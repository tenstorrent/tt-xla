#loc1 = loc("-1|unknown|unknown|-1|unknownxla__device_data")
module @SyncTensorsGraph.13945 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.13945 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
      func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<2048xbf16>>, ttir.name = "l__self___resampler_norm_out_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg1: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<2048xbf16>>, ttir.name = "l__self___resampler_norm_out_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<2048xbf16>>, ttir.name = "l__self___resampler_proj_out_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg3: tensor<2048x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<2048x1280xbf16>>, ttir.name = "l__self___resampler_proj_out_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg4: tensor<1x16x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x16x1280xbf16>>, ttir.name = "l__self___resampler_latents"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg5: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___resampler_layers_0_attn_to_out_0_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg6: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___resampler_layers_0_attn_to_v_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg7: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___resampler_layers_0_ln1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg8: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___resampler_layers_0_ln1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg9: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___resampler_layers_0_ln0_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg10: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___resampler_layers_0_ln0_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg11: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___resampler_proj_in_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg12: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___resampler_proj_in_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg13: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg14: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg15: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg16: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg17: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg18: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg19: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg20: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg21: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg22: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg23: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg24: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg25: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg26: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg27: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg28: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg29: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg30: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg31: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg32: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg33: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg34: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg35: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg36: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg37: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg38: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg39: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg40: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg41: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg42: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg43: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg44: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg45: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg46: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg47: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg48: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg49: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg50: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg51: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg52: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg53: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg54: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg55: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg56: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg57: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg58: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg59: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg60: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg61: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg62: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg63: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg64: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg65: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg66: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg67: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg68: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg69: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg70: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg71: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg72: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg73: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg74: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg75: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg76: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg77: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg78: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg79: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg80: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg81: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg82: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg83: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg84: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg85: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg86: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg87: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg88: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg89: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg90: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg91: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg92: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg93: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg94: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg95: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg96: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg97: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg98: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg99: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg100: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg101: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg102: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg103: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg104: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg105: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg106: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg107: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg108: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg109: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg110: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg111: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg112: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg113: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg114: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg115: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg116: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg117: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg118: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg119: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg120: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg121: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg122: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg123: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg124: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg125: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg126: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg127: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg128: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg129: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg130: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg131: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg132: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg133: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg134: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg135: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg136: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg137: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg138: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg139: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg140: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg141: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg142: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg143: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg144: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg145: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg146: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg147: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg148: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg149: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg150: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg151: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg152: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg153: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg154: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg155: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg156: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg157: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg158: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg159: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg160: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg161: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg162: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg163: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg164: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg165: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg166: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg167: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg168: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg169: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg170: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg171: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg172: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg173: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg174: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg175: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg176: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg177: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg178: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg179: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg180: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg181: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg182: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg183: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg184: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg185: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg186: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg187: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg188: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg189: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg190: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg191: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg192: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg193: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg194: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg195: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg196: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg197: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg198: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg199: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg200: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg201: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg202: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg203: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg204: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg205: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg206: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg207: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg208: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg209: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg210: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg211: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg212: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg213: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg214: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg215: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg216: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg217: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg218: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg219: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg220: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg221: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg222: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg223: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg224: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg225: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg226: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg227: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg228: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg229: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg230: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg231: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg232: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg233: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg234: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg235: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg236: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg237: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg238: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg239: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg240: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg241: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg242: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg243: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg244: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg245: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg246: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg247: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg248: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg249: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg250: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg251: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg252: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg253: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg254: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg255: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg256: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg257: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg258: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg259: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg260: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg261: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg262: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg263: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg264: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg265: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg266: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg267: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg268: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg269: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg270: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg271: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg272: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg273: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg274: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg275: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg276: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg277: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg278: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg279: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg280: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg281: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg282: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg283: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg284: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg285: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg286: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg287: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg288: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg289: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg290: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg291: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg292: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg293: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg294: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg295: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg296: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg297: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg298: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg299: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg300: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg301: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg302: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg303: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg304: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg305: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg306: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg307: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg308: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg309: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg310: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg311: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg312: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg313: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg314: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg315: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg316: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg317: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg318: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg319: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg320: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg321: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg322: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg323: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg324: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg325: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg326: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg327: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg328: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg329: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg330: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg331: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg332: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg333: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg334: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg335: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg336: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg337: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg338: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg339: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg340: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg341: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg342: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg343: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg344: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg345: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg346: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg347: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg348: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg349: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg350: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg351: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg352: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg353: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg354: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg355: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg356: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg357: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg358: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg359: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg360: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg361: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg362: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg363: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg364: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg365: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg366: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg367: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg368: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg369: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg370: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg371: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg372: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg373: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg374: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg375: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg376: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg377: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg378: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg379: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg380: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg381: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg382: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg383: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg384: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg385: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_pre_layrnorm_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg386: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_pre_layrnorm_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg387: tensor<1x257xi64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x257xi64>>, ttir.name = "l__self___image_encoder_vision_model_embeddings_position_ids"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg388: tensor<257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<257x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_embeddings_position_embedding_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg389: tensor<1280x3x14x14xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x3x14x14xbf16>>, ttir.name = "l__self___image_encoder_vision_model_embeddings_patch_embedding_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg390: tensor<1x3x224x224xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x3x224x224xbf16>>, ttir.name = "args_0"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg391: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_embeddings_class_embedding"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg392: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg393: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg394: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg395: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg396: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg397: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg398: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg399: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg400: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg401: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg402: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg403: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg404: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg405: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg406: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg407: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg408: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg409: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg410: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg411: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg412: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg413: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg414: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg415: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg416: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg417: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg418: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg419: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg420: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg421: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg422: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg423: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg424: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg425: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg426: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg427: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg428: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg429: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg430: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg431: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg432: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg433: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg434: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg435: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg436: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg437: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg438: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg439: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg440: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg441: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg442: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg443: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg444: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg445: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg446: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg447: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg448: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg449: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg450: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg451: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg452: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg453: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg454: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg455: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg456: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg457: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg458: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg459: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg460: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg461: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg462: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg463: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg464: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg465: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg466: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg467: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg468: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg469: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg470: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg471: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg472: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg473: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg474: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg475: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg476: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg477: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg478: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg479: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg480: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg481: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg482: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg483: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg484: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg485: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg486: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg487: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg488: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg489: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg490: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg491: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg492: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg493: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg494: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg495: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg496: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg497: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg498: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg499: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg500: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg501: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg502: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg503: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg504: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg505: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg506: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg507: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg508: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg509: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg510: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg511: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg512: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg513: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg514: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg515: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg516: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___resampler_layers_0_attn_to_k_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg517: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___resampler_layers_0_attn_to_q_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg518: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "getattr_l__self___resampler_layers_0_ff___1___net_2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg519: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "getattr_l__self___resampler_layers_0_ff___1___net_0_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg520: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___resampler_layers_0_ff_0_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg521: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___resampler_layers_0_ff_0_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg522: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___resampler_layers_1_attn_to_out_0_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg523: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___resampler_layers_1_attn_to_v_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg524: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___resampler_layers_1_ln1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg525: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___resampler_layers_1_ln1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg526: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___resampler_layers_1_ln0_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg527: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___resampler_layers_1_ln0_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg528: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___resampler_layers_1_attn_to_k_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg529: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___resampler_layers_1_attn_to_q_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg530: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "getattr_l__self___resampler_layers_1_ff___1___net_2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg531: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "getattr_l__self___resampler_layers_1_ff___1___net_0_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg532: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___resampler_layers_1_ff_0_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg533: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___resampler_layers_1_ff_0_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg534: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___resampler_layers_2_attn_to_out_0_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg535: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___resampler_layers_2_attn_to_v_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg536: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___resampler_layers_2_ln1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg537: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___resampler_layers_2_ln1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg538: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___resampler_layers_2_ln0_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg539: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___resampler_layers_2_ln0_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg540: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___resampler_layers_2_attn_to_k_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg541: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___resampler_layers_2_attn_to_q_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg542: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "getattr_l__self___resampler_layers_2_ff___1___net_2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg543: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "getattr_l__self___resampler_layers_2_ff___1___net_0_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg544: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___resampler_layers_2_ff_0_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg545: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___resampler_layers_2_ff_0_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg546: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___resampler_layers_3_attn_to_out_0_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg547: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___resampler_layers_3_attn_to_v_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg548: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___resampler_layers_3_ln1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg549: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___resampler_layers_3_ln1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg550: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___resampler_layers_3_ln0_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg551: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___resampler_layers_3_ln0_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg552: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___resampler_layers_3_attn_to_k_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg553: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x1280xbf16>>, ttir.name = "l__self___resampler_layers_3_attn_to_q_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg554: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280x5120xbf16>>, ttir.name = "getattr_l__self___resampler_layers_3_ff___1___net_2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg555: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<5120x1280xbf16>>, ttir.name = "getattr_l__self___resampler_layers_3_ff___1___net_0_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg556: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___resampler_layers_3_ff_0_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg557: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1280xbf16>>, ttir.name = "l__self___resampler_layers_3_ff_0_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data")) -> (tensor<1x16x2048xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x16x2048xbf16>>}) {
        %0 = "ttir.constant"() <{value = dense<1.000000e+00> : tensor<1x16x1280xbf16>}> : () -> tensor<1x16x1280xbf16> loc(#loc)
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x20x16x273xf32>}> : () -> tensor<1x20x16x273xf32> loc(#loc)
        %2 = "ttir.constant"() <{value = dense<0xFFF0000000000000> : tensor<1x20x16x273xf64>}> : () -> tensor<1x20x16x273xf64> loc(#loc)
        %3 = "ttir.constant"() <{value = dense<0.353553385> : tensor<1x20x64x273xf32>}> : () -> tensor<1x20x64x273xf32> loc(#loc)
        %4 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x16x257x257xf32>}> : () -> tensor<1x16x257x257xf32> loc(#loc)
        %5 = "ttir.constant"() <{value = dense<0xFFF0000000000000> : tensor<1x16x257x257xf64>}> : () -> tensor<1x16x257x257xf64> loc(#loc)
        %6 = "ttir.constant"() <{value = dense<0.334370166> : tensor<1x16x80x257xf32>}> : () -> tensor<1x16x80x257xf32> loc(#loc)
        %7 = "ttir.constant"() <{value = dense<0.334370166> : tensor<1x16x257x80xf32>}> : () -> tensor<1x16x257x80xf32> loc(#loc)
        %8 = "ttir.constant"() <{value = dense<0.353553385> : tensor<1x20x16x64xf32>}> : () -> tensor<1x20x16x64xf32> loc(#loc)
        %9 = "ttir.reshape"(%arg8) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %10 = "ttir.reshape"(%9) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %11 = "ttir.reshape"(%arg7) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %12 = "ttir.reshape"(%11) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %13 = "ttir.layer_norm"(%arg4, %10, %12) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc3)
        %14 = "ttir.reshape"(%13) <{shape = [16 : i32, 1280 : i32]}> : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc4)
        %15 = "ttir.reshape"(%arg517) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %16 = "ttir.reshape"(%15) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %17 = "ttir.permute"(%16) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc5)
        %18 = "ttir.dot_general"(%14, %17) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc6)
        %19 = "ttir.reshape"(%18) <{shape = [1 : i32, 16 : i32, 20 : i32, 64 : i32]}> : (tensor<16x1280xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc7)
        %20 = "ttir.permute"(%19) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x20x64xbf16>) -> tensor<1x20x16x64xbf16> loc(#loc8)
        %21 = "ttir.typecast"(%20) <{conservative_folding = false}> : (tensor<1x20x16x64xbf16>) -> tensor<1x20x16x64xf32> loc(#loc9)
        %22 = "ttir.multiply"(%21, %8) : (tensor<1x20x16x64xf32>, tensor<1x20x16x64xf32>) -> tensor<1x20x16x64xf32> loc(#loc10)
        %23 = "ttir.reshape"(%arg391) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %24 = "ttir.permute"(%arg389) <{permutation = array<i64: 0, 1, 2, 3>}> : (tensor<1280x3x14x14xbf16>) -> tensor<1280x3x14x14xbf16> loc(#loc1828)
        %25 = "ttir.conv2d"(%arg390, %24) <{batch_dim = 0 : i64, channel_dim = 1 : i64, dilation = array<i32: 1, 1>, groups = 1 : i32, height_dim = 2 : i64, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 14, 14>, width_dim = 3 : i64}> : (tensor<1x3x224x224xbf16>, tensor<1280x3x14x14xbf16>) -> tensor<1x1280x16x16xbf16> loc(#loc11)
        %26 = "ttir.reshape"(%25) <{shape = [1 : i32, 1280 : i32, 256 : i32]}> : (tensor<1x1280x16x16xbf16>) -> tensor<1x1280x256xbf16> loc(#loc12)
        %27 = "ttir.permute"(%26) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x1280x256xbf16>) -> tensor<1x256x1280xbf16> loc(#loc13)
        %28 = "ttir.concat"(%23, %27) <{dim = 1 : si32}> : (tensor<1x1x1280xbf16>, tensor<1x256x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc14)
        %29 = "ttir.reshape"(%arg388) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2)
        %30 = "ttir.reshape"(%29) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2)
        %31 = "ttir.reshape"(%arg387) <{shape = [1 : i32, 1 : i32, 257 : i32]}> : (tensor<1x257xi64>) -> tensor<1x1x257xi64> loc(#loc2)
        %32 = "ttir.reshape"(%31) <{shape = [257 : i32]}> : (tensor<1x1x257xi64>) -> tensor<257xi64> loc(#loc15)
        %33 = "ttir.typecast"(%32) <{conservative_folding = false}> : (tensor<257xi64>) -> tensor<257xui32> loc(#loc16)
        %34 = "ttir.gather"(%30, %33) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 1280>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<257x1280xbf16>, tensor<257xui32>) -> tensor<257x1280xbf16> loc(#loc16)
        %35 = "ttir.reshape"(%34) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc15)
        %36 = "ttir.add"(%28, %35) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc17)
        %37 = "ttir.reshape"(%arg386) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %38 = "ttir.reshape"(%37) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %39 = "ttir.reshape"(%arg385) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %40 = "ttir.reshape"(%39) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %41 = "ttir.layer_norm"(%36, %38, %40) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc18)
        %42 = "ttir.reshape"(%arg384) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %43 = "ttir.reshape"(%42) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %44 = "ttir.reshape"(%arg383) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %45 = "ttir.reshape"(%44) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %46 = "ttir.layer_norm"(%41, %43, %45) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc19)
        %47 = "ttir.reshape"(%46) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc20)
        %48 = "ttir.reshape"(%arg395) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %49 = "ttir.reshape"(%48) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %50 = "ttir.permute"(%49) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc21)
        %51 = "ttir.dot_general"(%47, %50) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc22)
        %52 = "ttir.reshape"(%51) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc20)
        %53 = "ttir.reshape"(%arg394) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %54 = "ttir.reshape"(%53) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %55 = "ttir.reshape"(%54) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc23)
        %56 = "ttir.broadcast"(%55) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc23)
        %57 = "ttir.add"(%52, %56) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc23)
        %58 = "ttir.reshape"(%57) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc24)
        %59 = "ttir.permute"(%58) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc25)
        %60 = "ttir.typecast"(%59) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc26)
        %61 = "ttir.multiply"(%60, %7) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc27)
        %62 = "ttir.reshape"(%arg393) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %63 = "ttir.reshape"(%62) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %64 = "ttir.permute"(%63) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc28)
        %65 = "ttir.dot_general"(%47, %64) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc29)
        %66 = "ttir.reshape"(%65) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc30)
        %67 = "ttir.reshape"(%arg392) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %68 = "ttir.reshape"(%67) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %69 = "ttir.reshape"(%68) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc31)
        %70 = "ttir.broadcast"(%69) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc31)
        %71 = "ttir.add"(%66, %70) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc31)
        %72 = "ttir.reshape"(%71) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc32)
        %73 = "ttir.permute"(%72) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc33)
        %74 = "ttir.typecast"(%73) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc34)
        %75 = "ttir.permute"(%74) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc35)
        %76 = "ttir.multiply"(%75, %6) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32> loc(#loc36)
        %77 = "ttir.dot_general"(%61, %76) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc37)
        %78 = "ttir.typecast"(%77) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc38)
        %79 = "ttir.eq"(%78, %5) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc38)
        %80 = "ttir.logical_not"(%79) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc39)
        %81 = "ttir.reduce_or"(%80) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1> loc(#loc40)
        %82 = "ttir.reshape"(%81) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc40)
        %83 = "ttir.logical_not"(%82) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1> loc(#loc41)
        %84 = "ttir.reshape"(%83) <{shape = [1 : i32, 16 : i32, 257 : i32]}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc42)
        %85 = "ttir.reshape"(%84) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc42)
        %86 = "ttir.broadcast"(%85) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1> loc(#loc42)
        %87 = "ttir.max"(%77) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc43)
        %88 = "ttir.reshape"(%87) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc43)
        %89 = "ttir.broadcast"(%88) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc43)
        %90 = "ttir.subtract"(%77, %89) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc43)
        %91 = "ttir.exp"(%90) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc43)
        %92 = "ttir.sum"(%91) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc43)
        %93 = "ttir.reshape"(%92) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc43)
        %94 = "ttir.broadcast"(%93) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc43)
        %95 = "ttir.div"(%91, %94) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc43)
        %96 = "ttir.where"(%86, %4, %95) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc44)
        %97 = "ttir.reshape"(%arg382) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %98 = "ttir.reshape"(%97) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %99 = "ttir.permute"(%98) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc45)
        %100 = "ttir.dot_general"(%47, %99) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc46)
        %101 = "ttir.reshape"(%100) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc47)
        %102 = "ttir.reshape"(%arg381) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %103 = "ttir.reshape"(%102) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %104 = "ttir.reshape"(%103) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc48)
        %105 = "ttir.broadcast"(%104) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc48)
        %106 = "ttir.add"(%101, %105) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc48)
        %107 = "ttir.reshape"(%106) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc49)
        %108 = "ttir.permute"(%107) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc50)
        %109 = "ttir.typecast"(%108) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc51)
        %110 = "ttir.dot_general"(%96, %109) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc52)
        %111 = "ttir.typecast"(%110) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc53)
        %112 = "ttir.permute"(%111) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc54)
        %113 = "ttir.reshape"(%112) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc55)
        %114 = "ttir.reshape"(%arg380) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %115 = "ttir.reshape"(%114) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %116 = "ttir.permute"(%115) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc56)
        %117 = "ttir.dot_general"(%113, %116) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc57)
        %118 = "ttir.reshape"(%117) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc55)
        %119 = "ttir.reshape"(%arg379) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %120 = "ttir.reshape"(%119) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %121 = "ttir.reshape"(%120) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc58)
        %122 = "ttir.broadcast"(%121) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc58)
        %123 = "ttir.add"(%118, %122) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc58)
        %124 = "ttir.add"(%41, %123) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc59)
        %125 = "ttir.reshape"(%arg378) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %126 = "ttir.reshape"(%125) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %127 = "ttir.reshape"(%arg377) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %128 = "ttir.reshape"(%127) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %129 = "ttir.layer_norm"(%124, %126, %128) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc60)
        %130 = "ttir.reshape"(%129) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc61)
        %131 = "ttir.reshape"(%arg376) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %132 = "ttir.reshape"(%131) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %133 = "ttir.permute"(%132) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc62)
        %134 = "ttir.dot_general"(%130, %133) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc63)
        %135 = "ttir.reshape"(%134) <{shape = [1 : i32, 257 : i32, 5120 : i32]}> : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc61)
        %136 = "ttir.reshape"(%arg375) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
        %137 = "ttir.reshape"(%136) <{shape = [5120 : i32]}> : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
        %138 = "ttir.reshape"(%137) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc64)
        %139 = "ttir.broadcast"(%138) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc64)
        %140 = "ttir.add"(%135, %139) : (tensor<1x257x5120xbf16>, tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc64)
        %141 = "ttir.gelu"(%140) : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc65)
        %142 = "ttir.reshape"(%141) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc66)
        %143 = "ttir.reshape"(%arg374) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %144 = "ttir.reshape"(%143) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %145 = "ttir.permute"(%144) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc67)
        %146 = "ttir.dot_general"(%142, %145) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc68)
        %147 = "ttir.reshape"(%146) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc66)
        %148 = "ttir.reshape"(%arg373) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %149 = "ttir.reshape"(%148) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %150 = "ttir.reshape"(%149) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc69)
        %151 = "ttir.broadcast"(%150) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc69)
        %152 = "ttir.add"(%147, %151) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc69)
        %153 = "ttir.add"(%124, %152) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc70)
        %154 = "ttir.reshape"(%arg372) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %155 = "ttir.reshape"(%154) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %156 = "ttir.reshape"(%arg371) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %157 = "ttir.reshape"(%156) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %158 = "ttir.layer_norm"(%153, %155, %157) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc71)
        %159 = "ttir.reshape"(%158) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc72)
        %160 = "ttir.reshape"(%arg399) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %161 = "ttir.reshape"(%160) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %162 = "ttir.permute"(%161) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc73)
        %163 = "ttir.dot_general"(%159, %162) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc74)
        %164 = "ttir.reshape"(%163) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc72)
        %165 = "ttir.reshape"(%arg398) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %166 = "ttir.reshape"(%165) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %167 = "ttir.reshape"(%166) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc75)
        %168 = "ttir.broadcast"(%167) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc75)
        %169 = "ttir.add"(%164, %168) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc75)
        %170 = "ttir.reshape"(%169) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc76)
        %171 = "ttir.permute"(%170) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc77)
        %172 = "ttir.typecast"(%171) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc78)
        %173 = "ttir.multiply"(%172, %7) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc79)
        %174 = "ttir.reshape"(%arg397) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %175 = "ttir.reshape"(%174) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %176 = "ttir.permute"(%175) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc80)
        %177 = "ttir.dot_general"(%159, %176) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc81)
        %178 = "ttir.reshape"(%177) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc82)
        %179 = "ttir.reshape"(%arg396) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %180 = "ttir.reshape"(%179) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %181 = "ttir.reshape"(%180) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc83)
        %182 = "ttir.broadcast"(%181) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc83)
        %183 = "ttir.add"(%178, %182) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc83)
        %184 = "ttir.reshape"(%183) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc84)
        %185 = "ttir.permute"(%184) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc85)
        %186 = "ttir.typecast"(%185) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc86)
        %187 = "ttir.permute"(%186) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc87)
        %188 = "ttir.multiply"(%187, %6) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32> loc(#loc88)
        %189 = "ttir.dot_general"(%173, %188) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc89)
        %190 = "ttir.typecast"(%189) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc90)
        %191 = "ttir.eq"(%190, %5) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc90)
        %192 = "ttir.logical_not"(%191) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc91)
        %193 = "ttir.reduce_or"(%192) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1> loc(#loc92)
        %194 = "ttir.reshape"(%193) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc92)
        %195 = "ttir.logical_not"(%194) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1> loc(#loc93)
        %196 = "ttir.reshape"(%195) <{shape = [1 : i32, 16 : i32, 257 : i32]}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc94)
        %197 = "ttir.reshape"(%196) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc94)
        %198 = "ttir.broadcast"(%197) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1> loc(#loc94)
        %199 = "ttir.max"(%189) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc95)
        %200 = "ttir.reshape"(%199) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc95)
        %201 = "ttir.broadcast"(%200) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc95)
        %202 = "ttir.subtract"(%189, %201) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc95)
        %203 = "ttir.exp"(%202) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc95)
        %204 = "ttir.sum"(%203) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc95)
        %205 = "ttir.reshape"(%204) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc95)
        %206 = "ttir.broadcast"(%205) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc95)
        %207 = "ttir.div"(%203, %206) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc95)
        %208 = "ttir.where"(%198, %4, %207) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc96)
        %209 = "ttir.reshape"(%arg370) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %210 = "ttir.reshape"(%209) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %211 = "ttir.permute"(%210) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc97)
        %212 = "ttir.dot_general"(%159, %211) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc98)
        %213 = "ttir.reshape"(%212) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc99)
        %214 = "ttir.reshape"(%arg369) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %215 = "ttir.reshape"(%214) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %216 = "ttir.reshape"(%215) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc100)
        %217 = "ttir.broadcast"(%216) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc100)
        %218 = "ttir.add"(%213, %217) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc100)
        %219 = "ttir.reshape"(%218) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc101)
        %220 = "ttir.permute"(%219) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc102)
        %221 = "ttir.typecast"(%220) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc103)
        %222 = "ttir.dot_general"(%208, %221) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc104)
        %223 = "ttir.typecast"(%222) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc105)
        %224 = "ttir.permute"(%223) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc106)
        %225 = "ttir.reshape"(%224) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc107)
        %226 = "ttir.reshape"(%arg368) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %227 = "ttir.reshape"(%226) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %228 = "ttir.permute"(%227) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc108)
        %229 = "ttir.dot_general"(%225, %228) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc109)
        %230 = "ttir.reshape"(%229) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc107)
        %231 = "ttir.reshape"(%arg367) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %232 = "ttir.reshape"(%231) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %233 = "ttir.reshape"(%232) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc110)
        %234 = "ttir.broadcast"(%233) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc110)
        %235 = "ttir.add"(%230, %234) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc110)
        %236 = "ttir.add"(%153, %235) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc111)
        %237 = "ttir.reshape"(%arg366) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %238 = "ttir.reshape"(%237) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %239 = "ttir.reshape"(%arg365) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %240 = "ttir.reshape"(%239) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %241 = "ttir.layer_norm"(%236, %238, %240) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc112)
        %242 = "ttir.reshape"(%241) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc113)
        %243 = "ttir.reshape"(%arg364) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %244 = "ttir.reshape"(%243) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %245 = "ttir.permute"(%244) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc114)
        %246 = "ttir.dot_general"(%242, %245) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc115)
        %247 = "ttir.reshape"(%246) <{shape = [1 : i32, 257 : i32, 5120 : i32]}> : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc113)
        %248 = "ttir.reshape"(%arg363) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
        %249 = "ttir.reshape"(%248) <{shape = [5120 : i32]}> : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
        %250 = "ttir.reshape"(%249) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc116)
        %251 = "ttir.broadcast"(%250) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc116)
        %252 = "ttir.add"(%247, %251) : (tensor<1x257x5120xbf16>, tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc116)
        %253 = "ttir.gelu"(%252) : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc117)
        %254 = "ttir.reshape"(%253) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc118)
        %255 = "ttir.reshape"(%arg362) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %256 = "ttir.reshape"(%255) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %257 = "ttir.permute"(%256) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc119)
        %258 = "ttir.dot_general"(%254, %257) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc120)
        %259 = "ttir.reshape"(%258) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc118)
        %260 = "ttir.reshape"(%arg361) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %261 = "ttir.reshape"(%260) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %262 = "ttir.reshape"(%261) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc121)
        %263 = "ttir.broadcast"(%262) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc121)
        %264 = "ttir.add"(%259, %263) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc121)
        %265 = "ttir.add"(%236, %264) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc122)
        %266 = "ttir.reshape"(%arg360) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %267 = "ttir.reshape"(%266) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %268 = "ttir.reshape"(%arg359) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %269 = "ttir.reshape"(%268) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %270 = "ttir.layer_norm"(%265, %267, %269) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc123)
        %271 = "ttir.reshape"(%270) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc124)
        %272 = "ttir.reshape"(%arg403) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %273 = "ttir.reshape"(%272) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %274 = "ttir.permute"(%273) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc125)
        %275 = "ttir.dot_general"(%271, %274) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc126)
        %276 = "ttir.reshape"(%275) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc124)
        %277 = "ttir.reshape"(%arg402) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %278 = "ttir.reshape"(%277) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %279 = "ttir.reshape"(%278) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc127)
        %280 = "ttir.broadcast"(%279) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc127)
        %281 = "ttir.add"(%276, %280) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc127)
        %282 = "ttir.reshape"(%281) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc128)
        %283 = "ttir.permute"(%282) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc129)
        %284 = "ttir.typecast"(%283) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc130)
        %285 = "ttir.multiply"(%284, %7) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc131)
        %286 = "ttir.reshape"(%arg401) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %287 = "ttir.reshape"(%286) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %288 = "ttir.permute"(%287) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc132)
        %289 = "ttir.dot_general"(%271, %288) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc133)
        %290 = "ttir.reshape"(%289) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc134)
        %291 = "ttir.reshape"(%arg400) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %292 = "ttir.reshape"(%291) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %293 = "ttir.reshape"(%292) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc135)
        %294 = "ttir.broadcast"(%293) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc135)
        %295 = "ttir.add"(%290, %294) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc135)
        %296 = "ttir.reshape"(%295) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc136)
        %297 = "ttir.permute"(%296) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc137)
        %298 = "ttir.typecast"(%297) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc138)
        %299 = "ttir.permute"(%298) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc139)
        %300 = "ttir.multiply"(%299, %6) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32> loc(#loc140)
        %301 = "ttir.dot_general"(%285, %300) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc141)
        %302 = "ttir.typecast"(%301) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc142)
        %303 = "ttir.eq"(%302, %5) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc142)
        %304 = "ttir.logical_not"(%303) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc143)
        %305 = "ttir.reduce_or"(%304) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1> loc(#loc144)
        %306 = "ttir.reshape"(%305) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc144)
        %307 = "ttir.logical_not"(%306) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1> loc(#loc145)
        %308 = "ttir.reshape"(%307) <{shape = [1 : i32, 16 : i32, 257 : i32]}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc146)
        %309 = "ttir.reshape"(%308) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc146)
        %310 = "ttir.broadcast"(%309) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1> loc(#loc146)
        %311 = "ttir.max"(%301) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc147)
        %312 = "ttir.reshape"(%311) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc147)
        %313 = "ttir.broadcast"(%312) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc147)
        %314 = "ttir.subtract"(%301, %313) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc147)
        %315 = "ttir.exp"(%314) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc147)
        %316 = "ttir.sum"(%315) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc147)
        %317 = "ttir.reshape"(%316) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc147)
        %318 = "ttir.broadcast"(%317) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc147)
        %319 = "ttir.div"(%315, %318) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc147)
        %320 = "ttir.where"(%310, %4, %319) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc148)
        %321 = "ttir.reshape"(%arg358) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %322 = "ttir.reshape"(%321) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %323 = "ttir.permute"(%322) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc149)
        %324 = "ttir.dot_general"(%271, %323) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc150)
        %325 = "ttir.reshape"(%324) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc151)
        %326 = "ttir.reshape"(%arg357) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %327 = "ttir.reshape"(%326) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %328 = "ttir.reshape"(%327) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc152)
        %329 = "ttir.broadcast"(%328) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc152)
        %330 = "ttir.add"(%325, %329) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc152)
        %331 = "ttir.reshape"(%330) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc153)
        %332 = "ttir.permute"(%331) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc154)
        %333 = "ttir.typecast"(%332) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc155)
        %334 = "ttir.dot_general"(%320, %333) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc156)
        %335 = "ttir.typecast"(%334) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc157)
        %336 = "ttir.permute"(%335) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc158)
        %337 = "ttir.reshape"(%336) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc159)
        %338 = "ttir.reshape"(%arg356) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %339 = "ttir.reshape"(%338) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %340 = "ttir.permute"(%339) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc160)
        %341 = "ttir.dot_general"(%337, %340) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc161)
        %342 = "ttir.reshape"(%341) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc159)
        %343 = "ttir.reshape"(%arg355) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %344 = "ttir.reshape"(%343) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %345 = "ttir.reshape"(%344) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc162)
        %346 = "ttir.broadcast"(%345) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc162)
        %347 = "ttir.add"(%342, %346) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc162)
        %348 = "ttir.add"(%265, %347) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc163)
        %349 = "ttir.reshape"(%arg354) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %350 = "ttir.reshape"(%349) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %351 = "ttir.reshape"(%arg353) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %352 = "ttir.reshape"(%351) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %353 = "ttir.layer_norm"(%348, %350, %352) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc164)
        %354 = "ttir.reshape"(%353) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc165)
        %355 = "ttir.reshape"(%arg352) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %356 = "ttir.reshape"(%355) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %357 = "ttir.permute"(%356) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc166)
        %358 = "ttir.dot_general"(%354, %357) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc167)
        %359 = "ttir.reshape"(%358) <{shape = [1 : i32, 257 : i32, 5120 : i32]}> : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc165)
        %360 = "ttir.reshape"(%arg351) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
        %361 = "ttir.reshape"(%360) <{shape = [5120 : i32]}> : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
        %362 = "ttir.reshape"(%361) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc168)
        %363 = "ttir.broadcast"(%362) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc168)
        %364 = "ttir.add"(%359, %363) : (tensor<1x257x5120xbf16>, tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc168)
        %365 = "ttir.gelu"(%364) : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc169)
        %366 = "ttir.reshape"(%365) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc170)
        %367 = "ttir.reshape"(%arg350) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %368 = "ttir.reshape"(%367) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %369 = "ttir.permute"(%368) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc171)
        %370 = "ttir.dot_general"(%366, %369) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc172)
        %371 = "ttir.reshape"(%370) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc170)
        %372 = "ttir.reshape"(%arg349) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %373 = "ttir.reshape"(%372) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %374 = "ttir.reshape"(%373) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc173)
        %375 = "ttir.broadcast"(%374) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc173)
        %376 = "ttir.add"(%371, %375) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc173)
        %377 = "ttir.add"(%348, %376) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc174)
        %378 = "ttir.reshape"(%arg348) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %379 = "ttir.reshape"(%378) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %380 = "ttir.reshape"(%arg347) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %381 = "ttir.reshape"(%380) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %382 = "ttir.layer_norm"(%377, %379, %381) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc175)
        %383 = "ttir.reshape"(%382) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc176)
        %384 = "ttir.reshape"(%arg407) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %385 = "ttir.reshape"(%384) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %386 = "ttir.permute"(%385) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc177)
        %387 = "ttir.dot_general"(%383, %386) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc178)
        %388 = "ttir.reshape"(%387) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc176)
        %389 = "ttir.reshape"(%arg406) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %390 = "ttir.reshape"(%389) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %391 = "ttir.reshape"(%390) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc179)
        %392 = "ttir.broadcast"(%391) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc179)
        %393 = "ttir.add"(%388, %392) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc179)
        %394 = "ttir.reshape"(%393) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc180)
        %395 = "ttir.permute"(%394) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc181)
        %396 = "ttir.typecast"(%395) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc182)
        %397 = "ttir.multiply"(%396, %7) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc183)
        %398 = "ttir.reshape"(%arg405) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %399 = "ttir.reshape"(%398) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %400 = "ttir.permute"(%399) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc184)
        %401 = "ttir.dot_general"(%383, %400) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc185)
        %402 = "ttir.reshape"(%401) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc186)
        %403 = "ttir.reshape"(%arg404) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %404 = "ttir.reshape"(%403) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %405 = "ttir.reshape"(%404) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc187)
        %406 = "ttir.broadcast"(%405) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc187)
        %407 = "ttir.add"(%402, %406) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc187)
        %408 = "ttir.reshape"(%407) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc188)
        %409 = "ttir.permute"(%408) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc189)
        %410 = "ttir.typecast"(%409) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc190)
        %411 = "ttir.permute"(%410) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc191)
        %412 = "ttir.multiply"(%411, %6) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32> loc(#loc192)
        %413 = "ttir.dot_general"(%397, %412) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc193)
        %414 = "ttir.typecast"(%413) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc194)
        %415 = "ttir.eq"(%414, %5) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc194)
        %416 = "ttir.logical_not"(%415) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc195)
        %417 = "ttir.reduce_or"(%416) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1> loc(#loc196)
        %418 = "ttir.reshape"(%417) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc196)
        %419 = "ttir.logical_not"(%418) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1> loc(#loc197)
        %420 = "ttir.reshape"(%419) <{shape = [1 : i32, 16 : i32, 257 : i32]}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc198)
        %421 = "ttir.reshape"(%420) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc198)
        %422 = "ttir.broadcast"(%421) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1> loc(#loc198)
        %423 = "ttir.max"(%413) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc199)
        %424 = "ttir.reshape"(%423) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc199)
        %425 = "ttir.broadcast"(%424) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc199)
        %426 = "ttir.subtract"(%413, %425) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc199)
        %427 = "ttir.exp"(%426) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc199)
        %428 = "ttir.sum"(%427) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc199)
        %429 = "ttir.reshape"(%428) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc199)
        %430 = "ttir.broadcast"(%429) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc199)
        %431 = "ttir.div"(%427, %430) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc199)
        %432 = "ttir.where"(%422, %4, %431) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc200)
        %433 = "ttir.reshape"(%arg346) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %434 = "ttir.reshape"(%433) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %435 = "ttir.permute"(%434) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc201)
        %436 = "ttir.dot_general"(%383, %435) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc202)
        %437 = "ttir.reshape"(%436) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc203)
        %438 = "ttir.reshape"(%arg345) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %439 = "ttir.reshape"(%438) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %440 = "ttir.reshape"(%439) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc204)
        %441 = "ttir.broadcast"(%440) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc204)
        %442 = "ttir.add"(%437, %441) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc204)
        %443 = "ttir.reshape"(%442) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc205)
        %444 = "ttir.permute"(%443) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc206)
        %445 = "ttir.typecast"(%444) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc207)
        %446 = "ttir.dot_general"(%432, %445) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc208)
        %447 = "ttir.typecast"(%446) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc209)
        %448 = "ttir.permute"(%447) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc210)
        %449 = "ttir.reshape"(%448) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc211)
        %450 = "ttir.reshape"(%arg344) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %451 = "ttir.reshape"(%450) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %452 = "ttir.permute"(%451) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc212)
        %453 = "ttir.dot_general"(%449, %452) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc213)
        %454 = "ttir.reshape"(%453) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc211)
        %455 = "ttir.reshape"(%arg343) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %456 = "ttir.reshape"(%455) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %457 = "ttir.reshape"(%456) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc214)
        %458 = "ttir.broadcast"(%457) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc214)
        %459 = "ttir.add"(%454, %458) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc214)
        %460 = "ttir.add"(%377, %459) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc215)
        %461 = "ttir.reshape"(%arg342) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %462 = "ttir.reshape"(%461) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %463 = "ttir.reshape"(%arg341) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %464 = "ttir.reshape"(%463) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %465 = "ttir.layer_norm"(%460, %462, %464) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc216)
        %466 = "ttir.reshape"(%465) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc217)
        %467 = "ttir.reshape"(%arg340) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %468 = "ttir.reshape"(%467) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %469 = "ttir.permute"(%468) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc218)
        %470 = "ttir.dot_general"(%466, %469) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc219)
        %471 = "ttir.reshape"(%470) <{shape = [1 : i32, 257 : i32, 5120 : i32]}> : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc217)
        %472 = "ttir.reshape"(%arg339) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
        %473 = "ttir.reshape"(%472) <{shape = [5120 : i32]}> : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
        %474 = "ttir.reshape"(%473) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc220)
        %475 = "ttir.broadcast"(%474) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc220)
        %476 = "ttir.add"(%471, %475) : (tensor<1x257x5120xbf16>, tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc220)
        %477 = "ttir.gelu"(%476) : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc221)
        %478 = "ttir.reshape"(%477) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc222)
        %479 = "ttir.reshape"(%arg338) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %480 = "ttir.reshape"(%479) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %481 = "ttir.permute"(%480) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc223)
        %482 = "ttir.dot_general"(%478, %481) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc224)
        %483 = "ttir.reshape"(%482) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc222)
        %484 = "ttir.reshape"(%arg337) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %485 = "ttir.reshape"(%484) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %486 = "ttir.reshape"(%485) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc225)
        %487 = "ttir.broadcast"(%486) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc225)
        %488 = "ttir.add"(%483, %487) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc225)
        %489 = "ttir.add"(%460, %488) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc226)
        %490 = "ttir.reshape"(%arg336) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %491 = "ttir.reshape"(%490) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %492 = "ttir.reshape"(%arg335) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %493 = "ttir.reshape"(%492) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %494 = "ttir.layer_norm"(%489, %491, %493) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc227)
        %495 = "ttir.reshape"(%494) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc228)
        %496 = "ttir.reshape"(%arg411) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %497 = "ttir.reshape"(%496) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %498 = "ttir.permute"(%497) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc229)
        %499 = "ttir.dot_general"(%495, %498) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc230)
        %500 = "ttir.reshape"(%499) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc228)
        %501 = "ttir.reshape"(%arg410) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %502 = "ttir.reshape"(%501) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %503 = "ttir.reshape"(%502) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc231)
        %504 = "ttir.broadcast"(%503) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc231)
        %505 = "ttir.add"(%500, %504) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc231)
        %506 = "ttir.reshape"(%505) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc232)
        %507 = "ttir.permute"(%506) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc233)
        %508 = "ttir.typecast"(%507) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc234)
        %509 = "ttir.multiply"(%508, %7) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc235)
        %510 = "ttir.reshape"(%arg409) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %511 = "ttir.reshape"(%510) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %512 = "ttir.permute"(%511) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc236)
        %513 = "ttir.dot_general"(%495, %512) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc237)
        %514 = "ttir.reshape"(%513) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc238)
        %515 = "ttir.reshape"(%arg408) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %516 = "ttir.reshape"(%515) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %517 = "ttir.reshape"(%516) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc239)
        %518 = "ttir.broadcast"(%517) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc239)
        %519 = "ttir.add"(%514, %518) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc239)
        %520 = "ttir.reshape"(%519) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc240)
        %521 = "ttir.permute"(%520) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc241)
        %522 = "ttir.typecast"(%521) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc242)
        %523 = "ttir.permute"(%522) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc243)
        %524 = "ttir.multiply"(%523, %6) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32> loc(#loc244)
        %525 = "ttir.dot_general"(%509, %524) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc245)
        %526 = "ttir.typecast"(%525) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc246)
        %527 = "ttir.eq"(%526, %5) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc246)
        %528 = "ttir.logical_not"(%527) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc247)
        %529 = "ttir.reduce_or"(%528) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1> loc(#loc248)
        %530 = "ttir.reshape"(%529) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc248)
        %531 = "ttir.logical_not"(%530) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1> loc(#loc249)
        %532 = "ttir.reshape"(%531) <{shape = [1 : i32, 16 : i32, 257 : i32]}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc250)
        %533 = "ttir.reshape"(%532) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc250)
        %534 = "ttir.broadcast"(%533) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1> loc(#loc250)
        %535 = "ttir.max"(%525) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc251)
        %536 = "ttir.reshape"(%535) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc251)
        %537 = "ttir.broadcast"(%536) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc251)
        %538 = "ttir.subtract"(%525, %537) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc251)
        %539 = "ttir.exp"(%538) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc251)
        %540 = "ttir.sum"(%539) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc251)
        %541 = "ttir.reshape"(%540) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc251)
        %542 = "ttir.broadcast"(%541) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc251)
        %543 = "ttir.div"(%539, %542) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc251)
        %544 = "ttir.where"(%534, %4, %543) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc252)
        %545 = "ttir.reshape"(%arg334) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %546 = "ttir.reshape"(%545) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %547 = "ttir.permute"(%546) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc253)
        %548 = "ttir.dot_general"(%495, %547) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc254)
        %549 = "ttir.reshape"(%548) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc255)
        %550 = "ttir.reshape"(%arg333) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %551 = "ttir.reshape"(%550) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %552 = "ttir.reshape"(%551) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc256)
        %553 = "ttir.broadcast"(%552) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc256)
        %554 = "ttir.add"(%549, %553) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc256)
        %555 = "ttir.reshape"(%554) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc257)
        %556 = "ttir.permute"(%555) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc258)
        %557 = "ttir.typecast"(%556) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc259)
        %558 = "ttir.dot_general"(%544, %557) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc260)
        %559 = "ttir.typecast"(%558) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc261)
        %560 = "ttir.permute"(%559) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc262)
        %561 = "ttir.reshape"(%560) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc263)
        %562 = "ttir.reshape"(%arg332) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %563 = "ttir.reshape"(%562) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %564 = "ttir.permute"(%563) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc264)
        %565 = "ttir.dot_general"(%561, %564) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc265)
        %566 = "ttir.reshape"(%565) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc263)
        %567 = "ttir.reshape"(%arg331) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %568 = "ttir.reshape"(%567) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %569 = "ttir.reshape"(%568) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc266)
        %570 = "ttir.broadcast"(%569) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc266)
        %571 = "ttir.add"(%566, %570) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc266)
        %572 = "ttir.add"(%489, %571) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc267)
        %573 = "ttir.reshape"(%arg330) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %574 = "ttir.reshape"(%573) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %575 = "ttir.reshape"(%arg329) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %576 = "ttir.reshape"(%575) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %577 = "ttir.layer_norm"(%572, %574, %576) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc268)
        %578 = "ttir.reshape"(%577) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc269)
        %579 = "ttir.reshape"(%arg328) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %580 = "ttir.reshape"(%579) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %581 = "ttir.permute"(%580) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc270)
        %582 = "ttir.dot_general"(%578, %581) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc271)
        %583 = "ttir.reshape"(%582) <{shape = [1 : i32, 257 : i32, 5120 : i32]}> : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc269)
        %584 = "ttir.reshape"(%arg327) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
        %585 = "ttir.reshape"(%584) <{shape = [5120 : i32]}> : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
        %586 = "ttir.reshape"(%585) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc272)
        %587 = "ttir.broadcast"(%586) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc272)
        %588 = "ttir.add"(%583, %587) : (tensor<1x257x5120xbf16>, tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc272)
        %589 = "ttir.gelu"(%588) : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc273)
        %590 = "ttir.reshape"(%589) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc274)
        %591 = "ttir.reshape"(%arg326) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %592 = "ttir.reshape"(%591) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %593 = "ttir.permute"(%592) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc275)
        %594 = "ttir.dot_general"(%590, %593) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc276)
        %595 = "ttir.reshape"(%594) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc274)
        %596 = "ttir.reshape"(%arg325) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %597 = "ttir.reshape"(%596) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %598 = "ttir.reshape"(%597) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc277)
        %599 = "ttir.broadcast"(%598) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc277)
        %600 = "ttir.add"(%595, %599) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc277)
        %601 = "ttir.add"(%572, %600) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc278)
        %602 = "ttir.reshape"(%arg324) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %603 = "ttir.reshape"(%602) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %604 = "ttir.reshape"(%arg323) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %605 = "ttir.reshape"(%604) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %606 = "ttir.layer_norm"(%601, %603, %605) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc279)
        %607 = "ttir.reshape"(%606) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc280)
        %608 = "ttir.reshape"(%arg415) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %609 = "ttir.reshape"(%608) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %610 = "ttir.permute"(%609) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc281)
        %611 = "ttir.dot_general"(%607, %610) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc282)
        %612 = "ttir.reshape"(%611) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc280)
        %613 = "ttir.reshape"(%arg414) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %614 = "ttir.reshape"(%613) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %615 = "ttir.reshape"(%614) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc283)
        %616 = "ttir.broadcast"(%615) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc283)
        %617 = "ttir.add"(%612, %616) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc283)
        %618 = "ttir.reshape"(%617) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc284)
        %619 = "ttir.permute"(%618) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc285)
        %620 = "ttir.typecast"(%619) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc286)
        %621 = "ttir.multiply"(%620, %7) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc287)
        %622 = "ttir.reshape"(%arg413) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %623 = "ttir.reshape"(%622) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %624 = "ttir.permute"(%623) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc288)
        %625 = "ttir.dot_general"(%607, %624) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc289)
        %626 = "ttir.reshape"(%625) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc290)
        %627 = "ttir.reshape"(%arg412) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %628 = "ttir.reshape"(%627) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %629 = "ttir.reshape"(%628) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc291)
        %630 = "ttir.broadcast"(%629) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc291)
        %631 = "ttir.add"(%626, %630) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc291)
        %632 = "ttir.reshape"(%631) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc292)
        %633 = "ttir.permute"(%632) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc293)
        %634 = "ttir.typecast"(%633) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc294)
        %635 = "ttir.permute"(%634) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc295)
        %636 = "ttir.multiply"(%635, %6) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32> loc(#loc296)
        %637 = "ttir.dot_general"(%621, %636) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc297)
        %638 = "ttir.typecast"(%637) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc298)
        %639 = "ttir.eq"(%638, %5) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc298)
        %640 = "ttir.logical_not"(%639) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc299)
        %641 = "ttir.reduce_or"(%640) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1> loc(#loc300)
        %642 = "ttir.reshape"(%641) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc300)
        %643 = "ttir.logical_not"(%642) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1> loc(#loc301)
        %644 = "ttir.reshape"(%643) <{shape = [1 : i32, 16 : i32, 257 : i32]}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc302)
        %645 = "ttir.reshape"(%644) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc302)
        %646 = "ttir.broadcast"(%645) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1> loc(#loc302)
        %647 = "ttir.max"(%637) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc303)
        %648 = "ttir.reshape"(%647) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc303)
        %649 = "ttir.broadcast"(%648) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc303)
        %650 = "ttir.subtract"(%637, %649) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc303)
        %651 = "ttir.exp"(%650) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc303)
        %652 = "ttir.sum"(%651) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc303)
        %653 = "ttir.reshape"(%652) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc303)
        %654 = "ttir.broadcast"(%653) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc303)
        %655 = "ttir.div"(%651, %654) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc303)
        %656 = "ttir.where"(%646, %4, %655) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc304)
        %657 = "ttir.reshape"(%arg322) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %658 = "ttir.reshape"(%657) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %659 = "ttir.permute"(%658) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc305)
        %660 = "ttir.dot_general"(%607, %659) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc306)
        %661 = "ttir.reshape"(%660) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc307)
        %662 = "ttir.reshape"(%arg321) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %663 = "ttir.reshape"(%662) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %664 = "ttir.reshape"(%663) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc308)
        %665 = "ttir.broadcast"(%664) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc308)
        %666 = "ttir.add"(%661, %665) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc308)
        %667 = "ttir.reshape"(%666) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc309)
        %668 = "ttir.permute"(%667) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc310)
        %669 = "ttir.typecast"(%668) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc311)
        %670 = "ttir.dot_general"(%656, %669) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc312)
        %671 = "ttir.typecast"(%670) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc313)
        %672 = "ttir.permute"(%671) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc314)
        %673 = "ttir.reshape"(%672) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc315)
        %674 = "ttir.reshape"(%arg320) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %675 = "ttir.reshape"(%674) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %676 = "ttir.permute"(%675) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc316)
        %677 = "ttir.dot_general"(%673, %676) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc317)
        %678 = "ttir.reshape"(%677) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc315)
        %679 = "ttir.reshape"(%arg319) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %680 = "ttir.reshape"(%679) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %681 = "ttir.reshape"(%680) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc318)
        %682 = "ttir.broadcast"(%681) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc318)
        %683 = "ttir.add"(%678, %682) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc318)
        %684 = "ttir.add"(%601, %683) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc319)
        %685 = "ttir.reshape"(%arg318) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %686 = "ttir.reshape"(%685) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %687 = "ttir.reshape"(%arg317) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %688 = "ttir.reshape"(%687) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %689 = "ttir.layer_norm"(%684, %686, %688) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc320)
        %690 = "ttir.reshape"(%689) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc321)
        %691 = "ttir.reshape"(%arg316) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %692 = "ttir.reshape"(%691) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %693 = "ttir.permute"(%692) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc322)
        %694 = "ttir.dot_general"(%690, %693) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc323)
        %695 = "ttir.reshape"(%694) <{shape = [1 : i32, 257 : i32, 5120 : i32]}> : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc321)
        %696 = "ttir.reshape"(%arg315) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
        %697 = "ttir.reshape"(%696) <{shape = [5120 : i32]}> : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
        %698 = "ttir.reshape"(%697) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc324)
        %699 = "ttir.broadcast"(%698) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc324)
        %700 = "ttir.add"(%695, %699) : (tensor<1x257x5120xbf16>, tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc324)
        %701 = "ttir.gelu"(%700) : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc325)
        %702 = "ttir.reshape"(%701) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc326)
        %703 = "ttir.reshape"(%arg314) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %704 = "ttir.reshape"(%703) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %705 = "ttir.permute"(%704) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc327)
        %706 = "ttir.dot_general"(%702, %705) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc328)
        %707 = "ttir.reshape"(%706) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc326)
        %708 = "ttir.reshape"(%arg313) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %709 = "ttir.reshape"(%708) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %710 = "ttir.reshape"(%709) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc329)
        %711 = "ttir.broadcast"(%710) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc329)
        %712 = "ttir.add"(%707, %711) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc329)
        %713 = "ttir.add"(%684, %712) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc330)
        %714 = "ttir.reshape"(%arg312) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %715 = "ttir.reshape"(%714) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %716 = "ttir.reshape"(%arg311) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %717 = "ttir.reshape"(%716) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %718 = "ttir.layer_norm"(%713, %715, %717) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc331)
        %719 = "ttir.reshape"(%718) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc332)
        %720 = "ttir.reshape"(%arg419) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %721 = "ttir.reshape"(%720) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %722 = "ttir.permute"(%721) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc333)
        %723 = "ttir.dot_general"(%719, %722) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc334)
        %724 = "ttir.reshape"(%723) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc332)
        %725 = "ttir.reshape"(%arg418) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %726 = "ttir.reshape"(%725) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %727 = "ttir.reshape"(%726) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc335)
        %728 = "ttir.broadcast"(%727) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc335)
        %729 = "ttir.add"(%724, %728) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc335)
        %730 = "ttir.reshape"(%729) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc336)
        %731 = "ttir.permute"(%730) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc337)
        %732 = "ttir.typecast"(%731) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc338)
        %733 = "ttir.multiply"(%732, %7) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc339)
        %734 = "ttir.reshape"(%arg417) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %735 = "ttir.reshape"(%734) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %736 = "ttir.permute"(%735) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc340)
        %737 = "ttir.dot_general"(%719, %736) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc341)
        %738 = "ttir.reshape"(%737) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc342)
        %739 = "ttir.reshape"(%arg416) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %740 = "ttir.reshape"(%739) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %741 = "ttir.reshape"(%740) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc343)
        %742 = "ttir.broadcast"(%741) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc343)
        %743 = "ttir.add"(%738, %742) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc343)
        %744 = "ttir.reshape"(%743) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc344)
        %745 = "ttir.permute"(%744) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc345)
        %746 = "ttir.typecast"(%745) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc346)
        %747 = "ttir.permute"(%746) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc347)
        %748 = "ttir.multiply"(%747, %6) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32> loc(#loc348)
        %749 = "ttir.dot_general"(%733, %748) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc349)
        %750 = "ttir.typecast"(%749) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc350)
        %751 = "ttir.eq"(%750, %5) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc350)
        %752 = "ttir.logical_not"(%751) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc351)
        %753 = "ttir.reduce_or"(%752) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1> loc(#loc352)
        %754 = "ttir.reshape"(%753) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc352)
        %755 = "ttir.logical_not"(%754) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1> loc(#loc353)
        %756 = "ttir.reshape"(%755) <{shape = [1 : i32, 16 : i32, 257 : i32]}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc354)
        %757 = "ttir.reshape"(%756) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc354)
        %758 = "ttir.broadcast"(%757) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1> loc(#loc354)
        %759 = "ttir.max"(%749) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc355)
        %760 = "ttir.reshape"(%759) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc355)
        %761 = "ttir.broadcast"(%760) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc355)
        %762 = "ttir.subtract"(%749, %761) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc355)
        %763 = "ttir.exp"(%762) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc355)
        %764 = "ttir.sum"(%763) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc355)
        %765 = "ttir.reshape"(%764) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc355)
        %766 = "ttir.broadcast"(%765) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc355)
        %767 = "ttir.div"(%763, %766) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc355)
        %768 = "ttir.where"(%758, %4, %767) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc356)
        %769 = "ttir.reshape"(%arg310) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %770 = "ttir.reshape"(%769) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %771 = "ttir.permute"(%770) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc357)
        %772 = "ttir.dot_general"(%719, %771) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc358)
        %773 = "ttir.reshape"(%772) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc359)
        %774 = "ttir.reshape"(%arg309) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %775 = "ttir.reshape"(%774) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %776 = "ttir.reshape"(%775) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc360)
        %777 = "ttir.broadcast"(%776) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc360)
        %778 = "ttir.add"(%773, %777) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc360)
        %779 = "ttir.reshape"(%778) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc361)
        %780 = "ttir.permute"(%779) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc362)
        %781 = "ttir.typecast"(%780) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc363)
        %782 = "ttir.dot_general"(%768, %781) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc364)
        %783 = "ttir.typecast"(%782) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc365)
        %784 = "ttir.permute"(%783) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc366)
        %785 = "ttir.reshape"(%784) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc367)
        %786 = "ttir.reshape"(%arg308) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %787 = "ttir.reshape"(%786) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %788 = "ttir.permute"(%787) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc368)
        %789 = "ttir.dot_general"(%785, %788) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc369)
        %790 = "ttir.reshape"(%789) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc367)
        %791 = "ttir.reshape"(%arg307) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %792 = "ttir.reshape"(%791) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %793 = "ttir.reshape"(%792) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc370)
        %794 = "ttir.broadcast"(%793) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc370)
        %795 = "ttir.add"(%790, %794) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc370)
        %796 = "ttir.add"(%713, %795) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc371)
        %797 = "ttir.reshape"(%arg306) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %798 = "ttir.reshape"(%797) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %799 = "ttir.reshape"(%arg305) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %800 = "ttir.reshape"(%799) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %801 = "ttir.layer_norm"(%796, %798, %800) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc372)
        %802 = "ttir.reshape"(%801) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc373)
        %803 = "ttir.reshape"(%arg304) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %804 = "ttir.reshape"(%803) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %805 = "ttir.permute"(%804) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc374)
        %806 = "ttir.dot_general"(%802, %805) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc375)
        %807 = "ttir.reshape"(%806) <{shape = [1 : i32, 257 : i32, 5120 : i32]}> : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc373)
        %808 = "ttir.reshape"(%arg303) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
        %809 = "ttir.reshape"(%808) <{shape = [5120 : i32]}> : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
        %810 = "ttir.reshape"(%809) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc376)
        %811 = "ttir.broadcast"(%810) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc376)
        %812 = "ttir.add"(%807, %811) : (tensor<1x257x5120xbf16>, tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc376)
        %813 = "ttir.gelu"(%812) : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc377)
        %814 = "ttir.reshape"(%813) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc378)
        %815 = "ttir.reshape"(%arg302) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %816 = "ttir.reshape"(%815) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %817 = "ttir.permute"(%816) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc379)
        %818 = "ttir.dot_general"(%814, %817) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc380)
        %819 = "ttir.reshape"(%818) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc378)
        %820 = "ttir.reshape"(%arg301) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %821 = "ttir.reshape"(%820) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %822 = "ttir.reshape"(%821) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc381)
        %823 = "ttir.broadcast"(%822) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc381)
        %824 = "ttir.add"(%819, %823) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc381)
        %825 = "ttir.add"(%796, %824) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc382)
        %826 = "ttir.reshape"(%arg300) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %827 = "ttir.reshape"(%826) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %828 = "ttir.reshape"(%arg299) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %829 = "ttir.reshape"(%828) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %830 = "ttir.layer_norm"(%825, %827, %829) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc383)
        %831 = "ttir.reshape"(%830) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc384)
        %832 = "ttir.reshape"(%arg423) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %833 = "ttir.reshape"(%832) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %834 = "ttir.permute"(%833) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc385)
        %835 = "ttir.dot_general"(%831, %834) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc386)
        %836 = "ttir.reshape"(%835) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc384)
        %837 = "ttir.reshape"(%arg422) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %838 = "ttir.reshape"(%837) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %839 = "ttir.reshape"(%838) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc387)
        %840 = "ttir.broadcast"(%839) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc387)
        %841 = "ttir.add"(%836, %840) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc387)
        %842 = "ttir.reshape"(%841) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc388)
        %843 = "ttir.permute"(%842) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc389)
        %844 = "ttir.typecast"(%843) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc390)
        %845 = "ttir.multiply"(%844, %7) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc391)
        %846 = "ttir.reshape"(%arg421) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %847 = "ttir.reshape"(%846) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %848 = "ttir.permute"(%847) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc392)
        %849 = "ttir.dot_general"(%831, %848) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc393)
        %850 = "ttir.reshape"(%849) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc394)
        %851 = "ttir.reshape"(%arg420) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %852 = "ttir.reshape"(%851) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %853 = "ttir.reshape"(%852) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc395)
        %854 = "ttir.broadcast"(%853) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc395)
        %855 = "ttir.add"(%850, %854) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc395)
        %856 = "ttir.reshape"(%855) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc396)
        %857 = "ttir.permute"(%856) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc397)
        %858 = "ttir.typecast"(%857) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc398)
        %859 = "ttir.permute"(%858) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc399)
        %860 = "ttir.multiply"(%859, %6) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32> loc(#loc400)
        %861 = "ttir.dot_general"(%845, %860) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc401)
        %862 = "ttir.typecast"(%861) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc402)
        %863 = "ttir.eq"(%862, %5) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc402)
        %864 = "ttir.logical_not"(%863) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc403)
        %865 = "ttir.reduce_or"(%864) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1> loc(#loc404)
        %866 = "ttir.reshape"(%865) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc404)
        %867 = "ttir.logical_not"(%866) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1> loc(#loc405)
        %868 = "ttir.reshape"(%867) <{shape = [1 : i32, 16 : i32, 257 : i32]}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc406)
        %869 = "ttir.reshape"(%868) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc406)
        %870 = "ttir.broadcast"(%869) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1> loc(#loc406)
        %871 = "ttir.max"(%861) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc407)
        %872 = "ttir.reshape"(%871) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc407)
        %873 = "ttir.broadcast"(%872) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc407)
        %874 = "ttir.subtract"(%861, %873) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc407)
        %875 = "ttir.exp"(%874) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc407)
        %876 = "ttir.sum"(%875) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc407)
        %877 = "ttir.reshape"(%876) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc407)
        %878 = "ttir.broadcast"(%877) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc407)
        %879 = "ttir.div"(%875, %878) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc407)
        %880 = "ttir.where"(%870, %4, %879) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc408)
        %881 = "ttir.reshape"(%arg298) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %882 = "ttir.reshape"(%881) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %883 = "ttir.permute"(%882) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc409)
        %884 = "ttir.dot_general"(%831, %883) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc410)
        %885 = "ttir.reshape"(%884) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc411)
        %886 = "ttir.reshape"(%arg297) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %887 = "ttir.reshape"(%886) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %888 = "ttir.reshape"(%887) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc412)
        %889 = "ttir.broadcast"(%888) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc412)
        %890 = "ttir.add"(%885, %889) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc412)
        %891 = "ttir.reshape"(%890) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc413)
        %892 = "ttir.permute"(%891) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc414)
        %893 = "ttir.typecast"(%892) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc415)
        %894 = "ttir.dot_general"(%880, %893) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc416)
        %895 = "ttir.typecast"(%894) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc417)
        %896 = "ttir.permute"(%895) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc418)
        %897 = "ttir.reshape"(%896) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc419)
        %898 = "ttir.reshape"(%arg296) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %899 = "ttir.reshape"(%898) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %900 = "ttir.permute"(%899) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc420)
        %901 = "ttir.dot_general"(%897, %900) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc421)
        %902 = "ttir.reshape"(%901) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc419)
        %903 = "ttir.reshape"(%arg295) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %904 = "ttir.reshape"(%903) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %905 = "ttir.reshape"(%904) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc422)
        %906 = "ttir.broadcast"(%905) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc422)
        %907 = "ttir.add"(%902, %906) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc422)
        %908 = "ttir.add"(%825, %907) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc423)
        %909 = "ttir.reshape"(%arg294) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %910 = "ttir.reshape"(%909) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %911 = "ttir.reshape"(%arg293) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %912 = "ttir.reshape"(%911) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %913 = "ttir.layer_norm"(%908, %910, %912) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc424)
        %914 = "ttir.reshape"(%913) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc425)
        %915 = "ttir.reshape"(%arg292) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %916 = "ttir.reshape"(%915) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %917 = "ttir.permute"(%916) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc426)
        %918 = "ttir.dot_general"(%914, %917) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc427)
        %919 = "ttir.reshape"(%918) <{shape = [1 : i32, 257 : i32, 5120 : i32]}> : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc425)
        %920 = "ttir.reshape"(%arg291) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
        %921 = "ttir.reshape"(%920) <{shape = [5120 : i32]}> : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
        %922 = "ttir.reshape"(%921) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc428)
        %923 = "ttir.broadcast"(%922) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc428)
        %924 = "ttir.add"(%919, %923) : (tensor<1x257x5120xbf16>, tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc428)
        %925 = "ttir.gelu"(%924) : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc429)
        %926 = "ttir.reshape"(%925) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc430)
        %927 = "ttir.reshape"(%arg290) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %928 = "ttir.reshape"(%927) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %929 = "ttir.permute"(%928) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc431)
        %930 = "ttir.dot_general"(%926, %929) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc432)
        %931 = "ttir.reshape"(%930) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc430)
        %932 = "ttir.reshape"(%arg289) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %933 = "ttir.reshape"(%932) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %934 = "ttir.reshape"(%933) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc433)
        %935 = "ttir.broadcast"(%934) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc433)
        %936 = "ttir.add"(%931, %935) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc433)
        %937 = "ttir.add"(%908, %936) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc434)
        %938 = "ttir.reshape"(%arg288) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %939 = "ttir.reshape"(%938) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %940 = "ttir.reshape"(%arg287) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %941 = "ttir.reshape"(%940) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %942 = "ttir.layer_norm"(%937, %939, %941) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc435)
        %943 = "ttir.reshape"(%942) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc436)
        %944 = "ttir.reshape"(%arg427) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %945 = "ttir.reshape"(%944) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %946 = "ttir.permute"(%945) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc437)
        %947 = "ttir.dot_general"(%943, %946) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc438)
        %948 = "ttir.reshape"(%947) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc436)
        %949 = "ttir.reshape"(%arg426) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %950 = "ttir.reshape"(%949) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %951 = "ttir.reshape"(%950) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc439)
        %952 = "ttir.broadcast"(%951) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc439)
        %953 = "ttir.add"(%948, %952) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc439)
        %954 = "ttir.reshape"(%953) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc440)
        %955 = "ttir.permute"(%954) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc441)
        %956 = "ttir.typecast"(%955) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc442)
        %957 = "ttir.multiply"(%956, %7) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc443)
        %958 = "ttir.reshape"(%arg425) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %959 = "ttir.reshape"(%958) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %960 = "ttir.permute"(%959) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc444)
        %961 = "ttir.dot_general"(%943, %960) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc445)
        %962 = "ttir.reshape"(%961) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc446)
        %963 = "ttir.reshape"(%arg424) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %964 = "ttir.reshape"(%963) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %965 = "ttir.reshape"(%964) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc447)
        %966 = "ttir.broadcast"(%965) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc447)
        %967 = "ttir.add"(%962, %966) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc447)
        %968 = "ttir.reshape"(%967) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc448)
        %969 = "ttir.permute"(%968) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc449)
        %970 = "ttir.typecast"(%969) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc450)
        %971 = "ttir.permute"(%970) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc451)
        %972 = "ttir.multiply"(%971, %6) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32> loc(#loc452)
        %973 = "ttir.dot_general"(%957, %972) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc453)
        %974 = "ttir.typecast"(%973) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc454)
        %975 = "ttir.eq"(%974, %5) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc454)
        %976 = "ttir.logical_not"(%975) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc455)
        %977 = "ttir.reduce_or"(%976) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1> loc(#loc456)
        %978 = "ttir.reshape"(%977) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc456)
        %979 = "ttir.logical_not"(%978) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1> loc(#loc457)
        %980 = "ttir.reshape"(%979) <{shape = [1 : i32, 16 : i32, 257 : i32]}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc458)
        %981 = "ttir.reshape"(%980) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc458)
        %982 = "ttir.broadcast"(%981) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1> loc(#loc458)
        %983 = "ttir.max"(%973) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc459)
        %984 = "ttir.reshape"(%983) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc459)
        %985 = "ttir.broadcast"(%984) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc459)
        %986 = "ttir.subtract"(%973, %985) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc459)
        %987 = "ttir.exp"(%986) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc459)
        %988 = "ttir.sum"(%987) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc459)
        %989 = "ttir.reshape"(%988) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc459)
        %990 = "ttir.broadcast"(%989) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc459)
        %991 = "ttir.div"(%987, %990) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc459)
        %992 = "ttir.where"(%982, %4, %991) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc460)
        %993 = "ttir.reshape"(%arg286) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %994 = "ttir.reshape"(%993) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %995 = "ttir.permute"(%994) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc461)
        %996 = "ttir.dot_general"(%943, %995) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc462)
        %997 = "ttir.reshape"(%996) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc463)
        %998 = "ttir.reshape"(%arg285) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %999 = "ttir.reshape"(%998) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1000 = "ttir.reshape"(%999) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc464)
        %1001 = "ttir.broadcast"(%1000) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc464)
        %1002 = "ttir.add"(%997, %1001) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc464)
        %1003 = "ttir.reshape"(%1002) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc465)
        %1004 = "ttir.permute"(%1003) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc466)
        %1005 = "ttir.typecast"(%1004) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc467)
        %1006 = "ttir.dot_general"(%992, %1005) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc468)
        %1007 = "ttir.typecast"(%1006) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc469)
        %1008 = "ttir.permute"(%1007) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc470)
        %1009 = "ttir.reshape"(%1008) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc471)
        %1010 = "ttir.reshape"(%arg284) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1011 = "ttir.reshape"(%1010) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1012 = "ttir.permute"(%1011) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc472)
        %1013 = "ttir.dot_general"(%1009, %1012) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc473)
        %1014 = "ttir.reshape"(%1013) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc471)
        %1015 = "ttir.reshape"(%arg283) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1016 = "ttir.reshape"(%1015) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1017 = "ttir.reshape"(%1016) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc474)
        %1018 = "ttir.broadcast"(%1017) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc474)
        %1019 = "ttir.add"(%1014, %1018) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc474)
        %1020 = "ttir.add"(%937, %1019) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc475)
        %1021 = "ttir.reshape"(%arg282) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1022 = "ttir.reshape"(%1021) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1023 = "ttir.reshape"(%arg281) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1024 = "ttir.reshape"(%1023) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1025 = "ttir.layer_norm"(%1020, %1022, %1024) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc476)
        %1026 = "ttir.reshape"(%1025) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc477)
        %1027 = "ttir.reshape"(%arg280) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %1028 = "ttir.reshape"(%1027) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %1029 = "ttir.permute"(%1028) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc478)
        %1030 = "ttir.dot_general"(%1026, %1029) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc479)
        %1031 = "ttir.reshape"(%1030) <{shape = [1 : i32, 257 : i32, 5120 : i32]}> : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc477)
        %1032 = "ttir.reshape"(%arg279) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
        %1033 = "ttir.reshape"(%1032) <{shape = [5120 : i32]}> : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
        %1034 = "ttir.reshape"(%1033) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc480)
        %1035 = "ttir.broadcast"(%1034) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc480)
        %1036 = "ttir.add"(%1031, %1035) : (tensor<1x257x5120xbf16>, tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc480)
        %1037 = "ttir.gelu"(%1036) : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc481)
        %1038 = "ttir.reshape"(%1037) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc482)
        %1039 = "ttir.reshape"(%arg278) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %1040 = "ttir.reshape"(%1039) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %1041 = "ttir.permute"(%1040) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc483)
        %1042 = "ttir.dot_general"(%1038, %1041) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc484)
        %1043 = "ttir.reshape"(%1042) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc482)
        %1044 = "ttir.reshape"(%arg277) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1045 = "ttir.reshape"(%1044) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1046 = "ttir.reshape"(%1045) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc485)
        %1047 = "ttir.broadcast"(%1046) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc485)
        %1048 = "ttir.add"(%1043, %1047) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc485)
        %1049 = "ttir.add"(%1020, %1048) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc486)
        %1050 = "ttir.reshape"(%arg276) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1051 = "ttir.reshape"(%1050) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1052 = "ttir.reshape"(%arg275) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1053 = "ttir.reshape"(%1052) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1054 = "ttir.layer_norm"(%1049, %1051, %1053) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc487)
        %1055 = "ttir.reshape"(%1054) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc488)
        %1056 = "ttir.reshape"(%arg431) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1057 = "ttir.reshape"(%1056) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1058 = "ttir.permute"(%1057) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc489)
        %1059 = "ttir.dot_general"(%1055, %1058) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc490)
        %1060 = "ttir.reshape"(%1059) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc488)
        %1061 = "ttir.reshape"(%arg430) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1062 = "ttir.reshape"(%1061) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1063 = "ttir.reshape"(%1062) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc491)
        %1064 = "ttir.broadcast"(%1063) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc491)
        %1065 = "ttir.add"(%1060, %1064) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc491)
        %1066 = "ttir.reshape"(%1065) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc492)
        %1067 = "ttir.permute"(%1066) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc493)
        %1068 = "ttir.typecast"(%1067) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc494)
        %1069 = "ttir.multiply"(%1068, %7) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc495)
        %1070 = "ttir.reshape"(%arg429) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1071 = "ttir.reshape"(%1070) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1072 = "ttir.permute"(%1071) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc496)
        %1073 = "ttir.dot_general"(%1055, %1072) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc497)
        %1074 = "ttir.reshape"(%1073) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc498)
        %1075 = "ttir.reshape"(%arg428) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1076 = "ttir.reshape"(%1075) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1077 = "ttir.reshape"(%1076) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc499)
        %1078 = "ttir.broadcast"(%1077) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc499)
        %1079 = "ttir.add"(%1074, %1078) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc499)
        %1080 = "ttir.reshape"(%1079) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc500)
        %1081 = "ttir.permute"(%1080) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc501)
        %1082 = "ttir.typecast"(%1081) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc502)
        %1083 = "ttir.permute"(%1082) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc503)
        %1084 = "ttir.multiply"(%1083, %6) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32> loc(#loc504)
        %1085 = "ttir.dot_general"(%1069, %1084) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc505)
        %1086 = "ttir.typecast"(%1085) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc506)
        %1087 = "ttir.eq"(%1086, %5) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc506)
        %1088 = "ttir.logical_not"(%1087) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc507)
        %1089 = "ttir.reduce_or"(%1088) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1> loc(#loc508)
        %1090 = "ttir.reshape"(%1089) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc508)
        %1091 = "ttir.logical_not"(%1090) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1> loc(#loc509)
        %1092 = "ttir.reshape"(%1091) <{shape = [1 : i32, 16 : i32, 257 : i32]}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc510)
        %1093 = "ttir.reshape"(%1092) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc510)
        %1094 = "ttir.broadcast"(%1093) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1> loc(#loc510)
        %1095 = "ttir.max"(%1085) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc511)
        %1096 = "ttir.reshape"(%1095) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc511)
        %1097 = "ttir.broadcast"(%1096) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc511)
        %1098 = "ttir.subtract"(%1085, %1097) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc511)
        %1099 = "ttir.exp"(%1098) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc511)
        %1100 = "ttir.sum"(%1099) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc511)
        %1101 = "ttir.reshape"(%1100) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc511)
        %1102 = "ttir.broadcast"(%1101) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc511)
        %1103 = "ttir.div"(%1099, %1102) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc511)
        %1104 = "ttir.where"(%1094, %4, %1103) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc512)
        %1105 = "ttir.reshape"(%arg274) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1106 = "ttir.reshape"(%1105) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1107 = "ttir.permute"(%1106) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc513)
        %1108 = "ttir.dot_general"(%1055, %1107) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc514)
        %1109 = "ttir.reshape"(%1108) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc515)
        %1110 = "ttir.reshape"(%arg273) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1111 = "ttir.reshape"(%1110) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1112 = "ttir.reshape"(%1111) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc516)
        %1113 = "ttir.broadcast"(%1112) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc516)
        %1114 = "ttir.add"(%1109, %1113) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc516)
        %1115 = "ttir.reshape"(%1114) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc517)
        %1116 = "ttir.permute"(%1115) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc518)
        %1117 = "ttir.typecast"(%1116) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc519)
        %1118 = "ttir.dot_general"(%1104, %1117) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc520)
        %1119 = "ttir.typecast"(%1118) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc521)
        %1120 = "ttir.permute"(%1119) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc522)
        %1121 = "ttir.reshape"(%1120) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc523)
        %1122 = "ttir.reshape"(%arg272) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1123 = "ttir.reshape"(%1122) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1124 = "ttir.permute"(%1123) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc524)
        %1125 = "ttir.dot_general"(%1121, %1124) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc525)
        %1126 = "ttir.reshape"(%1125) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc523)
        %1127 = "ttir.reshape"(%arg271) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1128 = "ttir.reshape"(%1127) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1129 = "ttir.reshape"(%1128) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc526)
        %1130 = "ttir.broadcast"(%1129) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc526)
        %1131 = "ttir.add"(%1126, %1130) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc526)
        %1132 = "ttir.add"(%1049, %1131) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc527)
        %1133 = "ttir.reshape"(%arg270) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1134 = "ttir.reshape"(%1133) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1135 = "ttir.reshape"(%arg269) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1136 = "ttir.reshape"(%1135) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1137 = "ttir.layer_norm"(%1132, %1134, %1136) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc528)
        %1138 = "ttir.reshape"(%1137) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc529)
        %1139 = "ttir.reshape"(%arg268) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %1140 = "ttir.reshape"(%1139) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %1141 = "ttir.permute"(%1140) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc530)
        %1142 = "ttir.dot_general"(%1138, %1141) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc531)
        %1143 = "ttir.reshape"(%1142) <{shape = [1 : i32, 257 : i32, 5120 : i32]}> : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc529)
        %1144 = "ttir.reshape"(%arg267) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
        %1145 = "ttir.reshape"(%1144) <{shape = [5120 : i32]}> : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
        %1146 = "ttir.reshape"(%1145) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc532)
        %1147 = "ttir.broadcast"(%1146) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc532)
        %1148 = "ttir.add"(%1143, %1147) : (tensor<1x257x5120xbf16>, tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc532)
        %1149 = "ttir.gelu"(%1148) : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc533)
        %1150 = "ttir.reshape"(%1149) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc534)
        %1151 = "ttir.reshape"(%arg266) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %1152 = "ttir.reshape"(%1151) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %1153 = "ttir.permute"(%1152) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc535)
        %1154 = "ttir.dot_general"(%1150, %1153) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc536)
        %1155 = "ttir.reshape"(%1154) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc534)
        %1156 = "ttir.reshape"(%arg265) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1157 = "ttir.reshape"(%1156) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1158 = "ttir.reshape"(%1157) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc537)
        %1159 = "ttir.broadcast"(%1158) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc537)
        %1160 = "ttir.add"(%1155, %1159) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc537)
        %1161 = "ttir.add"(%1132, %1160) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc538)
        %1162 = "ttir.reshape"(%arg264) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1163 = "ttir.reshape"(%1162) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1164 = "ttir.reshape"(%arg263) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1165 = "ttir.reshape"(%1164) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1166 = "ttir.layer_norm"(%1161, %1163, %1165) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc539)
        %1167 = "ttir.reshape"(%1166) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc540)
        %1168 = "ttir.reshape"(%arg435) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1169 = "ttir.reshape"(%1168) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1170 = "ttir.permute"(%1169) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc541)
        %1171 = "ttir.dot_general"(%1167, %1170) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc542)
        %1172 = "ttir.reshape"(%1171) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc540)
        %1173 = "ttir.reshape"(%arg434) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1174 = "ttir.reshape"(%1173) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1175 = "ttir.reshape"(%1174) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc543)
        %1176 = "ttir.broadcast"(%1175) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc543)
        %1177 = "ttir.add"(%1172, %1176) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc543)
        %1178 = "ttir.reshape"(%1177) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc544)
        %1179 = "ttir.permute"(%1178) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc545)
        %1180 = "ttir.typecast"(%1179) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc546)
        %1181 = "ttir.multiply"(%1180, %7) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc547)
        %1182 = "ttir.reshape"(%arg433) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1183 = "ttir.reshape"(%1182) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1184 = "ttir.permute"(%1183) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc548)
        %1185 = "ttir.dot_general"(%1167, %1184) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc549)
        %1186 = "ttir.reshape"(%1185) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc550)
        %1187 = "ttir.reshape"(%arg432) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1188 = "ttir.reshape"(%1187) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1189 = "ttir.reshape"(%1188) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc551)
        %1190 = "ttir.broadcast"(%1189) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc551)
        %1191 = "ttir.add"(%1186, %1190) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc551)
        %1192 = "ttir.reshape"(%1191) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc552)
        %1193 = "ttir.permute"(%1192) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc553)
        %1194 = "ttir.typecast"(%1193) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc554)
        %1195 = "ttir.permute"(%1194) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc555)
        %1196 = "ttir.multiply"(%1195, %6) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32> loc(#loc556)
        %1197 = "ttir.dot_general"(%1181, %1196) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc557)
        %1198 = "ttir.typecast"(%1197) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc558)
        %1199 = "ttir.eq"(%1198, %5) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc558)
        %1200 = "ttir.logical_not"(%1199) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc559)
        %1201 = "ttir.reduce_or"(%1200) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1> loc(#loc560)
        %1202 = "ttir.reshape"(%1201) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc560)
        %1203 = "ttir.logical_not"(%1202) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1> loc(#loc561)
        %1204 = "ttir.reshape"(%1203) <{shape = [1 : i32, 16 : i32, 257 : i32]}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc562)
        %1205 = "ttir.reshape"(%1204) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc562)
        %1206 = "ttir.broadcast"(%1205) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1> loc(#loc562)
        %1207 = "ttir.max"(%1197) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc563)
        %1208 = "ttir.reshape"(%1207) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc563)
        %1209 = "ttir.broadcast"(%1208) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc563)
        %1210 = "ttir.subtract"(%1197, %1209) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc563)
        %1211 = "ttir.exp"(%1210) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc563)
        %1212 = "ttir.sum"(%1211) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc563)
        %1213 = "ttir.reshape"(%1212) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc563)
        %1214 = "ttir.broadcast"(%1213) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc563)
        %1215 = "ttir.div"(%1211, %1214) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc563)
        %1216 = "ttir.where"(%1206, %4, %1215) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc564)
        %1217 = "ttir.reshape"(%arg262) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1218 = "ttir.reshape"(%1217) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1219 = "ttir.permute"(%1218) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc565)
        %1220 = "ttir.dot_general"(%1167, %1219) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc566)
        %1221 = "ttir.reshape"(%1220) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc567)
        %1222 = "ttir.reshape"(%arg261) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1223 = "ttir.reshape"(%1222) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1224 = "ttir.reshape"(%1223) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc568)
        %1225 = "ttir.broadcast"(%1224) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc568)
        %1226 = "ttir.add"(%1221, %1225) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc568)
        %1227 = "ttir.reshape"(%1226) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc569)
        %1228 = "ttir.permute"(%1227) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc570)
        %1229 = "ttir.typecast"(%1228) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc571)
        %1230 = "ttir.dot_general"(%1216, %1229) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc572)
        %1231 = "ttir.typecast"(%1230) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc573)
        %1232 = "ttir.permute"(%1231) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc574)
        %1233 = "ttir.reshape"(%1232) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc575)
        %1234 = "ttir.reshape"(%arg260) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1235 = "ttir.reshape"(%1234) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1236 = "ttir.permute"(%1235) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc576)
        %1237 = "ttir.dot_general"(%1233, %1236) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc577)
        %1238 = "ttir.reshape"(%1237) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc575)
        %1239 = "ttir.reshape"(%arg259) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1240 = "ttir.reshape"(%1239) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1241 = "ttir.reshape"(%1240) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc578)
        %1242 = "ttir.broadcast"(%1241) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc578)
        %1243 = "ttir.add"(%1238, %1242) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc578)
        %1244 = "ttir.add"(%1161, %1243) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc579)
        %1245 = "ttir.reshape"(%arg258) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1246 = "ttir.reshape"(%1245) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1247 = "ttir.reshape"(%arg257) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1248 = "ttir.reshape"(%1247) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1249 = "ttir.layer_norm"(%1244, %1246, %1248) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc580)
        %1250 = "ttir.reshape"(%1249) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc581)
        %1251 = "ttir.reshape"(%arg256) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %1252 = "ttir.reshape"(%1251) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %1253 = "ttir.permute"(%1252) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc582)
        %1254 = "ttir.dot_general"(%1250, %1253) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc583)
        %1255 = "ttir.reshape"(%1254) <{shape = [1 : i32, 257 : i32, 5120 : i32]}> : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc581)
        %1256 = "ttir.reshape"(%arg255) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
        %1257 = "ttir.reshape"(%1256) <{shape = [5120 : i32]}> : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
        %1258 = "ttir.reshape"(%1257) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc584)
        %1259 = "ttir.broadcast"(%1258) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc584)
        %1260 = "ttir.add"(%1255, %1259) : (tensor<1x257x5120xbf16>, tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc584)
        %1261 = "ttir.gelu"(%1260) : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc585)
        %1262 = "ttir.reshape"(%1261) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc586)
        %1263 = "ttir.reshape"(%arg254) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %1264 = "ttir.reshape"(%1263) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %1265 = "ttir.permute"(%1264) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc587)
        %1266 = "ttir.dot_general"(%1262, %1265) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc588)
        %1267 = "ttir.reshape"(%1266) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc586)
        %1268 = "ttir.reshape"(%arg253) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1269 = "ttir.reshape"(%1268) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1270 = "ttir.reshape"(%1269) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc589)
        %1271 = "ttir.broadcast"(%1270) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc589)
        %1272 = "ttir.add"(%1267, %1271) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc589)
        %1273 = "ttir.add"(%1244, %1272) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc590)
        %1274 = "ttir.reshape"(%arg252) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1275 = "ttir.reshape"(%1274) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1276 = "ttir.reshape"(%arg251) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1277 = "ttir.reshape"(%1276) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1278 = "ttir.layer_norm"(%1273, %1275, %1277) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc591)
        %1279 = "ttir.reshape"(%1278) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc592)
        %1280 = "ttir.reshape"(%arg439) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1281 = "ttir.reshape"(%1280) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1282 = "ttir.permute"(%1281) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc593)
        %1283 = "ttir.dot_general"(%1279, %1282) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc594)
        %1284 = "ttir.reshape"(%1283) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc592)
        %1285 = "ttir.reshape"(%arg438) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1286 = "ttir.reshape"(%1285) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1287 = "ttir.reshape"(%1286) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc595)
        %1288 = "ttir.broadcast"(%1287) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc595)
        %1289 = "ttir.add"(%1284, %1288) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc595)
        %1290 = "ttir.reshape"(%1289) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc596)
        %1291 = "ttir.permute"(%1290) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc597)
        %1292 = "ttir.typecast"(%1291) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc598)
        %1293 = "ttir.multiply"(%1292, %7) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc599)
        %1294 = "ttir.reshape"(%arg437) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1295 = "ttir.reshape"(%1294) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1296 = "ttir.permute"(%1295) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc600)
        %1297 = "ttir.dot_general"(%1279, %1296) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc601)
        %1298 = "ttir.reshape"(%1297) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc602)
        %1299 = "ttir.reshape"(%arg436) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1300 = "ttir.reshape"(%1299) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1301 = "ttir.reshape"(%1300) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc603)
        %1302 = "ttir.broadcast"(%1301) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc603)
        %1303 = "ttir.add"(%1298, %1302) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc603)
        %1304 = "ttir.reshape"(%1303) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc604)
        %1305 = "ttir.permute"(%1304) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc605)
        %1306 = "ttir.typecast"(%1305) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc606)
        %1307 = "ttir.permute"(%1306) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc607)
        %1308 = "ttir.multiply"(%1307, %6) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32> loc(#loc608)
        %1309 = "ttir.dot_general"(%1293, %1308) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc609)
        %1310 = "ttir.typecast"(%1309) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc610)
        %1311 = "ttir.eq"(%1310, %5) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc610)
        %1312 = "ttir.logical_not"(%1311) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc611)
        %1313 = "ttir.reduce_or"(%1312) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1> loc(#loc612)
        %1314 = "ttir.reshape"(%1313) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc612)
        %1315 = "ttir.logical_not"(%1314) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1> loc(#loc613)
        %1316 = "ttir.reshape"(%1315) <{shape = [1 : i32, 16 : i32, 257 : i32]}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc614)
        %1317 = "ttir.reshape"(%1316) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc614)
        %1318 = "ttir.broadcast"(%1317) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1> loc(#loc614)
        %1319 = "ttir.max"(%1309) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc615)
        %1320 = "ttir.reshape"(%1319) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc615)
        %1321 = "ttir.broadcast"(%1320) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc615)
        %1322 = "ttir.subtract"(%1309, %1321) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc615)
        %1323 = "ttir.exp"(%1322) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc615)
        %1324 = "ttir.sum"(%1323) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc615)
        %1325 = "ttir.reshape"(%1324) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc615)
        %1326 = "ttir.broadcast"(%1325) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc615)
        %1327 = "ttir.div"(%1323, %1326) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc615)
        %1328 = "ttir.where"(%1318, %4, %1327) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc616)
        %1329 = "ttir.reshape"(%arg250) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1330 = "ttir.reshape"(%1329) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1331 = "ttir.permute"(%1330) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc617)
        %1332 = "ttir.dot_general"(%1279, %1331) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc618)
        %1333 = "ttir.reshape"(%1332) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc619)
        %1334 = "ttir.reshape"(%arg249) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1335 = "ttir.reshape"(%1334) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1336 = "ttir.reshape"(%1335) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc620)
        %1337 = "ttir.broadcast"(%1336) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc620)
        %1338 = "ttir.add"(%1333, %1337) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc620)
        %1339 = "ttir.reshape"(%1338) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc621)
        %1340 = "ttir.permute"(%1339) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc622)
        %1341 = "ttir.typecast"(%1340) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc623)
        %1342 = "ttir.dot_general"(%1328, %1341) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc624)
        %1343 = "ttir.typecast"(%1342) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc625)
        %1344 = "ttir.permute"(%1343) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc626)
        %1345 = "ttir.reshape"(%1344) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc627)
        %1346 = "ttir.reshape"(%arg248) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1347 = "ttir.reshape"(%1346) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1348 = "ttir.permute"(%1347) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc628)
        %1349 = "ttir.dot_general"(%1345, %1348) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc629)
        %1350 = "ttir.reshape"(%1349) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc627)
        %1351 = "ttir.reshape"(%arg247) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1352 = "ttir.reshape"(%1351) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1353 = "ttir.reshape"(%1352) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc630)
        %1354 = "ttir.broadcast"(%1353) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc630)
        %1355 = "ttir.add"(%1350, %1354) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc630)
        %1356 = "ttir.add"(%1273, %1355) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc631)
        %1357 = "ttir.reshape"(%arg246) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1358 = "ttir.reshape"(%1357) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1359 = "ttir.reshape"(%arg245) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1360 = "ttir.reshape"(%1359) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1361 = "ttir.layer_norm"(%1356, %1358, %1360) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc632)
        %1362 = "ttir.reshape"(%1361) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc633)
        %1363 = "ttir.reshape"(%arg244) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %1364 = "ttir.reshape"(%1363) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %1365 = "ttir.permute"(%1364) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc634)
        %1366 = "ttir.dot_general"(%1362, %1365) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc635)
        %1367 = "ttir.reshape"(%1366) <{shape = [1 : i32, 257 : i32, 5120 : i32]}> : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc633)
        %1368 = "ttir.reshape"(%arg243) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
        %1369 = "ttir.reshape"(%1368) <{shape = [5120 : i32]}> : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
        %1370 = "ttir.reshape"(%1369) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc636)
        %1371 = "ttir.broadcast"(%1370) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc636)
        %1372 = "ttir.add"(%1367, %1371) : (tensor<1x257x5120xbf16>, tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc636)
        %1373 = "ttir.gelu"(%1372) : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc637)
        %1374 = "ttir.reshape"(%1373) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc638)
        %1375 = "ttir.reshape"(%arg242) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %1376 = "ttir.reshape"(%1375) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %1377 = "ttir.permute"(%1376) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc639)
        %1378 = "ttir.dot_general"(%1374, %1377) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc640)
        %1379 = "ttir.reshape"(%1378) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc638)
        %1380 = "ttir.reshape"(%arg241) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1381 = "ttir.reshape"(%1380) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1382 = "ttir.reshape"(%1381) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc641)
        %1383 = "ttir.broadcast"(%1382) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc641)
        %1384 = "ttir.add"(%1379, %1383) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc641)
        %1385 = "ttir.add"(%1356, %1384) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc642)
        %1386 = "ttir.reshape"(%arg240) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1387 = "ttir.reshape"(%1386) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1388 = "ttir.reshape"(%arg239) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1389 = "ttir.reshape"(%1388) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1390 = "ttir.layer_norm"(%1385, %1387, %1389) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc643)
        %1391 = "ttir.reshape"(%1390) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc644)
        %1392 = "ttir.reshape"(%arg443) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1393 = "ttir.reshape"(%1392) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1394 = "ttir.permute"(%1393) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc645)
        %1395 = "ttir.dot_general"(%1391, %1394) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc646)
        %1396 = "ttir.reshape"(%1395) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc644)
        %1397 = "ttir.reshape"(%arg442) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1398 = "ttir.reshape"(%1397) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1399 = "ttir.reshape"(%1398) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc647)
        %1400 = "ttir.broadcast"(%1399) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc647)
        %1401 = "ttir.add"(%1396, %1400) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc647)
        %1402 = "ttir.reshape"(%1401) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc648)
        %1403 = "ttir.permute"(%1402) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc649)
        %1404 = "ttir.typecast"(%1403) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc650)
        %1405 = "ttir.multiply"(%1404, %7) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc651)
        %1406 = "ttir.reshape"(%arg441) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1407 = "ttir.reshape"(%1406) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1408 = "ttir.permute"(%1407) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc652)
        %1409 = "ttir.dot_general"(%1391, %1408) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc653)
        %1410 = "ttir.reshape"(%1409) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc654)
        %1411 = "ttir.reshape"(%arg440) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1412 = "ttir.reshape"(%1411) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1413 = "ttir.reshape"(%1412) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc655)
        %1414 = "ttir.broadcast"(%1413) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc655)
        %1415 = "ttir.add"(%1410, %1414) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc655)
        %1416 = "ttir.reshape"(%1415) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc656)
        %1417 = "ttir.permute"(%1416) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc657)
        %1418 = "ttir.typecast"(%1417) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc658)
        %1419 = "ttir.permute"(%1418) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc659)
        %1420 = "ttir.multiply"(%1419, %6) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32> loc(#loc660)
        %1421 = "ttir.dot_general"(%1405, %1420) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc661)
        %1422 = "ttir.typecast"(%1421) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc662)
        %1423 = "ttir.eq"(%1422, %5) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc662)
        %1424 = "ttir.logical_not"(%1423) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc663)
        %1425 = "ttir.reduce_or"(%1424) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1> loc(#loc664)
        %1426 = "ttir.reshape"(%1425) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc664)
        %1427 = "ttir.logical_not"(%1426) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1> loc(#loc665)
        %1428 = "ttir.reshape"(%1427) <{shape = [1 : i32, 16 : i32, 257 : i32]}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc666)
        %1429 = "ttir.reshape"(%1428) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc666)
        %1430 = "ttir.broadcast"(%1429) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1> loc(#loc666)
        %1431 = "ttir.max"(%1421) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc667)
        %1432 = "ttir.reshape"(%1431) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc667)
        %1433 = "ttir.broadcast"(%1432) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc667)
        %1434 = "ttir.subtract"(%1421, %1433) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc667)
        %1435 = "ttir.exp"(%1434) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc667)
        %1436 = "ttir.sum"(%1435) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc667)
        %1437 = "ttir.reshape"(%1436) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc667)
        %1438 = "ttir.broadcast"(%1437) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc667)
        %1439 = "ttir.div"(%1435, %1438) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc667)
        %1440 = "ttir.where"(%1430, %4, %1439) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc668)
        %1441 = "ttir.reshape"(%arg238) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1442 = "ttir.reshape"(%1441) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1443 = "ttir.permute"(%1442) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc669)
        %1444 = "ttir.dot_general"(%1391, %1443) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc670)
        %1445 = "ttir.reshape"(%1444) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc671)
        %1446 = "ttir.reshape"(%arg237) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1447 = "ttir.reshape"(%1446) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1448 = "ttir.reshape"(%1447) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc672)
        %1449 = "ttir.broadcast"(%1448) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc672)
        %1450 = "ttir.add"(%1445, %1449) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc672)
        %1451 = "ttir.reshape"(%1450) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc673)
        %1452 = "ttir.permute"(%1451) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc674)
        %1453 = "ttir.typecast"(%1452) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc675)
        %1454 = "ttir.dot_general"(%1440, %1453) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc676)
        %1455 = "ttir.typecast"(%1454) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc677)
        %1456 = "ttir.permute"(%1455) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc678)
        %1457 = "ttir.reshape"(%1456) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc679)
        %1458 = "ttir.reshape"(%arg236) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1459 = "ttir.reshape"(%1458) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1460 = "ttir.permute"(%1459) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc680)
        %1461 = "ttir.dot_general"(%1457, %1460) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc681)
        %1462 = "ttir.reshape"(%1461) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc679)
        %1463 = "ttir.reshape"(%arg235) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1464 = "ttir.reshape"(%1463) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1465 = "ttir.reshape"(%1464) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc682)
        %1466 = "ttir.broadcast"(%1465) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc682)
        %1467 = "ttir.add"(%1462, %1466) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc682)
        %1468 = "ttir.add"(%1385, %1467) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc683)
        %1469 = "ttir.reshape"(%arg234) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1470 = "ttir.reshape"(%1469) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1471 = "ttir.reshape"(%arg233) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1472 = "ttir.reshape"(%1471) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1473 = "ttir.layer_norm"(%1468, %1470, %1472) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc684)
        %1474 = "ttir.reshape"(%1473) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc685)
        %1475 = "ttir.reshape"(%arg232) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %1476 = "ttir.reshape"(%1475) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %1477 = "ttir.permute"(%1476) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc686)
        %1478 = "ttir.dot_general"(%1474, %1477) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc687)
        %1479 = "ttir.reshape"(%1478) <{shape = [1 : i32, 257 : i32, 5120 : i32]}> : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc685)
        %1480 = "ttir.reshape"(%arg231) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
        %1481 = "ttir.reshape"(%1480) <{shape = [5120 : i32]}> : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
        %1482 = "ttir.reshape"(%1481) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc688)
        %1483 = "ttir.broadcast"(%1482) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc688)
        %1484 = "ttir.add"(%1479, %1483) : (tensor<1x257x5120xbf16>, tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc688)
        %1485 = "ttir.gelu"(%1484) : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc689)
        %1486 = "ttir.reshape"(%1485) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc690)
        %1487 = "ttir.reshape"(%arg230) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %1488 = "ttir.reshape"(%1487) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %1489 = "ttir.permute"(%1488) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc691)
        %1490 = "ttir.dot_general"(%1486, %1489) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc692)
        %1491 = "ttir.reshape"(%1490) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc690)
        %1492 = "ttir.reshape"(%arg229) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1493 = "ttir.reshape"(%1492) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1494 = "ttir.reshape"(%1493) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc693)
        %1495 = "ttir.broadcast"(%1494) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc693)
        %1496 = "ttir.add"(%1491, %1495) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc693)
        %1497 = "ttir.add"(%1468, %1496) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc694)
        %1498 = "ttir.reshape"(%arg228) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1499 = "ttir.reshape"(%1498) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1500 = "ttir.reshape"(%arg227) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1501 = "ttir.reshape"(%1500) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1502 = "ttir.layer_norm"(%1497, %1499, %1501) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc695)
        %1503 = "ttir.reshape"(%1502) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc696)
        %1504 = "ttir.reshape"(%arg447) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1505 = "ttir.reshape"(%1504) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1506 = "ttir.permute"(%1505) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc697)
        %1507 = "ttir.dot_general"(%1503, %1506) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc698)
        %1508 = "ttir.reshape"(%1507) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc696)
        %1509 = "ttir.reshape"(%arg446) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1510 = "ttir.reshape"(%1509) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1511 = "ttir.reshape"(%1510) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc699)
        %1512 = "ttir.broadcast"(%1511) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc699)
        %1513 = "ttir.add"(%1508, %1512) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc699)
        %1514 = "ttir.reshape"(%1513) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc700)
        %1515 = "ttir.permute"(%1514) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc701)
        %1516 = "ttir.typecast"(%1515) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc702)
        %1517 = "ttir.multiply"(%1516, %7) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc703)
        %1518 = "ttir.reshape"(%arg445) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1519 = "ttir.reshape"(%1518) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1520 = "ttir.permute"(%1519) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc704)
        %1521 = "ttir.dot_general"(%1503, %1520) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc705)
        %1522 = "ttir.reshape"(%1521) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc706)
        %1523 = "ttir.reshape"(%arg444) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1524 = "ttir.reshape"(%1523) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1525 = "ttir.reshape"(%1524) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc707)
        %1526 = "ttir.broadcast"(%1525) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc707)
        %1527 = "ttir.add"(%1522, %1526) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc707)
        %1528 = "ttir.reshape"(%1527) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc708)
        %1529 = "ttir.permute"(%1528) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc709)
        %1530 = "ttir.typecast"(%1529) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc710)
        %1531 = "ttir.permute"(%1530) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc711)
        %1532 = "ttir.multiply"(%1531, %6) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32> loc(#loc712)
        %1533 = "ttir.dot_general"(%1517, %1532) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc713)
        %1534 = "ttir.typecast"(%1533) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc714)
        %1535 = "ttir.eq"(%1534, %5) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc714)
        %1536 = "ttir.logical_not"(%1535) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc715)
        %1537 = "ttir.reduce_or"(%1536) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1> loc(#loc716)
        %1538 = "ttir.reshape"(%1537) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc716)
        %1539 = "ttir.logical_not"(%1538) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1> loc(#loc717)
        %1540 = "ttir.reshape"(%1539) <{shape = [1 : i32, 16 : i32, 257 : i32]}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc718)
        %1541 = "ttir.reshape"(%1540) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc718)
        %1542 = "ttir.broadcast"(%1541) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1> loc(#loc718)
        %1543 = "ttir.max"(%1533) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc719)
        %1544 = "ttir.reshape"(%1543) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc719)
        %1545 = "ttir.broadcast"(%1544) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc719)
        %1546 = "ttir.subtract"(%1533, %1545) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc719)
        %1547 = "ttir.exp"(%1546) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc719)
        %1548 = "ttir.sum"(%1547) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc719)
        %1549 = "ttir.reshape"(%1548) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc719)
        %1550 = "ttir.broadcast"(%1549) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc719)
        %1551 = "ttir.div"(%1547, %1550) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc719)
        %1552 = "ttir.where"(%1542, %4, %1551) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc720)
        %1553 = "ttir.reshape"(%arg226) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1554 = "ttir.reshape"(%1553) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1555 = "ttir.permute"(%1554) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc721)
        %1556 = "ttir.dot_general"(%1503, %1555) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc722)
        %1557 = "ttir.reshape"(%1556) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc723)
        %1558 = "ttir.reshape"(%arg225) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1559 = "ttir.reshape"(%1558) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1560 = "ttir.reshape"(%1559) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc724)
        %1561 = "ttir.broadcast"(%1560) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc724)
        %1562 = "ttir.add"(%1557, %1561) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc724)
        %1563 = "ttir.reshape"(%1562) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc725)
        %1564 = "ttir.permute"(%1563) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc726)
        %1565 = "ttir.typecast"(%1564) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc727)
        %1566 = "ttir.dot_general"(%1552, %1565) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc728)
        %1567 = "ttir.typecast"(%1566) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc729)
        %1568 = "ttir.permute"(%1567) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc730)
        %1569 = "ttir.reshape"(%1568) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc731)
        %1570 = "ttir.reshape"(%arg224) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1571 = "ttir.reshape"(%1570) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1572 = "ttir.permute"(%1571) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc732)
        %1573 = "ttir.dot_general"(%1569, %1572) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc733)
        %1574 = "ttir.reshape"(%1573) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc731)
        %1575 = "ttir.reshape"(%arg223) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1576 = "ttir.reshape"(%1575) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1577 = "ttir.reshape"(%1576) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc734)
        %1578 = "ttir.broadcast"(%1577) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc734)
        %1579 = "ttir.add"(%1574, %1578) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc734)
        %1580 = "ttir.add"(%1497, %1579) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc735)
        %1581 = "ttir.reshape"(%arg222) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1582 = "ttir.reshape"(%1581) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1583 = "ttir.reshape"(%arg221) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1584 = "ttir.reshape"(%1583) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1585 = "ttir.layer_norm"(%1580, %1582, %1584) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc736)
        %1586 = "ttir.reshape"(%1585) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc737)
        %1587 = "ttir.reshape"(%arg220) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %1588 = "ttir.reshape"(%1587) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %1589 = "ttir.permute"(%1588) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc738)
        %1590 = "ttir.dot_general"(%1586, %1589) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc739)
        %1591 = "ttir.reshape"(%1590) <{shape = [1 : i32, 257 : i32, 5120 : i32]}> : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc737)
        %1592 = "ttir.reshape"(%arg219) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
        %1593 = "ttir.reshape"(%1592) <{shape = [5120 : i32]}> : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
        %1594 = "ttir.reshape"(%1593) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc740)
        %1595 = "ttir.broadcast"(%1594) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc740)
        %1596 = "ttir.add"(%1591, %1595) : (tensor<1x257x5120xbf16>, tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc740)
        %1597 = "ttir.gelu"(%1596) : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc741)
        %1598 = "ttir.reshape"(%1597) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc742)
        %1599 = "ttir.reshape"(%arg218) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %1600 = "ttir.reshape"(%1599) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %1601 = "ttir.permute"(%1600) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc743)
        %1602 = "ttir.dot_general"(%1598, %1601) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc744)
        %1603 = "ttir.reshape"(%1602) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc742)
        %1604 = "ttir.reshape"(%arg217) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1605 = "ttir.reshape"(%1604) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1606 = "ttir.reshape"(%1605) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc745)
        %1607 = "ttir.broadcast"(%1606) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc745)
        %1608 = "ttir.add"(%1603, %1607) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc745)
        %1609 = "ttir.add"(%1580, %1608) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc746)
        %1610 = "ttir.reshape"(%arg216) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1611 = "ttir.reshape"(%1610) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1612 = "ttir.reshape"(%arg215) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1613 = "ttir.reshape"(%1612) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1614 = "ttir.layer_norm"(%1609, %1611, %1613) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc747)
        %1615 = "ttir.reshape"(%1614) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc748)
        %1616 = "ttir.reshape"(%arg451) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1617 = "ttir.reshape"(%1616) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1618 = "ttir.permute"(%1617) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc749)
        %1619 = "ttir.dot_general"(%1615, %1618) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc750)
        %1620 = "ttir.reshape"(%1619) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc748)
        %1621 = "ttir.reshape"(%arg450) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1622 = "ttir.reshape"(%1621) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1623 = "ttir.reshape"(%1622) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc751)
        %1624 = "ttir.broadcast"(%1623) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc751)
        %1625 = "ttir.add"(%1620, %1624) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc751)
        %1626 = "ttir.reshape"(%1625) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc752)
        %1627 = "ttir.permute"(%1626) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc753)
        %1628 = "ttir.typecast"(%1627) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc754)
        %1629 = "ttir.multiply"(%1628, %7) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc755)
        %1630 = "ttir.reshape"(%arg449) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1631 = "ttir.reshape"(%1630) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1632 = "ttir.permute"(%1631) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc756)
        %1633 = "ttir.dot_general"(%1615, %1632) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc757)
        %1634 = "ttir.reshape"(%1633) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc758)
        %1635 = "ttir.reshape"(%arg448) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1636 = "ttir.reshape"(%1635) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1637 = "ttir.reshape"(%1636) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc759)
        %1638 = "ttir.broadcast"(%1637) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc759)
        %1639 = "ttir.add"(%1634, %1638) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc759)
        %1640 = "ttir.reshape"(%1639) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc760)
        %1641 = "ttir.permute"(%1640) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc761)
        %1642 = "ttir.typecast"(%1641) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc762)
        %1643 = "ttir.permute"(%1642) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc763)
        %1644 = "ttir.multiply"(%1643, %6) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32> loc(#loc764)
        %1645 = "ttir.dot_general"(%1629, %1644) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc765)
        %1646 = "ttir.typecast"(%1645) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc766)
        %1647 = "ttir.eq"(%1646, %5) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc766)
        %1648 = "ttir.logical_not"(%1647) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc767)
        %1649 = "ttir.reduce_or"(%1648) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1> loc(#loc768)
        %1650 = "ttir.reshape"(%1649) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc768)
        %1651 = "ttir.logical_not"(%1650) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1> loc(#loc769)
        %1652 = "ttir.reshape"(%1651) <{shape = [1 : i32, 16 : i32, 257 : i32]}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc770)
        %1653 = "ttir.reshape"(%1652) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc770)
        %1654 = "ttir.broadcast"(%1653) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1> loc(#loc770)
        %1655 = "ttir.max"(%1645) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc771)
        %1656 = "ttir.reshape"(%1655) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc771)
        %1657 = "ttir.broadcast"(%1656) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc771)
        %1658 = "ttir.subtract"(%1645, %1657) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc771)
        %1659 = "ttir.exp"(%1658) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc771)
        %1660 = "ttir.sum"(%1659) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc771)
        %1661 = "ttir.reshape"(%1660) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc771)
        %1662 = "ttir.broadcast"(%1661) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc771)
        %1663 = "ttir.div"(%1659, %1662) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc771)
        %1664 = "ttir.where"(%1654, %4, %1663) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc772)
        %1665 = "ttir.reshape"(%arg214) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1666 = "ttir.reshape"(%1665) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1667 = "ttir.permute"(%1666) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc773)
        %1668 = "ttir.dot_general"(%1615, %1667) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc774)
        %1669 = "ttir.reshape"(%1668) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc775)
        %1670 = "ttir.reshape"(%arg213) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1671 = "ttir.reshape"(%1670) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1672 = "ttir.reshape"(%1671) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc776)
        %1673 = "ttir.broadcast"(%1672) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc776)
        %1674 = "ttir.add"(%1669, %1673) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc776)
        %1675 = "ttir.reshape"(%1674) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc777)
        %1676 = "ttir.permute"(%1675) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc778)
        %1677 = "ttir.typecast"(%1676) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc779)
        %1678 = "ttir.dot_general"(%1664, %1677) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc780)
        %1679 = "ttir.typecast"(%1678) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc781)
        %1680 = "ttir.permute"(%1679) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc782)
        %1681 = "ttir.reshape"(%1680) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc783)
        %1682 = "ttir.reshape"(%arg212) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1683 = "ttir.reshape"(%1682) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1684 = "ttir.permute"(%1683) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc784)
        %1685 = "ttir.dot_general"(%1681, %1684) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc785)
        %1686 = "ttir.reshape"(%1685) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc783)
        %1687 = "ttir.reshape"(%arg211) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1688 = "ttir.reshape"(%1687) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1689 = "ttir.reshape"(%1688) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc786)
        %1690 = "ttir.broadcast"(%1689) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc786)
        %1691 = "ttir.add"(%1686, %1690) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc786)
        %1692 = "ttir.add"(%1609, %1691) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc787)
        %1693 = "ttir.reshape"(%arg210) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1694 = "ttir.reshape"(%1693) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1695 = "ttir.reshape"(%arg209) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1696 = "ttir.reshape"(%1695) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1697 = "ttir.layer_norm"(%1692, %1694, %1696) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc788)
        %1698 = "ttir.reshape"(%1697) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc789)
        %1699 = "ttir.reshape"(%arg208) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %1700 = "ttir.reshape"(%1699) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %1701 = "ttir.permute"(%1700) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc790)
        %1702 = "ttir.dot_general"(%1698, %1701) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc791)
        %1703 = "ttir.reshape"(%1702) <{shape = [1 : i32, 257 : i32, 5120 : i32]}> : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc789)
        %1704 = "ttir.reshape"(%arg207) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
        %1705 = "ttir.reshape"(%1704) <{shape = [5120 : i32]}> : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
        %1706 = "ttir.reshape"(%1705) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc792)
        %1707 = "ttir.broadcast"(%1706) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc792)
        %1708 = "ttir.add"(%1703, %1707) : (tensor<1x257x5120xbf16>, tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc792)
        %1709 = "ttir.gelu"(%1708) : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc793)
        %1710 = "ttir.reshape"(%1709) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc794)
        %1711 = "ttir.reshape"(%arg206) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %1712 = "ttir.reshape"(%1711) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %1713 = "ttir.permute"(%1712) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc795)
        %1714 = "ttir.dot_general"(%1710, %1713) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc796)
        %1715 = "ttir.reshape"(%1714) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc794)
        %1716 = "ttir.reshape"(%arg205) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1717 = "ttir.reshape"(%1716) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1718 = "ttir.reshape"(%1717) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc797)
        %1719 = "ttir.broadcast"(%1718) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc797)
        %1720 = "ttir.add"(%1715, %1719) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc797)
        %1721 = "ttir.add"(%1692, %1720) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc798)
        %1722 = "ttir.reshape"(%arg204) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1723 = "ttir.reshape"(%1722) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1724 = "ttir.reshape"(%arg203) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1725 = "ttir.reshape"(%1724) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1726 = "ttir.layer_norm"(%1721, %1723, %1725) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc799)
        %1727 = "ttir.reshape"(%1726) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc800)
        %1728 = "ttir.reshape"(%arg455) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1729 = "ttir.reshape"(%1728) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1730 = "ttir.permute"(%1729) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc801)
        %1731 = "ttir.dot_general"(%1727, %1730) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc802)
        %1732 = "ttir.reshape"(%1731) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc800)
        %1733 = "ttir.reshape"(%arg454) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1734 = "ttir.reshape"(%1733) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1735 = "ttir.reshape"(%1734) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc803)
        %1736 = "ttir.broadcast"(%1735) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc803)
        %1737 = "ttir.add"(%1732, %1736) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc803)
        %1738 = "ttir.reshape"(%1737) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc804)
        %1739 = "ttir.permute"(%1738) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc805)
        %1740 = "ttir.typecast"(%1739) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc806)
        %1741 = "ttir.multiply"(%1740, %7) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc807)
        %1742 = "ttir.reshape"(%arg453) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1743 = "ttir.reshape"(%1742) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1744 = "ttir.permute"(%1743) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc808)
        %1745 = "ttir.dot_general"(%1727, %1744) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc809)
        %1746 = "ttir.reshape"(%1745) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc810)
        %1747 = "ttir.reshape"(%arg452) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1748 = "ttir.reshape"(%1747) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1749 = "ttir.reshape"(%1748) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc811)
        %1750 = "ttir.broadcast"(%1749) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc811)
        %1751 = "ttir.add"(%1746, %1750) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc811)
        %1752 = "ttir.reshape"(%1751) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc812)
        %1753 = "ttir.permute"(%1752) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc813)
        %1754 = "ttir.typecast"(%1753) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc814)
        %1755 = "ttir.permute"(%1754) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc815)
        %1756 = "ttir.multiply"(%1755, %6) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32> loc(#loc816)
        %1757 = "ttir.dot_general"(%1741, %1756) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc817)
        %1758 = "ttir.typecast"(%1757) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc818)
        %1759 = "ttir.eq"(%1758, %5) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc818)
        %1760 = "ttir.logical_not"(%1759) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc819)
        %1761 = "ttir.reduce_or"(%1760) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1> loc(#loc820)
        %1762 = "ttir.reshape"(%1761) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc820)
        %1763 = "ttir.logical_not"(%1762) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1> loc(#loc821)
        %1764 = "ttir.reshape"(%1763) <{shape = [1 : i32, 16 : i32, 257 : i32]}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc822)
        %1765 = "ttir.reshape"(%1764) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc822)
        %1766 = "ttir.broadcast"(%1765) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1> loc(#loc822)
        %1767 = "ttir.max"(%1757) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc823)
        %1768 = "ttir.reshape"(%1767) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc823)
        %1769 = "ttir.broadcast"(%1768) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc823)
        %1770 = "ttir.subtract"(%1757, %1769) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc823)
        %1771 = "ttir.exp"(%1770) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc823)
        %1772 = "ttir.sum"(%1771) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc823)
        %1773 = "ttir.reshape"(%1772) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc823)
        %1774 = "ttir.broadcast"(%1773) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc823)
        %1775 = "ttir.div"(%1771, %1774) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc823)
        %1776 = "ttir.where"(%1766, %4, %1775) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc824)
        %1777 = "ttir.reshape"(%arg202) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1778 = "ttir.reshape"(%1777) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1779 = "ttir.permute"(%1778) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc825)
        %1780 = "ttir.dot_general"(%1727, %1779) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc826)
        %1781 = "ttir.reshape"(%1780) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc827)
        %1782 = "ttir.reshape"(%arg201) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1783 = "ttir.reshape"(%1782) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1784 = "ttir.reshape"(%1783) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc828)
        %1785 = "ttir.broadcast"(%1784) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc828)
        %1786 = "ttir.add"(%1781, %1785) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc828)
        %1787 = "ttir.reshape"(%1786) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc829)
        %1788 = "ttir.permute"(%1787) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc830)
        %1789 = "ttir.typecast"(%1788) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc831)
        %1790 = "ttir.dot_general"(%1776, %1789) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc832)
        %1791 = "ttir.typecast"(%1790) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc833)
        %1792 = "ttir.permute"(%1791) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc834)
        %1793 = "ttir.reshape"(%1792) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc835)
        %1794 = "ttir.reshape"(%arg200) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1795 = "ttir.reshape"(%1794) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1796 = "ttir.permute"(%1795) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc836)
        %1797 = "ttir.dot_general"(%1793, %1796) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc837)
        %1798 = "ttir.reshape"(%1797) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc835)
        %1799 = "ttir.reshape"(%arg199) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1800 = "ttir.reshape"(%1799) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1801 = "ttir.reshape"(%1800) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc838)
        %1802 = "ttir.broadcast"(%1801) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc838)
        %1803 = "ttir.add"(%1798, %1802) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc838)
        %1804 = "ttir.add"(%1721, %1803) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc839)
        %1805 = "ttir.reshape"(%arg198) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1806 = "ttir.reshape"(%1805) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1807 = "ttir.reshape"(%arg197) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1808 = "ttir.reshape"(%1807) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1809 = "ttir.layer_norm"(%1804, %1806, %1808) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc840)
        %1810 = "ttir.reshape"(%1809) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc841)
        %1811 = "ttir.reshape"(%arg196) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %1812 = "ttir.reshape"(%1811) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %1813 = "ttir.permute"(%1812) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc842)
        %1814 = "ttir.dot_general"(%1810, %1813) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc843)
        %1815 = "ttir.reshape"(%1814) <{shape = [1 : i32, 257 : i32, 5120 : i32]}> : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc841)
        %1816 = "ttir.reshape"(%arg195) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
        %1817 = "ttir.reshape"(%1816) <{shape = [5120 : i32]}> : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
        %1818 = "ttir.reshape"(%1817) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc844)
        %1819 = "ttir.broadcast"(%1818) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc844)
        %1820 = "ttir.add"(%1815, %1819) : (tensor<1x257x5120xbf16>, tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc844)
        %1821 = "ttir.gelu"(%1820) : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc845)
        %1822 = "ttir.reshape"(%1821) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc846)
        %1823 = "ttir.reshape"(%arg194) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %1824 = "ttir.reshape"(%1823) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %1825 = "ttir.permute"(%1824) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc847)
        %1826 = "ttir.dot_general"(%1822, %1825) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc848)
        %1827 = "ttir.reshape"(%1826) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc846)
        %1828 = "ttir.reshape"(%arg193) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1829 = "ttir.reshape"(%1828) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1830 = "ttir.reshape"(%1829) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc849)
        %1831 = "ttir.broadcast"(%1830) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc849)
        %1832 = "ttir.add"(%1827, %1831) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc849)
        %1833 = "ttir.add"(%1804, %1832) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc850)
        %1834 = "ttir.reshape"(%arg192) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1835 = "ttir.reshape"(%1834) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1836 = "ttir.reshape"(%arg191) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1837 = "ttir.reshape"(%1836) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1838 = "ttir.layer_norm"(%1833, %1835, %1837) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc851)
        %1839 = "ttir.reshape"(%1838) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc852)
        %1840 = "ttir.reshape"(%arg459) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1841 = "ttir.reshape"(%1840) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1842 = "ttir.permute"(%1841) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc853)
        %1843 = "ttir.dot_general"(%1839, %1842) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc854)
        %1844 = "ttir.reshape"(%1843) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc852)
        %1845 = "ttir.reshape"(%arg458) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1846 = "ttir.reshape"(%1845) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1847 = "ttir.reshape"(%1846) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc855)
        %1848 = "ttir.broadcast"(%1847) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc855)
        %1849 = "ttir.add"(%1844, %1848) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc855)
        %1850 = "ttir.reshape"(%1849) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc856)
        %1851 = "ttir.permute"(%1850) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc857)
        %1852 = "ttir.typecast"(%1851) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc858)
        %1853 = "ttir.multiply"(%1852, %7) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc859)
        %1854 = "ttir.reshape"(%arg457) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1855 = "ttir.reshape"(%1854) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1856 = "ttir.permute"(%1855) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc860)
        %1857 = "ttir.dot_general"(%1839, %1856) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc861)
        %1858 = "ttir.reshape"(%1857) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc862)
        %1859 = "ttir.reshape"(%arg456) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1860 = "ttir.reshape"(%1859) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1861 = "ttir.reshape"(%1860) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc863)
        %1862 = "ttir.broadcast"(%1861) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc863)
        %1863 = "ttir.add"(%1858, %1862) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc863)
        %1864 = "ttir.reshape"(%1863) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc864)
        %1865 = "ttir.permute"(%1864) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc865)
        %1866 = "ttir.typecast"(%1865) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc866)
        %1867 = "ttir.permute"(%1866) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc867)
        %1868 = "ttir.multiply"(%1867, %6) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32> loc(#loc868)
        %1869 = "ttir.dot_general"(%1853, %1868) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc869)
        %1870 = "ttir.typecast"(%1869) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc870)
        %1871 = "ttir.eq"(%1870, %5) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc870)
        %1872 = "ttir.logical_not"(%1871) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc871)
        %1873 = "ttir.reduce_or"(%1872) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1> loc(#loc872)
        %1874 = "ttir.reshape"(%1873) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc872)
        %1875 = "ttir.logical_not"(%1874) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1> loc(#loc873)
        %1876 = "ttir.reshape"(%1875) <{shape = [1 : i32, 16 : i32, 257 : i32]}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc874)
        %1877 = "ttir.reshape"(%1876) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc874)
        %1878 = "ttir.broadcast"(%1877) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1> loc(#loc874)
        %1879 = "ttir.max"(%1869) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc875)
        %1880 = "ttir.reshape"(%1879) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc875)
        %1881 = "ttir.broadcast"(%1880) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc875)
        %1882 = "ttir.subtract"(%1869, %1881) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc875)
        %1883 = "ttir.exp"(%1882) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc875)
        %1884 = "ttir.sum"(%1883) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc875)
        %1885 = "ttir.reshape"(%1884) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc875)
        %1886 = "ttir.broadcast"(%1885) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc875)
        %1887 = "ttir.div"(%1883, %1886) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc875)
        %1888 = "ttir.where"(%1878, %4, %1887) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc876)
        %1889 = "ttir.reshape"(%arg190) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1890 = "ttir.reshape"(%1889) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1891 = "ttir.permute"(%1890) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc877)
        %1892 = "ttir.dot_general"(%1839, %1891) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc878)
        %1893 = "ttir.reshape"(%1892) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc879)
        %1894 = "ttir.reshape"(%arg189) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1895 = "ttir.reshape"(%1894) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1896 = "ttir.reshape"(%1895) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc880)
        %1897 = "ttir.broadcast"(%1896) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc880)
        %1898 = "ttir.add"(%1893, %1897) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc880)
        %1899 = "ttir.reshape"(%1898) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc881)
        %1900 = "ttir.permute"(%1899) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc882)
        %1901 = "ttir.typecast"(%1900) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc883)
        %1902 = "ttir.dot_general"(%1888, %1901) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc884)
        %1903 = "ttir.typecast"(%1902) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc885)
        %1904 = "ttir.permute"(%1903) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc886)
        %1905 = "ttir.reshape"(%1904) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc887)
        %1906 = "ttir.reshape"(%arg188) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1907 = "ttir.reshape"(%1906) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1908 = "ttir.permute"(%1907) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc888)
        %1909 = "ttir.dot_general"(%1905, %1908) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc889)
        %1910 = "ttir.reshape"(%1909) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc887)
        %1911 = "ttir.reshape"(%arg187) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1912 = "ttir.reshape"(%1911) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1913 = "ttir.reshape"(%1912) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc890)
        %1914 = "ttir.broadcast"(%1913) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc890)
        %1915 = "ttir.add"(%1910, %1914) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc890)
        %1916 = "ttir.add"(%1833, %1915) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc891)
        %1917 = "ttir.reshape"(%arg186) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1918 = "ttir.reshape"(%1917) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1919 = "ttir.reshape"(%arg185) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1920 = "ttir.reshape"(%1919) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1921 = "ttir.layer_norm"(%1916, %1918, %1920) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc892)
        %1922 = "ttir.reshape"(%1921) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc893)
        %1923 = "ttir.reshape"(%arg184) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %1924 = "ttir.reshape"(%1923) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %1925 = "ttir.permute"(%1924) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc894)
        %1926 = "ttir.dot_general"(%1922, %1925) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc895)
        %1927 = "ttir.reshape"(%1926) <{shape = [1 : i32, 257 : i32, 5120 : i32]}> : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc893)
        %1928 = "ttir.reshape"(%arg183) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
        %1929 = "ttir.reshape"(%1928) <{shape = [5120 : i32]}> : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
        %1930 = "ttir.reshape"(%1929) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc896)
        %1931 = "ttir.broadcast"(%1930) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc896)
        %1932 = "ttir.add"(%1927, %1931) : (tensor<1x257x5120xbf16>, tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc896)
        %1933 = "ttir.gelu"(%1932) : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc897)
        %1934 = "ttir.reshape"(%1933) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc898)
        %1935 = "ttir.reshape"(%arg182) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %1936 = "ttir.reshape"(%1935) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %1937 = "ttir.permute"(%1936) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc899)
        %1938 = "ttir.dot_general"(%1934, %1937) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc900)
        %1939 = "ttir.reshape"(%1938) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc898)
        %1940 = "ttir.reshape"(%arg181) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1941 = "ttir.reshape"(%1940) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1942 = "ttir.reshape"(%1941) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc901)
        %1943 = "ttir.broadcast"(%1942) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc901)
        %1944 = "ttir.add"(%1939, %1943) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc901)
        %1945 = "ttir.add"(%1916, %1944) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc902)
        %1946 = "ttir.reshape"(%arg180) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1947 = "ttir.reshape"(%1946) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1948 = "ttir.reshape"(%arg179) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1949 = "ttir.reshape"(%1948) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1950 = "ttir.layer_norm"(%1945, %1947, %1949) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc903)
        %1951 = "ttir.reshape"(%1950) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc904)
        %1952 = "ttir.reshape"(%arg463) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1953 = "ttir.reshape"(%1952) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1954 = "ttir.permute"(%1953) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc905)
        %1955 = "ttir.dot_general"(%1951, %1954) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc906)
        %1956 = "ttir.reshape"(%1955) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc904)
        %1957 = "ttir.reshape"(%arg462) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1958 = "ttir.reshape"(%1957) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1959 = "ttir.reshape"(%1958) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc907)
        %1960 = "ttir.broadcast"(%1959) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc907)
        %1961 = "ttir.add"(%1956, %1960) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc907)
        %1962 = "ttir.reshape"(%1961) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc908)
        %1963 = "ttir.permute"(%1962) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc909)
        %1964 = "ttir.typecast"(%1963) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc910)
        %1965 = "ttir.multiply"(%1964, %7) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc911)
        %1966 = "ttir.reshape"(%arg461) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %1967 = "ttir.reshape"(%1966) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %1968 = "ttir.permute"(%1967) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc912)
        %1969 = "ttir.dot_general"(%1951, %1968) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc913)
        %1970 = "ttir.reshape"(%1969) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc914)
        %1971 = "ttir.reshape"(%arg460) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %1972 = "ttir.reshape"(%1971) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %1973 = "ttir.reshape"(%1972) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc915)
        %1974 = "ttir.broadcast"(%1973) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc915)
        %1975 = "ttir.add"(%1970, %1974) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc915)
        %1976 = "ttir.reshape"(%1975) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc916)
        %1977 = "ttir.permute"(%1976) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc917)
        %1978 = "ttir.typecast"(%1977) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc918)
        %1979 = "ttir.permute"(%1978) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc919)
        %1980 = "ttir.multiply"(%1979, %6) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32> loc(#loc920)
        %1981 = "ttir.dot_general"(%1965, %1980) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc921)
        %1982 = "ttir.typecast"(%1981) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc922)
        %1983 = "ttir.eq"(%1982, %5) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc922)
        %1984 = "ttir.logical_not"(%1983) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc923)
        %1985 = "ttir.reduce_or"(%1984) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1> loc(#loc924)
        %1986 = "ttir.reshape"(%1985) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc924)
        %1987 = "ttir.logical_not"(%1986) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1> loc(#loc925)
        %1988 = "ttir.reshape"(%1987) <{shape = [1 : i32, 16 : i32, 257 : i32]}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc926)
        %1989 = "ttir.reshape"(%1988) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc926)
        %1990 = "ttir.broadcast"(%1989) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1> loc(#loc926)
        %1991 = "ttir.max"(%1981) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc927)
        %1992 = "ttir.reshape"(%1991) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc927)
        %1993 = "ttir.broadcast"(%1992) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc927)
        %1994 = "ttir.subtract"(%1981, %1993) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc927)
        %1995 = "ttir.exp"(%1994) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc927)
        %1996 = "ttir.sum"(%1995) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc927)
        %1997 = "ttir.reshape"(%1996) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc927)
        %1998 = "ttir.broadcast"(%1997) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc927)
        %1999 = "ttir.div"(%1995, %1998) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc927)
        %2000 = "ttir.where"(%1990, %4, %1999) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc928)
        %2001 = "ttir.reshape"(%arg178) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2002 = "ttir.reshape"(%2001) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2003 = "ttir.permute"(%2002) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc929)
        %2004 = "ttir.dot_general"(%1951, %2003) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc930)
        %2005 = "ttir.reshape"(%2004) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc931)
        %2006 = "ttir.reshape"(%arg177) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2007 = "ttir.reshape"(%2006) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2008 = "ttir.reshape"(%2007) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc932)
        %2009 = "ttir.broadcast"(%2008) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc932)
        %2010 = "ttir.add"(%2005, %2009) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc932)
        %2011 = "ttir.reshape"(%2010) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc933)
        %2012 = "ttir.permute"(%2011) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc934)
        %2013 = "ttir.typecast"(%2012) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc935)
        %2014 = "ttir.dot_general"(%2000, %2013) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc936)
        %2015 = "ttir.typecast"(%2014) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc937)
        %2016 = "ttir.permute"(%2015) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc938)
        %2017 = "ttir.reshape"(%2016) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc939)
        %2018 = "ttir.reshape"(%arg176) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2019 = "ttir.reshape"(%2018) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2020 = "ttir.permute"(%2019) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc940)
        %2021 = "ttir.dot_general"(%2017, %2020) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc941)
        %2022 = "ttir.reshape"(%2021) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc939)
        %2023 = "ttir.reshape"(%arg175) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2024 = "ttir.reshape"(%2023) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2025 = "ttir.reshape"(%2024) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc942)
        %2026 = "ttir.broadcast"(%2025) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc942)
        %2027 = "ttir.add"(%2022, %2026) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc942)
        %2028 = "ttir.add"(%1945, %2027) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc943)
        %2029 = "ttir.reshape"(%arg174) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2030 = "ttir.reshape"(%2029) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2031 = "ttir.reshape"(%arg173) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2032 = "ttir.reshape"(%2031) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2033 = "ttir.layer_norm"(%2028, %2030, %2032) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc944)
        %2034 = "ttir.reshape"(%2033) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc945)
        %2035 = "ttir.reshape"(%arg172) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %2036 = "ttir.reshape"(%2035) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %2037 = "ttir.permute"(%2036) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc946)
        %2038 = "ttir.dot_general"(%2034, %2037) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc947)
        %2039 = "ttir.reshape"(%2038) <{shape = [1 : i32, 257 : i32, 5120 : i32]}> : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc945)
        %2040 = "ttir.reshape"(%arg171) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
        %2041 = "ttir.reshape"(%2040) <{shape = [5120 : i32]}> : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
        %2042 = "ttir.reshape"(%2041) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc948)
        %2043 = "ttir.broadcast"(%2042) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc948)
        %2044 = "ttir.add"(%2039, %2043) : (tensor<1x257x5120xbf16>, tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc948)
        %2045 = "ttir.gelu"(%2044) : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc949)
        %2046 = "ttir.reshape"(%2045) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc950)
        %2047 = "ttir.reshape"(%arg170) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %2048 = "ttir.reshape"(%2047) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %2049 = "ttir.permute"(%2048) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc951)
        %2050 = "ttir.dot_general"(%2046, %2049) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc952)
        %2051 = "ttir.reshape"(%2050) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc950)
        %2052 = "ttir.reshape"(%arg169) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2053 = "ttir.reshape"(%2052) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2054 = "ttir.reshape"(%2053) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc953)
        %2055 = "ttir.broadcast"(%2054) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc953)
        %2056 = "ttir.add"(%2051, %2055) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc953)
        %2057 = "ttir.add"(%2028, %2056) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc954)
        %2058 = "ttir.reshape"(%arg168) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2059 = "ttir.reshape"(%2058) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2060 = "ttir.reshape"(%arg167) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2061 = "ttir.reshape"(%2060) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2062 = "ttir.layer_norm"(%2057, %2059, %2061) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc955)
        %2063 = "ttir.reshape"(%2062) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc956)
        %2064 = "ttir.reshape"(%arg467) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2065 = "ttir.reshape"(%2064) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2066 = "ttir.permute"(%2065) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc957)
        %2067 = "ttir.dot_general"(%2063, %2066) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc958)
        %2068 = "ttir.reshape"(%2067) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc956)
        %2069 = "ttir.reshape"(%arg466) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2070 = "ttir.reshape"(%2069) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2071 = "ttir.reshape"(%2070) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc959)
        %2072 = "ttir.broadcast"(%2071) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc959)
        %2073 = "ttir.add"(%2068, %2072) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc959)
        %2074 = "ttir.reshape"(%2073) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc960)
        %2075 = "ttir.permute"(%2074) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc961)
        %2076 = "ttir.typecast"(%2075) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc962)
        %2077 = "ttir.multiply"(%2076, %7) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc963)
        %2078 = "ttir.reshape"(%arg465) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2079 = "ttir.reshape"(%2078) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2080 = "ttir.permute"(%2079) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc964)
        %2081 = "ttir.dot_general"(%2063, %2080) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc965)
        %2082 = "ttir.reshape"(%2081) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc966)
        %2083 = "ttir.reshape"(%arg464) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2084 = "ttir.reshape"(%2083) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2085 = "ttir.reshape"(%2084) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc967)
        %2086 = "ttir.broadcast"(%2085) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc967)
        %2087 = "ttir.add"(%2082, %2086) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc967)
        %2088 = "ttir.reshape"(%2087) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc968)
        %2089 = "ttir.permute"(%2088) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc969)
        %2090 = "ttir.typecast"(%2089) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc970)
        %2091 = "ttir.permute"(%2090) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc971)
        %2092 = "ttir.multiply"(%2091, %6) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32> loc(#loc972)
        %2093 = "ttir.dot_general"(%2077, %2092) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc973)
        %2094 = "ttir.typecast"(%2093) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc974)
        %2095 = "ttir.eq"(%2094, %5) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc974)
        %2096 = "ttir.logical_not"(%2095) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc975)
        %2097 = "ttir.reduce_or"(%2096) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1> loc(#loc976)
        %2098 = "ttir.reshape"(%2097) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc976)
        %2099 = "ttir.logical_not"(%2098) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1> loc(#loc977)
        %2100 = "ttir.reshape"(%2099) <{shape = [1 : i32, 16 : i32, 257 : i32]}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc978)
        %2101 = "ttir.reshape"(%2100) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc978)
        %2102 = "ttir.broadcast"(%2101) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1> loc(#loc978)
        %2103 = "ttir.max"(%2093) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc979)
        %2104 = "ttir.reshape"(%2103) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc979)
        %2105 = "ttir.broadcast"(%2104) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc979)
        %2106 = "ttir.subtract"(%2093, %2105) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc979)
        %2107 = "ttir.exp"(%2106) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc979)
        %2108 = "ttir.sum"(%2107) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc979)
        %2109 = "ttir.reshape"(%2108) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc979)
        %2110 = "ttir.broadcast"(%2109) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc979)
        %2111 = "ttir.div"(%2107, %2110) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc979)
        %2112 = "ttir.where"(%2102, %4, %2111) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc980)
        %2113 = "ttir.reshape"(%arg166) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2114 = "ttir.reshape"(%2113) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2115 = "ttir.permute"(%2114) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc981)
        %2116 = "ttir.dot_general"(%2063, %2115) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc982)
        %2117 = "ttir.reshape"(%2116) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc983)
        %2118 = "ttir.reshape"(%arg165) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2119 = "ttir.reshape"(%2118) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2120 = "ttir.reshape"(%2119) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc984)
        %2121 = "ttir.broadcast"(%2120) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc984)
        %2122 = "ttir.add"(%2117, %2121) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc984)
        %2123 = "ttir.reshape"(%2122) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc985)
        %2124 = "ttir.permute"(%2123) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc986)
        %2125 = "ttir.typecast"(%2124) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc987)
        %2126 = "ttir.dot_general"(%2112, %2125) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc988)
        %2127 = "ttir.typecast"(%2126) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc989)
        %2128 = "ttir.permute"(%2127) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc990)
        %2129 = "ttir.reshape"(%2128) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc991)
        %2130 = "ttir.reshape"(%arg164) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2131 = "ttir.reshape"(%2130) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2132 = "ttir.permute"(%2131) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc992)
        %2133 = "ttir.dot_general"(%2129, %2132) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc993)
        %2134 = "ttir.reshape"(%2133) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc991)
        %2135 = "ttir.reshape"(%arg163) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2136 = "ttir.reshape"(%2135) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2137 = "ttir.reshape"(%2136) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc994)
        %2138 = "ttir.broadcast"(%2137) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc994)
        %2139 = "ttir.add"(%2134, %2138) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc994)
        %2140 = "ttir.add"(%2057, %2139) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc995)
        %2141 = "ttir.reshape"(%arg162) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2142 = "ttir.reshape"(%2141) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2143 = "ttir.reshape"(%arg161) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2144 = "ttir.reshape"(%2143) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2145 = "ttir.layer_norm"(%2140, %2142, %2144) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc996)
        %2146 = "ttir.reshape"(%2145) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc997)
        %2147 = "ttir.reshape"(%arg160) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %2148 = "ttir.reshape"(%2147) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %2149 = "ttir.permute"(%2148) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc998)
        %2150 = "ttir.dot_general"(%2146, %2149) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc999)
        %2151 = "ttir.reshape"(%2150) <{shape = [1 : i32, 257 : i32, 5120 : i32]}> : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc997)
        %2152 = "ttir.reshape"(%arg159) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
        %2153 = "ttir.reshape"(%2152) <{shape = [5120 : i32]}> : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
        %2154 = "ttir.reshape"(%2153) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1000)
        %2155 = "ttir.broadcast"(%2154) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1000)
        %2156 = "ttir.add"(%2151, %2155) : (tensor<1x257x5120xbf16>, tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1000)
        %2157 = "ttir.gelu"(%2156) : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1001)
        %2158 = "ttir.reshape"(%2157) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1002)
        %2159 = "ttir.reshape"(%arg158) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %2160 = "ttir.reshape"(%2159) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %2161 = "ttir.permute"(%2160) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1003)
        %2162 = "ttir.dot_general"(%2158, %2161) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1004)
        %2163 = "ttir.reshape"(%2162) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1002)
        %2164 = "ttir.reshape"(%arg157) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2165 = "ttir.reshape"(%2164) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2166 = "ttir.reshape"(%2165) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1005)
        %2167 = "ttir.broadcast"(%2166) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1005)
        %2168 = "ttir.add"(%2163, %2167) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1005)
        %2169 = "ttir.add"(%2140, %2168) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1006)
        %2170 = "ttir.reshape"(%arg156) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2171 = "ttir.reshape"(%2170) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2172 = "ttir.reshape"(%arg155) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2173 = "ttir.reshape"(%2172) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2174 = "ttir.layer_norm"(%2169, %2171, %2173) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1007)
        %2175 = "ttir.reshape"(%2174) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1008)
        %2176 = "ttir.reshape"(%arg471) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2177 = "ttir.reshape"(%2176) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2178 = "ttir.permute"(%2177) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1009)
        %2179 = "ttir.dot_general"(%2175, %2178) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1010)
        %2180 = "ttir.reshape"(%2179) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1008)
        %2181 = "ttir.reshape"(%arg470) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2182 = "ttir.reshape"(%2181) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2183 = "ttir.reshape"(%2182) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1011)
        %2184 = "ttir.broadcast"(%2183) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1011)
        %2185 = "ttir.add"(%2180, %2184) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1011)
        %2186 = "ttir.reshape"(%2185) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1012)
        %2187 = "ttir.permute"(%2186) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1013)
        %2188 = "ttir.typecast"(%2187) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1014)
        %2189 = "ttir.multiply"(%2188, %7) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1015)
        %2190 = "ttir.reshape"(%arg469) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2191 = "ttir.reshape"(%2190) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2192 = "ttir.permute"(%2191) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1016)
        %2193 = "ttir.dot_general"(%2175, %2192) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1017)
        %2194 = "ttir.reshape"(%2193) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1018)
        %2195 = "ttir.reshape"(%arg468) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2196 = "ttir.reshape"(%2195) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2197 = "ttir.reshape"(%2196) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1019)
        %2198 = "ttir.broadcast"(%2197) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1019)
        %2199 = "ttir.add"(%2194, %2198) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1019)
        %2200 = "ttir.reshape"(%2199) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1020)
        %2201 = "ttir.permute"(%2200) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1021)
        %2202 = "ttir.typecast"(%2201) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1022)
        %2203 = "ttir.permute"(%2202) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1023)
        %2204 = "ttir.multiply"(%2203, %6) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32> loc(#loc1024)
        %2205 = "ttir.dot_general"(%2189, %2204) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1025)
        %2206 = "ttir.typecast"(%2205) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1026)
        %2207 = "ttir.eq"(%2206, %5) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1026)
        %2208 = "ttir.logical_not"(%2207) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1027)
        %2209 = "ttir.reduce_or"(%2208) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1> loc(#loc1028)
        %2210 = "ttir.reshape"(%2209) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1028)
        %2211 = "ttir.logical_not"(%2210) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1> loc(#loc1029)
        %2212 = "ttir.reshape"(%2211) <{shape = [1 : i32, 16 : i32, 257 : i32]}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1030)
        %2213 = "ttir.reshape"(%2212) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1030)
        %2214 = "ttir.broadcast"(%2213) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1> loc(#loc1030)
        %2215 = "ttir.max"(%2205) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc1031)
        %2216 = "ttir.reshape"(%2215) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc1031)
        %2217 = "ttir.broadcast"(%2216) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc1031)
        %2218 = "ttir.subtract"(%2205, %2217) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1031)
        %2219 = "ttir.exp"(%2218) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1031)
        %2220 = "ttir.sum"(%2219) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc1031)
        %2221 = "ttir.reshape"(%2220) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc1031)
        %2222 = "ttir.broadcast"(%2221) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc1031)
        %2223 = "ttir.div"(%2219, %2222) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1031)
        %2224 = "ttir.where"(%2214, %4, %2223) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1032)
        %2225 = "ttir.reshape"(%arg154) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2226 = "ttir.reshape"(%2225) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2227 = "ttir.permute"(%2226) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1033)
        %2228 = "ttir.dot_general"(%2175, %2227) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1034)
        %2229 = "ttir.reshape"(%2228) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1035)
        %2230 = "ttir.reshape"(%arg153) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2231 = "ttir.reshape"(%2230) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2232 = "ttir.reshape"(%2231) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1036)
        %2233 = "ttir.broadcast"(%2232) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1036)
        %2234 = "ttir.add"(%2229, %2233) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1036)
        %2235 = "ttir.reshape"(%2234) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1037)
        %2236 = "ttir.permute"(%2235) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1038)
        %2237 = "ttir.typecast"(%2236) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1039)
        %2238 = "ttir.dot_general"(%2224, %2237) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1040)
        %2239 = "ttir.typecast"(%2238) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1041)
        %2240 = "ttir.permute"(%2239) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1042)
        %2241 = "ttir.reshape"(%2240) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1043)
        %2242 = "ttir.reshape"(%arg152) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2243 = "ttir.reshape"(%2242) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2244 = "ttir.permute"(%2243) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1044)
        %2245 = "ttir.dot_general"(%2241, %2244) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1045)
        %2246 = "ttir.reshape"(%2245) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1043)
        %2247 = "ttir.reshape"(%arg151) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2248 = "ttir.reshape"(%2247) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2249 = "ttir.reshape"(%2248) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1046)
        %2250 = "ttir.broadcast"(%2249) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1046)
        %2251 = "ttir.add"(%2246, %2250) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1046)
        %2252 = "ttir.add"(%2169, %2251) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1047)
        %2253 = "ttir.reshape"(%arg150) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2254 = "ttir.reshape"(%2253) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2255 = "ttir.reshape"(%arg149) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2256 = "ttir.reshape"(%2255) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2257 = "ttir.layer_norm"(%2252, %2254, %2256) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1048)
        %2258 = "ttir.reshape"(%2257) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1049)
        %2259 = "ttir.reshape"(%arg148) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %2260 = "ttir.reshape"(%2259) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %2261 = "ttir.permute"(%2260) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1050)
        %2262 = "ttir.dot_general"(%2258, %2261) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1051)
        %2263 = "ttir.reshape"(%2262) <{shape = [1 : i32, 257 : i32, 5120 : i32]}> : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1049)
        %2264 = "ttir.reshape"(%arg147) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
        %2265 = "ttir.reshape"(%2264) <{shape = [5120 : i32]}> : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
        %2266 = "ttir.reshape"(%2265) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1052)
        %2267 = "ttir.broadcast"(%2266) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1052)
        %2268 = "ttir.add"(%2263, %2267) : (tensor<1x257x5120xbf16>, tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1052)
        %2269 = "ttir.gelu"(%2268) : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1053)
        %2270 = "ttir.reshape"(%2269) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1054)
        %2271 = "ttir.reshape"(%arg146) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %2272 = "ttir.reshape"(%2271) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %2273 = "ttir.permute"(%2272) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1055)
        %2274 = "ttir.dot_general"(%2270, %2273) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1056)
        %2275 = "ttir.reshape"(%2274) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1054)
        %2276 = "ttir.reshape"(%arg145) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2277 = "ttir.reshape"(%2276) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2278 = "ttir.reshape"(%2277) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1057)
        %2279 = "ttir.broadcast"(%2278) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1057)
        %2280 = "ttir.add"(%2275, %2279) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1057)
        %2281 = "ttir.add"(%2252, %2280) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1058)
        %2282 = "ttir.reshape"(%arg144) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2283 = "ttir.reshape"(%2282) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2284 = "ttir.reshape"(%arg143) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2285 = "ttir.reshape"(%2284) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2286 = "ttir.layer_norm"(%2281, %2283, %2285) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1059)
        %2287 = "ttir.reshape"(%2286) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1060)
        %2288 = "ttir.reshape"(%arg475) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2289 = "ttir.reshape"(%2288) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2290 = "ttir.permute"(%2289) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1061)
        %2291 = "ttir.dot_general"(%2287, %2290) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1062)
        %2292 = "ttir.reshape"(%2291) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1060)
        %2293 = "ttir.reshape"(%arg474) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2294 = "ttir.reshape"(%2293) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2295 = "ttir.reshape"(%2294) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1063)
        %2296 = "ttir.broadcast"(%2295) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1063)
        %2297 = "ttir.add"(%2292, %2296) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1063)
        %2298 = "ttir.reshape"(%2297) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1064)
        %2299 = "ttir.permute"(%2298) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1065)
        %2300 = "ttir.typecast"(%2299) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1066)
        %2301 = "ttir.multiply"(%2300, %7) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1067)
        %2302 = "ttir.reshape"(%arg473) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2303 = "ttir.reshape"(%2302) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2304 = "ttir.permute"(%2303) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1068)
        %2305 = "ttir.dot_general"(%2287, %2304) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1069)
        %2306 = "ttir.reshape"(%2305) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1070)
        %2307 = "ttir.reshape"(%arg472) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2308 = "ttir.reshape"(%2307) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2309 = "ttir.reshape"(%2308) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1071)
        %2310 = "ttir.broadcast"(%2309) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1071)
        %2311 = "ttir.add"(%2306, %2310) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1071)
        %2312 = "ttir.reshape"(%2311) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1072)
        %2313 = "ttir.permute"(%2312) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1073)
        %2314 = "ttir.typecast"(%2313) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1074)
        %2315 = "ttir.permute"(%2314) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1075)
        %2316 = "ttir.multiply"(%2315, %6) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32> loc(#loc1076)
        %2317 = "ttir.dot_general"(%2301, %2316) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1077)
        %2318 = "ttir.typecast"(%2317) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1078)
        %2319 = "ttir.eq"(%2318, %5) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1078)
        %2320 = "ttir.logical_not"(%2319) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1079)
        %2321 = "ttir.reduce_or"(%2320) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1> loc(#loc1080)
        %2322 = "ttir.reshape"(%2321) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1080)
        %2323 = "ttir.logical_not"(%2322) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1> loc(#loc1081)
        %2324 = "ttir.reshape"(%2323) <{shape = [1 : i32, 16 : i32, 257 : i32]}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1082)
        %2325 = "ttir.reshape"(%2324) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1082)
        %2326 = "ttir.broadcast"(%2325) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1> loc(#loc1082)
        %2327 = "ttir.max"(%2317) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc1083)
        %2328 = "ttir.reshape"(%2327) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc1083)
        %2329 = "ttir.broadcast"(%2328) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc1083)
        %2330 = "ttir.subtract"(%2317, %2329) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1083)
        %2331 = "ttir.exp"(%2330) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1083)
        %2332 = "ttir.sum"(%2331) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc1083)
        %2333 = "ttir.reshape"(%2332) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc1083)
        %2334 = "ttir.broadcast"(%2333) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc1083)
        %2335 = "ttir.div"(%2331, %2334) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1083)
        %2336 = "ttir.where"(%2326, %4, %2335) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1084)
        %2337 = "ttir.reshape"(%arg142) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2338 = "ttir.reshape"(%2337) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2339 = "ttir.permute"(%2338) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1085)
        %2340 = "ttir.dot_general"(%2287, %2339) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1086)
        %2341 = "ttir.reshape"(%2340) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1087)
        %2342 = "ttir.reshape"(%arg141) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2343 = "ttir.reshape"(%2342) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2344 = "ttir.reshape"(%2343) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1088)
        %2345 = "ttir.broadcast"(%2344) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1088)
        %2346 = "ttir.add"(%2341, %2345) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1088)
        %2347 = "ttir.reshape"(%2346) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1089)
        %2348 = "ttir.permute"(%2347) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1090)
        %2349 = "ttir.typecast"(%2348) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1091)
        %2350 = "ttir.dot_general"(%2336, %2349) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1092)
        %2351 = "ttir.typecast"(%2350) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1093)
        %2352 = "ttir.permute"(%2351) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1094)
        %2353 = "ttir.reshape"(%2352) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1095)
        %2354 = "ttir.reshape"(%arg140) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2355 = "ttir.reshape"(%2354) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2356 = "ttir.permute"(%2355) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1096)
        %2357 = "ttir.dot_general"(%2353, %2356) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1097)
        %2358 = "ttir.reshape"(%2357) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1095)
        %2359 = "ttir.reshape"(%arg139) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2360 = "ttir.reshape"(%2359) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2361 = "ttir.reshape"(%2360) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1098)
        %2362 = "ttir.broadcast"(%2361) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1098)
        %2363 = "ttir.add"(%2358, %2362) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1098)
        %2364 = "ttir.add"(%2281, %2363) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1099)
        %2365 = "ttir.reshape"(%arg138) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2366 = "ttir.reshape"(%2365) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2367 = "ttir.reshape"(%arg137) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2368 = "ttir.reshape"(%2367) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2369 = "ttir.layer_norm"(%2364, %2366, %2368) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1100)
        %2370 = "ttir.reshape"(%2369) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1101)
        %2371 = "ttir.reshape"(%arg136) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %2372 = "ttir.reshape"(%2371) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %2373 = "ttir.permute"(%2372) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1102)
        %2374 = "ttir.dot_general"(%2370, %2373) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1103)
        %2375 = "ttir.reshape"(%2374) <{shape = [1 : i32, 257 : i32, 5120 : i32]}> : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1101)
        %2376 = "ttir.reshape"(%arg135) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
        %2377 = "ttir.reshape"(%2376) <{shape = [5120 : i32]}> : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
        %2378 = "ttir.reshape"(%2377) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1104)
        %2379 = "ttir.broadcast"(%2378) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1104)
        %2380 = "ttir.add"(%2375, %2379) : (tensor<1x257x5120xbf16>, tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1104)
        %2381 = "ttir.gelu"(%2380) : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1105)
        %2382 = "ttir.reshape"(%2381) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1106)
        %2383 = "ttir.reshape"(%arg134) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %2384 = "ttir.reshape"(%2383) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %2385 = "ttir.permute"(%2384) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1107)
        %2386 = "ttir.dot_general"(%2382, %2385) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1108)
        %2387 = "ttir.reshape"(%2386) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1106)
        %2388 = "ttir.reshape"(%arg133) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2389 = "ttir.reshape"(%2388) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2390 = "ttir.reshape"(%2389) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1109)
        %2391 = "ttir.broadcast"(%2390) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1109)
        %2392 = "ttir.add"(%2387, %2391) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1109)
        %2393 = "ttir.add"(%2364, %2392) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1110)
        %2394 = "ttir.reshape"(%arg132) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2395 = "ttir.reshape"(%2394) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2396 = "ttir.reshape"(%arg131) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2397 = "ttir.reshape"(%2396) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2398 = "ttir.layer_norm"(%2393, %2395, %2397) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1111)
        %2399 = "ttir.reshape"(%2398) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1112)
        %2400 = "ttir.reshape"(%arg479) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2401 = "ttir.reshape"(%2400) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2402 = "ttir.permute"(%2401) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1113)
        %2403 = "ttir.dot_general"(%2399, %2402) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1114)
        %2404 = "ttir.reshape"(%2403) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1112)
        %2405 = "ttir.reshape"(%arg478) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2406 = "ttir.reshape"(%2405) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2407 = "ttir.reshape"(%2406) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1115)
        %2408 = "ttir.broadcast"(%2407) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1115)
        %2409 = "ttir.add"(%2404, %2408) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1115)
        %2410 = "ttir.reshape"(%2409) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1116)
        %2411 = "ttir.permute"(%2410) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1117)
        %2412 = "ttir.typecast"(%2411) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1118)
        %2413 = "ttir.multiply"(%2412, %7) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1119)
        %2414 = "ttir.reshape"(%arg477) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2415 = "ttir.reshape"(%2414) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2416 = "ttir.permute"(%2415) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1120)
        %2417 = "ttir.dot_general"(%2399, %2416) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1121)
        %2418 = "ttir.reshape"(%2417) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1122)
        %2419 = "ttir.reshape"(%arg476) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2420 = "ttir.reshape"(%2419) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2421 = "ttir.reshape"(%2420) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1123)
        %2422 = "ttir.broadcast"(%2421) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1123)
        %2423 = "ttir.add"(%2418, %2422) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1123)
        %2424 = "ttir.reshape"(%2423) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1124)
        %2425 = "ttir.permute"(%2424) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1125)
        %2426 = "ttir.typecast"(%2425) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1126)
        %2427 = "ttir.permute"(%2426) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1127)
        %2428 = "ttir.multiply"(%2427, %6) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32> loc(#loc1128)
        %2429 = "ttir.dot_general"(%2413, %2428) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1129)
        %2430 = "ttir.typecast"(%2429) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1130)
        %2431 = "ttir.eq"(%2430, %5) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1130)
        %2432 = "ttir.logical_not"(%2431) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1131)
        %2433 = "ttir.reduce_or"(%2432) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1> loc(#loc1132)
        %2434 = "ttir.reshape"(%2433) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1132)
        %2435 = "ttir.logical_not"(%2434) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1> loc(#loc1133)
        %2436 = "ttir.reshape"(%2435) <{shape = [1 : i32, 16 : i32, 257 : i32]}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1134)
        %2437 = "ttir.reshape"(%2436) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1134)
        %2438 = "ttir.broadcast"(%2437) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1> loc(#loc1134)
        %2439 = "ttir.max"(%2429) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc1135)
        %2440 = "ttir.reshape"(%2439) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc1135)
        %2441 = "ttir.broadcast"(%2440) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc1135)
        %2442 = "ttir.subtract"(%2429, %2441) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1135)
        %2443 = "ttir.exp"(%2442) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1135)
        %2444 = "ttir.sum"(%2443) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc1135)
        %2445 = "ttir.reshape"(%2444) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc1135)
        %2446 = "ttir.broadcast"(%2445) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc1135)
        %2447 = "ttir.div"(%2443, %2446) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1135)
        %2448 = "ttir.where"(%2438, %4, %2447) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1136)
        %2449 = "ttir.reshape"(%arg130) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2450 = "ttir.reshape"(%2449) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2451 = "ttir.permute"(%2450) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1137)
        %2452 = "ttir.dot_general"(%2399, %2451) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1138)
        %2453 = "ttir.reshape"(%2452) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1139)
        %2454 = "ttir.reshape"(%arg129) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2455 = "ttir.reshape"(%2454) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2456 = "ttir.reshape"(%2455) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1140)
        %2457 = "ttir.broadcast"(%2456) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1140)
        %2458 = "ttir.add"(%2453, %2457) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1140)
        %2459 = "ttir.reshape"(%2458) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1141)
        %2460 = "ttir.permute"(%2459) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1142)
        %2461 = "ttir.typecast"(%2460) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1143)
        %2462 = "ttir.dot_general"(%2448, %2461) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1144)
        %2463 = "ttir.typecast"(%2462) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1145)
        %2464 = "ttir.permute"(%2463) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1146)
        %2465 = "ttir.reshape"(%2464) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1147)
        %2466 = "ttir.reshape"(%arg128) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2467 = "ttir.reshape"(%2466) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2468 = "ttir.permute"(%2467) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1148)
        %2469 = "ttir.dot_general"(%2465, %2468) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1149)
        %2470 = "ttir.reshape"(%2469) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1147)
        %2471 = "ttir.reshape"(%arg127) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2472 = "ttir.reshape"(%2471) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2473 = "ttir.reshape"(%2472) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1150)
        %2474 = "ttir.broadcast"(%2473) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1150)
        %2475 = "ttir.add"(%2470, %2474) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1150)
        %2476 = "ttir.add"(%2393, %2475) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1151)
        %2477 = "ttir.reshape"(%arg126) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2478 = "ttir.reshape"(%2477) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2479 = "ttir.reshape"(%arg125) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2480 = "ttir.reshape"(%2479) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2481 = "ttir.layer_norm"(%2476, %2478, %2480) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1152)
        %2482 = "ttir.reshape"(%2481) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1153)
        %2483 = "ttir.reshape"(%arg124) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %2484 = "ttir.reshape"(%2483) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %2485 = "ttir.permute"(%2484) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1154)
        %2486 = "ttir.dot_general"(%2482, %2485) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1155)
        %2487 = "ttir.reshape"(%2486) <{shape = [1 : i32, 257 : i32, 5120 : i32]}> : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1153)
        %2488 = "ttir.reshape"(%arg123) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
        %2489 = "ttir.reshape"(%2488) <{shape = [5120 : i32]}> : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
        %2490 = "ttir.reshape"(%2489) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1156)
        %2491 = "ttir.broadcast"(%2490) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1156)
        %2492 = "ttir.add"(%2487, %2491) : (tensor<1x257x5120xbf16>, tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1156)
        %2493 = "ttir.gelu"(%2492) : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1157)
        %2494 = "ttir.reshape"(%2493) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1158)
        %2495 = "ttir.reshape"(%arg122) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %2496 = "ttir.reshape"(%2495) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %2497 = "ttir.permute"(%2496) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1159)
        %2498 = "ttir.dot_general"(%2494, %2497) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1160)
        %2499 = "ttir.reshape"(%2498) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1158)
        %2500 = "ttir.reshape"(%arg121) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2501 = "ttir.reshape"(%2500) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2502 = "ttir.reshape"(%2501) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1161)
        %2503 = "ttir.broadcast"(%2502) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1161)
        %2504 = "ttir.add"(%2499, %2503) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1161)
        %2505 = "ttir.add"(%2476, %2504) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1162)
        %2506 = "ttir.reshape"(%arg120) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2507 = "ttir.reshape"(%2506) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2508 = "ttir.reshape"(%arg119) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2509 = "ttir.reshape"(%2508) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2510 = "ttir.layer_norm"(%2505, %2507, %2509) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1163)
        %2511 = "ttir.reshape"(%2510) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1164)
        %2512 = "ttir.reshape"(%arg483) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2513 = "ttir.reshape"(%2512) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2514 = "ttir.permute"(%2513) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1165)
        %2515 = "ttir.dot_general"(%2511, %2514) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1166)
        %2516 = "ttir.reshape"(%2515) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1164)
        %2517 = "ttir.reshape"(%arg482) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2518 = "ttir.reshape"(%2517) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2519 = "ttir.reshape"(%2518) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1167)
        %2520 = "ttir.broadcast"(%2519) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1167)
        %2521 = "ttir.add"(%2516, %2520) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1167)
        %2522 = "ttir.reshape"(%2521) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1168)
        %2523 = "ttir.permute"(%2522) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1169)
        %2524 = "ttir.typecast"(%2523) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1170)
        %2525 = "ttir.multiply"(%2524, %7) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1171)
        %2526 = "ttir.reshape"(%arg481) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2527 = "ttir.reshape"(%2526) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2528 = "ttir.permute"(%2527) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1172)
        %2529 = "ttir.dot_general"(%2511, %2528) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1173)
        %2530 = "ttir.reshape"(%2529) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1174)
        %2531 = "ttir.reshape"(%arg480) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2532 = "ttir.reshape"(%2531) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2533 = "ttir.reshape"(%2532) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1175)
        %2534 = "ttir.broadcast"(%2533) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1175)
        %2535 = "ttir.add"(%2530, %2534) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1175)
        %2536 = "ttir.reshape"(%2535) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1176)
        %2537 = "ttir.permute"(%2536) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1177)
        %2538 = "ttir.typecast"(%2537) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1178)
        %2539 = "ttir.permute"(%2538) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1179)
        %2540 = "ttir.multiply"(%2539, %6) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32> loc(#loc1180)
        %2541 = "ttir.dot_general"(%2525, %2540) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1181)
        %2542 = "ttir.typecast"(%2541) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1182)
        %2543 = "ttir.eq"(%2542, %5) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1182)
        %2544 = "ttir.logical_not"(%2543) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1183)
        %2545 = "ttir.reduce_or"(%2544) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1> loc(#loc1184)
        %2546 = "ttir.reshape"(%2545) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1184)
        %2547 = "ttir.logical_not"(%2546) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1> loc(#loc1185)
        %2548 = "ttir.reshape"(%2547) <{shape = [1 : i32, 16 : i32, 257 : i32]}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1186)
        %2549 = "ttir.reshape"(%2548) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1186)
        %2550 = "ttir.broadcast"(%2549) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1> loc(#loc1186)
        %2551 = "ttir.max"(%2541) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc1187)
        %2552 = "ttir.reshape"(%2551) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc1187)
        %2553 = "ttir.broadcast"(%2552) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc1187)
        %2554 = "ttir.subtract"(%2541, %2553) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1187)
        %2555 = "ttir.exp"(%2554) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1187)
        %2556 = "ttir.sum"(%2555) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc1187)
        %2557 = "ttir.reshape"(%2556) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc1187)
        %2558 = "ttir.broadcast"(%2557) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc1187)
        %2559 = "ttir.div"(%2555, %2558) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1187)
        %2560 = "ttir.where"(%2550, %4, %2559) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1188)
        %2561 = "ttir.reshape"(%arg118) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2562 = "ttir.reshape"(%2561) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2563 = "ttir.permute"(%2562) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1189)
        %2564 = "ttir.dot_general"(%2511, %2563) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1190)
        %2565 = "ttir.reshape"(%2564) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1191)
        %2566 = "ttir.reshape"(%arg117) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2567 = "ttir.reshape"(%2566) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2568 = "ttir.reshape"(%2567) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1192)
        %2569 = "ttir.broadcast"(%2568) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1192)
        %2570 = "ttir.add"(%2565, %2569) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1192)
        %2571 = "ttir.reshape"(%2570) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1193)
        %2572 = "ttir.permute"(%2571) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1194)
        %2573 = "ttir.typecast"(%2572) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1195)
        %2574 = "ttir.dot_general"(%2560, %2573) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1196)
        %2575 = "ttir.typecast"(%2574) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1197)
        %2576 = "ttir.permute"(%2575) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1198)
        %2577 = "ttir.reshape"(%2576) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1199)
        %2578 = "ttir.reshape"(%arg116) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2579 = "ttir.reshape"(%2578) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2580 = "ttir.permute"(%2579) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1200)
        %2581 = "ttir.dot_general"(%2577, %2580) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1201)
        %2582 = "ttir.reshape"(%2581) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1199)
        %2583 = "ttir.reshape"(%arg115) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2584 = "ttir.reshape"(%2583) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2585 = "ttir.reshape"(%2584) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1202)
        %2586 = "ttir.broadcast"(%2585) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1202)
        %2587 = "ttir.add"(%2582, %2586) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1202)
        %2588 = "ttir.add"(%2505, %2587) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1203)
        %2589 = "ttir.reshape"(%arg114) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2590 = "ttir.reshape"(%2589) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2591 = "ttir.reshape"(%arg113) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2592 = "ttir.reshape"(%2591) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2593 = "ttir.layer_norm"(%2588, %2590, %2592) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1204)
        %2594 = "ttir.reshape"(%2593) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1205)
        %2595 = "ttir.reshape"(%arg112) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %2596 = "ttir.reshape"(%2595) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %2597 = "ttir.permute"(%2596) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1206)
        %2598 = "ttir.dot_general"(%2594, %2597) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1207)
        %2599 = "ttir.reshape"(%2598) <{shape = [1 : i32, 257 : i32, 5120 : i32]}> : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1205)
        %2600 = "ttir.reshape"(%arg111) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
        %2601 = "ttir.reshape"(%2600) <{shape = [5120 : i32]}> : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
        %2602 = "ttir.reshape"(%2601) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1208)
        %2603 = "ttir.broadcast"(%2602) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1208)
        %2604 = "ttir.add"(%2599, %2603) : (tensor<1x257x5120xbf16>, tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1208)
        %2605 = "ttir.gelu"(%2604) : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1209)
        %2606 = "ttir.reshape"(%2605) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1210)
        %2607 = "ttir.reshape"(%arg110) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %2608 = "ttir.reshape"(%2607) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %2609 = "ttir.permute"(%2608) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1211)
        %2610 = "ttir.dot_general"(%2606, %2609) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1212)
        %2611 = "ttir.reshape"(%2610) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1210)
        %2612 = "ttir.reshape"(%arg109) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2613 = "ttir.reshape"(%2612) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2614 = "ttir.reshape"(%2613) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1213)
        %2615 = "ttir.broadcast"(%2614) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1213)
        %2616 = "ttir.add"(%2611, %2615) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1213)
        %2617 = "ttir.add"(%2588, %2616) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1214)
        %2618 = "ttir.reshape"(%arg108) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2619 = "ttir.reshape"(%2618) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2620 = "ttir.reshape"(%arg107) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2621 = "ttir.reshape"(%2620) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2622 = "ttir.layer_norm"(%2617, %2619, %2621) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1215)
        %2623 = "ttir.reshape"(%2622) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1216)
        %2624 = "ttir.reshape"(%arg487) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2625 = "ttir.reshape"(%2624) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2626 = "ttir.permute"(%2625) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1217)
        %2627 = "ttir.dot_general"(%2623, %2626) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1218)
        %2628 = "ttir.reshape"(%2627) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1216)
        %2629 = "ttir.reshape"(%arg486) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2630 = "ttir.reshape"(%2629) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2631 = "ttir.reshape"(%2630) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1219)
        %2632 = "ttir.broadcast"(%2631) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1219)
        %2633 = "ttir.add"(%2628, %2632) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1219)
        %2634 = "ttir.reshape"(%2633) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1220)
        %2635 = "ttir.permute"(%2634) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1221)
        %2636 = "ttir.typecast"(%2635) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1222)
        %2637 = "ttir.multiply"(%2636, %7) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1223)
        %2638 = "ttir.reshape"(%arg485) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2639 = "ttir.reshape"(%2638) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2640 = "ttir.permute"(%2639) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1224)
        %2641 = "ttir.dot_general"(%2623, %2640) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1225)
        %2642 = "ttir.reshape"(%2641) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1226)
        %2643 = "ttir.reshape"(%arg484) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2644 = "ttir.reshape"(%2643) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2645 = "ttir.reshape"(%2644) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1227)
        %2646 = "ttir.broadcast"(%2645) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1227)
        %2647 = "ttir.add"(%2642, %2646) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1227)
        %2648 = "ttir.reshape"(%2647) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1228)
        %2649 = "ttir.permute"(%2648) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1229)
        %2650 = "ttir.typecast"(%2649) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1230)
        %2651 = "ttir.permute"(%2650) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1231)
        %2652 = "ttir.multiply"(%2651, %6) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32> loc(#loc1232)
        %2653 = "ttir.dot_general"(%2637, %2652) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1233)
        %2654 = "ttir.typecast"(%2653) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1234)
        %2655 = "ttir.eq"(%2654, %5) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1234)
        %2656 = "ttir.logical_not"(%2655) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1235)
        %2657 = "ttir.reduce_or"(%2656) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1> loc(#loc1236)
        %2658 = "ttir.reshape"(%2657) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1236)
        %2659 = "ttir.logical_not"(%2658) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1> loc(#loc1237)
        %2660 = "ttir.reshape"(%2659) <{shape = [1 : i32, 16 : i32, 257 : i32]}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1238)
        %2661 = "ttir.reshape"(%2660) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1238)
        %2662 = "ttir.broadcast"(%2661) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1> loc(#loc1238)
        %2663 = "ttir.max"(%2653) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc1239)
        %2664 = "ttir.reshape"(%2663) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc1239)
        %2665 = "ttir.broadcast"(%2664) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc1239)
        %2666 = "ttir.subtract"(%2653, %2665) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1239)
        %2667 = "ttir.exp"(%2666) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1239)
        %2668 = "ttir.sum"(%2667) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc1239)
        %2669 = "ttir.reshape"(%2668) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc1239)
        %2670 = "ttir.broadcast"(%2669) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc1239)
        %2671 = "ttir.div"(%2667, %2670) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1239)
        %2672 = "ttir.where"(%2662, %4, %2671) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1240)
        %2673 = "ttir.reshape"(%arg106) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2674 = "ttir.reshape"(%2673) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2675 = "ttir.permute"(%2674) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1241)
        %2676 = "ttir.dot_general"(%2623, %2675) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1242)
        %2677 = "ttir.reshape"(%2676) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1243)
        %2678 = "ttir.reshape"(%arg105) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2679 = "ttir.reshape"(%2678) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2680 = "ttir.reshape"(%2679) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1244)
        %2681 = "ttir.broadcast"(%2680) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1244)
        %2682 = "ttir.add"(%2677, %2681) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1244)
        %2683 = "ttir.reshape"(%2682) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1245)
        %2684 = "ttir.permute"(%2683) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1246)
        %2685 = "ttir.typecast"(%2684) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1247)
        %2686 = "ttir.dot_general"(%2672, %2685) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1248)
        %2687 = "ttir.typecast"(%2686) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1249)
        %2688 = "ttir.permute"(%2687) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1250)
        %2689 = "ttir.reshape"(%2688) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1251)
        %2690 = "ttir.reshape"(%arg104) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2691 = "ttir.reshape"(%2690) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2692 = "ttir.permute"(%2691) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1252)
        %2693 = "ttir.dot_general"(%2689, %2692) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1253)
        %2694 = "ttir.reshape"(%2693) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1251)
        %2695 = "ttir.reshape"(%arg103) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2696 = "ttir.reshape"(%2695) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2697 = "ttir.reshape"(%2696) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1254)
        %2698 = "ttir.broadcast"(%2697) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1254)
        %2699 = "ttir.add"(%2694, %2698) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1254)
        %2700 = "ttir.add"(%2617, %2699) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1255)
        %2701 = "ttir.reshape"(%arg102) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2702 = "ttir.reshape"(%2701) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2703 = "ttir.reshape"(%arg101) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2704 = "ttir.reshape"(%2703) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2705 = "ttir.layer_norm"(%2700, %2702, %2704) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1256)
        %2706 = "ttir.reshape"(%2705) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1257)
        %2707 = "ttir.reshape"(%arg100) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %2708 = "ttir.reshape"(%2707) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %2709 = "ttir.permute"(%2708) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1258)
        %2710 = "ttir.dot_general"(%2706, %2709) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1259)
        %2711 = "ttir.reshape"(%2710) <{shape = [1 : i32, 257 : i32, 5120 : i32]}> : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1257)
        %2712 = "ttir.reshape"(%arg99) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
        %2713 = "ttir.reshape"(%2712) <{shape = [5120 : i32]}> : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
        %2714 = "ttir.reshape"(%2713) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1260)
        %2715 = "ttir.broadcast"(%2714) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1260)
        %2716 = "ttir.add"(%2711, %2715) : (tensor<1x257x5120xbf16>, tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1260)
        %2717 = "ttir.gelu"(%2716) : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1261)
        %2718 = "ttir.reshape"(%2717) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1262)
        %2719 = "ttir.reshape"(%arg98) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %2720 = "ttir.reshape"(%2719) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %2721 = "ttir.permute"(%2720) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1263)
        %2722 = "ttir.dot_general"(%2718, %2721) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1264)
        %2723 = "ttir.reshape"(%2722) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1262)
        %2724 = "ttir.reshape"(%arg97) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2725 = "ttir.reshape"(%2724) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2726 = "ttir.reshape"(%2725) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1265)
        %2727 = "ttir.broadcast"(%2726) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1265)
        %2728 = "ttir.add"(%2723, %2727) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1265)
        %2729 = "ttir.add"(%2700, %2728) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1266)
        %2730 = "ttir.reshape"(%arg96) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2731 = "ttir.reshape"(%2730) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2732 = "ttir.reshape"(%arg95) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2733 = "ttir.reshape"(%2732) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2734 = "ttir.layer_norm"(%2729, %2731, %2733) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1267)
        %2735 = "ttir.reshape"(%2734) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1268)
        %2736 = "ttir.reshape"(%arg491) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2737 = "ttir.reshape"(%2736) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2738 = "ttir.permute"(%2737) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1269)
        %2739 = "ttir.dot_general"(%2735, %2738) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1270)
        %2740 = "ttir.reshape"(%2739) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1268)
        %2741 = "ttir.reshape"(%arg490) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2742 = "ttir.reshape"(%2741) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2743 = "ttir.reshape"(%2742) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1271)
        %2744 = "ttir.broadcast"(%2743) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1271)
        %2745 = "ttir.add"(%2740, %2744) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1271)
        %2746 = "ttir.reshape"(%2745) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1272)
        %2747 = "ttir.permute"(%2746) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1273)
        %2748 = "ttir.typecast"(%2747) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1274)
        %2749 = "ttir.multiply"(%2748, %7) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1275)
        %2750 = "ttir.reshape"(%arg489) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2751 = "ttir.reshape"(%2750) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2752 = "ttir.permute"(%2751) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1276)
        %2753 = "ttir.dot_general"(%2735, %2752) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1277)
        %2754 = "ttir.reshape"(%2753) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1278)
        %2755 = "ttir.reshape"(%arg488) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2756 = "ttir.reshape"(%2755) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2757 = "ttir.reshape"(%2756) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1279)
        %2758 = "ttir.broadcast"(%2757) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1279)
        %2759 = "ttir.add"(%2754, %2758) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1279)
        %2760 = "ttir.reshape"(%2759) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1280)
        %2761 = "ttir.permute"(%2760) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1281)
        %2762 = "ttir.typecast"(%2761) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1282)
        %2763 = "ttir.permute"(%2762) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1283)
        %2764 = "ttir.multiply"(%2763, %6) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32> loc(#loc1284)
        %2765 = "ttir.dot_general"(%2749, %2764) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1285)
        %2766 = "ttir.typecast"(%2765) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1286)
        %2767 = "ttir.eq"(%2766, %5) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1286)
        %2768 = "ttir.logical_not"(%2767) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1287)
        %2769 = "ttir.reduce_or"(%2768) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1> loc(#loc1288)
        %2770 = "ttir.reshape"(%2769) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1288)
        %2771 = "ttir.logical_not"(%2770) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1> loc(#loc1289)
        %2772 = "ttir.reshape"(%2771) <{shape = [1 : i32, 16 : i32, 257 : i32]}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1290)
        %2773 = "ttir.reshape"(%2772) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1290)
        %2774 = "ttir.broadcast"(%2773) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1> loc(#loc1290)
        %2775 = "ttir.max"(%2765) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc1291)
        %2776 = "ttir.reshape"(%2775) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc1291)
        %2777 = "ttir.broadcast"(%2776) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc1291)
        %2778 = "ttir.subtract"(%2765, %2777) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1291)
        %2779 = "ttir.exp"(%2778) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1291)
        %2780 = "ttir.sum"(%2779) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc1291)
        %2781 = "ttir.reshape"(%2780) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc1291)
        %2782 = "ttir.broadcast"(%2781) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc1291)
        %2783 = "ttir.div"(%2779, %2782) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1291)
        %2784 = "ttir.where"(%2774, %4, %2783) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1292)
        %2785 = "ttir.reshape"(%arg94) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2786 = "ttir.reshape"(%2785) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2787 = "ttir.permute"(%2786) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1293)
        %2788 = "ttir.dot_general"(%2735, %2787) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1294)
        %2789 = "ttir.reshape"(%2788) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1295)
        %2790 = "ttir.reshape"(%arg93) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2791 = "ttir.reshape"(%2790) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2792 = "ttir.reshape"(%2791) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1296)
        %2793 = "ttir.broadcast"(%2792) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1296)
        %2794 = "ttir.add"(%2789, %2793) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1296)
        %2795 = "ttir.reshape"(%2794) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1297)
        %2796 = "ttir.permute"(%2795) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1298)
        %2797 = "ttir.typecast"(%2796) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1299)
        %2798 = "ttir.dot_general"(%2784, %2797) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1300)
        %2799 = "ttir.typecast"(%2798) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1301)
        %2800 = "ttir.permute"(%2799) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1302)
        %2801 = "ttir.reshape"(%2800) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1303)
        %2802 = "ttir.reshape"(%arg92) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2803 = "ttir.reshape"(%2802) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2804 = "ttir.permute"(%2803) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1304)
        %2805 = "ttir.dot_general"(%2801, %2804) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1305)
        %2806 = "ttir.reshape"(%2805) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1303)
        %2807 = "ttir.reshape"(%arg91) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2808 = "ttir.reshape"(%2807) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2809 = "ttir.reshape"(%2808) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1306)
        %2810 = "ttir.broadcast"(%2809) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1306)
        %2811 = "ttir.add"(%2806, %2810) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1306)
        %2812 = "ttir.add"(%2729, %2811) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1307)
        %2813 = "ttir.reshape"(%arg90) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2814 = "ttir.reshape"(%2813) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2815 = "ttir.reshape"(%arg89) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2816 = "ttir.reshape"(%2815) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2817 = "ttir.layer_norm"(%2812, %2814, %2816) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1308)
        %2818 = "ttir.reshape"(%2817) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1309)
        %2819 = "ttir.reshape"(%arg88) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %2820 = "ttir.reshape"(%2819) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %2821 = "ttir.permute"(%2820) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1310)
        %2822 = "ttir.dot_general"(%2818, %2821) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1311)
        %2823 = "ttir.reshape"(%2822) <{shape = [1 : i32, 257 : i32, 5120 : i32]}> : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1309)
        %2824 = "ttir.reshape"(%arg87) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
        %2825 = "ttir.reshape"(%2824) <{shape = [5120 : i32]}> : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
        %2826 = "ttir.reshape"(%2825) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1312)
        %2827 = "ttir.broadcast"(%2826) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1312)
        %2828 = "ttir.add"(%2823, %2827) : (tensor<1x257x5120xbf16>, tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1312)
        %2829 = "ttir.gelu"(%2828) : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1313)
        %2830 = "ttir.reshape"(%2829) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1314)
        %2831 = "ttir.reshape"(%arg86) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %2832 = "ttir.reshape"(%2831) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %2833 = "ttir.permute"(%2832) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1315)
        %2834 = "ttir.dot_general"(%2830, %2833) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1316)
        %2835 = "ttir.reshape"(%2834) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1314)
        %2836 = "ttir.reshape"(%arg85) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2837 = "ttir.reshape"(%2836) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2838 = "ttir.reshape"(%2837) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1317)
        %2839 = "ttir.broadcast"(%2838) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1317)
        %2840 = "ttir.add"(%2835, %2839) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1317)
        %2841 = "ttir.add"(%2812, %2840) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1318)
        %2842 = "ttir.reshape"(%arg84) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2843 = "ttir.reshape"(%2842) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2844 = "ttir.reshape"(%arg83) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2845 = "ttir.reshape"(%2844) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2846 = "ttir.layer_norm"(%2841, %2843, %2845) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1319)
        %2847 = "ttir.reshape"(%2846) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1320)
        %2848 = "ttir.reshape"(%arg495) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2849 = "ttir.reshape"(%2848) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2850 = "ttir.permute"(%2849) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1321)
        %2851 = "ttir.dot_general"(%2847, %2850) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1322)
        %2852 = "ttir.reshape"(%2851) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1320)
        %2853 = "ttir.reshape"(%arg494) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2854 = "ttir.reshape"(%2853) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2855 = "ttir.reshape"(%2854) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1323)
        %2856 = "ttir.broadcast"(%2855) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1323)
        %2857 = "ttir.add"(%2852, %2856) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1323)
        %2858 = "ttir.reshape"(%2857) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1324)
        %2859 = "ttir.permute"(%2858) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1325)
        %2860 = "ttir.typecast"(%2859) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1326)
        %2861 = "ttir.multiply"(%2860, %7) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1327)
        %2862 = "ttir.reshape"(%arg493) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2863 = "ttir.reshape"(%2862) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2864 = "ttir.permute"(%2863) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1328)
        %2865 = "ttir.dot_general"(%2847, %2864) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1329)
        %2866 = "ttir.reshape"(%2865) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1330)
        %2867 = "ttir.reshape"(%arg492) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2868 = "ttir.reshape"(%2867) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2869 = "ttir.reshape"(%2868) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1331)
        %2870 = "ttir.broadcast"(%2869) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1331)
        %2871 = "ttir.add"(%2866, %2870) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1331)
        %2872 = "ttir.reshape"(%2871) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1332)
        %2873 = "ttir.permute"(%2872) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1333)
        %2874 = "ttir.typecast"(%2873) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1334)
        %2875 = "ttir.permute"(%2874) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1335)
        %2876 = "ttir.multiply"(%2875, %6) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32> loc(#loc1336)
        %2877 = "ttir.dot_general"(%2861, %2876) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1337)
        %2878 = "ttir.typecast"(%2877) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1338)
        %2879 = "ttir.eq"(%2878, %5) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1338)
        %2880 = "ttir.logical_not"(%2879) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1339)
        %2881 = "ttir.reduce_or"(%2880) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1> loc(#loc1340)
        %2882 = "ttir.reshape"(%2881) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1340)
        %2883 = "ttir.logical_not"(%2882) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1> loc(#loc1341)
        %2884 = "ttir.reshape"(%2883) <{shape = [1 : i32, 16 : i32, 257 : i32]}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1342)
        %2885 = "ttir.reshape"(%2884) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1342)
        %2886 = "ttir.broadcast"(%2885) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1> loc(#loc1342)
        %2887 = "ttir.max"(%2877) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc1343)
        %2888 = "ttir.reshape"(%2887) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc1343)
        %2889 = "ttir.broadcast"(%2888) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc1343)
        %2890 = "ttir.subtract"(%2877, %2889) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1343)
        %2891 = "ttir.exp"(%2890) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1343)
        %2892 = "ttir.sum"(%2891) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc1343)
        %2893 = "ttir.reshape"(%2892) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc1343)
        %2894 = "ttir.broadcast"(%2893) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc1343)
        %2895 = "ttir.div"(%2891, %2894) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1343)
        %2896 = "ttir.where"(%2886, %4, %2895) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1344)
        %2897 = "ttir.reshape"(%arg82) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2898 = "ttir.reshape"(%2897) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2899 = "ttir.permute"(%2898) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1345)
        %2900 = "ttir.dot_general"(%2847, %2899) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1346)
        %2901 = "ttir.reshape"(%2900) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1347)
        %2902 = "ttir.reshape"(%arg81) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2903 = "ttir.reshape"(%2902) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2904 = "ttir.reshape"(%2903) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1348)
        %2905 = "ttir.broadcast"(%2904) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1348)
        %2906 = "ttir.add"(%2901, %2905) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1348)
        %2907 = "ttir.reshape"(%2906) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1349)
        %2908 = "ttir.permute"(%2907) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1350)
        %2909 = "ttir.typecast"(%2908) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1351)
        %2910 = "ttir.dot_general"(%2896, %2909) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1352)
        %2911 = "ttir.typecast"(%2910) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1353)
        %2912 = "ttir.permute"(%2911) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1354)
        %2913 = "ttir.reshape"(%2912) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1355)
        %2914 = "ttir.reshape"(%arg80) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2915 = "ttir.reshape"(%2914) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2916 = "ttir.permute"(%2915) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1356)
        %2917 = "ttir.dot_general"(%2913, %2916) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1357)
        %2918 = "ttir.reshape"(%2917) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1355)
        %2919 = "ttir.reshape"(%arg79) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2920 = "ttir.reshape"(%2919) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2921 = "ttir.reshape"(%2920) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1358)
        %2922 = "ttir.broadcast"(%2921) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1358)
        %2923 = "ttir.add"(%2918, %2922) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1358)
        %2924 = "ttir.add"(%2841, %2923) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1359)
        %2925 = "ttir.reshape"(%arg78) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2926 = "ttir.reshape"(%2925) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2927 = "ttir.reshape"(%arg77) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2928 = "ttir.reshape"(%2927) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2929 = "ttir.layer_norm"(%2924, %2926, %2928) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1360)
        %2930 = "ttir.reshape"(%2929) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1361)
        %2931 = "ttir.reshape"(%arg76) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %2932 = "ttir.reshape"(%2931) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %2933 = "ttir.permute"(%2932) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1362)
        %2934 = "ttir.dot_general"(%2930, %2933) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1363)
        %2935 = "ttir.reshape"(%2934) <{shape = [1 : i32, 257 : i32, 5120 : i32]}> : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1361)
        %2936 = "ttir.reshape"(%arg75) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
        %2937 = "ttir.reshape"(%2936) <{shape = [5120 : i32]}> : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
        %2938 = "ttir.reshape"(%2937) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1364)
        %2939 = "ttir.broadcast"(%2938) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1364)
        %2940 = "ttir.add"(%2935, %2939) : (tensor<1x257x5120xbf16>, tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1364)
        %2941 = "ttir.gelu"(%2940) : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1365)
        %2942 = "ttir.reshape"(%2941) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1366)
        %2943 = "ttir.reshape"(%arg74) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %2944 = "ttir.reshape"(%2943) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %2945 = "ttir.permute"(%2944) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1367)
        %2946 = "ttir.dot_general"(%2942, %2945) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1368)
        %2947 = "ttir.reshape"(%2946) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1366)
        %2948 = "ttir.reshape"(%arg73) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2949 = "ttir.reshape"(%2948) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2950 = "ttir.reshape"(%2949) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1369)
        %2951 = "ttir.broadcast"(%2950) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1369)
        %2952 = "ttir.add"(%2947, %2951) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1369)
        %2953 = "ttir.add"(%2924, %2952) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1370)
        %2954 = "ttir.reshape"(%arg72) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2955 = "ttir.reshape"(%2954) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2956 = "ttir.reshape"(%arg71) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2957 = "ttir.reshape"(%2956) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2958 = "ttir.layer_norm"(%2953, %2955, %2957) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1371)
        %2959 = "ttir.reshape"(%2958) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1372)
        %2960 = "ttir.reshape"(%arg499) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2961 = "ttir.reshape"(%2960) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2962 = "ttir.permute"(%2961) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1373)
        %2963 = "ttir.dot_general"(%2959, %2962) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1374)
        %2964 = "ttir.reshape"(%2963) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1372)
        %2965 = "ttir.reshape"(%arg498) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2966 = "ttir.reshape"(%2965) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2967 = "ttir.reshape"(%2966) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1375)
        %2968 = "ttir.broadcast"(%2967) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1375)
        %2969 = "ttir.add"(%2964, %2968) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1375)
        %2970 = "ttir.reshape"(%2969) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1376)
        %2971 = "ttir.permute"(%2970) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1377)
        %2972 = "ttir.typecast"(%2971) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1378)
        %2973 = "ttir.multiply"(%2972, %7) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1379)
        %2974 = "ttir.reshape"(%arg497) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %2975 = "ttir.reshape"(%2974) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %2976 = "ttir.permute"(%2975) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1380)
        %2977 = "ttir.dot_general"(%2959, %2976) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1381)
        %2978 = "ttir.reshape"(%2977) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1382)
        %2979 = "ttir.reshape"(%arg496) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %2980 = "ttir.reshape"(%2979) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %2981 = "ttir.reshape"(%2980) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1383)
        %2982 = "ttir.broadcast"(%2981) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1383)
        %2983 = "ttir.add"(%2978, %2982) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1383)
        %2984 = "ttir.reshape"(%2983) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1384)
        %2985 = "ttir.permute"(%2984) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1385)
        %2986 = "ttir.typecast"(%2985) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1386)
        %2987 = "ttir.permute"(%2986) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1387)
        %2988 = "ttir.multiply"(%2987, %6) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32> loc(#loc1388)
        %2989 = "ttir.dot_general"(%2973, %2988) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1389)
        %2990 = "ttir.typecast"(%2989) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1390)
        %2991 = "ttir.eq"(%2990, %5) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1390)
        %2992 = "ttir.logical_not"(%2991) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1391)
        %2993 = "ttir.reduce_or"(%2992) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1> loc(#loc1392)
        %2994 = "ttir.reshape"(%2993) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1392)
        %2995 = "ttir.logical_not"(%2994) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1> loc(#loc1393)
        %2996 = "ttir.reshape"(%2995) <{shape = [1 : i32, 16 : i32, 257 : i32]}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1394)
        %2997 = "ttir.reshape"(%2996) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1394)
        %2998 = "ttir.broadcast"(%2997) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1> loc(#loc1394)
        %2999 = "ttir.max"(%2989) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc1395)
        %3000 = "ttir.reshape"(%2999) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc1395)
        %3001 = "ttir.broadcast"(%3000) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc1395)
        %3002 = "ttir.subtract"(%2989, %3001) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1395)
        %3003 = "ttir.exp"(%3002) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1395)
        %3004 = "ttir.sum"(%3003) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc1395)
        %3005 = "ttir.reshape"(%3004) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc1395)
        %3006 = "ttir.broadcast"(%3005) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc1395)
        %3007 = "ttir.div"(%3003, %3006) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1395)
        %3008 = "ttir.where"(%2998, %4, %3007) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1396)
        %3009 = "ttir.reshape"(%arg70) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3010 = "ttir.reshape"(%3009) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3011 = "ttir.permute"(%3010) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1397)
        %3012 = "ttir.dot_general"(%2959, %3011) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1398)
        %3013 = "ttir.reshape"(%3012) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1399)
        %3014 = "ttir.reshape"(%arg69) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3015 = "ttir.reshape"(%3014) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3016 = "ttir.reshape"(%3015) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1400)
        %3017 = "ttir.broadcast"(%3016) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1400)
        %3018 = "ttir.add"(%3013, %3017) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1400)
        %3019 = "ttir.reshape"(%3018) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1401)
        %3020 = "ttir.permute"(%3019) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1402)
        %3021 = "ttir.typecast"(%3020) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1403)
        %3022 = "ttir.dot_general"(%3008, %3021) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1404)
        %3023 = "ttir.typecast"(%3022) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1405)
        %3024 = "ttir.permute"(%3023) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1406)
        %3025 = "ttir.reshape"(%3024) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1407)
        %3026 = "ttir.reshape"(%arg68) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3027 = "ttir.reshape"(%3026) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3028 = "ttir.permute"(%3027) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1408)
        %3029 = "ttir.dot_general"(%3025, %3028) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1409)
        %3030 = "ttir.reshape"(%3029) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1407)
        %3031 = "ttir.reshape"(%arg67) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3032 = "ttir.reshape"(%3031) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3033 = "ttir.reshape"(%3032) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1410)
        %3034 = "ttir.broadcast"(%3033) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1410)
        %3035 = "ttir.add"(%3030, %3034) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1410)
        %3036 = "ttir.add"(%2953, %3035) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1411)
        %3037 = "ttir.reshape"(%arg66) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3038 = "ttir.reshape"(%3037) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3039 = "ttir.reshape"(%arg65) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3040 = "ttir.reshape"(%3039) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3041 = "ttir.layer_norm"(%3036, %3038, %3040) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1412)
        %3042 = "ttir.reshape"(%3041) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1413)
        %3043 = "ttir.reshape"(%arg64) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %3044 = "ttir.reshape"(%3043) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %3045 = "ttir.permute"(%3044) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1414)
        %3046 = "ttir.dot_general"(%3042, %3045) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1415)
        %3047 = "ttir.reshape"(%3046) <{shape = [1 : i32, 257 : i32, 5120 : i32]}> : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1413)
        %3048 = "ttir.reshape"(%arg63) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
        %3049 = "ttir.reshape"(%3048) <{shape = [5120 : i32]}> : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
        %3050 = "ttir.reshape"(%3049) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1416)
        %3051 = "ttir.broadcast"(%3050) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1416)
        %3052 = "ttir.add"(%3047, %3051) : (tensor<1x257x5120xbf16>, tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1416)
        %3053 = "ttir.gelu"(%3052) : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1417)
        %3054 = "ttir.reshape"(%3053) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1418)
        %3055 = "ttir.reshape"(%arg62) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %3056 = "ttir.reshape"(%3055) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %3057 = "ttir.permute"(%3056) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1419)
        %3058 = "ttir.dot_general"(%3054, %3057) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1420)
        %3059 = "ttir.reshape"(%3058) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1418)
        %3060 = "ttir.reshape"(%arg61) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3061 = "ttir.reshape"(%3060) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3062 = "ttir.reshape"(%3061) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1421)
        %3063 = "ttir.broadcast"(%3062) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1421)
        %3064 = "ttir.add"(%3059, %3063) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1421)
        %3065 = "ttir.add"(%3036, %3064) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1422)
        %3066 = "ttir.reshape"(%arg60) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3067 = "ttir.reshape"(%3066) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3068 = "ttir.reshape"(%arg59) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3069 = "ttir.reshape"(%3068) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3070 = "ttir.layer_norm"(%3065, %3067, %3069) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1423)
        %3071 = "ttir.reshape"(%3070) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1424)
        %3072 = "ttir.reshape"(%arg503) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3073 = "ttir.reshape"(%3072) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3074 = "ttir.permute"(%3073) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1425)
        %3075 = "ttir.dot_general"(%3071, %3074) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1426)
        %3076 = "ttir.reshape"(%3075) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1424)
        %3077 = "ttir.reshape"(%arg502) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3078 = "ttir.reshape"(%3077) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3079 = "ttir.reshape"(%3078) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1427)
        %3080 = "ttir.broadcast"(%3079) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1427)
        %3081 = "ttir.add"(%3076, %3080) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1427)
        %3082 = "ttir.reshape"(%3081) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1428)
        %3083 = "ttir.permute"(%3082) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1429)
        %3084 = "ttir.typecast"(%3083) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1430)
        %3085 = "ttir.multiply"(%3084, %7) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1431)
        %3086 = "ttir.reshape"(%arg501) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3087 = "ttir.reshape"(%3086) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3088 = "ttir.permute"(%3087) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1432)
        %3089 = "ttir.dot_general"(%3071, %3088) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1433)
        %3090 = "ttir.reshape"(%3089) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1434)
        %3091 = "ttir.reshape"(%arg500) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3092 = "ttir.reshape"(%3091) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3093 = "ttir.reshape"(%3092) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1435)
        %3094 = "ttir.broadcast"(%3093) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1435)
        %3095 = "ttir.add"(%3090, %3094) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1435)
        %3096 = "ttir.reshape"(%3095) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1436)
        %3097 = "ttir.permute"(%3096) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1437)
        %3098 = "ttir.typecast"(%3097) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1438)
        %3099 = "ttir.permute"(%3098) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1439)
        %3100 = "ttir.multiply"(%3099, %6) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32> loc(#loc1440)
        %3101 = "ttir.dot_general"(%3085, %3100) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1441)
        %3102 = "ttir.typecast"(%3101) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1442)
        %3103 = "ttir.eq"(%3102, %5) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1442)
        %3104 = "ttir.logical_not"(%3103) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1443)
        %3105 = "ttir.reduce_or"(%3104) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1> loc(#loc1444)
        %3106 = "ttir.reshape"(%3105) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1444)
        %3107 = "ttir.logical_not"(%3106) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1> loc(#loc1445)
        %3108 = "ttir.reshape"(%3107) <{shape = [1 : i32, 16 : i32, 257 : i32]}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1446)
        %3109 = "ttir.reshape"(%3108) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1446)
        %3110 = "ttir.broadcast"(%3109) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1> loc(#loc1446)
        %3111 = "ttir.max"(%3101) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc1447)
        %3112 = "ttir.reshape"(%3111) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc1447)
        %3113 = "ttir.broadcast"(%3112) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc1447)
        %3114 = "ttir.subtract"(%3101, %3113) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1447)
        %3115 = "ttir.exp"(%3114) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1447)
        %3116 = "ttir.sum"(%3115) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc1447)
        %3117 = "ttir.reshape"(%3116) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc1447)
        %3118 = "ttir.broadcast"(%3117) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc1447)
        %3119 = "ttir.div"(%3115, %3118) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1447)
        %3120 = "ttir.where"(%3110, %4, %3119) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1448)
        %3121 = "ttir.reshape"(%arg58) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3122 = "ttir.reshape"(%3121) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3123 = "ttir.permute"(%3122) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1449)
        %3124 = "ttir.dot_general"(%3071, %3123) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1450)
        %3125 = "ttir.reshape"(%3124) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1451)
        %3126 = "ttir.reshape"(%arg57) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3127 = "ttir.reshape"(%3126) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3128 = "ttir.reshape"(%3127) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1452)
        %3129 = "ttir.broadcast"(%3128) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1452)
        %3130 = "ttir.add"(%3125, %3129) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1452)
        %3131 = "ttir.reshape"(%3130) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1453)
        %3132 = "ttir.permute"(%3131) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1454)
        %3133 = "ttir.typecast"(%3132) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1455)
        %3134 = "ttir.dot_general"(%3120, %3133) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1456)
        %3135 = "ttir.typecast"(%3134) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1457)
        %3136 = "ttir.permute"(%3135) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1458)
        %3137 = "ttir.reshape"(%3136) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1459)
        %3138 = "ttir.reshape"(%arg56) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3139 = "ttir.reshape"(%3138) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3140 = "ttir.permute"(%3139) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1460)
        %3141 = "ttir.dot_general"(%3137, %3140) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1461)
        %3142 = "ttir.reshape"(%3141) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1459)
        %3143 = "ttir.reshape"(%arg55) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3144 = "ttir.reshape"(%3143) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3145 = "ttir.reshape"(%3144) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1462)
        %3146 = "ttir.broadcast"(%3145) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1462)
        %3147 = "ttir.add"(%3142, %3146) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1462)
        %3148 = "ttir.add"(%3065, %3147) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1463)
        %3149 = "ttir.reshape"(%arg54) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3150 = "ttir.reshape"(%3149) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3151 = "ttir.reshape"(%arg53) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3152 = "ttir.reshape"(%3151) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3153 = "ttir.layer_norm"(%3148, %3150, %3152) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1464)
        %3154 = "ttir.reshape"(%3153) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1465)
        %3155 = "ttir.reshape"(%arg52) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %3156 = "ttir.reshape"(%3155) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %3157 = "ttir.permute"(%3156) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1466)
        %3158 = "ttir.dot_general"(%3154, %3157) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1467)
        %3159 = "ttir.reshape"(%3158) <{shape = [1 : i32, 257 : i32, 5120 : i32]}> : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1465)
        %3160 = "ttir.reshape"(%arg51) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
        %3161 = "ttir.reshape"(%3160) <{shape = [5120 : i32]}> : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
        %3162 = "ttir.reshape"(%3161) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1468)
        %3163 = "ttir.broadcast"(%3162) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1468)
        %3164 = "ttir.add"(%3159, %3163) : (tensor<1x257x5120xbf16>, tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1468)
        %3165 = "ttir.gelu"(%3164) : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1469)
        %3166 = "ttir.reshape"(%3165) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1470)
        %3167 = "ttir.reshape"(%arg50) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %3168 = "ttir.reshape"(%3167) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %3169 = "ttir.permute"(%3168) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1471)
        %3170 = "ttir.dot_general"(%3166, %3169) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1472)
        %3171 = "ttir.reshape"(%3170) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1470)
        %3172 = "ttir.reshape"(%arg49) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3173 = "ttir.reshape"(%3172) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3174 = "ttir.reshape"(%3173) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1473)
        %3175 = "ttir.broadcast"(%3174) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1473)
        %3176 = "ttir.add"(%3171, %3175) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1473)
        %3177 = "ttir.add"(%3148, %3176) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1474)
        %3178 = "ttir.reshape"(%arg48) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3179 = "ttir.reshape"(%3178) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3180 = "ttir.reshape"(%arg47) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3181 = "ttir.reshape"(%3180) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3182 = "ttir.layer_norm"(%3177, %3179, %3181) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1475)
        %3183 = "ttir.reshape"(%3182) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1476)
        %3184 = "ttir.reshape"(%arg507) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3185 = "ttir.reshape"(%3184) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3186 = "ttir.permute"(%3185) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1477)
        %3187 = "ttir.dot_general"(%3183, %3186) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1478)
        %3188 = "ttir.reshape"(%3187) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1476)
        %3189 = "ttir.reshape"(%arg506) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3190 = "ttir.reshape"(%3189) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3191 = "ttir.reshape"(%3190) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1479)
        %3192 = "ttir.broadcast"(%3191) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1479)
        %3193 = "ttir.add"(%3188, %3192) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1479)
        %3194 = "ttir.reshape"(%3193) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1480)
        %3195 = "ttir.permute"(%3194) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1481)
        %3196 = "ttir.typecast"(%3195) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1482)
        %3197 = "ttir.multiply"(%3196, %7) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1483)
        %3198 = "ttir.reshape"(%arg505) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3199 = "ttir.reshape"(%3198) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3200 = "ttir.permute"(%3199) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1484)
        %3201 = "ttir.dot_general"(%3183, %3200) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1485)
        %3202 = "ttir.reshape"(%3201) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1486)
        %3203 = "ttir.reshape"(%arg504) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3204 = "ttir.reshape"(%3203) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3205 = "ttir.reshape"(%3204) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1487)
        %3206 = "ttir.broadcast"(%3205) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1487)
        %3207 = "ttir.add"(%3202, %3206) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1487)
        %3208 = "ttir.reshape"(%3207) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1488)
        %3209 = "ttir.permute"(%3208) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1489)
        %3210 = "ttir.typecast"(%3209) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1490)
        %3211 = "ttir.permute"(%3210) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1491)
        %3212 = "ttir.multiply"(%3211, %6) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32> loc(#loc1492)
        %3213 = "ttir.dot_general"(%3197, %3212) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1493)
        %3214 = "ttir.typecast"(%3213) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1494)
        %3215 = "ttir.eq"(%3214, %5) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1494)
        %3216 = "ttir.logical_not"(%3215) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1495)
        %3217 = "ttir.reduce_or"(%3216) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1> loc(#loc1496)
        %3218 = "ttir.reshape"(%3217) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1496)
        %3219 = "ttir.logical_not"(%3218) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1> loc(#loc1497)
        %3220 = "ttir.reshape"(%3219) <{shape = [1 : i32, 16 : i32, 257 : i32]}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1498)
        %3221 = "ttir.reshape"(%3220) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1498)
        %3222 = "ttir.broadcast"(%3221) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1> loc(#loc1498)
        %3223 = "ttir.max"(%3213) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc1499)
        %3224 = "ttir.reshape"(%3223) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc1499)
        %3225 = "ttir.broadcast"(%3224) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc1499)
        %3226 = "ttir.subtract"(%3213, %3225) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1499)
        %3227 = "ttir.exp"(%3226) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1499)
        %3228 = "ttir.sum"(%3227) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc1499)
        %3229 = "ttir.reshape"(%3228) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc1499)
        %3230 = "ttir.broadcast"(%3229) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc1499)
        %3231 = "ttir.div"(%3227, %3230) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1499)
        %3232 = "ttir.where"(%3222, %4, %3231) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1500)
        %3233 = "ttir.reshape"(%arg46) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3234 = "ttir.reshape"(%3233) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3235 = "ttir.permute"(%3234) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1501)
        %3236 = "ttir.dot_general"(%3183, %3235) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1502)
        %3237 = "ttir.reshape"(%3236) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1503)
        %3238 = "ttir.reshape"(%arg45) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3239 = "ttir.reshape"(%3238) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3240 = "ttir.reshape"(%3239) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1504)
        %3241 = "ttir.broadcast"(%3240) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1504)
        %3242 = "ttir.add"(%3237, %3241) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1504)
        %3243 = "ttir.reshape"(%3242) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1505)
        %3244 = "ttir.permute"(%3243) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1506)
        %3245 = "ttir.typecast"(%3244) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1507)
        %3246 = "ttir.dot_general"(%3232, %3245) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1508)
        %3247 = "ttir.typecast"(%3246) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1509)
        %3248 = "ttir.permute"(%3247) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1510)
        %3249 = "ttir.reshape"(%3248) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1511)
        %3250 = "ttir.reshape"(%arg44) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3251 = "ttir.reshape"(%3250) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3252 = "ttir.permute"(%3251) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1512)
        %3253 = "ttir.dot_general"(%3249, %3252) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1513)
        %3254 = "ttir.reshape"(%3253) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1511)
        %3255 = "ttir.reshape"(%arg43) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3256 = "ttir.reshape"(%3255) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3257 = "ttir.reshape"(%3256) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1514)
        %3258 = "ttir.broadcast"(%3257) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1514)
        %3259 = "ttir.add"(%3254, %3258) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1514)
        %3260 = "ttir.add"(%3177, %3259) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1515)
        %3261 = "ttir.reshape"(%arg42) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3262 = "ttir.reshape"(%3261) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3263 = "ttir.reshape"(%arg41) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3264 = "ttir.reshape"(%3263) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3265 = "ttir.layer_norm"(%3260, %3262, %3264) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1516)
        %3266 = "ttir.reshape"(%3265) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1517)
        %3267 = "ttir.reshape"(%arg40) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %3268 = "ttir.reshape"(%3267) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %3269 = "ttir.permute"(%3268) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1518)
        %3270 = "ttir.dot_general"(%3266, %3269) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1519)
        %3271 = "ttir.reshape"(%3270) <{shape = [1 : i32, 257 : i32, 5120 : i32]}> : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1517)
        %3272 = "ttir.reshape"(%arg39) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
        %3273 = "ttir.reshape"(%3272) <{shape = [5120 : i32]}> : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
        %3274 = "ttir.reshape"(%3273) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1520)
        %3275 = "ttir.broadcast"(%3274) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1520)
        %3276 = "ttir.add"(%3271, %3275) : (tensor<1x257x5120xbf16>, tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1520)
        %3277 = "ttir.gelu"(%3276) : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1521)
        %3278 = "ttir.reshape"(%3277) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1522)
        %3279 = "ttir.reshape"(%arg38) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %3280 = "ttir.reshape"(%3279) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %3281 = "ttir.permute"(%3280) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1523)
        %3282 = "ttir.dot_general"(%3278, %3281) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1524)
        %3283 = "ttir.reshape"(%3282) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1522)
        %3284 = "ttir.reshape"(%arg37) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3285 = "ttir.reshape"(%3284) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3286 = "ttir.reshape"(%3285) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1525)
        %3287 = "ttir.broadcast"(%3286) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1525)
        %3288 = "ttir.add"(%3283, %3287) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1525)
        %3289 = "ttir.add"(%3260, %3288) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1526)
        %3290 = "ttir.reshape"(%arg36) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3291 = "ttir.reshape"(%3290) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3292 = "ttir.reshape"(%arg35) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3293 = "ttir.reshape"(%3292) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3294 = "ttir.layer_norm"(%3289, %3291, %3293) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1527)
        %3295 = "ttir.reshape"(%3294) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1528)
        %3296 = "ttir.reshape"(%arg511) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3297 = "ttir.reshape"(%3296) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3298 = "ttir.permute"(%3297) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1529)
        %3299 = "ttir.dot_general"(%3295, %3298) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1530)
        %3300 = "ttir.reshape"(%3299) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1528)
        %3301 = "ttir.reshape"(%arg510) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3302 = "ttir.reshape"(%3301) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3303 = "ttir.reshape"(%3302) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1531)
        %3304 = "ttir.broadcast"(%3303) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1531)
        %3305 = "ttir.add"(%3300, %3304) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1531)
        %3306 = "ttir.reshape"(%3305) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1532)
        %3307 = "ttir.permute"(%3306) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1533)
        %3308 = "ttir.typecast"(%3307) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1534)
        %3309 = "ttir.multiply"(%3308, %7) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1535)
        %3310 = "ttir.reshape"(%arg509) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3311 = "ttir.reshape"(%3310) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3312 = "ttir.permute"(%3311) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1536)
        %3313 = "ttir.dot_general"(%3295, %3312) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1537)
        %3314 = "ttir.reshape"(%3313) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1538)
        %3315 = "ttir.reshape"(%arg508) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3316 = "ttir.reshape"(%3315) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3317 = "ttir.reshape"(%3316) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1539)
        %3318 = "ttir.broadcast"(%3317) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1539)
        %3319 = "ttir.add"(%3314, %3318) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1539)
        %3320 = "ttir.reshape"(%3319) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1540)
        %3321 = "ttir.permute"(%3320) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1541)
        %3322 = "ttir.typecast"(%3321) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1542)
        %3323 = "ttir.permute"(%3322) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1543)
        %3324 = "ttir.multiply"(%3323, %6) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32> loc(#loc1544)
        %3325 = "ttir.dot_general"(%3309, %3324) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1545)
        %3326 = "ttir.typecast"(%3325) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1546)
        %3327 = "ttir.eq"(%3326, %5) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1546)
        %3328 = "ttir.logical_not"(%3327) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1547)
        %3329 = "ttir.reduce_or"(%3328) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1> loc(#loc1548)
        %3330 = "ttir.reshape"(%3329) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1548)
        %3331 = "ttir.logical_not"(%3330) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1> loc(#loc1549)
        %3332 = "ttir.reshape"(%3331) <{shape = [1 : i32, 16 : i32, 257 : i32]}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1550)
        %3333 = "ttir.reshape"(%3332) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1550)
        %3334 = "ttir.broadcast"(%3333) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1> loc(#loc1550)
        %3335 = "ttir.max"(%3325) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc1551)
        %3336 = "ttir.reshape"(%3335) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc1551)
        %3337 = "ttir.broadcast"(%3336) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc1551)
        %3338 = "ttir.subtract"(%3325, %3337) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1551)
        %3339 = "ttir.exp"(%3338) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1551)
        %3340 = "ttir.sum"(%3339) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc1551)
        %3341 = "ttir.reshape"(%3340) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc1551)
        %3342 = "ttir.broadcast"(%3341) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc1551)
        %3343 = "ttir.div"(%3339, %3342) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1551)
        %3344 = "ttir.where"(%3334, %4, %3343) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1552)
        %3345 = "ttir.reshape"(%arg34) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3346 = "ttir.reshape"(%3345) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3347 = "ttir.permute"(%3346) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1553)
        %3348 = "ttir.dot_general"(%3295, %3347) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1554)
        %3349 = "ttir.reshape"(%3348) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1555)
        %3350 = "ttir.reshape"(%arg33) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3351 = "ttir.reshape"(%3350) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3352 = "ttir.reshape"(%3351) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1556)
        %3353 = "ttir.broadcast"(%3352) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1556)
        %3354 = "ttir.add"(%3349, %3353) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1556)
        %3355 = "ttir.reshape"(%3354) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1557)
        %3356 = "ttir.permute"(%3355) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1558)
        %3357 = "ttir.typecast"(%3356) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1559)
        %3358 = "ttir.dot_general"(%3344, %3357) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1560)
        %3359 = "ttir.typecast"(%3358) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1561)
        %3360 = "ttir.permute"(%3359) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1562)
        %3361 = "ttir.reshape"(%3360) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1563)
        %3362 = "ttir.reshape"(%arg32) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3363 = "ttir.reshape"(%3362) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3364 = "ttir.permute"(%3363) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1564)
        %3365 = "ttir.dot_general"(%3361, %3364) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1565)
        %3366 = "ttir.reshape"(%3365) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1563)
        %3367 = "ttir.reshape"(%arg31) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3368 = "ttir.reshape"(%3367) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3369 = "ttir.reshape"(%3368) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1566)
        %3370 = "ttir.broadcast"(%3369) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1566)
        %3371 = "ttir.add"(%3366, %3370) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1566)
        %3372 = "ttir.add"(%3289, %3371) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1567)
        %3373 = "ttir.reshape"(%arg30) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3374 = "ttir.reshape"(%3373) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3375 = "ttir.reshape"(%arg29) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3376 = "ttir.reshape"(%3375) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3377 = "ttir.layer_norm"(%3372, %3374, %3376) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1568)
        %3378 = "ttir.reshape"(%3377) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1569)
        %3379 = "ttir.reshape"(%arg28) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %3380 = "ttir.reshape"(%3379) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %3381 = "ttir.permute"(%3380) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1570)
        %3382 = "ttir.dot_general"(%3378, %3381) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1571)
        %3383 = "ttir.reshape"(%3382) <{shape = [1 : i32, 257 : i32, 5120 : i32]}> : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1569)
        %3384 = "ttir.reshape"(%arg27) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
        %3385 = "ttir.reshape"(%3384) <{shape = [5120 : i32]}> : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
        %3386 = "ttir.reshape"(%3385) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1572)
        %3387 = "ttir.broadcast"(%3386) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1572)
        %3388 = "ttir.add"(%3383, %3387) : (tensor<1x257x5120xbf16>, tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1572)
        %3389 = "ttir.gelu"(%3388) : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1573)
        %3390 = "ttir.reshape"(%3389) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1574)
        %3391 = "ttir.reshape"(%arg26) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %3392 = "ttir.reshape"(%3391) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %3393 = "ttir.permute"(%3392) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1575)
        %3394 = "ttir.dot_general"(%3390, %3393) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1576)
        %3395 = "ttir.reshape"(%3394) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1574)
        %3396 = "ttir.reshape"(%arg25) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3397 = "ttir.reshape"(%3396) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3398 = "ttir.reshape"(%3397) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1577)
        %3399 = "ttir.broadcast"(%3398) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1577)
        %3400 = "ttir.add"(%3395, %3399) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1577)
        %3401 = "ttir.add"(%3372, %3400) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1578)
        %3402 = "ttir.reshape"(%arg24) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3403 = "ttir.reshape"(%3402) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3404 = "ttir.reshape"(%arg23) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3405 = "ttir.reshape"(%3404) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3406 = "ttir.layer_norm"(%3401, %3403, %3405) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1579)
        %3407 = "ttir.reshape"(%3406) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1580)
        %3408 = "ttir.reshape"(%arg515) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3409 = "ttir.reshape"(%3408) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3410 = "ttir.permute"(%3409) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1581)
        %3411 = "ttir.dot_general"(%3407, %3410) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1582)
        %3412 = "ttir.reshape"(%3411) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1580)
        %3413 = "ttir.reshape"(%arg514) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3414 = "ttir.reshape"(%3413) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3415 = "ttir.reshape"(%3414) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1583)
        %3416 = "ttir.broadcast"(%3415) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1583)
        %3417 = "ttir.add"(%3412, %3416) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1583)
        %3418 = "ttir.reshape"(%3417) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1584)
        %3419 = "ttir.permute"(%3418) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1585)
        %3420 = "ttir.typecast"(%3419) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1586)
        %3421 = "ttir.multiply"(%3420, %7) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1587)
        %3422 = "ttir.reshape"(%arg513) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3423 = "ttir.reshape"(%3422) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3424 = "ttir.permute"(%3423) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1588)
        %3425 = "ttir.dot_general"(%3407, %3424) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1589)
        %3426 = "ttir.reshape"(%3425) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1590)
        %3427 = "ttir.reshape"(%arg512) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3428 = "ttir.reshape"(%3427) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3429 = "ttir.reshape"(%3428) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1591)
        %3430 = "ttir.broadcast"(%3429) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1591)
        %3431 = "ttir.add"(%3426, %3430) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1591)
        %3432 = "ttir.reshape"(%3431) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1592)
        %3433 = "ttir.permute"(%3432) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1593)
        %3434 = "ttir.typecast"(%3433) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1594)
        %3435 = "ttir.permute"(%3434) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1595)
        %3436 = "ttir.multiply"(%3435, %6) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32> loc(#loc1596)
        %3437 = "ttir.dot_general"(%3421, %3436) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1597)
        %3438 = "ttir.typecast"(%3437) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1598)
        %3439 = "ttir.eq"(%3438, %5) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1598)
        %3440 = "ttir.logical_not"(%3439) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1599)
        %3441 = "ttir.reduce_or"(%3440) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1> loc(#loc1600)
        %3442 = "ttir.reshape"(%3441) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1600)
        %3443 = "ttir.logical_not"(%3442) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1> loc(#loc1601)
        %3444 = "ttir.reshape"(%3443) <{shape = [1 : i32, 16 : i32, 257 : i32]}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1602)
        %3445 = "ttir.reshape"(%3444) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1602)
        %3446 = "ttir.broadcast"(%3445) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1> loc(#loc1602)
        %3447 = "ttir.max"(%3437) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc1603)
        %3448 = "ttir.reshape"(%3447) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc1603)
        %3449 = "ttir.broadcast"(%3448) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc1603)
        %3450 = "ttir.subtract"(%3437, %3449) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1603)
        %3451 = "ttir.exp"(%3450) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1603)
        %3452 = "ttir.sum"(%3451) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32> loc(#loc1603)
        %3453 = "ttir.reshape"(%3452) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32> loc(#loc1603)
        %3454 = "ttir.broadcast"(%3453) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32> loc(#loc1603)
        %3455 = "ttir.div"(%3451, %3454) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1603)
        %3456 = "ttir.where"(%3446, %4, %3455) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1604)
        %3457 = "ttir.reshape"(%arg22) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3458 = "ttir.reshape"(%3457) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3459 = "ttir.permute"(%3458) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1605)
        %3460 = "ttir.dot_general"(%3407, %3459) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1606)
        %3461 = "ttir.reshape"(%3460) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1607)
        %3462 = "ttir.reshape"(%arg21) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3463 = "ttir.reshape"(%3462) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3464 = "ttir.reshape"(%3463) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1608)
        %3465 = "ttir.broadcast"(%3464) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1608)
        %3466 = "ttir.add"(%3461, %3465) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1608)
        %3467 = "ttir.reshape"(%3466) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1609)
        %3468 = "ttir.permute"(%3467) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1610)
        %3469 = "ttir.typecast"(%3468) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1611)
        %3470 = "ttir.dot_general"(%3456, %3469) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1612)
        %3471 = "ttir.typecast"(%3470) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1613)
        %3472 = "ttir.permute"(%3471) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1614)
        %3473 = "ttir.reshape"(%3472) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1615)
        %3474 = "ttir.reshape"(%arg20) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3475 = "ttir.reshape"(%3474) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3476 = "ttir.permute"(%3475) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1616)
        %3477 = "ttir.dot_general"(%3473, %3476) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1617)
        %3478 = "ttir.reshape"(%3477) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1615)
        %3479 = "ttir.reshape"(%arg19) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3480 = "ttir.reshape"(%3479) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3481 = "ttir.reshape"(%3480) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1618)
        %3482 = "ttir.broadcast"(%3481) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1618)
        %3483 = "ttir.add"(%3478, %3482) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1618)
        %3484 = "ttir.add"(%3401, %3483) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1619)
        %3485 = "ttir.reshape"(%arg18) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3486 = "ttir.reshape"(%3485) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3487 = "ttir.reshape"(%arg17) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3488 = "ttir.reshape"(%3487) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3489 = "ttir.layer_norm"(%3484, %3486, %3488) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1620)
        %3490 = "ttir.reshape"(%3489) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1621)
        %3491 = "ttir.reshape"(%arg16) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %3492 = "ttir.reshape"(%3491) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %3493 = "ttir.permute"(%3492) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1622)
        %3494 = "ttir.dot_general"(%3490, %3493) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1623)
        %3495 = "ttir.reshape"(%3494) <{shape = [1 : i32, 257 : i32, 5120 : i32]}> : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1621)
        %3496 = "ttir.reshape"(%arg15) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
        %3497 = "ttir.reshape"(%3496) <{shape = [5120 : i32]}> : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
        %3498 = "ttir.reshape"(%3497) <{shape = [1 : i32, 1 : i32, 5120 : i32]}> : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1624)
        %3499 = "ttir.broadcast"(%3498) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1624)
        %3500 = "ttir.add"(%3495, %3499) : (tensor<1x257x5120xbf16>, tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1624)
        %3501 = "ttir.gelu"(%3500) : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1625)
        %3502 = "ttir.reshape"(%3501) <{shape = [257 : i32, 5120 : i32]}> : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1626)
        %3503 = "ttir.reshape"(%arg14) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %3504 = "ttir.reshape"(%3503) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %3505 = "ttir.permute"(%3504) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1627)
        %3506 = "ttir.dot_general"(%3502, %3505) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1628)
        %3507 = "ttir.reshape"(%3506) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1626)
        %3508 = "ttir.reshape"(%arg13) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3509 = "ttir.reshape"(%3508) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3510 = "ttir.reshape"(%3509) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1629)
        %3511 = "ttir.broadcast"(%3510) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1629)
        %3512 = "ttir.add"(%3507, %3511) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1629)
        %3513 = "ttir.add"(%3484, %3512) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1630)
        %3514 = "ttir.reshape"(%3513) <{shape = [257 : i32, 1280 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1631)
        %3515 = "ttir.reshape"(%arg12) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3516 = "ttir.reshape"(%3515) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3517 = "ttir.permute"(%3516) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1632)
        %3518 = "ttir.dot_general"(%3514, %3517) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1633)
        %3519 = "ttir.reshape"(%3518) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1631)
        %3520 = "ttir.reshape"(%arg11) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3521 = "ttir.reshape"(%3520) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3522 = "ttir.reshape"(%3521) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc1634)
        %3523 = "ttir.broadcast"(%3522) <{broadcast_dimensions = array<i64: 1, 257, 1>}> : (tensor<1x1x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1634)
        %3524 = "ttir.add"(%3519, %3523) : (tensor<1x257x1280xbf16>, tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1634)
        %3525 = "ttir.reshape"(%arg10) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3526 = "ttir.reshape"(%3525) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3527 = "ttir.reshape"(%arg9) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3528 = "ttir.reshape"(%3527) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3529 = "ttir.layer_norm"(%3524, %3526, %3528) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1635)
        %3530 = "ttir.concat"(%3529, %13) <{dim = 1 : si32}> : (tensor<1x257x1280xbf16>, tensor<1x16x1280xbf16>) -> tensor<1x273x1280xbf16> loc(#loc1636)
        %3531 = "ttir.reshape"(%3530) <{shape = [273 : i32, 1280 : i32]}> : (tensor<1x273x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1637)
        %3532 = "ttir.reshape"(%arg516) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3533 = "ttir.reshape"(%3532) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3534 = "ttir.permute"(%3533) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1638)
        %3535 = "ttir.dot_general"(%3531, %3534) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1639)
        %3536 = "ttir.reshape"(%3535) <{shape = [1 : i32, 273 : i32, 20 : i32, 64 : i32]}> : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc1640)
        %3537 = "ttir.permute"(%3536) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc1641)
        %3538 = "ttir.typecast"(%3537) <{conservative_folding = false}> : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc1642)
        %3539 = "ttir.permute"(%3538) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x20x273x64xf32>) -> tensor<1x20x64x273xf32> loc(#loc1643)
        %3540 = "ttir.multiply"(%3539, %3) : (tensor<1x20x64x273xf32>, tensor<1x20x64x273xf32>) -> tensor<1x20x64x273xf32> loc(#loc1644)
        %3541 = "ttir.dot_general"(%22, %3540) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x20x16x64xf32>, tensor<1x20x64x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc1645)
        %3542 = "ttir.typecast"(%3541) <{conservative_folding = false}> : (tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf64> loc(#loc1646)
        %3543 = "ttir.eq"(%3542, %2) : (tensor<1x20x16x273xf64>, tensor<1x20x16x273xf64>) -> tensor<1x20x16x273xi1> loc(#loc1646)
        %3544 = "ttir.logical_not"(%3543) : (tensor<1x20x16x273xi1>) -> tensor<1x20x16x273xi1> loc(#loc1647)
        %3545 = "ttir.reduce_or"(%3544) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x20x16x273xi1>) -> tensor<1x20x16xi1> loc(#loc1648)
        %3546 = "ttir.reshape"(%3545) <{shape = [1 : i32, 20 : i32, 16 : i32, 1 : i32]}> : (tensor<1x20x16xi1>) -> tensor<1x20x16x1xi1> loc(#loc1648)
        %3547 = "ttir.logical_not"(%3546) : (tensor<1x20x16x1xi1>) -> tensor<1x20x16x1xi1> loc(#loc1649)
        %3548 = "ttir.reshape"(%3547) <{shape = [1 : i32, 20 : i32, 16 : i32]}> : (tensor<1x20x16x1xi1>) -> tensor<1x20x16xi1> loc(#loc1650)
        %3549 = "ttir.reshape"(%3548) <{shape = [1 : i32, 20 : i32, 16 : i32, 1 : i32]}> : (tensor<1x20x16xi1>) -> tensor<1x20x16x1xi1> loc(#loc1650)
        %3550 = "ttir.broadcast"(%3549) <{broadcast_dimensions = array<i64: 1, 1, 1, 273>}> : (tensor<1x20x16x1xi1>) -> tensor<1x20x16x273xi1> loc(#loc1650)
        %3551 = "ttir.max"(%3541) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x20x16x273xf32>) -> tensor<1x20x16xf32> loc(#loc1651)
        %3552 = "ttir.reshape"(%3551) <{shape = [1 : i32, 20 : i32, 16 : i32, 1 : i32]}> : (tensor<1x20x16xf32>) -> tensor<1x20x16x1xf32> loc(#loc1651)
        %3553 = "ttir.broadcast"(%3552) <{broadcast_dimensions = array<i64: 1, 1, 1, 273>}> : (tensor<1x20x16x1xf32>) -> tensor<1x20x16x273xf32> loc(#loc1651)
        %3554 = "ttir.subtract"(%3541, %3553) : (tensor<1x20x16x273xf32>, tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc1651)
        %3555 = "ttir.exp"(%3554) : (tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc1651)
        %3556 = "ttir.sum"(%3555) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x20x16x273xf32>) -> tensor<1x20x16xf32> loc(#loc1651)
        %3557 = "ttir.reshape"(%3556) <{shape = [1 : i32, 20 : i32, 16 : i32, 1 : i32]}> : (tensor<1x20x16xf32>) -> tensor<1x20x16x1xf32> loc(#loc1651)
        %3558 = "ttir.broadcast"(%3557) <{broadcast_dimensions = array<i64: 1, 1, 1, 273>}> : (tensor<1x20x16x1xf32>) -> tensor<1x20x16x273xf32> loc(#loc1651)
        %3559 = "ttir.div"(%3555, %3558) : (tensor<1x20x16x273xf32>, tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc1651)
        %3560 = "ttir.where"(%3550, %1, %3559) : (tensor<1x20x16x273xi1>, tensor<1x20x16x273xf32>, tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc1652)
        %3561 = "ttir.reshape"(%arg6) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3562 = "ttir.reshape"(%3561) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3563 = "ttir.permute"(%3562) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1653)
        %3564 = "ttir.dot_general"(%3531, %3563) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1654)
        %3565 = "ttir.reshape"(%3564) <{shape = [1 : i32, 273 : i32, 20 : i32, 64 : i32]}> : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc1655)
        %3566 = "ttir.permute"(%3565) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc1656)
        %3567 = "ttir.typecast"(%3566) <{conservative_folding = false}> : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc1657)
        %3568 = "ttir.dot_general"(%3560, %3567) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x20x16x273xf32>, tensor<1x20x273x64xf32>) -> tensor<1x20x16x64xf32> loc(#loc1658)
        %3569 = "ttir.typecast"(%3568) <{conservative_folding = false}> : (tensor<1x20x16x64xf32>) -> tensor<1x20x16x64xbf16> loc(#loc1659)
        %3570 = "ttir.permute"(%3569) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x20x16x64xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc1660)
        %3571 = "ttir.reshape"(%3570) <{shape = [16 : i32, 1280 : i32]}> : (tensor<1x16x20x64xbf16>) -> tensor<16x1280xbf16> loc(#loc1661)
        %3572 = "ttir.reshape"(%arg5) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3573 = "ttir.reshape"(%3572) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3574 = "ttir.permute"(%3573) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1662)
        %3575 = "ttir.dot_general"(%3571, %3574) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1663)
        %3576 = "ttir.reshape"(%3575) <{shape = [1 : i32, 16 : i32, 1280 : i32]}> : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1664)
        %3577 = "ttir.div"(%3576, %0) : (tensor<1x16x1280xbf16>, tensor<1x16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1665)
        %3578 = "ttir.add"(%3577, %arg4) : (tensor<1x16x1280xbf16>, tensor<1x16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1666)
        %3579 = "ttir.reshape"(%arg521) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3580 = "ttir.reshape"(%3579) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3581 = "ttir.reshape"(%arg520) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3582 = "ttir.reshape"(%3581) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3583 = "ttir.layer_norm"(%3578, %3580, %3582) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1667)
        %3584 = "ttir.reshape"(%3583) <{shape = [16 : i32, 1280 : i32]}> : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1668)
        %3585 = "ttir.reshape"(%arg519) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %3586 = "ttir.reshape"(%3585) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %3587 = "ttir.permute"(%3586) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1669)
        %3588 = "ttir.dot_general"(%3584, %3587) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<16x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc1670)
        %3589 = "ttir.reshape"(%3588) <{shape = [1 : i32, 16 : i32, 5120 : i32]}> : (tensor<16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc1668)
        %3590 = "ttir.gelu"(%3589) : (tensor<1x16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc1671)
        %3591 = "ttir.reshape"(%3590) <{shape = [16 : i32, 5120 : i32]}> : (tensor<1x16x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc1672)
        %3592 = "ttir.reshape"(%arg518) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %3593 = "ttir.reshape"(%3592) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %3594 = "ttir.permute"(%3593) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1673)
        %3595 = "ttir.dot_general"(%3591, %3594) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<16x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1674)
        %3596 = "ttir.reshape"(%3595) <{shape = [1 : i32, 16 : i32, 1280 : i32]}> : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1672)
        %3597 = "ttir.add"(%3596, %3578) : (tensor<1x16x1280xbf16>, tensor<1x16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1675)
        %3598 = "ttir.reshape"(%arg525) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3599 = "ttir.reshape"(%3598) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3600 = "ttir.reshape"(%arg524) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3601 = "ttir.reshape"(%3600) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3602 = "ttir.layer_norm"(%3597, %3599, %3601) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1676)
        %3603 = "ttir.reshape"(%3602) <{shape = [16 : i32, 1280 : i32]}> : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1677)
        %3604 = "ttir.reshape"(%arg529) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3605 = "ttir.reshape"(%3604) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3606 = "ttir.permute"(%3605) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1678)
        %3607 = "ttir.dot_general"(%3603, %3606) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1679)
        %3608 = "ttir.reshape"(%3607) <{shape = [1 : i32, 16 : i32, 20 : i32, 64 : i32]}> : (tensor<16x1280xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc1680)
        %3609 = "ttir.permute"(%3608) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x20x64xbf16>) -> tensor<1x20x16x64xbf16> loc(#loc1681)
        %3610 = "ttir.typecast"(%3609) <{conservative_folding = false}> : (tensor<1x20x16x64xbf16>) -> tensor<1x20x16x64xf32> loc(#loc1682)
        %3611 = "ttir.multiply"(%3610, %8) : (tensor<1x20x16x64xf32>, tensor<1x20x16x64xf32>) -> tensor<1x20x16x64xf32> loc(#loc1683)
        %3612 = "ttir.reshape"(%arg527) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3613 = "ttir.reshape"(%3612) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3614 = "ttir.reshape"(%arg526) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3615 = "ttir.reshape"(%3614) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3616 = "ttir.layer_norm"(%3524, %3613, %3615) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1684)
        %3617 = "ttir.concat"(%3616, %3602) <{dim = 1 : si32}> : (tensor<1x257x1280xbf16>, tensor<1x16x1280xbf16>) -> tensor<1x273x1280xbf16> loc(#loc1685)
        %3618 = "ttir.reshape"(%3617) <{shape = [273 : i32, 1280 : i32]}> : (tensor<1x273x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1686)
        %3619 = "ttir.reshape"(%arg528) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3620 = "ttir.reshape"(%3619) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3621 = "ttir.permute"(%3620) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1687)
        %3622 = "ttir.dot_general"(%3618, %3621) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1688)
        %3623 = "ttir.reshape"(%3622) <{shape = [1 : i32, 273 : i32, 20 : i32, 64 : i32]}> : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc1689)
        %3624 = "ttir.permute"(%3623) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc1690)
        %3625 = "ttir.typecast"(%3624) <{conservative_folding = false}> : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc1691)
        %3626 = "ttir.permute"(%3625) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x20x273x64xf32>) -> tensor<1x20x64x273xf32> loc(#loc1692)
        %3627 = "ttir.multiply"(%3626, %3) : (tensor<1x20x64x273xf32>, tensor<1x20x64x273xf32>) -> tensor<1x20x64x273xf32> loc(#loc1693)
        %3628 = "ttir.dot_general"(%3611, %3627) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x20x16x64xf32>, tensor<1x20x64x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc1694)
        %3629 = "ttir.typecast"(%3628) <{conservative_folding = false}> : (tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf64> loc(#loc1695)
        %3630 = "ttir.eq"(%3629, %2) : (tensor<1x20x16x273xf64>, tensor<1x20x16x273xf64>) -> tensor<1x20x16x273xi1> loc(#loc1695)
        %3631 = "ttir.logical_not"(%3630) : (tensor<1x20x16x273xi1>) -> tensor<1x20x16x273xi1> loc(#loc1696)
        %3632 = "ttir.reduce_or"(%3631) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x20x16x273xi1>) -> tensor<1x20x16xi1> loc(#loc1697)
        %3633 = "ttir.reshape"(%3632) <{shape = [1 : i32, 20 : i32, 16 : i32, 1 : i32]}> : (tensor<1x20x16xi1>) -> tensor<1x20x16x1xi1> loc(#loc1697)
        %3634 = "ttir.logical_not"(%3633) : (tensor<1x20x16x1xi1>) -> tensor<1x20x16x1xi1> loc(#loc1698)
        %3635 = "ttir.reshape"(%3634) <{shape = [1 : i32, 20 : i32, 16 : i32]}> : (tensor<1x20x16x1xi1>) -> tensor<1x20x16xi1> loc(#loc1699)
        %3636 = "ttir.reshape"(%3635) <{shape = [1 : i32, 20 : i32, 16 : i32, 1 : i32]}> : (tensor<1x20x16xi1>) -> tensor<1x20x16x1xi1> loc(#loc1699)
        %3637 = "ttir.broadcast"(%3636) <{broadcast_dimensions = array<i64: 1, 1, 1, 273>}> : (tensor<1x20x16x1xi1>) -> tensor<1x20x16x273xi1> loc(#loc1699)
        %3638 = "ttir.max"(%3628) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x20x16x273xf32>) -> tensor<1x20x16xf32> loc(#loc1700)
        %3639 = "ttir.reshape"(%3638) <{shape = [1 : i32, 20 : i32, 16 : i32, 1 : i32]}> : (tensor<1x20x16xf32>) -> tensor<1x20x16x1xf32> loc(#loc1700)
        %3640 = "ttir.broadcast"(%3639) <{broadcast_dimensions = array<i64: 1, 1, 1, 273>}> : (tensor<1x20x16x1xf32>) -> tensor<1x20x16x273xf32> loc(#loc1700)
        %3641 = "ttir.subtract"(%3628, %3640) : (tensor<1x20x16x273xf32>, tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc1700)
        %3642 = "ttir.exp"(%3641) : (tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc1700)
        %3643 = "ttir.sum"(%3642) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x20x16x273xf32>) -> tensor<1x20x16xf32> loc(#loc1700)
        %3644 = "ttir.reshape"(%3643) <{shape = [1 : i32, 20 : i32, 16 : i32, 1 : i32]}> : (tensor<1x20x16xf32>) -> tensor<1x20x16x1xf32> loc(#loc1700)
        %3645 = "ttir.broadcast"(%3644) <{broadcast_dimensions = array<i64: 1, 1, 1, 273>}> : (tensor<1x20x16x1xf32>) -> tensor<1x20x16x273xf32> loc(#loc1700)
        %3646 = "ttir.div"(%3642, %3645) : (tensor<1x20x16x273xf32>, tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc1700)
        %3647 = "ttir.where"(%3637, %1, %3646) : (tensor<1x20x16x273xi1>, tensor<1x20x16x273xf32>, tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc1701)
        %3648 = "ttir.reshape"(%arg523) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3649 = "ttir.reshape"(%3648) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3650 = "ttir.permute"(%3649) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1702)
        %3651 = "ttir.dot_general"(%3618, %3650) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1703)
        %3652 = "ttir.reshape"(%3651) <{shape = [1 : i32, 273 : i32, 20 : i32, 64 : i32]}> : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc1704)
        %3653 = "ttir.permute"(%3652) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc1705)
        %3654 = "ttir.typecast"(%3653) <{conservative_folding = false}> : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc1706)
        %3655 = "ttir.dot_general"(%3647, %3654) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x20x16x273xf32>, tensor<1x20x273x64xf32>) -> tensor<1x20x16x64xf32> loc(#loc1707)
        %3656 = "ttir.typecast"(%3655) <{conservative_folding = false}> : (tensor<1x20x16x64xf32>) -> tensor<1x20x16x64xbf16> loc(#loc1708)
        %3657 = "ttir.permute"(%3656) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x20x16x64xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc1709)
        %3658 = "ttir.reshape"(%3657) <{shape = [16 : i32, 1280 : i32]}> : (tensor<1x16x20x64xbf16>) -> tensor<16x1280xbf16> loc(#loc1710)
        %3659 = "ttir.reshape"(%arg522) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3660 = "ttir.reshape"(%3659) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3661 = "ttir.permute"(%3660) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1711)
        %3662 = "ttir.dot_general"(%3658, %3661) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1712)
        %3663 = "ttir.reshape"(%3662) <{shape = [1 : i32, 16 : i32, 1280 : i32]}> : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1713)
        %3664 = "ttir.div"(%3663, %0) : (tensor<1x16x1280xbf16>, tensor<1x16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1714)
        %3665 = "ttir.add"(%3664, %3597) : (tensor<1x16x1280xbf16>, tensor<1x16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1715)
        %3666 = "ttir.reshape"(%arg533) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3667 = "ttir.reshape"(%3666) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3668 = "ttir.reshape"(%arg532) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3669 = "ttir.reshape"(%3668) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3670 = "ttir.layer_norm"(%3665, %3667, %3669) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1716)
        %3671 = "ttir.reshape"(%3670) <{shape = [16 : i32, 1280 : i32]}> : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1717)
        %3672 = "ttir.reshape"(%arg531) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %3673 = "ttir.reshape"(%3672) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %3674 = "ttir.permute"(%3673) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1718)
        %3675 = "ttir.dot_general"(%3671, %3674) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<16x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc1719)
        %3676 = "ttir.reshape"(%3675) <{shape = [1 : i32, 16 : i32, 5120 : i32]}> : (tensor<16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc1717)
        %3677 = "ttir.gelu"(%3676) : (tensor<1x16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc1720)
        %3678 = "ttir.reshape"(%3677) <{shape = [16 : i32, 5120 : i32]}> : (tensor<1x16x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc1721)
        %3679 = "ttir.reshape"(%arg530) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %3680 = "ttir.reshape"(%3679) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %3681 = "ttir.permute"(%3680) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1722)
        %3682 = "ttir.dot_general"(%3678, %3681) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<16x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1723)
        %3683 = "ttir.reshape"(%3682) <{shape = [1 : i32, 16 : i32, 1280 : i32]}> : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1721)
        %3684 = "ttir.add"(%3683, %3665) : (tensor<1x16x1280xbf16>, tensor<1x16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1724)
        %3685 = "ttir.reshape"(%arg537) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3686 = "ttir.reshape"(%3685) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3687 = "ttir.reshape"(%arg536) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3688 = "ttir.reshape"(%3687) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3689 = "ttir.layer_norm"(%3684, %3686, %3688) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1725)
        %3690 = "ttir.reshape"(%3689) <{shape = [16 : i32, 1280 : i32]}> : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1726)
        %3691 = "ttir.reshape"(%arg541) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3692 = "ttir.reshape"(%3691) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3693 = "ttir.permute"(%3692) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1727)
        %3694 = "ttir.dot_general"(%3690, %3693) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1728)
        %3695 = "ttir.reshape"(%3694) <{shape = [1 : i32, 16 : i32, 20 : i32, 64 : i32]}> : (tensor<16x1280xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc1729)
        %3696 = "ttir.permute"(%3695) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x20x64xbf16>) -> tensor<1x20x16x64xbf16> loc(#loc1730)
        %3697 = "ttir.typecast"(%3696) <{conservative_folding = false}> : (tensor<1x20x16x64xbf16>) -> tensor<1x20x16x64xf32> loc(#loc1731)
        %3698 = "ttir.multiply"(%3697, %8) : (tensor<1x20x16x64xf32>, tensor<1x20x16x64xf32>) -> tensor<1x20x16x64xf32> loc(#loc1732)
        %3699 = "ttir.reshape"(%arg539) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3700 = "ttir.reshape"(%3699) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3701 = "ttir.reshape"(%arg538) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3702 = "ttir.reshape"(%3701) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3703 = "ttir.layer_norm"(%3524, %3700, %3702) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1733)
        %3704 = "ttir.concat"(%3703, %3689) <{dim = 1 : si32}> : (tensor<1x257x1280xbf16>, tensor<1x16x1280xbf16>) -> tensor<1x273x1280xbf16> loc(#loc1734)
        %3705 = "ttir.reshape"(%3704) <{shape = [273 : i32, 1280 : i32]}> : (tensor<1x273x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1735)
        %3706 = "ttir.reshape"(%arg540) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3707 = "ttir.reshape"(%3706) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3708 = "ttir.permute"(%3707) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1736)
        %3709 = "ttir.dot_general"(%3705, %3708) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1737)
        %3710 = "ttir.reshape"(%3709) <{shape = [1 : i32, 273 : i32, 20 : i32, 64 : i32]}> : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc1738)
        %3711 = "ttir.permute"(%3710) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc1739)
        %3712 = "ttir.typecast"(%3711) <{conservative_folding = false}> : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc1740)
        %3713 = "ttir.permute"(%3712) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x20x273x64xf32>) -> tensor<1x20x64x273xf32> loc(#loc1741)
        %3714 = "ttir.multiply"(%3713, %3) : (tensor<1x20x64x273xf32>, tensor<1x20x64x273xf32>) -> tensor<1x20x64x273xf32> loc(#loc1742)
        %3715 = "ttir.dot_general"(%3698, %3714) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x20x16x64xf32>, tensor<1x20x64x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc1743)
        %3716 = "ttir.typecast"(%3715) <{conservative_folding = false}> : (tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf64> loc(#loc1744)
        %3717 = "ttir.eq"(%3716, %2) : (tensor<1x20x16x273xf64>, tensor<1x20x16x273xf64>) -> tensor<1x20x16x273xi1> loc(#loc1744)
        %3718 = "ttir.logical_not"(%3717) : (tensor<1x20x16x273xi1>) -> tensor<1x20x16x273xi1> loc(#loc1745)
        %3719 = "ttir.reduce_or"(%3718) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x20x16x273xi1>) -> tensor<1x20x16xi1> loc(#loc1746)
        %3720 = "ttir.reshape"(%3719) <{shape = [1 : i32, 20 : i32, 16 : i32, 1 : i32]}> : (tensor<1x20x16xi1>) -> tensor<1x20x16x1xi1> loc(#loc1746)
        %3721 = "ttir.logical_not"(%3720) : (tensor<1x20x16x1xi1>) -> tensor<1x20x16x1xi1> loc(#loc1747)
        %3722 = "ttir.reshape"(%3721) <{shape = [1 : i32, 20 : i32, 16 : i32]}> : (tensor<1x20x16x1xi1>) -> tensor<1x20x16xi1> loc(#loc1748)
        %3723 = "ttir.reshape"(%3722) <{shape = [1 : i32, 20 : i32, 16 : i32, 1 : i32]}> : (tensor<1x20x16xi1>) -> tensor<1x20x16x1xi1> loc(#loc1748)
        %3724 = "ttir.broadcast"(%3723) <{broadcast_dimensions = array<i64: 1, 1, 1, 273>}> : (tensor<1x20x16x1xi1>) -> tensor<1x20x16x273xi1> loc(#loc1748)
        %3725 = "ttir.max"(%3715) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x20x16x273xf32>) -> tensor<1x20x16xf32> loc(#loc1749)
        %3726 = "ttir.reshape"(%3725) <{shape = [1 : i32, 20 : i32, 16 : i32, 1 : i32]}> : (tensor<1x20x16xf32>) -> tensor<1x20x16x1xf32> loc(#loc1749)
        %3727 = "ttir.broadcast"(%3726) <{broadcast_dimensions = array<i64: 1, 1, 1, 273>}> : (tensor<1x20x16x1xf32>) -> tensor<1x20x16x273xf32> loc(#loc1749)
        %3728 = "ttir.subtract"(%3715, %3727) : (tensor<1x20x16x273xf32>, tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc1749)
        %3729 = "ttir.exp"(%3728) : (tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc1749)
        %3730 = "ttir.sum"(%3729) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x20x16x273xf32>) -> tensor<1x20x16xf32> loc(#loc1749)
        %3731 = "ttir.reshape"(%3730) <{shape = [1 : i32, 20 : i32, 16 : i32, 1 : i32]}> : (tensor<1x20x16xf32>) -> tensor<1x20x16x1xf32> loc(#loc1749)
        %3732 = "ttir.broadcast"(%3731) <{broadcast_dimensions = array<i64: 1, 1, 1, 273>}> : (tensor<1x20x16x1xf32>) -> tensor<1x20x16x273xf32> loc(#loc1749)
        %3733 = "ttir.div"(%3729, %3732) : (tensor<1x20x16x273xf32>, tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc1749)
        %3734 = "ttir.where"(%3724, %1, %3733) : (tensor<1x20x16x273xi1>, tensor<1x20x16x273xf32>, tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc1750)
        %3735 = "ttir.reshape"(%arg535) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3736 = "ttir.reshape"(%3735) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3737 = "ttir.permute"(%3736) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1751)
        %3738 = "ttir.dot_general"(%3705, %3737) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1752)
        %3739 = "ttir.reshape"(%3738) <{shape = [1 : i32, 273 : i32, 20 : i32, 64 : i32]}> : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc1753)
        %3740 = "ttir.permute"(%3739) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc1754)
        %3741 = "ttir.typecast"(%3740) <{conservative_folding = false}> : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc1755)
        %3742 = "ttir.dot_general"(%3734, %3741) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x20x16x273xf32>, tensor<1x20x273x64xf32>) -> tensor<1x20x16x64xf32> loc(#loc1756)
        %3743 = "ttir.typecast"(%3742) <{conservative_folding = false}> : (tensor<1x20x16x64xf32>) -> tensor<1x20x16x64xbf16> loc(#loc1757)
        %3744 = "ttir.permute"(%3743) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x20x16x64xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc1758)
        %3745 = "ttir.reshape"(%3744) <{shape = [16 : i32, 1280 : i32]}> : (tensor<1x16x20x64xbf16>) -> tensor<16x1280xbf16> loc(#loc1759)
        %3746 = "ttir.reshape"(%arg534) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3747 = "ttir.reshape"(%3746) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3748 = "ttir.permute"(%3747) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1760)
        %3749 = "ttir.dot_general"(%3745, %3748) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1761)
        %3750 = "ttir.reshape"(%3749) <{shape = [1 : i32, 16 : i32, 1280 : i32]}> : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1762)
        %3751 = "ttir.div"(%3750, %0) : (tensor<1x16x1280xbf16>, tensor<1x16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1763)
        %3752 = "ttir.add"(%3751, %3684) : (tensor<1x16x1280xbf16>, tensor<1x16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1764)
        %3753 = "ttir.reshape"(%arg545) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3754 = "ttir.reshape"(%3753) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3755 = "ttir.reshape"(%arg544) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3756 = "ttir.reshape"(%3755) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3757 = "ttir.layer_norm"(%3752, %3754, %3756) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1765)
        %3758 = "ttir.reshape"(%3757) <{shape = [16 : i32, 1280 : i32]}> : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1766)
        %3759 = "ttir.reshape"(%arg543) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %3760 = "ttir.reshape"(%3759) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %3761 = "ttir.permute"(%3760) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1767)
        %3762 = "ttir.dot_general"(%3758, %3761) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<16x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc1768)
        %3763 = "ttir.reshape"(%3762) <{shape = [1 : i32, 16 : i32, 5120 : i32]}> : (tensor<16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc1766)
        %3764 = "ttir.gelu"(%3763) : (tensor<1x16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc1769)
        %3765 = "ttir.reshape"(%3764) <{shape = [16 : i32, 5120 : i32]}> : (tensor<1x16x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc1770)
        %3766 = "ttir.reshape"(%arg542) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %3767 = "ttir.reshape"(%3766) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %3768 = "ttir.permute"(%3767) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1771)
        %3769 = "ttir.dot_general"(%3765, %3768) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<16x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1772)
        %3770 = "ttir.reshape"(%3769) <{shape = [1 : i32, 16 : i32, 1280 : i32]}> : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1770)
        %3771 = "ttir.add"(%3770, %3752) : (tensor<1x16x1280xbf16>, tensor<1x16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1773)
        %3772 = "ttir.reshape"(%arg549) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3773 = "ttir.reshape"(%3772) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3774 = "ttir.reshape"(%arg548) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3775 = "ttir.reshape"(%3774) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3776 = "ttir.layer_norm"(%3771, %3773, %3775) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1774)
        %3777 = "ttir.reshape"(%3776) <{shape = [16 : i32, 1280 : i32]}> : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1775)
        %3778 = "ttir.reshape"(%arg553) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3779 = "ttir.reshape"(%3778) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3780 = "ttir.permute"(%3779) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1776)
        %3781 = "ttir.dot_general"(%3777, %3780) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1777)
        %3782 = "ttir.reshape"(%3781) <{shape = [1 : i32, 16 : i32, 20 : i32, 64 : i32]}> : (tensor<16x1280xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc1778)
        %3783 = "ttir.permute"(%3782) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x20x64xbf16>) -> tensor<1x20x16x64xbf16> loc(#loc1779)
        %3784 = "ttir.typecast"(%3783) <{conservative_folding = false}> : (tensor<1x20x16x64xbf16>) -> tensor<1x20x16x64xf32> loc(#loc1780)
        %3785 = "ttir.multiply"(%3784, %8) : (tensor<1x20x16x64xf32>, tensor<1x20x16x64xf32>) -> tensor<1x20x16x64xf32> loc(#loc1781)
        %3786 = "ttir.reshape"(%arg551) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3787 = "ttir.reshape"(%3786) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3788 = "ttir.reshape"(%arg550) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3789 = "ttir.reshape"(%3788) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3790 = "ttir.layer_norm"(%3524, %3787, %3789) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1782)
        %3791 = "ttir.concat"(%3790, %3776) <{dim = 1 : si32}> : (tensor<1x257x1280xbf16>, tensor<1x16x1280xbf16>) -> tensor<1x273x1280xbf16> loc(#loc1783)
        %3792 = "ttir.reshape"(%3791) <{shape = [273 : i32, 1280 : i32]}> : (tensor<1x273x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1784)
        %3793 = "ttir.reshape"(%arg552) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3794 = "ttir.reshape"(%3793) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3795 = "ttir.permute"(%3794) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1785)
        %3796 = "ttir.dot_general"(%3792, %3795) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1786)
        %3797 = "ttir.reshape"(%3796) <{shape = [1 : i32, 273 : i32, 20 : i32, 64 : i32]}> : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc1787)
        %3798 = "ttir.permute"(%3797) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc1788)
        %3799 = "ttir.typecast"(%3798) <{conservative_folding = false}> : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc1789)
        %3800 = "ttir.permute"(%3799) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x20x273x64xf32>) -> tensor<1x20x64x273xf32> loc(#loc1790)
        %3801 = "ttir.multiply"(%3800, %3) : (tensor<1x20x64x273xf32>, tensor<1x20x64x273xf32>) -> tensor<1x20x64x273xf32> loc(#loc1791)
        %3802 = "ttir.dot_general"(%3785, %3801) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x20x16x64xf32>, tensor<1x20x64x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc1792)
        %3803 = "ttir.typecast"(%3802) <{conservative_folding = false}> : (tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf64> loc(#loc1793)
        %3804 = "ttir.eq"(%3803, %2) : (tensor<1x20x16x273xf64>, tensor<1x20x16x273xf64>) -> tensor<1x20x16x273xi1> loc(#loc1793)
        %3805 = "ttir.logical_not"(%3804) : (tensor<1x20x16x273xi1>) -> tensor<1x20x16x273xi1> loc(#loc1794)
        %3806 = "ttir.reduce_or"(%3805) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x20x16x273xi1>) -> tensor<1x20x16xi1> loc(#loc1795)
        %3807 = "ttir.reshape"(%3806) <{shape = [1 : i32, 20 : i32, 16 : i32, 1 : i32]}> : (tensor<1x20x16xi1>) -> tensor<1x20x16x1xi1> loc(#loc1795)
        %3808 = "ttir.logical_not"(%3807) : (tensor<1x20x16x1xi1>) -> tensor<1x20x16x1xi1> loc(#loc1796)
        %3809 = "ttir.reshape"(%3808) <{shape = [1 : i32, 20 : i32, 16 : i32]}> : (tensor<1x20x16x1xi1>) -> tensor<1x20x16xi1> loc(#loc1797)
        %3810 = "ttir.reshape"(%3809) <{shape = [1 : i32, 20 : i32, 16 : i32, 1 : i32]}> : (tensor<1x20x16xi1>) -> tensor<1x20x16x1xi1> loc(#loc1797)
        %3811 = "ttir.broadcast"(%3810) <{broadcast_dimensions = array<i64: 1, 1, 1, 273>}> : (tensor<1x20x16x1xi1>) -> tensor<1x20x16x273xi1> loc(#loc1797)
        %3812 = "ttir.max"(%3802) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x20x16x273xf32>) -> tensor<1x20x16xf32> loc(#loc1798)
        %3813 = "ttir.reshape"(%3812) <{shape = [1 : i32, 20 : i32, 16 : i32, 1 : i32]}> : (tensor<1x20x16xf32>) -> tensor<1x20x16x1xf32> loc(#loc1798)
        %3814 = "ttir.broadcast"(%3813) <{broadcast_dimensions = array<i64: 1, 1, 1, 273>}> : (tensor<1x20x16x1xf32>) -> tensor<1x20x16x273xf32> loc(#loc1798)
        %3815 = "ttir.subtract"(%3802, %3814) : (tensor<1x20x16x273xf32>, tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc1798)
        %3816 = "ttir.exp"(%3815) : (tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc1798)
        %3817 = "ttir.sum"(%3816) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x20x16x273xf32>) -> tensor<1x20x16xf32> loc(#loc1798)
        %3818 = "ttir.reshape"(%3817) <{shape = [1 : i32, 20 : i32, 16 : i32, 1 : i32]}> : (tensor<1x20x16xf32>) -> tensor<1x20x16x1xf32> loc(#loc1798)
        %3819 = "ttir.broadcast"(%3818) <{broadcast_dimensions = array<i64: 1, 1, 1, 273>}> : (tensor<1x20x16x1xf32>) -> tensor<1x20x16x273xf32> loc(#loc1798)
        %3820 = "ttir.div"(%3816, %3819) : (tensor<1x20x16x273xf32>, tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc1798)
        %3821 = "ttir.where"(%3811, %1, %3820) : (tensor<1x20x16x273xi1>, tensor<1x20x16x273xf32>, tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc1799)
        %3822 = "ttir.reshape"(%arg547) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3823 = "ttir.reshape"(%3822) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3824 = "ttir.permute"(%3823) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1800)
        %3825 = "ttir.dot_general"(%3792, %3824) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1801)
        %3826 = "ttir.reshape"(%3825) <{shape = [1 : i32, 273 : i32, 20 : i32, 64 : i32]}> : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc1802)
        %3827 = "ttir.permute"(%3826) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc1803)
        %3828 = "ttir.typecast"(%3827) <{conservative_folding = false}> : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc1804)
        %3829 = "ttir.dot_general"(%3821, %3828) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x20x16x273xf32>, tensor<1x20x273x64xf32>) -> tensor<1x20x16x64xf32> loc(#loc1805)
        %3830 = "ttir.typecast"(%3829) <{conservative_folding = false}> : (tensor<1x20x16x64xf32>) -> tensor<1x20x16x64xbf16> loc(#loc1806)
        %3831 = "ttir.permute"(%3830) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x20x16x64xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc1807)
        %3832 = "ttir.reshape"(%3831) <{shape = [16 : i32, 1280 : i32]}> : (tensor<1x16x20x64xbf16>) -> tensor<16x1280xbf16> loc(#loc1808)
        %3833 = "ttir.reshape"(%arg546) <{shape = [1 : i32, 1280 : i32, 1280 : i32]}> : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
        %3834 = "ttir.reshape"(%3833) <{shape = [1280 : i32, 1280 : i32]}> : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
        %3835 = "ttir.permute"(%3834) <{permutation = array<i64: 1, 0>}> : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1809)
        %3836 = "ttir.dot_general"(%3832, %3835) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1810)
        %3837 = "ttir.reshape"(%3836) <{shape = [1 : i32, 16 : i32, 1280 : i32]}> : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1811)
        %3838 = "ttir.div"(%3837, %0) : (tensor<1x16x1280xbf16>, tensor<1x16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1812)
        %3839 = "ttir.add"(%3838, %3771) : (tensor<1x16x1280xbf16>, tensor<1x16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1813)
        %3840 = "ttir.reshape"(%arg557) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3841 = "ttir.reshape"(%3840) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3842 = "ttir.reshape"(%arg556) <{shape = [1 : i32, 1 : i32, 1280 : i32]}> : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
        %3843 = "ttir.reshape"(%3842) <{shape = [1280 : i32]}> : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
        %3844 = "ttir.layer_norm"(%3839, %3841, %3843) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 1280>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1814)
        %3845 = "ttir.reshape"(%3844) <{shape = [16 : i32, 1280 : i32]}> : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1815)
        %3846 = "ttir.reshape"(%arg555) <{shape = [1 : i32, 5120 : i32, 1280 : i32]}> : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
        %3847 = "ttir.reshape"(%3846) <{shape = [5120 : i32, 1280 : i32]}> : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
        %3848 = "ttir.permute"(%3847) <{permutation = array<i64: 1, 0>}> : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1816)
        %3849 = "ttir.dot_general"(%3845, %3848) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<16x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc1817)
        %3850 = "ttir.reshape"(%3849) <{shape = [1 : i32, 16 : i32, 5120 : i32]}> : (tensor<16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc1815)
        %3851 = "ttir.gelu"(%3850) : (tensor<1x16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc1818)
        %3852 = "ttir.reshape"(%3851) <{shape = [16 : i32, 5120 : i32]}> : (tensor<1x16x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc1819)
        %3853 = "ttir.reshape"(%arg554) <{shape = [1 : i32, 1280 : i32, 5120 : i32]}> : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
        %3854 = "ttir.reshape"(%3853) <{shape = [1280 : i32, 5120 : i32]}> : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
        %3855 = "ttir.permute"(%3854) <{permutation = array<i64: 1, 0>}> : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1820)
        %3856 = "ttir.dot_general"(%3852, %3855) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<16x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1821)
        %3857 = "ttir.reshape"(%3856) <{shape = [1 : i32, 16 : i32, 1280 : i32]}> : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1819)
        %3858 = "ttir.add"(%3857, %3839) : (tensor<1x16x1280xbf16>, tensor<1x16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1822)
        %3859 = "ttir.reshape"(%3858) <{shape = [16 : i32, 1280 : i32]}> : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1823)
        %3860 = "ttir.reshape"(%arg3) <{shape = [1 : i32, 2048 : i32, 1280 : i32]}> : (tensor<2048x1280xbf16>) -> tensor<1x2048x1280xbf16> loc(#loc2)
        %3861 = "ttir.reshape"(%3860) <{shape = [2048 : i32, 1280 : i32]}> : (tensor<1x2048x1280xbf16>) -> tensor<2048x1280xbf16> loc(#loc2)
        %3862 = "ttir.permute"(%3861) <{permutation = array<i64: 1, 0>}> : (tensor<2048x1280xbf16>) -> tensor<1280x2048xbf16> loc(#loc1824)
        %3863 = "ttir.dot_general"(%3859, %3862) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<16x1280xbf16>, tensor<1280x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1825)
        %3864 = "ttir.reshape"(%3863) <{shape = [1 : i32, 16 : i32, 2048 : i32]}> : (tensor<16x2048xbf16>) -> tensor<1x16x2048xbf16> loc(#loc1823)
        %3865 = "ttir.reshape"(%arg2) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc2)
        %3866 = "ttir.reshape"(%3865) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>) -> tensor<2048xbf16> loc(#loc2)
        %3867 = "ttir.reshape"(%3866) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc1826)
        %3868 = "ttir.broadcast"(%3867) <{broadcast_dimensions = array<i64: 1, 16, 1>}> : (tensor<1x1x2048xbf16>) -> tensor<1x16x2048xbf16> loc(#loc1826)
        %3869 = "ttir.add"(%3864, %3868) : (tensor<1x16x2048xbf16>, tensor<1x16x2048xbf16>) -> tensor<1x16x2048xbf16> loc(#loc1826)
        %3870 = "ttir.reshape"(%arg1) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc2)
        %3871 = "ttir.reshape"(%3870) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>) -> tensor<2048xbf16> loc(#loc2)
        %3872 = "ttir.reshape"(%arg0) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc2)
        %3873 = "ttir.reshape"(%3872) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>) -> tensor<2048xbf16> loc(#loc2)
        %3874 = "ttir.layer_norm"(%3869, %3871, %3873) <{epsilon = 9.99999974E-6 : f32, normalized_shape = array<i64: 2048>, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1x16x2048xbf16>, tensor<2048xbf16>, tensor<2048xbf16>) -> tensor<1x16x2048xbf16> loc(#loc1827)
        return %3874 : tensor<1x16x2048xbf16> loc(#loc)
      } loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("-1|unknown|unknown|-1|unknownaten__view")
#loc3 = loc("2901|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_335xla__mark_tensor")
#loc4 = loc("2904|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_194aten__view")
#loc5 = loc("2903|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|permute_355aten__permute")
#loc6 = loc("2904|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_194aten__mm")
#loc7 = loc("2909|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|view_129aten__view")
#loc8 = loc("2910|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|permute_358aten__permute")
#loc9 = loc("2915|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_296xla__cast")
#loc10 = loc("2918|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_200aten__mul")
#loc11 = loc("558|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPVisionEmbeddings[image_encoder.vision_model.embeddings]|Conv2d[image_encoder.vision_model.embeddings.patch_embedding]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:195|forward|202|convolutionaten__convolution_overrideable")
#loc12 = loc("559|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPVisionEmbeddings[image_encoder.vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:195|forward|203|viewaten__view")
#loc13 = loc("560|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPVisionEmbeddings[image_encoder.vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:195|forward|203|permuteaten__permute")
#loc14 = loc("562|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPVisionEmbeddings[image_encoder.vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:195|forward|206|cataten__cat")
#loc15 = loc("563|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPVisionEmbeddings[image_encoder.vision_model.embeddings]|Embedding[image_encoder.vision_model.embeddings.position_embedding]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:195|forward|210|embeddingaten__view")
#loc16 = loc("563|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPVisionEmbeddings[image_encoder.vision_model.embeddings]|Embedding[image_encoder.vision_model.embeddings.position_embedding]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:195|forward|210|embeddingaten__index_select")
#loc17 = loc("564|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPVisionEmbeddings[image_encoder.vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:195|forward|210|addaten__add")
#loc18 = loc("577|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|mark_tensor_3xla__mark_tensor")
#loc19 = loc("590|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_7xla__mark_tensor")
#loc20 = loc("592|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmulaten__view")
#loc21 = loc("591|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_1aten__permute")
#loc22 = loc("592|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmulaten__mm")
#loc23 = loc("593|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_5aten__add")
#loc24 = loc("600|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_1aten__view")
#loc25 = loc("601|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_4aten__permute")
#loc26 = loc("606|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_4xla__cast")
#loc27 = loc("609|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_4aten__mul")
#loc28 = loc("594|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_2aten__permute")
#loc29 = loc("595|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_1aten__mm")
#loc30 = loc("595|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_1aten__view")
#loc31 = loc("596|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_6aten__add")
#loc32 = loc("602|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_2aten__view")
#loc33 = loc("603|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_5aten__permute")
#loc34 = loc("607|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_5xla__cast")
#loc35 = loc("610|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_7aten__permute")
#loc36 = loc("611|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_5aten__mul")
#loc37 = loc("613|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmaxaten__einsum")
#loc38 = loc("614|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eqaten__eq")
#loc39 = loc("615|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_notaten__logical_not")
#loc40 = loc("616|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_1aten__any")
#loc41 = loc("617|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_1aten__logical_not")
#loc42 = loc("619|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|whereaten__expand")
#loc43 = loc("613|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmaxaten__softmax")
#loc44 = loc("619|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|whereaten__where")
#loc45 = loc("597|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_3aten__permute")
#loc46 = loc("598|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_2aten__mm")
#loc47 = loc("598|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_2aten__view")
#loc48 = loc("599|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_7aten__add")
#loc49 = loc("604|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_3aten__view")
#loc50 = loc("605|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_6aten__permute")
#loc51 = loc("608|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_6xla__cast")
#loc52 = loc("621|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_8aten__einsum")
#loc53 = loc("621|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_8xla__cast")
#loc54 = loc("623|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|cloneaten__permute")
#loc55 = loc("626|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_3aten__view")
#loc56 = loc("625|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_9aten__permute")
#loc57 = loc("626|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_3aten__mm")
#loc58 = loc("627|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_8aten__add")
#loc59 = loc("628|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_9aten__add")
#loc60 = loc("641|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_11xla__mark_tensor")
#loc61 = loc("643|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_4aten__view")
#loc62 = loc("642|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_10aten__permute")
#loc63 = loc("643|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_4aten__mm")
#loc64 = loc("644|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_12aten__add")
#loc65 = loc("647|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[0].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_13xla__mark_tensor")
#loc66 = loc("649|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_5aten__view")
#loc67 = loc("648|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_11aten__permute")
#loc68 = loc("649|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_5aten__mm")
#loc69 = loc("650|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_13aten__add")
#loc70 = loc("651|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_14aten__add")
#loc71 = loc("664|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_17xla__mark_tensor")
#loc72 = loc("666|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_6aten__view")
#loc73 = loc("665|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_12aten__permute")
#loc74 = loc("666|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_6aten__mm")
#loc75 = loc("667|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_17aten__add")
#loc76 = loc("674|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_5aten__view")
#loc77 = loc("675|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_15aten__permute")
#loc78 = loc("680|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_13xla__cast")
#loc79 = loc("683|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_10aten__mul")
#loc80 = loc("668|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_13aten__permute")
#loc81 = loc("669|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_7aten__mm")
#loc82 = loc("669|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_7aten__view")
#loc83 = loc("670|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_18aten__add")
#loc84 = loc("676|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_6aten__view")
#loc85 = loc("677|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_16aten__permute")
#loc86 = loc("681|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_14xla__cast")
#loc87 = loc("684|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_18aten__permute")
#loc88 = loc("685|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_11aten__mul")
#loc89 = loc("687|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_1aten__einsum")
#loc90 = loc("688|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_1aten__eq")
#loc91 = loc("689|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_2aten__logical_not")
#loc92 = loc("690|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_2aten__any")
#loc93 = loc("691|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_3aten__logical_not")
#loc94 = loc("693|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_1aten__expand")
#loc95 = loc("687|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_1aten__softmax")
#loc96 = loc("693|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_1aten__where")
#loc97 = loc("671|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_14aten__permute")
#loc98 = loc("672|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_8aten__mm")
#loc99 = loc("672|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_8aten__view")
#loc100 = loc("673|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_19aten__add")
#loc101 = loc("678|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_7aten__view")
#loc102 = loc("679|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_17aten__permute")
#loc103 = loc("682|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_15xla__cast")
#loc104 = loc("695|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_17aten__einsum")
#loc105 = loc("695|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_17xla__cast")
#loc106 = loc("697|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_1aten__permute")
#loc107 = loc("700|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_9aten__view")
#loc108 = loc("699|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_20aten__permute")
#loc109 = loc("700|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_9aten__mm")
#loc110 = loc("701|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_20aten__add")
#loc111 = loc("702|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_21aten__add")
#loc112 = loc("715|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_21xla__mark_tensor")
#loc113 = loc("717|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_10aten__view")
#loc114 = loc("716|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_21aten__permute")
#loc115 = loc("717|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_10aten__mm")
#loc116 = loc("718|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_24aten__add")
#loc117 = loc("721|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[1].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_23xla__mark_tensor")
#loc118 = loc("723|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_11aten__view")
#loc119 = loc("722|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_22aten__permute")
#loc120 = loc("723|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_11aten__mm")
#loc121 = loc("724|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_25aten__add")
#loc122 = loc("725|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_26aten__add")
#loc123 = loc("738|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_27xla__mark_tensor")
#loc124 = loc("740|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_12aten__view")
#loc125 = loc("739|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_23aten__permute")
#loc126 = loc("740|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_12aten__mm")
#loc127 = loc("741|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_29aten__add")
#loc128 = loc("748|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_9aten__view")
#loc129 = loc("749|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_26aten__permute")
#loc130 = loc("754|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_22xla__cast")
#loc131 = loc("757|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_16aten__mul")
#loc132 = loc("742|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_24aten__permute")
#loc133 = loc("743|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_13aten__mm")
#loc134 = loc("743|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_13aten__view")
#loc135 = loc("744|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_30aten__add")
#loc136 = loc("750|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_10aten__view")
#loc137 = loc("751|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_27aten__permute")
#loc138 = loc("755|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_23xla__cast")
#loc139 = loc("758|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_29aten__permute")
#loc140 = loc("759|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_17aten__mul")
#loc141 = loc("761|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_2aten__einsum")
#loc142 = loc("762|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_2aten__eq")
#loc143 = loc("763|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_4aten__logical_not")
#loc144 = loc("764|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_3aten__any")
#loc145 = loc("765|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_5aten__logical_not")
#loc146 = loc("767|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_2aten__expand")
#loc147 = loc("761|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_2aten__softmax")
#loc148 = loc("767|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_2aten__where")
#loc149 = loc("745|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_25aten__permute")
#loc150 = loc("746|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_14aten__mm")
#loc151 = loc("746|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_14aten__view")
#loc152 = loc("747|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_31aten__add")
#loc153 = loc("752|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_11aten__view")
#loc154 = loc("753|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_28aten__permute")
#loc155 = loc("756|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_24xla__cast")
#loc156 = loc("769|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_26aten__einsum")
#loc157 = loc("769|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_26xla__cast")
#loc158 = loc("771|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_2aten__permute")
#loc159 = loc("774|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_15aten__view")
#loc160 = loc("773|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_31aten__permute")
#loc161 = loc("774|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_15aten__mm")
#loc162 = loc("775|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_32aten__add")
#loc163 = loc("776|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_33aten__add")
#loc164 = loc("789|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_31xla__mark_tensor")
#loc165 = loc("791|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_16aten__view")
#loc166 = loc("790|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_32aten__permute")
#loc167 = loc("791|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_16aten__mm")
#loc168 = loc("792|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_36aten__add")
#loc169 = loc("795|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[2].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_33xla__mark_tensor")
#loc170 = loc("797|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_17aten__view")
#loc171 = loc("796|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_33aten__permute")
#loc172 = loc("797|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_17aten__mm")
#loc173 = loc("798|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_37aten__add")
#loc174 = loc("799|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_38aten__add")
#loc175 = loc("812|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_37xla__mark_tensor")
#loc176 = loc("814|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_18aten__view")
#loc177 = loc("813|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_34aten__permute")
#loc178 = loc("814|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_18aten__mm")
#loc179 = loc("815|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_41aten__add")
#loc180 = loc("822|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_13aten__view")
#loc181 = loc("823|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_37aten__permute")
#loc182 = loc("828|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_31xla__cast")
#loc183 = loc("831|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_22aten__mul")
#loc184 = loc("816|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_35aten__permute")
#loc185 = loc("817|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_19aten__mm")
#loc186 = loc("817|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_19aten__view")
#loc187 = loc("818|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_42aten__add")
#loc188 = loc("824|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_14aten__view")
#loc189 = loc("825|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_38aten__permute")
#loc190 = loc("829|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_32xla__cast")
#loc191 = loc("832|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_40aten__permute")
#loc192 = loc("833|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_23aten__mul")
#loc193 = loc("835|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_3aten__einsum")
#loc194 = loc("836|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_3aten__eq")
#loc195 = loc("837|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_6aten__logical_not")
#loc196 = loc("838|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_4aten__any")
#loc197 = loc("839|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_7aten__logical_not")
#loc198 = loc("841|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_3aten__expand")
#loc199 = loc("835|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_3aten__softmax")
#loc200 = loc("841|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_3aten__where")
#loc201 = loc("819|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_36aten__permute")
#loc202 = loc("820|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_20aten__mm")
#loc203 = loc("820|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_20aten__view")
#loc204 = loc("821|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_43aten__add")
#loc205 = loc("826|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_15aten__view")
#loc206 = loc("827|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_39aten__permute")
#loc207 = loc("830|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_33xla__cast")
#loc208 = loc("843|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_35aten__einsum")
#loc209 = loc("843|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_35xla__cast")
#loc210 = loc("845|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_3aten__permute")
#loc211 = loc("848|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_21aten__view")
#loc212 = loc("847|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_42aten__permute")
#loc213 = loc("848|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_21aten__mm")
#loc214 = loc("849|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_44aten__add")
#loc215 = loc("850|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_45aten__add")
#loc216 = loc("863|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_41xla__mark_tensor")
#loc217 = loc("865|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_22aten__view")
#loc218 = loc("864|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_43aten__permute")
#loc219 = loc("865|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_22aten__mm")
#loc220 = loc("866|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_48aten__add")
#loc221 = loc("869|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[3].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_43xla__mark_tensor")
#loc222 = loc("871|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_23aten__view")
#loc223 = loc("870|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_44aten__permute")
#loc224 = loc("871|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_23aten__mm")
#loc225 = loc("872|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_49aten__add")
#loc226 = loc("873|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_50aten__add")
#loc227 = loc("886|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_47xla__mark_tensor")
#loc228 = loc("888|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_24aten__view")
#loc229 = loc("887|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_45aten__permute")
#loc230 = loc("888|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_24aten__mm")
#loc231 = loc("889|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_53aten__add")
#loc232 = loc("896|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_17aten__view")
#loc233 = loc("897|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_48aten__permute")
#loc234 = loc("902|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_40xla__cast")
#loc235 = loc("905|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_28aten__mul")
#loc236 = loc("890|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_46aten__permute")
#loc237 = loc("891|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_25aten__mm")
#loc238 = loc("891|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_25aten__view")
#loc239 = loc("892|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_54aten__add")
#loc240 = loc("898|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_18aten__view")
#loc241 = loc("899|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_49aten__permute")
#loc242 = loc("903|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_41xla__cast")
#loc243 = loc("906|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_51aten__permute")
#loc244 = loc("907|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_29aten__mul")
#loc245 = loc("909|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_4aten__einsum")
#loc246 = loc("910|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_4aten__eq")
#loc247 = loc("911|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_8aten__logical_not")
#loc248 = loc("912|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_5aten__any")
#loc249 = loc("913|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_9aten__logical_not")
#loc250 = loc("915|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_4aten__expand")
#loc251 = loc("909|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_4aten__softmax")
#loc252 = loc("915|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_4aten__where")
#loc253 = loc("893|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_47aten__permute")
#loc254 = loc("894|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_26aten__mm")
#loc255 = loc("894|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_26aten__view")
#loc256 = loc("895|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_55aten__add")
#loc257 = loc("900|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_19aten__view")
#loc258 = loc("901|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_50aten__permute")
#loc259 = loc("904|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_42xla__cast")
#loc260 = loc("917|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_44aten__einsum")
#loc261 = loc("917|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_44xla__cast")
#loc262 = loc("919|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_4aten__permute")
#loc263 = loc("922|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_27aten__view")
#loc264 = loc("921|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_53aten__permute")
#loc265 = loc("922|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_27aten__mm")
#loc266 = loc("923|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_56aten__add")
#loc267 = loc("924|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_57aten__add")
#loc268 = loc("937|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_51xla__mark_tensor")
#loc269 = loc("939|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_28aten__view")
#loc270 = loc("938|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_54aten__permute")
#loc271 = loc("939|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_28aten__mm")
#loc272 = loc("940|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_60aten__add")
#loc273 = loc("943|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[4].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_53xla__mark_tensor")
#loc274 = loc("945|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_29aten__view")
#loc275 = loc("944|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_55aten__permute")
#loc276 = loc("945|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_29aten__mm")
#loc277 = loc("946|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_61aten__add")
#loc278 = loc("947|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_62aten__add")
#loc279 = loc("960|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_57xla__mark_tensor")
#loc280 = loc("962|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_30aten__view")
#loc281 = loc("961|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_56aten__permute")
#loc282 = loc("962|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_30aten__mm")
#loc283 = loc("963|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_65aten__add")
#loc284 = loc("970|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_21aten__view")
#loc285 = loc("971|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_59aten__permute")
#loc286 = loc("976|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_49xla__cast")
#loc287 = loc("979|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_34aten__mul")
#loc288 = loc("964|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_57aten__permute")
#loc289 = loc("965|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_31aten__mm")
#loc290 = loc("965|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_31aten__view")
#loc291 = loc("966|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_66aten__add")
#loc292 = loc("972|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_22aten__view")
#loc293 = loc("973|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_60aten__permute")
#loc294 = loc("977|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_50xla__cast")
#loc295 = loc("980|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_62aten__permute")
#loc296 = loc("981|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_35aten__mul")
#loc297 = loc("983|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_5aten__einsum")
#loc298 = loc("984|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_5aten__eq")
#loc299 = loc("985|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_10aten__logical_not")
#loc300 = loc("986|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_6aten__any")
#loc301 = loc("987|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_11aten__logical_not")
#loc302 = loc("989|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_5aten__expand")
#loc303 = loc("983|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_5aten__softmax")
#loc304 = loc("989|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_5aten__where")
#loc305 = loc("967|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_58aten__permute")
#loc306 = loc("968|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_32aten__mm")
#loc307 = loc("968|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_32aten__view")
#loc308 = loc("969|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_67aten__add")
#loc309 = loc("974|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_23aten__view")
#loc310 = loc("975|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_61aten__permute")
#loc311 = loc("978|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_51xla__cast")
#loc312 = loc("991|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_53aten__einsum")
#loc313 = loc("991|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_53xla__cast")
#loc314 = loc("993|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_5aten__permute")
#loc315 = loc("996|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_33aten__view")
#loc316 = loc("995|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_64aten__permute")
#loc317 = loc("996|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_33aten__mm")
#loc318 = loc("997|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_68aten__add")
#loc319 = loc("998|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_69aten__add")
#loc320 = loc("1011|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_61xla__mark_tensor")
#loc321 = loc("1013|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_34aten__view")
#loc322 = loc("1012|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_65aten__permute")
#loc323 = loc("1013|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_34aten__mm")
#loc324 = loc("1014|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_72aten__add")
#loc325 = loc("1017|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[5].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_63xla__mark_tensor")
#loc326 = loc("1019|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_35aten__view")
#loc327 = loc("1018|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_66aten__permute")
#loc328 = loc("1019|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_35aten__mm")
#loc329 = loc("1020|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_73aten__add")
#loc330 = loc("1021|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_74aten__add")
#loc331 = loc("1034|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_67xla__mark_tensor")
#loc332 = loc("1036|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_36aten__view")
#loc333 = loc("1035|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_67aten__permute")
#loc334 = loc("1036|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_36aten__mm")
#loc335 = loc("1037|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_77aten__add")
#loc336 = loc("1044|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_25aten__view")
#loc337 = loc("1045|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_70aten__permute")
#loc338 = loc("1050|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_58xla__cast")
#loc339 = loc("1053|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_40aten__mul")
#loc340 = loc("1038|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_68aten__permute")
#loc341 = loc("1039|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_37aten__mm")
#loc342 = loc("1039|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_37aten__view")
#loc343 = loc("1040|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_78aten__add")
#loc344 = loc("1046|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_26aten__view")
#loc345 = loc("1047|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_71aten__permute")
#loc346 = loc("1051|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_59xla__cast")
#loc347 = loc("1054|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_73aten__permute")
#loc348 = loc("1055|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_41aten__mul")
#loc349 = loc("1057|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_6aten__einsum")
#loc350 = loc("1058|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_6aten__eq")
#loc351 = loc("1059|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_12aten__logical_not")
#loc352 = loc("1060|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_7aten__any")
#loc353 = loc("1061|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_13aten__logical_not")
#loc354 = loc("1063|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_6aten__expand")
#loc355 = loc("1057|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_6aten__softmax")
#loc356 = loc("1063|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_6aten__where")
#loc357 = loc("1041|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_69aten__permute")
#loc358 = loc("1042|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_38aten__mm")
#loc359 = loc("1042|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_38aten__view")
#loc360 = loc("1043|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_79aten__add")
#loc361 = loc("1048|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_27aten__view")
#loc362 = loc("1049|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_72aten__permute")
#loc363 = loc("1052|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_60xla__cast")
#loc364 = loc("1065|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_62aten__einsum")
#loc365 = loc("1065|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_62xla__cast")
#loc366 = loc("1067|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_6aten__permute")
#loc367 = loc("1070|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_39aten__view")
#loc368 = loc("1069|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_75aten__permute")
#loc369 = loc("1070|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_39aten__mm")
#loc370 = loc("1071|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_80aten__add")
#loc371 = loc("1072|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_81aten__add")
#loc372 = loc("1085|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_71xla__mark_tensor")
#loc373 = loc("1087|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_40aten__view")
#loc374 = loc("1086|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_76aten__permute")
#loc375 = loc("1087|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_40aten__mm")
#loc376 = loc("1088|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_84aten__add")
#loc377 = loc("1091|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[6].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_73xla__mark_tensor")
#loc378 = loc("1093|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_41aten__view")
#loc379 = loc("1092|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_77aten__permute")
#loc380 = loc("1093|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_41aten__mm")
#loc381 = loc("1094|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_85aten__add")
#loc382 = loc("1095|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_86aten__add")
#loc383 = loc("1108|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_77xla__mark_tensor")
#loc384 = loc("1110|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_42aten__view")
#loc385 = loc("1109|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_78aten__permute")
#loc386 = loc("1110|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_42aten__mm")
#loc387 = loc("1111|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_89aten__add")
#loc388 = loc("1118|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_29aten__view")
#loc389 = loc("1119|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_81aten__permute")
#loc390 = loc("1124|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_67xla__cast")
#loc391 = loc("1127|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_46aten__mul")
#loc392 = loc("1112|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_79aten__permute")
#loc393 = loc("1113|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_43aten__mm")
#loc394 = loc("1113|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_43aten__view")
#loc395 = loc("1114|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_90aten__add")
#loc396 = loc("1120|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_30aten__view")
#loc397 = loc("1121|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_82aten__permute")
#loc398 = loc("1125|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_68xla__cast")
#loc399 = loc("1128|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_84aten__permute")
#loc400 = loc("1129|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_47aten__mul")
#loc401 = loc("1131|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_7aten__einsum")
#loc402 = loc("1132|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_7aten__eq")
#loc403 = loc("1133|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_14aten__logical_not")
#loc404 = loc("1134|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_8aten__any")
#loc405 = loc("1135|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_15aten__logical_not")
#loc406 = loc("1137|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_7aten__expand")
#loc407 = loc("1131|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_7aten__softmax")
#loc408 = loc("1137|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_7aten__where")
#loc409 = loc("1115|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_80aten__permute")
#loc410 = loc("1116|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_44aten__mm")
#loc411 = loc("1116|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_44aten__view")
#loc412 = loc("1117|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_91aten__add")
#loc413 = loc("1122|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_31aten__view")
#loc414 = loc("1123|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_83aten__permute")
#loc415 = loc("1126|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_69xla__cast")
#loc416 = loc("1139|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_71aten__einsum")
#loc417 = loc("1139|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_71xla__cast")
#loc418 = loc("1141|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_7aten__permute")
#loc419 = loc("1144|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_45aten__view")
#loc420 = loc("1143|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_86aten__permute")
#loc421 = loc("1144|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_45aten__mm")
#loc422 = loc("1145|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_92aten__add")
#loc423 = loc("1146|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_93aten__add")
#loc424 = loc("1159|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_81xla__mark_tensor")
#loc425 = loc("1161|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_46aten__view")
#loc426 = loc("1160|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_87aten__permute")
#loc427 = loc("1161|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_46aten__mm")
#loc428 = loc("1162|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_96aten__add")
#loc429 = loc("1165|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[7].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_83xla__mark_tensor")
#loc430 = loc("1167|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_47aten__view")
#loc431 = loc("1166|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_88aten__permute")
#loc432 = loc("1167|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_47aten__mm")
#loc433 = loc("1168|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_97aten__add")
#loc434 = loc("1169|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_98aten__add")
#loc435 = loc("1182|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_87xla__mark_tensor")
#loc436 = loc("1184|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_48aten__view")
#loc437 = loc("1183|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_89aten__permute")
#loc438 = loc("1184|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_48aten__mm")
#loc439 = loc("1185|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_101aten__add")
#loc440 = loc("1192|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_33aten__view")
#loc441 = loc("1193|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_92aten__permute")
#loc442 = loc("1198|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_76xla__cast")
#loc443 = loc("1201|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_52aten__mul")
#loc444 = loc("1186|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_90aten__permute")
#loc445 = loc("1187|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_49aten__mm")
#loc446 = loc("1187|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_49aten__view")
#loc447 = loc("1188|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_102aten__add")
#loc448 = loc("1194|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_34aten__view")
#loc449 = loc("1195|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_93aten__permute")
#loc450 = loc("1199|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_77xla__cast")
#loc451 = loc("1202|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_95aten__permute")
#loc452 = loc("1203|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_53aten__mul")
#loc453 = loc("1205|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_8aten__einsum")
#loc454 = loc("1206|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_8aten__eq")
#loc455 = loc("1207|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_16aten__logical_not")
#loc456 = loc("1208|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_9aten__any")
#loc457 = loc("1209|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_17aten__logical_not")
#loc458 = loc("1211|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_8aten__expand")
#loc459 = loc("1205|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_8aten__softmax")
#loc460 = loc("1211|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_8aten__where")
#loc461 = loc("1189|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_91aten__permute")
#loc462 = loc("1190|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_50aten__mm")
#loc463 = loc("1190|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_50aten__view")
#loc464 = loc("1191|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_103aten__add")
#loc465 = loc("1196|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_35aten__view")
#loc466 = loc("1197|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_94aten__permute")
#loc467 = loc("1200|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_78xla__cast")
#loc468 = loc("1213|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_80aten__einsum")
#loc469 = loc("1213|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_80xla__cast")
#loc470 = loc("1215|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_8aten__permute")
#loc471 = loc("1218|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_51aten__view")
#loc472 = loc("1217|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_97aten__permute")
#loc473 = loc("1218|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_51aten__mm")
#loc474 = loc("1219|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_104aten__add")
#loc475 = loc("1220|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_105aten__add")
#loc476 = loc("1233|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_91xla__mark_tensor")
#loc477 = loc("1235|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_52aten__view")
#loc478 = loc("1234|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_98aten__permute")
#loc479 = loc("1235|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_52aten__mm")
#loc480 = loc("1236|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_108aten__add")
#loc481 = loc("1239|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[8].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_93xla__mark_tensor")
#loc482 = loc("1241|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_53aten__view")
#loc483 = loc("1240|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_99aten__permute")
#loc484 = loc("1241|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_53aten__mm")
#loc485 = loc("1242|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_109aten__add")
#loc486 = loc("1243|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_110aten__add")
#loc487 = loc("1256|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_97xla__mark_tensor")
#loc488 = loc("1258|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_54aten__view")
#loc489 = loc("1257|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_100aten__permute")
#loc490 = loc("1258|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_54aten__mm")
#loc491 = loc("1259|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_113aten__add")
#loc492 = loc("1266|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_37aten__view")
#loc493 = loc("1267|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_103aten__permute")
#loc494 = loc("1272|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_85xla__cast")
#loc495 = loc("1275|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_58aten__mul")
#loc496 = loc("1260|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_101aten__permute")
#loc497 = loc("1261|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_55aten__mm")
#loc498 = loc("1261|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_55aten__view")
#loc499 = loc("1262|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_114aten__add")
#loc500 = loc("1268|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_38aten__view")
#loc501 = loc("1269|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_104aten__permute")
#loc502 = loc("1273|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_86xla__cast")
#loc503 = loc("1276|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_106aten__permute")
#loc504 = loc("1277|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_59aten__mul")
#loc505 = loc("1279|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_9aten__einsum")
#loc506 = loc("1280|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_9aten__eq")
#loc507 = loc("1281|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_18aten__logical_not")
#loc508 = loc("1282|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_10aten__any")
#loc509 = loc("1283|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_19aten__logical_not")
#loc510 = loc("1285|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_9aten__expand")
#loc511 = loc("1279|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_9aten__softmax")
#loc512 = loc("1285|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_9aten__where")
#loc513 = loc("1263|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_102aten__permute")
#loc514 = loc("1264|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_56aten__mm")
#loc515 = loc("1264|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_56aten__view")
#loc516 = loc("1265|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_115aten__add")
#loc517 = loc("1270|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_39aten__view")
#loc518 = loc("1271|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_105aten__permute")
#loc519 = loc("1274|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_87xla__cast")
#loc520 = loc("1287|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_89aten__einsum")
#loc521 = loc("1287|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_89xla__cast")
#loc522 = loc("1289|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_9aten__permute")
#loc523 = loc("1292|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_57aten__view")
#loc524 = loc("1291|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_108aten__permute")
#loc525 = loc("1292|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_57aten__mm")
#loc526 = loc("1293|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_116aten__add")
#loc527 = loc("1294|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_117aten__add")
#loc528 = loc("1307|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_101xla__mark_tensor")
#loc529 = loc("1309|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_58aten__view")
#loc530 = loc("1308|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_109aten__permute")
#loc531 = loc("1309|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_58aten__mm")
#loc532 = loc("1310|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_120aten__add")
#loc533 = loc("1313|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[9].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_103xla__mark_tensor")
#loc534 = loc("1315|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_59aten__view")
#loc535 = loc("1314|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_110aten__permute")
#loc536 = loc("1315|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_59aten__mm")
#loc537 = loc("1316|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_121aten__add")
#loc538 = loc("1317|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_122aten__add")
#loc539 = loc("1330|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_107xla__mark_tensor")
#loc540 = loc("1332|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_60aten__view")
#loc541 = loc("1331|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_111aten__permute")
#loc542 = loc("1332|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_60aten__mm")
#loc543 = loc("1333|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_125aten__add")
#loc544 = loc("1340|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_41aten__view")
#loc545 = loc("1341|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_114aten__permute")
#loc546 = loc("1346|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_94xla__cast")
#loc547 = loc("1349|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_64aten__mul")
#loc548 = loc("1334|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_112aten__permute")
#loc549 = loc("1335|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_61aten__mm")
#loc550 = loc("1335|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_61aten__view")
#loc551 = loc("1336|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_126aten__add")
#loc552 = loc("1342|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_42aten__view")
#loc553 = loc("1343|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_115aten__permute")
#loc554 = loc("1347|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_95xla__cast")
#loc555 = loc("1350|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_117aten__permute")
#loc556 = loc("1351|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_65aten__mul")
#loc557 = loc("1353|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_10aten__einsum")
#loc558 = loc("1354|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_10aten__eq")
#loc559 = loc("1355|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_20aten__logical_not")
#loc560 = loc("1356|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_11aten__any")
#loc561 = loc("1357|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_21aten__logical_not")
#loc562 = loc("1359|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_10aten__expand")
#loc563 = loc("1353|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_10aten__softmax")
#loc564 = loc("1359|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_10aten__where")
#loc565 = loc("1337|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_113aten__permute")
#loc566 = loc("1338|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_62aten__mm")
#loc567 = loc("1338|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_62aten__view")
#loc568 = loc("1339|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_127aten__add")
#loc569 = loc("1344|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_43aten__view")
#loc570 = loc("1345|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_116aten__permute")
#loc571 = loc("1348|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_96xla__cast")
#loc572 = loc("1361|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_98aten__einsum")
#loc573 = loc("1361|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_98xla__cast")
#loc574 = loc("1363|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_10aten__permute")
#loc575 = loc("1366|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_63aten__view")
#loc576 = loc("1365|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_119aten__permute")
#loc577 = loc("1366|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_63aten__mm")
#loc578 = loc("1367|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_128aten__add")
#loc579 = loc("1368|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_129aten__add")
#loc580 = loc("1381|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_111xla__mark_tensor")
#loc581 = loc("1383|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_64aten__view")
#loc582 = loc("1382|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_120aten__permute")
#loc583 = loc("1383|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_64aten__mm")
#loc584 = loc("1384|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_132aten__add")
#loc585 = loc("1387|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[10].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_113xla__mark_tensor")
#loc586 = loc("1389|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_65aten__view")
#loc587 = loc("1388|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_121aten__permute")
#loc588 = loc("1389|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_65aten__mm")
#loc589 = loc("1390|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_133aten__add")
#loc590 = loc("1391|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_134aten__add")
#loc591 = loc("1404|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_117xla__mark_tensor")
#loc592 = loc("1406|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_66aten__view")
#loc593 = loc("1405|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_122aten__permute")
#loc594 = loc("1406|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_66aten__mm")
#loc595 = loc("1407|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_137aten__add")
#loc596 = loc("1414|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_45aten__view")
#loc597 = loc("1415|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_125aten__permute")
#loc598 = loc("1420|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_103xla__cast")
#loc599 = loc("1423|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_70aten__mul")
#loc600 = loc("1408|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_123aten__permute")
#loc601 = loc("1409|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_67aten__mm")
#loc602 = loc("1409|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_67aten__view")
#loc603 = loc("1410|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_138aten__add")
#loc604 = loc("1416|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_46aten__view")
#loc605 = loc("1417|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_126aten__permute")
#loc606 = loc("1421|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_104xla__cast")
#loc607 = loc("1424|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_128aten__permute")
#loc608 = loc("1425|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_71aten__mul")
#loc609 = loc("1427|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_11aten__einsum")
#loc610 = loc("1428|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_11aten__eq")
#loc611 = loc("1429|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_22aten__logical_not")
#loc612 = loc("1430|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_12aten__any")
#loc613 = loc("1431|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_23aten__logical_not")
#loc614 = loc("1433|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_11aten__expand")
#loc615 = loc("1427|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_11aten__softmax")
#loc616 = loc("1433|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_11aten__where")
#loc617 = loc("1411|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_124aten__permute")
#loc618 = loc("1412|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_68aten__mm")
#loc619 = loc("1412|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_68aten__view")
#loc620 = loc("1413|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_139aten__add")
#loc621 = loc("1418|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_47aten__view")
#loc622 = loc("1419|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_127aten__permute")
#loc623 = loc("1422|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_105xla__cast")
#loc624 = loc("1435|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_107aten__einsum")
#loc625 = loc("1435|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_107xla__cast")
#loc626 = loc("1437|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_11aten__permute")
#loc627 = loc("1440|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_69aten__view")
#loc628 = loc("1439|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_130aten__permute")
#loc629 = loc("1440|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_69aten__mm")
#loc630 = loc("1441|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_140aten__add")
#loc631 = loc("1442|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_141aten__add")
#loc632 = loc("1455|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_121xla__mark_tensor")
#loc633 = loc("1457|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_70aten__view")
#loc634 = loc("1456|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_131aten__permute")
#loc635 = loc("1457|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_70aten__mm")
#loc636 = loc("1458|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_144aten__add")
#loc637 = loc("1461|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[11].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_123xla__mark_tensor")
#loc638 = loc("1463|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_71aten__view")
#loc639 = loc("1462|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_132aten__permute")
#loc640 = loc("1463|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_71aten__mm")
#loc641 = loc("1464|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_145aten__add")
#loc642 = loc("1465|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_146aten__add")
#loc643 = loc("1478|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_127xla__mark_tensor")
#loc644 = loc("1480|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_72aten__view")
#loc645 = loc("1479|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_133aten__permute")
#loc646 = loc("1480|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_72aten__mm")
#loc647 = loc("1481|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_149aten__add")
#loc648 = loc("1488|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_49aten__view")
#loc649 = loc("1489|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_136aten__permute")
#loc650 = loc("1494|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_112xla__cast")
#loc651 = loc("1497|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_76aten__mul")
#loc652 = loc("1482|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_134aten__permute")
#loc653 = loc("1483|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_73aten__mm")
#loc654 = loc("1483|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_73aten__view")
#loc655 = loc("1484|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_150aten__add")
#loc656 = loc("1490|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_50aten__view")
#loc657 = loc("1491|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_137aten__permute")
#loc658 = loc("1495|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_113xla__cast")
#loc659 = loc("1498|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_139aten__permute")
#loc660 = loc("1499|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_77aten__mul")
#loc661 = loc("1501|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_12aten__einsum")
#loc662 = loc("1502|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_12aten__eq")
#loc663 = loc("1503|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_24aten__logical_not")
#loc664 = loc("1504|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_13aten__any")
#loc665 = loc("1505|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_25aten__logical_not")
#loc666 = loc("1507|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_12aten__expand")
#loc667 = loc("1501|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_12aten__softmax")
#loc668 = loc("1507|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_12aten__where")
#loc669 = loc("1485|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_135aten__permute")
#loc670 = loc("1486|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_74aten__mm")
#loc671 = loc("1486|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_74aten__view")
#loc672 = loc("1487|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_151aten__add")
#loc673 = loc("1492|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_51aten__view")
#loc674 = loc("1493|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_138aten__permute")
#loc675 = loc("1496|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_114xla__cast")
#loc676 = loc("1509|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_116aten__einsum")
#loc677 = loc("1509|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_116xla__cast")
#loc678 = loc("1511|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_12aten__permute")
#loc679 = loc("1514|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_75aten__view")
#loc680 = loc("1513|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_141aten__permute")
#loc681 = loc("1514|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_75aten__mm")
#loc682 = loc("1515|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_152aten__add")
#loc683 = loc("1516|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_153aten__add")
#loc684 = loc("1529|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_131xla__mark_tensor")
#loc685 = loc("1531|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_76aten__view")
#loc686 = loc("1530|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_142aten__permute")
#loc687 = loc("1531|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_76aten__mm")
#loc688 = loc("1532|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_156aten__add")
#loc689 = loc("1535|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[12].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_133xla__mark_tensor")
#loc690 = loc("1537|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_77aten__view")
#loc691 = loc("1536|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_143aten__permute")
#loc692 = loc("1537|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_77aten__mm")
#loc693 = loc("1538|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_157aten__add")
#loc694 = loc("1539|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_158aten__add")
#loc695 = loc("1552|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_137xla__mark_tensor")
#loc696 = loc("1554|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_78aten__view")
#loc697 = loc("1553|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_144aten__permute")
#loc698 = loc("1554|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_78aten__mm")
#loc699 = loc("1555|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_161aten__add")
#loc700 = loc("1562|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_53aten__view")
#loc701 = loc("1563|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_147aten__permute")
#loc702 = loc("1568|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_121xla__cast")
#loc703 = loc("1571|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_82aten__mul")
#loc704 = loc("1556|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_145aten__permute")
#loc705 = loc("1557|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_79aten__mm")
#loc706 = loc("1557|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_79aten__view")
#loc707 = loc("1558|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_162aten__add")
#loc708 = loc("1564|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_54aten__view")
#loc709 = loc("1565|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_148aten__permute")
#loc710 = loc("1569|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_122xla__cast")
#loc711 = loc("1572|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_150aten__permute")
#loc712 = loc("1573|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_83aten__mul")
#loc713 = loc("1575|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_13aten__einsum")
#loc714 = loc("1576|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_13aten__eq")
#loc715 = loc("1577|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_26aten__logical_not")
#loc716 = loc("1578|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_14aten__any")
#loc717 = loc("1579|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_27aten__logical_not")
#loc718 = loc("1581|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_13aten__expand")
#loc719 = loc("1575|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_13aten__softmax")
#loc720 = loc("1581|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_13aten__where")
#loc721 = loc("1559|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_146aten__permute")
#loc722 = loc("1560|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_80aten__mm")
#loc723 = loc("1560|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_80aten__view")
#loc724 = loc("1561|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_163aten__add")
#loc725 = loc("1566|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_55aten__view")
#loc726 = loc("1567|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_149aten__permute")
#loc727 = loc("1570|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_123xla__cast")
#loc728 = loc("1583|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_125aten__einsum")
#loc729 = loc("1583|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_125xla__cast")
#loc730 = loc("1585|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_13aten__permute")
#loc731 = loc("1588|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_81aten__view")
#loc732 = loc("1587|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_152aten__permute")
#loc733 = loc("1588|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_81aten__mm")
#loc734 = loc("1589|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_164aten__add")
#loc735 = loc("1590|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_165aten__add")
#loc736 = loc("1603|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_141xla__mark_tensor")
#loc737 = loc("1605|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_82aten__view")
#loc738 = loc("1604|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_153aten__permute")
#loc739 = loc("1605|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_82aten__mm")
#loc740 = loc("1606|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_168aten__add")
#loc741 = loc("1609|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[13].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_143xla__mark_tensor")
#loc742 = loc("1611|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_83aten__view")
#loc743 = loc("1610|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_154aten__permute")
#loc744 = loc("1611|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_83aten__mm")
#loc745 = loc("1612|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_169aten__add")
#loc746 = loc("1613|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_170aten__add")
#loc747 = loc("1626|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_147xla__mark_tensor")
#loc748 = loc("1628|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_84aten__view")
#loc749 = loc("1627|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_155aten__permute")
#loc750 = loc("1628|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_84aten__mm")
#loc751 = loc("1629|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_173aten__add")
#loc752 = loc("1636|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_57aten__view")
#loc753 = loc("1637|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_158aten__permute")
#loc754 = loc("1642|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_130xla__cast")
#loc755 = loc("1645|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_88aten__mul")
#loc756 = loc("1630|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_156aten__permute")
#loc757 = loc("1631|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_85aten__mm")
#loc758 = loc("1631|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_85aten__view")
#loc759 = loc("1632|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_174aten__add")
#loc760 = loc("1638|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_58aten__view")
#loc761 = loc("1639|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_159aten__permute")
#loc762 = loc("1643|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_131xla__cast")
#loc763 = loc("1646|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_161aten__permute")
#loc764 = loc("1647|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_89aten__mul")
#loc765 = loc("1649|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_14aten__einsum")
#loc766 = loc("1650|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_14aten__eq")
#loc767 = loc("1651|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_28aten__logical_not")
#loc768 = loc("1652|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_15aten__any")
#loc769 = loc("1653|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_29aten__logical_not")
#loc770 = loc("1655|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_14aten__expand")
#loc771 = loc("1649|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_14aten__softmax")
#loc772 = loc("1655|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_14aten__where")
#loc773 = loc("1633|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_157aten__permute")
#loc774 = loc("1634|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_86aten__mm")
#loc775 = loc("1634|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_86aten__view")
#loc776 = loc("1635|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_175aten__add")
#loc777 = loc("1640|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_59aten__view")
#loc778 = loc("1641|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_160aten__permute")
#loc779 = loc("1644|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_132xla__cast")
#loc780 = loc("1657|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_134aten__einsum")
#loc781 = loc("1657|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_134xla__cast")
#loc782 = loc("1659|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_14aten__permute")
#loc783 = loc("1662|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_87aten__view")
#loc784 = loc("1661|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_163aten__permute")
#loc785 = loc("1662|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_87aten__mm")
#loc786 = loc("1663|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_176aten__add")
#loc787 = loc("1664|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_177aten__add")
#loc788 = loc("1677|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_151xla__mark_tensor")
#loc789 = loc("1679|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_88aten__view")
#loc790 = loc("1678|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_164aten__permute")
#loc791 = loc("1679|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_88aten__mm")
#loc792 = loc("1680|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_180aten__add")
#loc793 = loc("1683|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[14].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_153xla__mark_tensor")
#loc794 = loc("1685|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_89aten__view")
#loc795 = loc("1684|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_165aten__permute")
#loc796 = loc("1685|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_89aten__mm")
#loc797 = loc("1686|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_181aten__add")
#loc798 = loc("1687|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_182aten__add")
#loc799 = loc("1700|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_157xla__mark_tensor")
#loc800 = loc("1702|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_90aten__view")
#loc801 = loc("1701|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_166aten__permute")
#loc802 = loc("1702|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_90aten__mm")
#loc803 = loc("1703|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_185aten__add")
#loc804 = loc("1710|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_61aten__view")
#loc805 = loc("1711|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_169aten__permute")
#loc806 = loc("1716|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_139xla__cast")
#loc807 = loc("1719|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_94aten__mul")
#loc808 = loc("1704|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_167aten__permute")
#loc809 = loc("1705|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_91aten__mm")
#loc810 = loc("1705|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_91aten__view")
#loc811 = loc("1706|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_186aten__add")
#loc812 = loc("1712|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_62aten__view")
#loc813 = loc("1713|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_170aten__permute")
#loc814 = loc("1717|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_140xla__cast")
#loc815 = loc("1720|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_172aten__permute")
#loc816 = loc("1721|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_95aten__mul")
#loc817 = loc("1723|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_15aten__einsum")
#loc818 = loc("1724|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_15aten__eq")
#loc819 = loc("1725|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_30aten__logical_not")
#loc820 = loc("1726|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_16aten__any")
#loc821 = loc("1727|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_31aten__logical_not")
#loc822 = loc("1729|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_15aten__expand")
#loc823 = loc("1723|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_15aten__softmax")
#loc824 = loc("1729|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_15aten__where")
#loc825 = loc("1707|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_168aten__permute")
#loc826 = loc("1708|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_92aten__mm")
#loc827 = loc("1708|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_92aten__view")
#loc828 = loc("1709|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_187aten__add")
#loc829 = loc("1714|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_63aten__view")
#loc830 = loc("1715|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_171aten__permute")
#loc831 = loc("1718|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_141xla__cast")
#loc832 = loc("1731|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_143aten__einsum")
#loc833 = loc("1731|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_143xla__cast")
#loc834 = loc("1733|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_15aten__permute")
#loc835 = loc("1736|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_93aten__view")
#loc836 = loc("1735|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_174aten__permute")
#loc837 = loc("1736|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_93aten__mm")
#loc838 = loc("1737|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_188aten__add")
#loc839 = loc("1738|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_189aten__add")
#loc840 = loc("1751|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_161xla__mark_tensor")
#loc841 = loc("1753|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_94aten__view")
#loc842 = loc("1752|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_175aten__permute")
#loc843 = loc("1753|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_94aten__mm")
#loc844 = loc("1754|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_192aten__add")
#loc845 = loc("1757|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[15].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_163xla__mark_tensor")
#loc846 = loc("1759|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_95aten__view")
#loc847 = loc("1758|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_176aten__permute")
#loc848 = loc("1759|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_95aten__mm")
#loc849 = loc("1760|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_193aten__add")
#loc850 = loc("1761|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_194aten__add")
#loc851 = loc("1774|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_167xla__mark_tensor")
#loc852 = loc("1776|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_96aten__view")
#loc853 = loc("1775|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_177aten__permute")
#loc854 = loc("1776|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_96aten__mm")
#loc855 = loc("1777|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_197aten__add")
#loc856 = loc("1784|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_65aten__view")
#loc857 = loc("1785|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_180aten__permute")
#loc858 = loc("1790|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_148xla__cast")
#loc859 = loc("1793|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_100aten__mul")
#loc860 = loc("1778|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_178aten__permute")
#loc861 = loc("1779|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_97aten__mm")
#loc862 = loc("1779|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_97aten__view")
#loc863 = loc("1780|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_198aten__add")
#loc864 = loc("1786|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_66aten__view")
#loc865 = loc("1787|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_181aten__permute")
#loc866 = loc("1791|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_149xla__cast")
#loc867 = loc("1794|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_183aten__permute")
#loc868 = loc("1795|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_101aten__mul")
#loc869 = loc("1797|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_16aten__einsum")
#loc870 = loc("1798|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_16aten__eq")
#loc871 = loc("1799|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_32aten__logical_not")
#loc872 = loc("1800|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_17aten__any")
#loc873 = loc("1801|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_33aten__logical_not")
#loc874 = loc("1803|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_16aten__expand")
#loc875 = loc("1797|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_16aten__softmax")
#loc876 = loc("1803|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_16aten__where")
#loc877 = loc("1781|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_179aten__permute")
#loc878 = loc("1782|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_98aten__mm")
#loc879 = loc("1782|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_98aten__view")
#loc880 = loc("1783|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_199aten__add")
#loc881 = loc("1788|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_67aten__view")
#loc882 = loc("1789|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_182aten__permute")
#loc883 = loc("1792|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_150xla__cast")
#loc884 = loc("1805|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_152aten__einsum")
#loc885 = loc("1805|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_152xla__cast")
#loc886 = loc("1807|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_16aten__permute")
#loc887 = loc("1810|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_99aten__view")
#loc888 = loc("1809|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_185aten__permute")
#loc889 = loc("1810|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_99aten__mm")
#loc890 = loc("1811|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_200aten__add")
#loc891 = loc("1812|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_201aten__add")
#loc892 = loc("1825|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_171xla__mark_tensor")
#loc893 = loc("1827|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_100aten__view")
#loc894 = loc("1826|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_186aten__permute")
#loc895 = loc("1827|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_100aten__mm")
#loc896 = loc("1828|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_204aten__add")
#loc897 = loc("1831|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[16].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_173xla__mark_tensor")
#loc898 = loc("1833|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_101aten__view")
#loc899 = loc("1832|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_187aten__permute")
#loc900 = loc("1833|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_101aten__mm")
#loc901 = loc("1834|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_205aten__add")
#loc902 = loc("1835|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_206aten__add")
#loc903 = loc("1848|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_177xla__mark_tensor")
#loc904 = loc("1850|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_102aten__view")
#loc905 = loc("1849|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_188aten__permute")
#loc906 = loc("1850|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_102aten__mm")
#loc907 = loc("1851|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_209aten__add")
#loc908 = loc("1858|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_69aten__view")
#loc909 = loc("1859|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_191aten__permute")
#loc910 = loc("1864|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_157xla__cast")
#loc911 = loc("1867|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_106aten__mul")
#loc912 = loc("1852|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_189aten__permute")
#loc913 = loc("1853|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_103aten__mm")
#loc914 = loc("1853|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_103aten__view")
#loc915 = loc("1854|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_210aten__add")
#loc916 = loc("1860|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_70aten__view")
#loc917 = loc("1861|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_192aten__permute")
#loc918 = loc("1865|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_158xla__cast")
#loc919 = loc("1868|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_194aten__permute")
#loc920 = loc("1869|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_107aten__mul")
#loc921 = loc("1871|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_17aten__einsum")
#loc922 = loc("1872|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_17aten__eq")
#loc923 = loc("1873|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_34aten__logical_not")
#loc924 = loc("1874|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_18aten__any")
#loc925 = loc("1875|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_35aten__logical_not")
#loc926 = loc("1877|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_17aten__expand")
#loc927 = loc("1871|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_17aten__softmax")
#loc928 = loc("1877|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_17aten__where")
#loc929 = loc("1855|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_190aten__permute")
#loc930 = loc("1856|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_104aten__mm")
#loc931 = loc("1856|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_104aten__view")
#loc932 = loc("1857|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_211aten__add")
#loc933 = loc("1862|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_71aten__view")
#loc934 = loc("1863|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_193aten__permute")
#loc935 = loc("1866|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_159xla__cast")
#loc936 = loc("1879|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_161aten__einsum")
#loc937 = loc("1879|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_161xla__cast")
#loc938 = loc("1881|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_17aten__permute")
#loc939 = loc("1884|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_105aten__view")
#loc940 = loc("1883|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_196aten__permute")
#loc941 = loc("1884|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_105aten__mm")
#loc942 = loc("1885|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_212aten__add")
#loc943 = loc("1886|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_213aten__add")
#loc944 = loc("1899|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_181xla__mark_tensor")
#loc945 = loc("1901|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_106aten__view")
#loc946 = loc("1900|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_197aten__permute")
#loc947 = loc("1901|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_106aten__mm")
#loc948 = loc("1902|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_216aten__add")
#loc949 = loc("1905|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[17].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_183xla__mark_tensor")
#loc950 = loc("1907|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_107aten__view")
#loc951 = loc("1906|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_198aten__permute")
#loc952 = loc("1907|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_107aten__mm")
#loc953 = loc("1908|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_217aten__add")
#loc954 = loc("1909|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_218aten__add")
#loc955 = loc("1922|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_187xla__mark_tensor")
#loc956 = loc("1924|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_108aten__view")
#loc957 = loc("1923|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_199aten__permute")
#loc958 = loc("1924|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_108aten__mm")
#loc959 = loc("1925|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_221aten__add")
#loc960 = loc("1932|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_73aten__view")
#loc961 = loc("1933|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_202aten__permute")
#loc962 = loc("1938|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_166xla__cast")
#loc963 = loc("1941|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_112aten__mul")
#loc964 = loc("1926|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_200aten__permute")
#loc965 = loc("1927|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_109aten__mm")
#loc966 = loc("1927|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_109aten__view")
#loc967 = loc("1928|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_222aten__add")
#loc968 = loc("1934|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_74aten__view")
#loc969 = loc("1935|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_203aten__permute")
#loc970 = loc("1939|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_167xla__cast")
#loc971 = loc("1942|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_205aten__permute")
#loc972 = loc("1943|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_113aten__mul")
#loc973 = loc("1945|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_18aten__einsum")
#loc974 = loc("1946|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_18aten__eq")
#loc975 = loc("1947|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_36aten__logical_not")
#loc976 = loc("1948|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_19aten__any")
#loc977 = loc("1949|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_37aten__logical_not")
#loc978 = loc("1951|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_18aten__expand")
#loc979 = loc("1945|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_18aten__softmax")
#loc980 = loc("1951|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_18aten__where")
#loc981 = loc("1929|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_201aten__permute")
#loc982 = loc("1930|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_110aten__mm")
#loc983 = loc("1930|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_110aten__view")
#loc984 = loc("1931|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_223aten__add")
#loc985 = loc("1936|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_75aten__view")
#loc986 = loc("1937|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_204aten__permute")
#loc987 = loc("1940|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_168xla__cast")
#loc988 = loc("1953|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_170aten__einsum")
#loc989 = loc("1953|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_170xla__cast")
#loc990 = loc("1955|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_18aten__permute")
#loc991 = loc("1958|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_111aten__view")
#loc992 = loc("1957|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_207aten__permute")
#loc993 = loc("1958|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_111aten__mm")
#loc994 = loc("1959|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_224aten__add")
#loc995 = loc("1960|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_225aten__add")
#loc996 = loc("1973|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_191xla__mark_tensor")
#loc997 = loc("1975|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_112aten__view")
#loc998 = loc("1974|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_208aten__permute")
#loc999 = loc("1975|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_112aten__mm")
#loc1000 = loc("1976|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_228aten__add")
#loc1001 = loc("1979|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[18].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_193xla__mark_tensor")
#loc1002 = loc("1981|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_113aten__view")
#loc1003 = loc("1980|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_209aten__permute")
#loc1004 = loc("1981|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_113aten__mm")
#loc1005 = loc("1982|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_229aten__add")
#loc1006 = loc("1983|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_230aten__add")
#loc1007 = loc("1996|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_197xla__mark_tensor")
#loc1008 = loc("1998|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_114aten__view")
#loc1009 = loc("1997|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_210aten__permute")
#loc1010 = loc("1998|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_114aten__mm")
#loc1011 = loc("1999|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_233aten__add")
#loc1012 = loc("2006|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_77aten__view")
#loc1013 = loc("2007|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_213aten__permute")
#loc1014 = loc("2012|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_175xla__cast")
#loc1015 = loc("2015|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_118aten__mul")
#loc1016 = loc("2000|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_211aten__permute")
#loc1017 = loc("2001|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_115aten__mm")
#loc1018 = loc("2001|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_115aten__view")
#loc1019 = loc("2002|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_234aten__add")
#loc1020 = loc("2008|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_78aten__view")
#loc1021 = loc("2009|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_214aten__permute")
#loc1022 = loc("2013|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_176xla__cast")
#loc1023 = loc("2016|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_216aten__permute")
#loc1024 = loc("2017|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_119aten__mul")
#loc1025 = loc("2019|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_19aten__einsum")
#loc1026 = loc("2020|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_19aten__eq")
#loc1027 = loc("2021|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_38aten__logical_not")
#loc1028 = loc("2022|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_20aten__any")
#loc1029 = loc("2023|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_39aten__logical_not")
#loc1030 = loc("2025|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_19aten__expand")
#loc1031 = loc("2019|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_19aten__softmax")
#loc1032 = loc("2025|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_19aten__where")
#loc1033 = loc("2003|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_212aten__permute")
#loc1034 = loc("2004|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_116aten__mm")
#loc1035 = loc("2004|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_116aten__view")
#loc1036 = loc("2005|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_235aten__add")
#loc1037 = loc("2010|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_79aten__view")
#loc1038 = loc("2011|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_215aten__permute")
#loc1039 = loc("2014|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_177xla__cast")
#loc1040 = loc("2027|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_179aten__einsum")
#loc1041 = loc("2027|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_179xla__cast")
#loc1042 = loc("2029|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_19aten__permute")
#loc1043 = loc("2032|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_117aten__view")
#loc1044 = loc("2031|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_218aten__permute")
#loc1045 = loc("2032|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_117aten__mm")
#loc1046 = loc("2033|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_236aten__add")
#loc1047 = loc("2034|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_237aten__add")
#loc1048 = loc("2047|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_201xla__mark_tensor")
#loc1049 = loc("2049|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_118aten__view")
#loc1050 = loc("2048|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_219aten__permute")
#loc1051 = loc("2049|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_118aten__mm")
#loc1052 = loc("2050|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_240aten__add")
#loc1053 = loc("2053|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[19].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_203xla__mark_tensor")
#loc1054 = loc("2055|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_119aten__view")
#loc1055 = loc("2054|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_220aten__permute")
#loc1056 = loc("2055|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_119aten__mm")
#loc1057 = loc("2056|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_241aten__add")
#loc1058 = loc("2057|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_242aten__add")
#loc1059 = loc("2070|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_207xla__mark_tensor")
#loc1060 = loc("2072|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_120aten__view")
#loc1061 = loc("2071|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_221aten__permute")
#loc1062 = loc("2072|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_120aten__mm")
#loc1063 = loc("2073|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_245aten__add")
#loc1064 = loc("2080|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_81aten__view")
#loc1065 = loc("2081|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_224aten__permute")
#loc1066 = loc("2086|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_184xla__cast")
#loc1067 = loc("2089|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_124aten__mul")
#loc1068 = loc("2074|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_222aten__permute")
#loc1069 = loc("2075|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_121aten__mm")
#loc1070 = loc("2075|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_121aten__view")
#loc1071 = loc("2076|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_246aten__add")
#loc1072 = loc("2082|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_82aten__view")
#loc1073 = loc("2083|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_225aten__permute")
#loc1074 = loc("2087|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_185xla__cast")
#loc1075 = loc("2090|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_227aten__permute")
#loc1076 = loc("2091|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_125aten__mul")
#loc1077 = loc("2093|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_20aten__einsum")
#loc1078 = loc("2094|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_20aten__eq")
#loc1079 = loc("2095|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_40aten__logical_not")
#loc1080 = loc("2096|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_21aten__any")
#loc1081 = loc("2097|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_41aten__logical_not")
#loc1082 = loc("2099|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_20aten__expand")
#loc1083 = loc("2093|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_20aten__softmax")
#loc1084 = loc("2099|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_20aten__where")
#loc1085 = loc("2077|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_223aten__permute")
#loc1086 = loc("2078|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_122aten__mm")
#loc1087 = loc("2078|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_122aten__view")
#loc1088 = loc("2079|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_247aten__add")
#loc1089 = loc("2084|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_83aten__view")
#loc1090 = loc("2085|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_226aten__permute")
#loc1091 = loc("2088|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_186xla__cast")
#loc1092 = loc("2101|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_188aten__einsum")
#loc1093 = loc("2101|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_188xla__cast")
#loc1094 = loc("2103|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_20aten__permute")
#loc1095 = loc("2106|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_123aten__view")
#loc1096 = loc("2105|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_229aten__permute")
#loc1097 = loc("2106|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_123aten__mm")
#loc1098 = loc("2107|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_248aten__add")
#loc1099 = loc("2108|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_249aten__add")
#loc1100 = loc("2121|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_211xla__mark_tensor")
#loc1101 = loc("2123|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_124aten__view")
#loc1102 = loc("2122|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_230aten__permute")
#loc1103 = loc("2123|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_124aten__mm")
#loc1104 = loc("2124|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_252aten__add")
#loc1105 = loc("2127|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[20].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_213xla__mark_tensor")
#loc1106 = loc("2129|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_125aten__view")
#loc1107 = loc("2128|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_231aten__permute")
#loc1108 = loc("2129|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_125aten__mm")
#loc1109 = loc("2130|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_253aten__add")
#loc1110 = loc("2131|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_254aten__add")
#loc1111 = loc("2144|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_217xla__mark_tensor")
#loc1112 = loc("2146|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_126aten__view")
#loc1113 = loc("2145|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_232aten__permute")
#loc1114 = loc("2146|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_126aten__mm")
#loc1115 = loc("2147|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_257aten__add")
#loc1116 = loc("2154|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_85aten__view")
#loc1117 = loc("2155|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_235aten__permute")
#loc1118 = loc("2160|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_193xla__cast")
#loc1119 = loc("2163|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_130aten__mul")
#loc1120 = loc("2148|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_233aten__permute")
#loc1121 = loc("2149|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_127aten__mm")
#loc1122 = loc("2149|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_127aten__view")
#loc1123 = loc("2150|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_258aten__add")
#loc1124 = loc("2156|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_86aten__view")
#loc1125 = loc("2157|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_236aten__permute")
#loc1126 = loc("2161|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_194xla__cast")
#loc1127 = loc("2164|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_238aten__permute")
#loc1128 = loc("2165|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_131aten__mul")
#loc1129 = loc("2167|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_21aten__einsum")
#loc1130 = loc("2168|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_21aten__eq")
#loc1131 = loc("2169|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_42aten__logical_not")
#loc1132 = loc("2170|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_22aten__any")
#loc1133 = loc("2171|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_43aten__logical_not")
#loc1134 = loc("2173|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_21aten__expand")
#loc1135 = loc("2167|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_21aten__softmax")
#loc1136 = loc("2173|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_21aten__where")
#loc1137 = loc("2151|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_234aten__permute")
#loc1138 = loc("2152|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_128aten__mm")
#loc1139 = loc("2152|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_128aten__view")
#loc1140 = loc("2153|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_259aten__add")
#loc1141 = loc("2158|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_87aten__view")
#loc1142 = loc("2159|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_237aten__permute")
#loc1143 = loc("2162|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_195xla__cast")
#loc1144 = loc("2175|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_197aten__einsum")
#loc1145 = loc("2175|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_197xla__cast")
#loc1146 = loc("2177|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_21aten__permute")
#loc1147 = loc("2180|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_129aten__view")
#loc1148 = loc("2179|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_240aten__permute")
#loc1149 = loc("2180|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_129aten__mm")
#loc1150 = loc("2181|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_260aten__add")
#loc1151 = loc("2182|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_261aten__add")
#loc1152 = loc("2195|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_221xla__mark_tensor")
#loc1153 = loc("2197|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_130aten__view")
#loc1154 = loc("2196|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_241aten__permute")
#loc1155 = loc("2197|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_130aten__mm")
#loc1156 = loc("2198|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_264aten__add")
#loc1157 = loc("2201|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[21].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_223xla__mark_tensor")
#loc1158 = loc("2203|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_131aten__view")
#loc1159 = loc("2202|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_242aten__permute")
#loc1160 = loc("2203|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_131aten__mm")
#loc1161 = loc("2204|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_265aten__add")
#loc1162 = loc("2205|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_266aten__add")
#loc1163 = loc("2218|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_227xla__mark_tensor")
#loc1164 = loc("2220|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_132aten__view")
#loc1165 = loc("2219|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_243aten__permute")
#loc1166 = loc("2220|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_132aten__mm")
#loc1167 = loc("2221|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_269aten__add")
#loc1168 = loc("2228|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_89aten__view")
#loc1169 = loc("2229|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_246aten__permute")
#loc1170 = loc("2234|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_202xla__cast")
#loc1171 = loc("2237|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_136aten__mul")
#loc1172 = loc("2222|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_244aten__permute")
#loc1173 = loc("2223|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_133aten__mm")
#loc1174 = loc("2223|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_133aten__view")
#loc1175 = loc("2224|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_270aten__add")
#loc1176 = loc("2230|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_90aten__view")
#loc1177 = loc("2231|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_247aten__permute")
#loc1178 = loc("2235|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_203xla__cast")
#loc1179 = loc("2238|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_249aten__permute")
#loc1180 = loc("2239|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_137aten__mul")
#loc1181 = loc("2241|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_22aten__einsum")
#loc1182 = loc("2242|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_22aten__eq")
#loc1183 = loc("2243|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_44aten__logical_not")
#loc1184 = loc("2244|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_23aten__any")
#loc1185 = loc("2245|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_45aten__logical_not")
#loc1186 = loc("2247|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_22aten__expand")
#loc1187 = loc("2241|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_22aten__softmax")
#loc1188 = loc("2247|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_22aten__where")
#loc1189 = loc("2225|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_245aten__permute")
#loc1190 = loc("2226|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_134aten__mm")
#loc1191 = loc("2226|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_134aten__view")
#loc1192 = loc("2227|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_271aten__add")
#loc1193 = loc("2232|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_91aten__view")
#loc1194 = loc("2233|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_248aten__permute")
#loc1195 = loc("2236|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_204xla__cast")
#loc1196 = loc("2249|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_206aten__einsum")
#loc1197 = loc("2249|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_206xla__cast")
#loc1198 = loc("2251|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_22aten__permute")
#loc1199 = loc("2254|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_135aten__view")
#loc1200 = loc("2253|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_251aten__permute")
#loc1201 = loc("2254|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_135aten__mm")
#loc1202 = loc("2255|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_272aten__add")
#loc1203 = loc("2256|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_273aten__add")
#loc1204 = loc("2269|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_231xla__mark_tensor")
#loc1205 = loc("2271|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_136aten__view")
#loc1206 = loc("2270|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_252aten__permute")
#loc1207 = loc("2271|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_136aten__mm")
#loc1208 = loc("2272|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_276aten__add")
#loc1209 = loc("2275|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[22].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_233xla__mark_tensor")
#loc1210 = loc("2277|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_137aten__view")
#loc1211 = loc("2276|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_253aten__permute")
#loc1212 = loc("2277|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_137aten__mm")
#loc1213 = loc("2278|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_277aten__add")
#loc1214 = loc("2279|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_278aten__add")
#loc1215 = loc("2292|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_237xla__mark_tensor")
#loc1216 = loc("2294|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_138aten__view")
#loc1217 = loc("2293|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_254aten__permute")
#loc1218 = loc("2294|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_138aten__mm")
#loc1219 = loc("2295|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_281aten__add")
#loc1220 = loc("2302|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_93aten__view")
#loc1221 = loc("2303|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_257aten__permute")
#loc1222 = loc("2308|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_211xla__cast")
#loc1223 = loc("2311|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_142aten__mul")
#loc1224 = loc("2296|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_255aten__permute")
#loc1225 = loc("2297|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_139aten__mm")
#loc1226 = loc("2297|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_139aten__view")
#loc1227 = loc("2298|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_282aten__add")
#loc1228 = loc("2304|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_94aten__view")
#loc1229 = loc("2305|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_258aten__permute")
#loc1230 = loc("2309|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_212xla__cast")
#loc1231 = loc("2312|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_260aten__permute")
#loc1232 = loc("2313|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_143aten__mul")
#loc1233 = loc("2315|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_23aten__einsum")
#loc1234 = loc("2316|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_23aten__eq")
#loc1235 = loc("2317|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_46aten__logical_not")
#loc1236 = loc("2318|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_24aten__any")
#loc1237 = loc("2319|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_47aten__logical_not")
#loc1238 = loc("2321|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_23aten__expand")
#loc1239 = loc("2315|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_23aten__softmax")
#loc1240 = loc("2321|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_23aten__where")
#loc1241 = loc("2299|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_256aten__permute")
#loc1242 = loc("2300|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_140aten__mm")
#loc1243 = loc("2300|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_140aten__view")
#loc1244 = loc("2301|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_283aten__add")
#loc1245 = loc("2306|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_95aten__view")
#loc1246 = loc("2307|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_259aten__permute")
#loc1247 = loc("2310|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_213xla__cast")
#loc1248 = loc("2323|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_215aten__einsum")
#loc1249 = loc("2323|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_215xla__cast")
#loc1250 = loc("2325|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_23aten__permute")
#loc1251 = loc("2328|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_141aten__view")
#loc1252 = loc("2327|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_262aten__permute")
#loc1253 = loc("2328|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_141aten__mm")
#loc1254 = loc("2329|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_284aten__add")
#loc1255 = loc("2330|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_285aten__add")
#loc1256 = loc("2343|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_241xla__mark_tensor")
#loc1257 = loc("2345|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_142aten__view")
#loc1258 = loc("2344|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_263aten__permute")
#loc1259 = loc("2345|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_142aten__mm")
#loc1260 = loc("2346|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_288aten__add")
#loc1261 = loc("2349|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[23].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_243xla__mark_tensor")
#loc1262 = loc("2351|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_143aten__view")
#loc1263 = loc("2350|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_264aten__permute")
#loc1264 = loc("2351|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_143aten__mm")
#loc1265 = loc("2352|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_289aten__add")
#loc1266 = loc("2353|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_290aten__add")
#loc1267 = loc("2366|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_247xla__mark_tensor")
#loc1268 = loc("2368|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_144aten__view")
#loc1269 = loc("2367|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_265aten__permute")
#loc1270 = loc("2368|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_144aten__mm")
#loc1271 = loc("2369|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_293aten__add")
#loc1272 = loc("2376|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_97aten__view")
#loc1273 = loc("2377|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_268aten__permute")
#loc1274 = loc("2382|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_220xla__cast")
#loc1275 = loc("2385|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_148aten__mul")
#loc1276 = loc("2370|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_266aten__permute")
#loc1277 = loc("2371|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_145aten__mm")
#loc1278 = loc("2371|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_145aten__view")
#loc1279 = loc("2372|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_294aten__add")
#loc1280 = loc("2378|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_98aten__view")
#loc1281 = loc("2379|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_269aten__permute")
#loc1282 = loc("2383|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_221xla__cast")
#loc1283 = loc("2386|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_271aten__permute")
#loc1284 = loc("2387|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_149aten__mul")
#loc1285 = loc("2389|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_24aten__einsum")
#loc1286 = loc("2390|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_24aten__eq")
#loc1287 = loc("2391|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_48aten__logical_not")
#loc1288 = loc("2392|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_25aten__any")
#loc1289 = loc("2393|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_49aten__logical_not")
#loc1290 = loc("2395|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_24aten__expand")
#loc1291 = loc("2389|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_24aten__softmax")
#loc1292 = loc("2395|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_24aten__where")
#loc1293 = loc("2373|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_267aten__permute")
#loc1294 = loc("2374|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_146aten__mm")
#loc1295 = loc("2374|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_146aten__view")
#loc1296 = loc("2375|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_295aten__add")
#loc1297 = loc("2380|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_99aten__view")
#loc1298 = loc("2381|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_270aten__permute")
#loc1299 = loc("2384|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_222xla__cast")
#loc1300 = loc("2397|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_224aten__einsum")
#loc1301 = loc("2397|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_224xla__cast")
#loc1302 = loc("2399|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_24aten__permute")
#loc1303 = loc("2402|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_147aten__view")
#loc1304 = loc("2401|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_273aten__permute")
#loc1305 = loc("2402|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_147aten__mm")
#loc1306 = loc("2403|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_296aten__add")
#loc1307 = loc("2404|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_297aten__add")
#loc1308 = loc("2417|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_251xla__mark_tensor")
#loc1309 = loc("2419|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_148aten__view")
#loc1310 = loc("2418|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_274aten__permute")
#loc1311 = loc("2419|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_148aten__mm")
#loc1312 = loc("2420|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_300aten__add")
#loc1313 = loc("2423|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[24].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_253xla__mark_tensor")
#loc1314 = loc("2425|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_149aten__view")
#loc1315 = loc("2424|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_275aten__permute")
#loc1316 = loc("2425|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_149aten__mm")
#loc1317 = loc("2426|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_301aten__add")
#loc1318 = loc("2427|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_302aten__add")
#loc1319 = loc("2440|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_257xla__mark_tensor")
#loc1320 = loc("2442|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_150aten__view")
#loc1321 = loc("2441|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_276aten__permute")
#loc1322 = loc("2442|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_150aten__mm")
#loc1323 = loc("2443|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_305aten__add")
#loc1324 = loc("2450|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_101aten__view")
#loc1325 = loc("2451|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_279aten__permute")
#loc1326 = loc("2456|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_229xla__cast")
#loc1327 = loc("2459|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_154aten__mul")
#loc1328 = loc("2444|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_277aten__permute")
#loc1329 = loc("2445|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_151aten__mm")
#loc1330 = loc("2445|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_151aten__view")
#loc1331 = loc("2446|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_306aten__add")
#loc1332 = loc("2452|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_102aten__view")
#loc1333 = loc("2453|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_280aten__permute")
#loc1334 = loc("2457|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_230xla__cast")
#loc1335 = loc("2460|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_282aten__permute")
#loc1336 = loc("2461|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_155aten__mul")
#loc1337 = loc("2463|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_25aten__einsum")
#loc1338 = loc("2464|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_25aten__eq")
#loc1339 = loc("2465|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_50aten__logical_not")
#loc1340 = loc("2466|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_26aten__any")
#loc1341 = loc("2467|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_51aten__logical_not")
#loc1342 = loc("2469|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_25aten__expand")
#loc1343 = loc("2463|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_25aten__softmax")
#loc1344 = loc("2469|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_25aten__where")
#loc1345 = loc("2447|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_278aten__permute")
#loc1346 = loc("2448|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_152aten__mm")
#loc1347 = loc("2448|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_152aten__view")
#loc1348 = loc("2449|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_307aten__add")
#loc1349 = loc("2454|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_103aten__view")
#loc1350 = loc("2455|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_281aten__permute")
#loc1351 = loc("2458|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_231xla__cast")
#loc1352 = loc("2471|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_233aten__einsum")
#loc1353 = loc("2471|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_233xla__cast")
#loc1354 = loc("2473|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_25aten__permute")
#loc1355 = loc("2476|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_153aten__view")
#loc1356 = loc("2475|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_284aten__permute")
#loc1357 = loc("2476|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_153aten__mm")
#loc1358 = loc("2477|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_308aten__add")
#loc1359 = loc("2478|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_309aten__add")
#loc1360 = loc("2491|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_261xla__mark_tensor")
#loc1361 = loc("2493|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_154aten__view")
#loc1362 = loc("2492|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_285aten__permute")
#loc1363 = loc("2493|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_154aten__mm")
#loc1364 = loc("2494|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_312aten__add")
#loc1365 = loc("2497|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[25].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_263xla__mark_tensor")
#loc1366 = loc("2499|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_155aten__view")
#loc1367 = loc("2498|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_286aten__permute")
#loc1368 = loc("2499|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_155aten__mm")
#loc1369 = loc("2500|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_313aten__add")
#loc1370 = loc("2501|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_314aten__add")
#loc1371 = loc("2514|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_267xla__mark_tensor")
#loc1372 = loc("2516|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_156aten__view")
#loc1373 = loc("2515|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_287aten__permute")
#loc1374 = loc("2516|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_156aten__mm")
#loc1375 = loc("2517|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_317aten__add")
#loc1376 = loc("2524|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_105aten__view")
#loc1377 = loc("2525|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_290aten__permute")
#loc1378 = loc("2530|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_238xla__cast")
#loc1379 = loc("2533|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_160aten__mul")
#loc1380 = loc("2518|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_288aten__permute")
#loc1381 = loc("2519|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_157aten__mm")
#loc1382 = loc("2519|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_157aten__view")
#loc1383 = loc("2520|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_318aten__add")
#loc1384 = loc("2526|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_106aten__view")
#loc1385 = loc("2527|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_291aten__permute")
#loc1386 = loc("2531|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_239xla__cast")
#loc1387 = loc("2534|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_293aten__permute")
#loc1388 = loc("2535|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_161aten__mul")
#loc1389 = loc("2537|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_26aten__einsum")
#loc1390 = loc("2538|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_26aten__eq")
#loc1391 = loc("2539|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_52aten__logical_not")
#loc1392 = loc("2540|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_27aten__any")
#loc1393 = loc("2541|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_53aten__logical_not")
#loc1394 = loc("2543|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_26aten__expand")
#loc1395 = loc("2537|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_26aten__softmax")
#loc1396 = loc("2543|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_26aten__where")
#loc1397 = loc("2521|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_289aten__permute")
#loc1398 = loc("2522|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_158aten__mm")
#loc1399 = loc("2522|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_158aten__view")
#loc1400 = loc("2523|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_319aten__add")
#loc1401 = loc("2528|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_107aten__view")
#loc1402 = loc("2529|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_292aten__permute")
#loc1403 = loc("2532|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_240xla__cast")
#loc1404 = loc("2545|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_242aten__einsum")
#loc1405 = loc("2545|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_242xla__cast")
#loc1406 = loc("2547|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_26aten__permute")
#loc1407 = loc("2550|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_159aten__view")
#loc1408 = loc("2549|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_295aten__permute")
#loc1409 = loc("2550|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_159aten__mm")
#loc1410 = loc("2551|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_320aten__add")
#loc1411 = loc("2552|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_321aten__add")
#loc1412 = loc("2565|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_271xla__mark_tensor")
#loc1413 = loc("2567|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_160aten__view")
#loc1414 = loc("2566|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_296aten__permute")
#loc1415 = loc("2567|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_160aten__mm")
#loc1416 = loc("2568|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_324aten__add")
#loc1417 = loc("2571|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[26].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_273xla__mark_tensor")
#loc1418 = loc("2573|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_161aten__view")
#loc1419 = loc("2572|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_297aten__permute")
#loc1420 = loc("2573|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_161aten__mm")
#loc1421 = loc("2574|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_325aten__add")
#loc1422 = loc("2575|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_326aten__add")
#loc1423 = loc("2588|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_277xla__mark_tensor")
#loc1424 = loc("2590|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_162aten__view")
#loc1425 = loc("2589|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_298aten__permute")
#loc1426 = loc("2590|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_162aten__mm")
#loc1427 = loc("2591|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_329aten__add")
#loc1428 = loc("2598|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_109aten__view")
#loc1429 = loc("2599|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_301aten__permute")
#loc1430 = loc("2604|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_247xla__cast")
#loc1431 = loc("2607|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_166aten__mul")
#loc1432 = loc("2592|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_299aten__permute")
#loc1433 = loc("2593|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_163aten__mm")
#loc1434 = loc("2593|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_163aten__view")
#loc1435 = loc("2594|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_330aten__add")
#loc1436 = loc("2600|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_110aten__view")
#loc1437 = loc("2601|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_302aten__permute")
#loc1438 = loc("2605|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_248xla__cast")
#loc1439 = loc("2608|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_304aten__permute")
#loc1440 = loc("2609|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_167aten__mul")
#loc1441 = loc("2611|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_27aten__einsum")
#loc1442 = loc("2612|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_27aten__eq")
#loc1443 = loc("2613|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_54aten__logical_not")
#loc1444 = loc("2614|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_28aten__any")
#loc1445 = loc("2615|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_55aten__logical_not")
#loc1446 = loc("2617|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_27aten__expand")
#loc1447 = loc("2611|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_27aten__softmax")
#loc1448 = loc("2617|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_27aten__where")
#loc1449 = loc("2595|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_300aten__permute")
#loc1450 = loc("2596|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_164aten__mm")
#loc1451 = loc("2596|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_164aten__view")
#loc1452 = loc("2597|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_331aten__add")
#loc1453 = loc("2602|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_111aten__view")
#loc1454 = loc("2603|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_303aten__permute")
#loc1455 = loc("2606|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_249xla__cast")
#loc1456 = loc("2619|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_251aten__einsum")
#loc1457 = loc("2619|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_251xla__cast")
#loc1458 = loc("2621|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_27aten__permute")
#loc1459 = loc("2624|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_165aten__view")
#loc1460 = loc("2623|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_306aten__permute")
#loc1461 = loc("2624|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_165aten__mm")
#loc1462 = loc("2625|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_332aten__add")
#loc1463 = loc("2626|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_333aten__add")
#loc1464 = loc("2639|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_281xla__mark_tensor")
#loc1465 = loc("2641|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_166aten__view")
#loc1466 = loc("2640|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_307aten__permute")
#loc1467 = loc("2641|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_166aten__mm")
#loc1468 = loc("2642|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_336aten__add")
#loc1469 = loc("2645|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[27].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_283xla__mark_tensor")
#loc1470 = loc("2647|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_167aten__view")
#loc1471 = loc("2646|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_308aten__permute")
#loc1472 = loc("2647|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_167aten__mm")
#loc1473 = loc("2648|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_337aten__add")
#loc1474 = loc("2649|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_338aten__add")
#loc1475 = loc("2662|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_287xla__mark_tensor")
#loc1476 = loc("2664|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_168aten__view")
#loc1477 = loc("2663|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_309aten__permute")
#loc1478 = loc("2664|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_168aten__mm")
#loc1479 = loc("2665|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_341aten__add")
#loc1480 = loc("2672|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_113aten__view")
#loc1481 = loc("2673|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_312aten__permute")
#loc1482 = loc("2678|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_256xla__cast")
#loc1483 = loc("2681|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_172aten__mul")
#loc1484 = loc("2666|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_310aten__permute")
#loc1485 = loc("2667|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_169aten__mm")
#loc1486 = loc("2667|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_169aten__view")
#loc1487 = loc("2668|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_342aten__add")
#loc1488 = loc("2674|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_114aten__view")
#loc1489 = loc("2675|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_313aten__permute")
#loc1490 = loc("2679|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_257xla__cast")
#loc1491 = loc("2682|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_315aten__permute")
#loc1492 = loc("2683|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_173aten__mul")
#loc1493 = loc("2685|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_28aten__einsum")
#loc1494 = loc("2686|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_28aten__eq")
#loc1495 = loc("2687|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_56aten__logical_not")
#loc1496 = loc("2688|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_29aten__any")
#loc1497 = loc("2689|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_57aten__logical_not")
#loc1498 = loc("2691|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_28aten__expand")
#loc1499 = loc("2685|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_28aten__softmax")
#loc1500 = loc("2691|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_28aten__where")
#loc1501 = loc("2669|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_311aten__permute")
#loc1502 = loc("2670|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_170aten__mm")
#loc1503 = loc("2670|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_170aten__view")
#loc1504 = loc("2671|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_343aten__add")
#loc1505 = loc("2676|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_115aten__view")
#loc1506 = loc("2677|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_314aten__permute")
#loc1507 = loc("2680|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_258xla__cast")
#loc1508 = loc("2693|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_260aten__einsum")
#loc1509 = loc("2693|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_260xla__cast")
#loc1510 = loc("2695|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_28aten__permute")
#loc1511 = loc("2698|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_171aten__view")
#loc1512 = loc("2697|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_317aten__permute")
#loc1513 = loc("2698|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_171aten__mm")
#loc1514 = loc("2699|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_344aten__add")
#loc1515 = loc("2700|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_345aten__add")
#loc1516 = loc("2713|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_291xla__mark_tensor")
#loc1517 = loc("2715|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_172aten__view")
#loc1518 = loc("2714|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_318aten__permute")
#loc1519 = loc("2715|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_172aten__mm")
#loc1520 = loc("2716|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_348aten__add")
#loc1521 = loc("2719|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[28].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_293xla__mark_tensor")
#loc1522 = loc("2721|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_173aten__view")
#loc1523 = loc("2720|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_319aten__permute")
#loc1524 = loc("2721|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_173aten__mm")
#loc1525 = loc("2722|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_349aten__add")
#loc1526 = loc("2723|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_350aten__add")
#loc1527 = loc("2736|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_297xla__mark_tensor")
#loc1528 = loc("2738|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_174aten__view")
#loc1529 = loc("2737|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_320aten__permute")
#loc1530 = loc("2738|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_174aten__mm")
#loc1531 = loc("2739|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_353aten__add")
#loc1532 = loc("2746|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_117aten__view")
#loc1533 = loc("2747|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_323aten__permute")
#loc1534 = loc("2752|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_265xla__cast")
#loc1535 = loc("2755|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_178aten__mul")
#loc1536 = loc("2740|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_321aten__permute")
#loc1537 = loc("2741|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_175aten__mm")
#loc1538 = loc("2741|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_175aten__view")
#loc1539 = loc("2742|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_354aten__add")
#loc1540 = loc("2748|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_118aten__view")
#loc1541 = loc("2749|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_324aten__permute")
#loc1542 = loc("2753|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_266xla__cast")
#loc1543 = loc("2756|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_326aten__permute")
#loc1544 = loc("2757|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_179aten__mul")
#loc1545 = loc("2759|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_29aten__einsum")
#loc1546 = loc("2760|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_29aten__eq")
#loc1547 = loc("2761|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_58aten__logical_not")
#loc1548 = loc("2762|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_30aten__any")
#loc1549 = loc("2763|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_59aten__logical_not")
#loc1550 = loc("2765|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_29aten__expand")
#loc1551 = loc("2759|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_29aten__softmax")
#loc1552 = loc("2765|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_29aten__where")
#loc1553 = loc("2743|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_322aten__permute")
#loc1554 = loc("2744|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_176aten__mm")
#loc1555 = loc("2744|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_176aten__view")
#loc1556 = loc("2745|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_355aten__add")
#loc1557 = loc("2750|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_119aten__view")
#loc1558 = loc("2751|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_325aten__permute")
#loc1559 = loc("2754|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_267xla__cast")
#loc1560 = loc("2767|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_269aten__einsum")
#loc1561 = loc("2767|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_269xla__cast")
#loc1562 = loc("2769|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_29aten__permute")
#loc1563 = loc("2772|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_177aten__view")
#loc1564 = loc("2771|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_328aten__permute")
#loc1565 = loc("2772|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_177aten__mm")
#loc1566 = loc("2773|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_356aten__add")
#loc1567 = loc("2774|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_357aten__add")
#loc1568 = loc("2787|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_301xla__mark_tensor")
#loc1569 = loc("2789|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_178aten__view")
#loc1570 = loc("2788|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_329aten__permute")
#loc1571 = loc("2789|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_178aten__mm")
#loc1572 = loc("2790|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_360aten__add")
#loc1573 = loc("2793|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[29].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_303xla__mark_tensor")
#loc1574 = loc("2795|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_179aten__view")
#loc1575 = loc("2794|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_330aten__permute")
#loc1576 = loc("2795|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_179aten__mm")
#loc1577 = loc("2796|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_361aten__add")
#loc1578 = loc("2797|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_362aten__add")
#loc1579 = loc("2810|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_307xla__mark_tensor")
#loc1580 = loc("2812|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_180aten__view")
#loc1581 = loc("2811|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_331aten__permute")
#loc1582 = loc("2812|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_180aten__mm")
#loc1583 = loc("2813|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_365aten__add")
#loc1584 = loc("2820|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_121aten__view")
#loc1585 = loc("2821|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_334aten__permute")
#loc1586 = loc("2826|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_274xla__cast")
#loc1587 = loc("2829|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_184aten__mul")
#loc1588 = loc("2814|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_332aten__permute")
#loc1589 = loc("2815|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_181aten__mm")
#loc1590 = loc("2815|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_181aten__view")
#loc1591 = loc("2816|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_366aten__add")
#loc1592 = loc("2822|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_122aten__view")
#loc1593 = loc("2823|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_335aten__permute")
#loc1594 = loc("2827|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_275xla__cast")
#loc1595 = loc("2830|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_337aten__permute")
#loc1596 = loc("2831|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_185aten__mul")
#loc1597 = loc("2833|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_30aten__einsum")
#loc1598 = loc("2834|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_30aten__eq")
#loc1599 = loc("2835|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_60aten__logical_not")
#loc1600 = loc("2836|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_31aten__any")
#loc1601 = loc("2837|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_61aten__logical_not")
#loc1602 = loc("2839|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_30aten__expand")
#loc1603 = loc("2833|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_30aten__softmax")
#loc1604 = loc("2839|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_30aten__where")
#loc1605 = loc("2817|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_333aten__permute")
#loc1606 = loc("2818|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_182aten__mm")
#loc1607 = loc("2818|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_182aten__view")
#loc1608 = loc("2819|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_367aten__add")
#loc1609 = loc("2824|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_123aten__view")
#loc1610 = loc("2825|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_336aten__permute")
#loc1611 = loc("2828|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_276xla__cast")
#loc1612 = loc("2841|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_278aten__einsum")
#loc1613 = loc("2841|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_278xla__cast")
#loc1614 = loc("2843|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_30aten__permute")
#loc1615 = loc("2846|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_183aten__view")
#loc1616 = loc("2845|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_339aten__permute")
#loc1617 = loc("2846|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_183aten__mm")
#loc1618 = loc("2847|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_368aten__add")
#loc1619 = loc("2848|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_369aten__add")
#loc1620 = loc("2861|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_311xla__mark_tensor")
#loc1621 = loc("2863|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_184aten__view")
#loc1622 = loc("2862|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_340aten__permute")
#loc1623 = loc("2863|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_184aten__mm")
#loc1624 = loc("2864|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_372aten__add")
#loc1625 = loc("2867|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[30].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_313xla__mark_tensor")
#loc1626 = loc("2869|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_185aten__view")
#loc1627 = loc("2868|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_341aten__permute")
#loc1628 = loc("2869|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_185aten__mm")
#loc1629 = loc("2870|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_373aten__add")
#loc1630 = loc("2871|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_374aten__add")
#loc1631 = loc("2874|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_in]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2303|matmul_193aten__view")
#loc1632 = loc("2873|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_in]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2303|permute_354aten__permute")
#loc1633 = loc("2874|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_in]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2303|matmul_193aten__mm")
#loc1634 = loc("2875|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_in]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2303|add_389aten__add")
#loc1635 = loc("2888|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_331xla__mark_tensor")
#loc1636 = loc("2902|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2248|cat_1aten__cat")
#loc1637 = loc("2906|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_195aten__view")
#loc1638 = loc("2905|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|permute_356aten__permute")
#loc1639 = loc("2906|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_195aten__mm")
#loc1640 = loc("2911|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|view_130aten__view")
#loc1641 = loc("2912|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|permute_359aten__permute")
#loc1642 = loc("2916|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_297xla__cast")
#loc1643 = loc("2919|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|permute_361aten__permute")
#loc1644 = loc("2920|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_201aten__mul")
#loc1645 = loc("2922|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_32aten__einsum")
#loc1646 = loc("2923|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|eq_32aten__eq")
#loc1647 = loc("2924|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_64aten__logical_not")
#loc1648 = loc("2925|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_33aten__any")
#loc1649 = loc("2926|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_65aten__logical_not")
#loc1650 = loc("2928|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_32aten__expand")
#loc1651 = loc("2922|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_32aten__softmax")
#loc1652 = loc("2928|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_32aten__where")
#loc1653 = loc("2907|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|permute_357aten__permute")
#loc1654 = loc("2908|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|matmul_196aten__mm")
#loc1655 = loc("2913|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|view_131aten__view")
#loc1656 = loc("2914|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|permute_360aten__permute")
#loc1657 = loc("2917|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_298xla__cast")
#loc1658 = loc("2930|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_300aten__einsum")
#loc1659 = loc("2930|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_300xla__cast")
#loc1660 = loc("2932|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2769|clone_32aten__permute")
#loc1661 = loc("2935|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_197aten__view")
#loc1662 = loc("2934|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|permute_363aten__permute")
#loc1663 = loc("2935|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_197aten__mm")
#loc1664 = loc("2936|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Dropout[resampler.layers[0].attn.to_out[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2775|clone_33aten__view")
#loc1665 = loc("2937|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2783|divaten__div")
#loc1666 = loc("2938|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2249|add_394aten__add")
#loc1667 = loc("2951|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_339xla__mark_tensor")
#loc1668 = loc("2953|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|GELU[getattr(resampler.layers[0].ff, '1').net[0]]|Linear[getattr(resampler.layers[0].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_198aten__view")
#loc1669 = loc("2952|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|GELU[getattr(resampler.layers[0].ff, '1').net[0]]|Linear[getattr(resampler.layers[0].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|permute_364aten__permute")
#loc1670 = loc("2953|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|GELU[getattr(resampler.layers[0].ff, '1').net[0]]|Linear[getattr(resampler.layers[0].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_198aten__mm")
#loc1671 = loc("2957|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|Dropout[getattr(resampler.layers[0].ff, '1').net[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|clone_34xla__mark_tensor")
#loc1672 = loc("2959|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|Linear[getattr(resampler.layers[0].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_199aten__view")
#loc1673 = loc("2958|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|Linear[getattr(resampler.layers[0].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|permute_365aten__permute")
#loc1674 = loc("2959|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|Linear[getattr(resampler.layers[0].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_199aten__mm")
#loc1675 = loc("2960|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_397aten__add")
#loc1676 = loc("2986|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_349xla__mark_tensor")
#loc1677 = loc("2989|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_200aten__view")
#loc1678 = loc("2988|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|permute_366aten__permute")
#loc1679 = loc("2989|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_200aten__mm")
#loc1680 = loc("2994|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|view_133aten__view")
#loc1681 = loc("2995|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|permute_369aten__permute")
#loc1682 = loc("3000|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_307xla__cast")
#loc1683 = loc("3003|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_208aten__mul")
#loc1684 = loc("2973|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_345xla__mark_tensor")
#loc1685 = loc("2987|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2248|cat_2aten__cat")
#loc1686 = loc("2991|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_201aten__view")
#loc1687 = loc("2990|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|permute_367aten__permute")
#loc1688 = loc("2991|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_201aten__mm")
#loc1689 = loc("2996|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|view_134aten__view")
#loc1690 = loc("2997|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|permute_370aten__permute")
#loc1691 = loc("3001|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_308xla__cast")
#loc1692 = loc("3004|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|permute_372aten__permute")
#loc1693 = loc("3005|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_209aten__mul")
#loc1694 = loc("3007|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_33aten__einsum")
#loc1695 = loc("3008|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|eq_33aten__eq")
#loc1696 = loc("3009|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_66aten__logical_not")
#loc1697 = loc("3010|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_34aten__any")
#loc1698 = loc("3011|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_67aten__logical_not")
#loc1699 = loc("3013|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_33aten__expand")
#loc1700 = loc("3007|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_33aten__softmax")
#loc1701 = loc("3013|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_33aten__where")
#loc1702 = loc("2992|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|permute_368aten__permute")
#loc1703 = loc("2993|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|matmul_202aten__mm")
#loc1704 = loc("2998|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|view_135aten__view")
#loc1705 = loc("2999|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|permute_371aten__permute")
#loc1706 = loc("3002|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_309xla__cast")
#loc1707 = loc("3015|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_311aten__einsum")
#loc1708 = loc("3015|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_311xla__cast")
#loc1709 = loc("3017|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2769|clone_35aten__permute")
#loc1710 = loc("3020|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_203aten__view")
#loc1711 = loc("3019|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|permute_374aten__permute")
#loc1712 = loc("3020|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_203aten__mm")
#loc1713 = loc("3021|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Dropout[resampler.layers[1].attn.to_out[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2775|clone_36aten__view")
#loc1714 = loc("3022|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2783|div_1aten__div")
#loc1715 = loc("3023|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2249|add_402aten__add")
#loc1716 = loc("3036|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_353xla__mark_tensor")
#loc1717 = loc("3038|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|GELU[getattr(resampler.layers[1].ff, '1').net[0]]|Linear[getattr(resampler.layers[1].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_204aten__view")
#loc1718 = loc("3037|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|GELU[getattr(resampler.layers[1].ff, '1').net[0]]|Linear[getattr(resampler.layers[1].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|permute_375aten__permute")
#loc1719 = loc("3038|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|GELU[getattr(resampler.layers[1].ff, '1').net[0]]|Linear[getattr(resampler.layers[1].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_204aten__mm")
#loc1720 = loc("3042|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|Dropout[getattr(resampler.layers[1].ff, '1').net[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|clone_37xla__mark_tensor")
#loc1721 = loc("3044|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|Linear[getattr(resampler.layers[1].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_205aten__view")
#loc1722 = loc("3043|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|Linear[getattr(resampler.layers[1].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|permute_376aten__permute")
#loc1723 = loc("3044|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|Linear[getattr(resampler.layers[1].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_205aten__mm")
#loc1724 = loc("3045|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_405aten__add")
#loc1725 = loc("3071|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_363xla__mark_tensor")
#loc1726 = loc("3074|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_206aten__view")
#loc1727 = loc("3073|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|permute_377aten__permute")
#loc1728 = loc("3074|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_206aten__mm")
#loc1729 = loc("3079|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|view_137aten__view")
#loc1730 = loc("3080|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|permute_380aten__permute")
#loc1731 = loc("3085|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_318xla__cast")
#loc1732 = loc("3088|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_216aten__mul")
#loc1733 = loc("3058|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_359xla__mark_tensor")
#loc1734 = loc("3072|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2248|cat_3aten__cat")
#loc1735 = loc("3076|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_207aten__view")
#loc1736 = loc("3075|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|permute_378aten__permute")
#loc1737 = loc("3076|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_207aten__mm")
#loc1738 = loc("3081|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|view_138aten__view")
#loc1739 = loc("3082|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|permute_381aten__permute")
#loc1740 = loc("3086|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_319xla__cast")
#loc1741 = loc("3089|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|permute_383aten__permute")
#loc1742 = loc("3090|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_217aten__mul")
#loc1743 = loc("3092|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_34aten__einsum")
#loc1744 = loc("3093|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|eq_34aten__eq")
#loc1745 = loc("3094|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_68aten__logical_not")
#loc1746 = loc("3095|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_35aten__any")
#loc1747 = loc("3096|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_69aten__logical_not")
#loc1748 = loc("3098|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_34aten__expand")
#loc1749 = loc("3092|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_34aten__softmax")
#loc1750 = loc("3098|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_34aten__where")
#loc1751 = loc("3077|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|permute_379aten__permute")
#loc1752 = loc("3078|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|matmul_208aten__mm")
#loc1753 = loc("3083|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|view_139aten__view")
#loc1754 = loc("3084|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|permute_382aten__permute")
#loc1755 = loc("3087|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_320xla__cast")
#loc1756 = loc("3100|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_322aten__einsum")
#loc1757 = loc("3100|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_322xla__cast")
#loc1758 = loc("3102|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2769|clone_38aten__permute")
#loc1759 = loc("3105|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_209aten__view")
#loc1760 = loc("3104|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|permute_385aten__permute")
#loc1761 = loc("3105|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_209aten__mm")
#loc1762 = loc("3106|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Dropout[resampler.layers[2].attn.to_out[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2775|clone_39aten__view")
#loc1763 = loc("3107|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2783|div_2aten__div")
#loc1764 = loc("3108|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2249|add_410aten__add")
#loc1765 = loc("3121|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_367xla__mark_tensor")
#loc1766 = loc("3123|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|GELU[getattr(resampler.layers[2].ff, '1').net[0]]|Linear[getattr(resampler.layers[2].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_210aten__view")
#loc1767 = loc("3122|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|GELU[getattr(resampler.layers[2].ff, '1').net[0]]|Linear[getattr(resampler.layers[2].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|permute_386aten__permute")
#loc1768 = loc("3123|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|GELU[getattr(resampler.layers[2].ff, '1').net[0]]|Linear[getattr(resampler.layers[2].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_210aten__mm")
#loc1769 = loc("3127|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|Dropout[getattr(resampler.layers[2].ff, '1').net[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|clone_40xla__mark_tensor")
#loc1770 = loc("3129|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|Linear[getattr(resampler.layers[2].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_211aten__view")
#loc1771 = loc("3128|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|Linear[getattr(resampler.layers[2].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|permute_387aten__permute")
#loc1772 = loc("3129|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|Linear[getattr(resampler.layers[2].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_211aten__mm")
#loc1773 = loc("3130|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_413aten__add")
#loc1774 = loc("3156|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_377xla__mark_tensor")
#loc1775 = loc("3159|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_212aten__view")
#loc1776 = loc("3158|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|permute_388aten__permute")
#loc1777 = loc("3159|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_212aten__mm")
#loc1778 = loc("3164|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|view_141aten__view")
#loc1779 = loc("3165|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|permute_391aten__permute")
#loc1780 = loc("3170|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_329xla__cast")
#loc1781 = loc("3173|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_224aten__mul")
#loc1782 = loc("3143|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_373xla__mark_tensor")
#loc1783 = loc("3157|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2248|cat_4aten__cat")
#loc1784 = loc("3161|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_213aten__view")
#loc1785 = loc("3160|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|permute_389aten__permute")
#loc1786 = loc("3161|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_213aten__mm")
#loc1787 = loc("3166|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|view_142aten__view")
#loc1788 = loc("3167|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|permute_392aten__permute")
#loc1789 = loc("3171|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_330xla__cast")
#loc1790 = loc("3174|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|permute_394aten__permute")
#loc1791 = loc("3175|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_225aten__mul")
#loc1792 = loc("3177|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_35aten__einsum")
#loc1793 = loc("3178|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|eq_35aten__eq")
#loc1794 = loc("3179|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_70aten__logical_not")
#loc1795 = loc("3180|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_36aten__any")
#loc1796 = loc("3181|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_71aten__logical_not")
#loc1797 = loc("3183|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_35aten__expand")
#loc1798 = loc("3177|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_35aten__softmax")
#loc1799 = loc("3183|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_35aten__where")
#loc1800 = loc("3162|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|permute_390aten__permute")
#loc1801 = loc("3163|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|matmul_214aten__mm")
#loc1802 = loc("3168|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|view_143aten__view")
#loc1803 = loc("3169|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|permute_393aten__permute")
#loc1804 = loc("3172|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_331xla__cast")
#loc1805 = loc("3185|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_333aten__einsum")
#loc1806 = loc("3185|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_333xla__cast")
#loc1807 = loc("3187|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2769|clone_41aten__permute")
#loc1808 = loc("3190|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_215aten__view")
#loc1809 = loc("3189|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|permute_396aten__permute")
#loc1810 = loc("3190|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_215aten__mm")
#loc1811 = loc("3191|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Dropout[resampler.layers[3].attn.to_out[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2775|clone_42aten__view")
#loc1812 = loc("3192|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2783|div_3aten__div")
#loc1813 = loc("3193|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2249|add_418aten__add")
#loc1814 = loc("3206|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_381xla__mark_tensor")
#loc1815 = loc("3208|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|GELU[getattr(resampler.layers[3].ff, '1').net[0]]|Linear[getattr(resampler.layers[3].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_216aten__view")
#loc1816 = loc("3207|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|GELU[getattr(resampler.layers[3].ff, '1').net[0]]|Linear[getattr(resampler.layers[3].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|permute_397aten__permute")
#loc1817 = loc("3208|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|GELU[getattr(resampler.layers[3].ff, '1').net[0]]|Linear[getattr(resampler.layers[3].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_216aten__mm")
#loc1818 = loc("3212|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|Dropout[getattr(resampler.layers[3].ff, '1').net[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|clone_43xla__mark_tensor")
#loc1819 = loc("3214|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|Linear[getattr(resampler.layers[3].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_217aten__view")
#loc1820 = loc("3213|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|Linear[getattr(resampler.layers[3].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|permute_398aten__permute")
#loc1821 = loc("3214|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|Linear[getattr(resampler.layers[3].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_217aten__mm")
#loc1822 = loc("3215|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_421aten__add")
#loc1823 = loc("3217|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2309|matmul_218aten__view")
#loc1824 = loc("3216|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2309|permute_399aten__permute")
#loc1825 = loc("3217|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2309|matmul_218aten__mm")
#loc1826 = loc("3218|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2309|add_422aten__add")
#loc1827 = loc("3231|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|mark_tensor_387xla__mark_tensor")
#loc1828 = loc("558|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPVisionEmbeddings[image_encoder.vision_model.embeddings]|Conv2d[image_encoder.vision_model.embeddings.patch_embedding]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:195|forward|202|convolutionaten__convolution_overrideable_weight"(#loc11))
