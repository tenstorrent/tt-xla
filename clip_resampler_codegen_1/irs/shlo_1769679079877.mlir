#loc1 = loc("-1|unknown|unknown|-1|unknownxla__device_data")
#loc41 = loc("616|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_1aten__any")
#loc95 = loc("690|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_2aten__any")
#loc149 = loc("764|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_3aten__any")
#loc203 = loc("838|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_4aten__any")
#loc257 = loc("912|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_5aten__any")
#loc311 = loc("986|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_6aten__any")
#loc365 = loc("1060|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_7aten__any")
#loc419 = loc("1134|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_8aten__any")
#loc473 = loc("1208|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_9aten__any")
#loc527 = loc("1282|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_10aten__any")
#loc581 = loc("1356|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_11aten__any")
#loc635 = loc("1430|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_12aten__any")
#loc689 = loc("1504|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_13aten__any")
#loc743 = loc("1578|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_14aten__any")
#loc797 = loc("1652|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_15aten__any")
#loc851 = loc("1726|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_16aten__any")
#loc905 = loc("1800|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_17aten__any")
#loc959 = loc("1874|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_18aten__any")
#loc1013 = loc("1948|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_19aten__any")
#loc1067 = loc("2022|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_20aten__any")
#loc1121 = loc("2096|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_21aten__any")
#loc1175 = loc("2170|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_22aten__any")
#loc1229 = loc("2244|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_23aten__any")
#loc1283 = loc("2318|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_24aten__any")
#loc1337 = loc("2392|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_25aten__any")
#loc1391 = loc("2466|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_26aten__any")
#loc1445 = loc("2540|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_27aten__any")
#loc1499 = loc("2614|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_28aten__any")
#loc1553 = loc("2688|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_29aten__any")
#loc1607 = loc("2762|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_30aten__any")
#loc1661 = loc("2836|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_31aten__any")
#loc1711 = loc("2925|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_33aten__any")
#loc1762 = loc("3010|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_34aten__any")
#loc1813 = loc("3095|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_35aten__any")
#loc1864 = loc("3180|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_36aten__any")
#loc1899 = loc("3219|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|mark_tensor_384xla__mark_tensor")
#loc1900 = loc("3220|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|mark_tensor_385xla__mark_tensor")
#loc1901 = loc("3221|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|mark_tensor_386xla__mark_tensor")
#loc1912 = loc("3194|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_378xla__mark_tensor")
#loc1913 = loc("3195|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_379xla__mark_tensor")
#loc1914 = loc("3196|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_380xla__mark_tensor")
#loc1925 = loc("3131|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_370xla__mark_tensor")
#loc1926 = loc("3132|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_371xla__mark_tensor")
#loc1927 = loc("3133|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_372xla__mark_tensor")
#loc1938 = loc("3046|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_356xla__mark_tensor")
#loc1939 = loc("3047|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_357xla__mark_tensor")
#loc1940 = loc("3048|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_358xla__mark_tensor")
#loc1951 = loc("3039|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|GELU[getattr(resampler.layers[1].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|mark_tensor_354xla__mark_tensor")
#loc1953 = loc("2954|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|GELU[getattr(resampler.layers[0].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|mark_tensor_340xla__mark_tensor")
#loc1955 = loc("2939|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_336xla__mark_tensor")
#loc1956 = loc("2940|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_337xla__mark_tensor")
#loc1957 = loc("2941|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_338xla__mark_tensor")
#loc1968 = loc("2791|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[29].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_302xla__mark_tensor")
#loc1970 = loc("2724|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_294xla__mark_tensor")
#loc1971 = loc("2725|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_295xla__mark_tensor")
#loc1972 = loc("2726|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_296xla__mark_tensor")
#loc1983 = loc("2717|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[28].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_292xla__mark_tensor")
#loc1985 = loc("2974|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_346xla__mark_tensor")
#loc1986 = loc("2975|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_347xla__mark_tensor")
#loc1987 = loc("2976|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_348xla__mark_tensor")
#loc1998 = loc("2701|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_288xla__mark_tensor")
#loc1999 = loc("2702|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_289xla__mark_tensor")
#loc2000 = loc("2703|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_290xla__mark_tensor")
#loc2011 = loc("2650|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_284xla__mark_tensor")
#loc2012 = loc("2651|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_285xla__mark_tensor")
#loc2013 = loc("2652|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_286xla__mark_tensor")
#loc2024 = loc("3124|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|GELU[getattr(resampler.layers[2].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|mark_tensor_368xla__mark_tensor")
#loc2026 = loc("2576|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_274xla__mark_tensor")
#loc2027 = loc("2577|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_275xla__mark_tensor")
#loc2028 = loc("2578|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_276xla__mark_tensor")
#loc2039 = loc("2553|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_268xla__mark_tensor")
#loc2040 = loc("2554|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_269xla__mark_tensor")
#loc2041 = loc("2555|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_270xla__mark_tensor")
#loc2052 = loc("2502|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_264xla__mark_tensor")
#loc2053 = loc("2503|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_265xla__mark_tensor")
#loc2054 = loc("2504|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_266xla__mark_tensor")
#loc2065 = loc("2495|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[25].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_262xla__mark_tensor")
#loc2067 = loc("2479|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_258xla__mark_tensor")
#loc2068 = loc("2480|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_259xla__mark_tensor")
#loc2069 = loc("2481|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_260xla__mark_tensor")
#loc2080 = loc("2428|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_254xla__mark_tensor")
#loc2081 = loc("2429|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_255xla__mark_tensor")
#loc2082 = loc("2430|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_256xla__mark_tensor")
#loc2093 = loc("2421|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[24].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_252xla__mark_tensor")
#loc2095 = loc("2405|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_248xla__mark_tensor")
#loc2096 = loc("2406|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_249xla__mark_tensor")
#loc2097 = loc("2407|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_250xla__mark_tensor")
#loc2108 = loc("2354|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_244xla__mark_tensor")
#loc2109 = loc("2355|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_245xla__mark_tensor")
#loc2110 = loc("2356|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_246xla__mark_tensor")
#loc2121 = loc("2347|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[23].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_242xla__mark_tensor")
#loc2123 = loc("2331|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_238xla__mark_tensor")
#loc2124 = loc("2332|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_239xla__mark_tensor")
#loc2125 = loc("2333|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_240xla__mark_tensor")
#loc2136 = loc("2206|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_224xla__mark_tensor")
#loc2137 = loc("2207|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_225xla__mark_tensor")
#loc2138 = loc("2208|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_226xla__mark_tensor")
#loc2149 = loc("2199|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[21].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_222xla__mark_tensor")
#loc2151 = loc("3024|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_350xla__mark_tensor")
#loc2152 = loc("3025|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_351xla__mark_tensor")
#loc2153 = loc("3026|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_352xla__mark_tensor")
#loc2164 = loc("2183|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_218xla__mark_tensor")
#loc2165 = loc("2184|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_219xla__mark_tensor")
#loc2166 = loc("2185|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_220xla__mark_tensor")
#loc2177 = loc("2125|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[20].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_212xla__mark_tensor")
#loc2179 = loc("2569|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[26].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_272xla__mark_tensor")
#loc2181 = loc("2058|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_204xla__mark_tensor")
#loc2182 = loc("2059|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_205xla__mark_tensor")
#loc2183 = loc("2060|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_206xla__mark_tensor")
#loc2194 = loc("2051|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[19].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_202xla__mark_tensor")
#loc2196 = loc("1984|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_194xla__mark_tensor")
#loc2197 = loc("1985|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_195xla__mark_tensor")
#loc2198 = loc("1986|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_196xla__mark_tensor")
#loc2209 = loc("1237|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[8].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_92xla__mark_tensor")
#loc2211 = loc("1318|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_104xla__mark_tensor")
#loc2212 = loc("1319|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_105xla__mark_tensor")
#loc2213 = loc("1320|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_106xla__mark_tensor")
#loc2224 = loc("1170|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_84xla__mark_tensor")
#loc2225 = loc("1171|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_85xla__mark_tensor")
#loc2226 = loc("1172|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_86xla__mark_tensor")
#loc2237 = loc("2865|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[30].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_312xla__mark_tensor")
#loc2239 = loc("1163|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[7].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_82xla__mark_tensor")
#loc2241 = loc("2849|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_308xla__mark_tensor")
#loc2242 = loc("2850|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_309xla__mark_tensor")
#loc2243 = loc("2851|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_310xla__mark_tensor")
#loc2254 = loc("2273|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[22].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_232xla__mark_tensor")
#loc2256 = loc("2257|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_228xla__mark_tensor")
#loc2257 = loc("2258|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_229xla__mark_tensor")
#loc2258 = loc("2259|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_230xla__mark_tensor")
#loc2269 = loc("1089|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[6].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_72xla__mark_tensor")
#loc2271 = loc("1073|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_68xla__mark_tensor")
#loc2272 = loc("1074|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_69xla__mark_tensor")
#loc2273 = loc("1075|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_70xla__mark_tensor")
#loc2284 = loc("645|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[0].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_12xla__mark_tensor")
#loc2286 = loc("3109|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_364xla__mark_tensor")
#loc2287 = loc("3110|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_365xla__mark_tensor")
#loc2288 = loc("3111|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_366xla__mark_tensor")
#loc2299 = loc("1221|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_88xla__mark_tensor")
#loc2300 = loc("1222|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_89xla__mark_tensor")
#loc2301 = loc("1223|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_90xla__mark_tensor")
#loc2312 = loc("726|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_24xla__mark_tensor")
#loc2313 = loc("727|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_25xla__mark_tensor")
#loc2314 = loc("728|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_26xla__mark_tensor")
#loc2325 = loc("1147|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_78xla__mark_tensor")
#loc2326 = loc("1148|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_79xla__mark_tensor")
#loc2327 = loc("1149|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_80xla__mark_tensor")
#loc2338 = loc("1836|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_174xla__mark_tensor")
#loc2339 = loc("1837|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_175xla__mark_tensor")
#loc2340 = loc("1838|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_176xla__mark_tensor")
#loc2351 = loc("941|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[4].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_52xla__mark_tensor")
#loc2353 = loc("2798|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_304xla__mark_tensor")
#loc2354 = loc("2799|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_305xla__mark_tensor")
#loc2355 = loc("2800|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_306xla__mark_tensor")
#loc2366 = loc("777|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_28xla__mark_tensor")
#loc2367 = loc("778|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_29xla__mark_tensor")
#loc2368 = loc("779|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_30xla__mark_tensor")
#loc2379 = loc("1665|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_148xla__mark_tensor")
#loc2380 = loc("1666|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_149xla__mark_tensor")
#loc2381 = loc("1667|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_150xla__mark_tensor")
#loc2392 = loc("578|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_4xla__mark_tensor")
#loc2393 = loc("579|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_5xla__mark_tensor")
#loc2394 = loc("580|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_6xla__mark_tensor")
#loc2405 = loc("1533|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[12].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_132xla__mark_tensor")
#loc2407 = loc("629|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_8xla__mark_tensor")
#loc2408 = loc("630|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_9xla__mark_tensor")
#loc2409 = loc("631|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_10xla__mark_tensor")
#loc2420 = loc("703|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_18xla__mark_tensor")
#loc2421 = loc("704|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_19xla__mark_tensor")
#loc2422 = loc("705|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_20xla__mark_tensor")
#loc2433 = loc("719|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[1].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_22xla__mark_tensor")
#loc2435 = loc("565|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|mark_tensorxla__mark_tensor")
#loc2436 = loc("566|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|mark_tensor_1xla__mark_tensor")
#loc2437 = loc("567|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|mark_tensor_2xla__mark_tensor")
#loc2448 = loc("925|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_48xla__mark_tensor")
#loc2449 = loc("926|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_49xla__mark_tensor")
#loc2450 = loc("927|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_50xla__mark_tensor")
#loc2461 = loc("652|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_14xla__mark_tensor")
#loc2462 = loc("653|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_15xla__mark_tensor")
#loc2463 = loc("654|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_16xla__mark_tensor")
#loc2474 = loc("1688|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_154xla__mark_tensor")
#loc2475 = loc("1689|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_155xla__mark_tensor")
#loc2476 = loc("1690|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_156xla__mark_tensor")
#loc2487 = loc("867|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[3].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_42xla__mark_tensor")
#loc2489 = loc("999|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_58xla__mark_tensor")
#loc2490 = loc("1000|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_59xla__mark_tensor")
#loc2491 = loc("1001|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_60xla__mark_tensor")
#loc2502 = loc("793|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[2].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_32xla__mark_tensor")
#loc2504 = loc("948|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_54xla__mark_tensor")
#loc2505 = loc("949|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_55xla__mark_tensor")
#loc2506 = loc("950|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_56xla__mark_tensor")
#loc2517 = loc("800|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_34xla__mark_tensor")
#loc2518 = loc("801|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_35xla__mark_tensor")
#loc2519 = loc("802|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_36xla__mark_tensor")
#loc2530 = loc("1762|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_164xla__mark_tensor")
#loc2531 = loc("1763|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_165xla__mark_tensor")
#loc2532 = loc("1764|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_166xla__mark_tensor")
#loc2543 = loc("1295|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_98xla__mark_tensor")
#loc2544 = loc("1296|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_99xla__mark_tensor")
#loc2545 = loc("1297|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_100xla__mark_tensor")
#loc2556 = loc("2627|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_278xla__mark_tensor")
#loc2557 = loc("2628|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_279xla__mark_tensor")
#loc2558 = loc("2629|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_280xla__mark_tensor")
#loc2569 = loc("2109|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_208xla__mark_tensor")
#loc2570 = loc("2110|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_209xla__mark_tensor")
#loc2571 = loc("2111|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_210xla__mark_tensor")
#loc2582 = loc("1739|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_158xla__mark_tensor")
#loc2583 = loc("1740|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_159xla__mark_tensor")
#loc2584 = loc("1741|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_160xla__mark_tensor")
#loc2595 = loc("1311|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[9].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_102xla__mark_tensor")
#loc2597 = loc("3059|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_360xla__mark_tensor")
#loc2598 = loc("3060|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_361xla__mark_tensor")
#loc2599 = loc("3061|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_362xla__mark_tensor")
#loc2610 = loc("1813|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_168xla__mark_tensor")
#loc2611 = loc("1814|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_169xla__mark_tensor")
#loc2612 = loc("1815|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_170xla__mark_tensor")
#loc2623 = loc("2035|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_198xla__mark_tensor")
#loc2624 = loc("2036|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_199xla__mark_tensor")
#loc2625 = loc("2037|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_200xla__mark_tensor")
#loc2636 = loc("851|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_38xla__mark_tensor")
#loc2637 = loc("852|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_39xla__mark_tensor")
#loc2638 = loc("853|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_40xla__mark_tensor")
#loc2649 = loc("1977|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[18].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_192xla__mark_tensor")
#loc2651 = loc("3209|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|GELU[getattr(resampler.layers[3].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|mark_tensor_382xla__mark_tensor")
#loc2653 = loc("1369|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_108xla__mark_tensor")
#loc2654 = loc("1370|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_109xla__mark_tensor")
#loc2655 = loc("1371|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_110xla__mark_tensor")
#loc2666 = loc("2889|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_332xla__mark_tensor")
#loc2667 = loc("2890|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_333xla__mark_tensor")
#loc2668 = loc("2891|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_334xla__mark_tensor")
#loc2679 = loc("1385|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[10].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_112xla__mark_tensor")
#loc2681 = loc("2132|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_214xla__mark_tensor")
#loc2682 = loc("2133|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_215xla__mark_tensor")
#loc2683 = loc("2134|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_216xla__mark_tensor")
#loc2694 = loc("1392|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_114xla__mark_tensor")
#loc2695 = loc("1393|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_115xla__mark_tensor")
#loc2696 = loc("1394|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_116xla__mark_tensor")
#loc2707 = loc("1443|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_118xla__mark_tensor")
#loc2708 = loc("1444|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_119xla__mark_tensor")
#loc2709 = loc("1445|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_120xla__mark_tensor")
#loc2720 = loc("2280|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_234xla__mark_tensor")
#loc2721 = loc("2281|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_235xla__mark_tensor")
#loc2722 = loc("2282|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_236xla__mark_tensor")
#loc2733 = loc("1244|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_94xla__mark_tensor")
#loc2734 = loc("1245|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_95xla__mark_tensor")
#loc2735 = loc("1246|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_96xla__mark_tensor")
#loc2746 = loc("1517|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_128xla__mark_tensor")
#loc2747 = loc("1518|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_129xla__mark_tensor")
#loc2748 = loc("1519|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_130xla__mark_tensor")
#loc2759 = loc("1015|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[5].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_62xla__mark_tensor")
#loc2761 = loc("1961|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_188xla__mark_tensor")
#loc2762 = loc("1962|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_189xla__mark_tensor")
#loc2763 = loc("1963|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_190xla__mark_tensor")
#loc2774 = loc("1591|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_138xla__mark_tensor")
#loc2775 = loc("1592|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_139xla__mark_tensor")
#loc2776 = loc("1593|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_140xla__mark_tensor")
#loc2787 = loc("2876|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_328xla__mark_tensor")
#loc2788 = loc("2877|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_329xla__mark_tensor")
#loc2789 = loc("2878|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_330xla__mark_tensor")
#loc2800 = loc("1022|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_64xla__mark_tensor")
#loc2801 = loc("1023|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_65xla__mark_tensor")
#loc2802 = loc("1024|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_66xla__mark_tensor")
#loc2813 = loc("1755|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[15].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_162xla__mark_tensor")
#loc2815 = loc("1607|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[13].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_142xla__mark_tensor")
#loc2817 = loc("2643|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[27].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_282xla__mark_tensor")
#loc2819 = loc("1096|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_74xla__mark_tensor")
#loc2820 = loc("1097|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_75xla__mark_tensor")
#loc2821 = loc("1098|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_76xla__mark_tensor")
#loc2832 = loc("1614|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_144xla__mark_tensor")
#loc2833 = loc("1615|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_145xla__mark_tensor")
#loc2834 = loc("1616|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_146xla__mark_tensor")
#loc2845 = loc("2961|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_342xla__mark_tensor")
#loc2846 = loc("2962|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_343xla__mark_tensor")
#loc2847 = loc("2963|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_344xla__mark_tensor")
#loc2858 = loc("1459|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[11].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_122xla__mark_tensor")
#loc2860 = loc("1681|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[14].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_152xla__mark_tensor")
#loc2862 = loc("874|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_44xla__mark_tensor")
#loc2863 = loc("875|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_45xla__mark_tensor")
#loc2864 = loc("876|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_46xla__mark_tensor")
#loc2875 = loc("1466|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_124xla__mark_tensor")
#loc2876 = loc("1467|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_125xla__mark_tensor")
#loc2877 = loc("1468|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_126xla__mark_tensor")
#loc2888 = loc("1829|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[16].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_172xla__mark_tensor")
#loc2890 = loc("1887|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_178xla__mark_tensor")
#loc2891 = loc("1888|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_179xla__mark_tensor")
#loc2892 = loc("1889|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_180xla__mark_tensor")
#loc2903 = loc("3144|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_374xla__mark_tensor")
#loc2904 = loc("3145|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_375xla__mark_tensor")
#loc2905 = loc("3146|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_376xla__mark_tensor")
#loc2916 = loc("2775|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_298xla__mark_tensor")
#loc2917 = loc("2776|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_299xla__mark_tensor")
#loc2918 = loc("2777|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_300xla__mark_tensor")
#loc2929 = loc("1540|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_134xla__mark_tensor")
#loc2930 = loc("1541|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_135xla__mark_tensor")
#loc2931 = loc("1542|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_136xla__mark_tensor")
#loc2942 = loc("1903|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[17].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_182xla__mark_tensor")
#loc2944 = loc("1910|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_184xla__mark_tensor")
#loc2945 = loc("1911|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_185xla__mark_tensor")
#loc2946 = loc("1912|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_186xla__mark_tensor")
module @SyncTensorsGraph.13945 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<2048xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg1: tensor<2048xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg2: tensor<2048xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg3: tensor<2048x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg4: tensor<1x16x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg5: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg6: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg7: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg8: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg9: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg10: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg11: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg12: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg13: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg14: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg15: tensor<5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg16: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg17: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg18: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg19: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg20: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg21: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg22: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg23: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg24: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg25: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg26: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg27: tensor<5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg28: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg29: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg30: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg31: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg32: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg33: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg34: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg35: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg36: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg37: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg38: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg39: tensor<5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg40: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg41: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg42: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg43: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg44: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg45: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg46: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg47: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg48: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg49: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg50: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg51: tensor<5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg52: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg53: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg54: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg55: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg56: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg57: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg58: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg59: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg60: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg61: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg62: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg63: tensor<5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg64: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg65: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg66: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg67: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg68: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg69: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg70: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg71: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg72: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg73: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg74: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg75: tensor<5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg76: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg77: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg78: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg79: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg80: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg81: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg82: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg83: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg84: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg85: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg86: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg87: tensor<5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg88: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg89: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg90: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg91: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg92: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg93: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg94: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg95: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg96: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg97: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg98: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg99: tensor<5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg100: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg101: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg102: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg103: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg104: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg105: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg106: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg107: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg108: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg109: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg110: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg111: tensor<5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg112: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg113: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg114: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg115: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg116: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg117: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg118: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg119: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg120: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg121: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg122: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg123: tensor<5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg124: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg125: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg126: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg127: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg128: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg129: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg130: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg131: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg132: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg133: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg134: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg135: tensor<5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg136: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg137: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg138: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg139: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg140: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg141: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg142: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg143: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg144: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg145: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg146: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg147: tensor<5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg148: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg149: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg150: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg151: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg152: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg153: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg154: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg155: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg156: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg157: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg158: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg159: tensor<5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg160: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg161: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg162: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg163: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg164: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg165: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg166: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg167: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg168: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg169: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg170: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg171: tensor<5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg172: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg173: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg174: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg175: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg176: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg177: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg178: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg179: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg180: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg181: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg182: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg183: tensor<5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg184: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg185: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg186: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg187: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg188: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg189: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg190: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg191: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg192: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg193: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg194: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg195: tensor<5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg196: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg197: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg198: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg199: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg200: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg201: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg202: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg203: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg204: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg205: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg206: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg207: tensor<5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg208: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg209: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg210: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg211: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg212: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg213: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg214: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg215: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg216: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg217: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg218: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg219: tensor<5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg220: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg221: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg222: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg223: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg224: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg225: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg226: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg227: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg228: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg229: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg230: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg231: tensor<5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg232: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg233: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg234: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg235: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg236: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg237: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg238: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg239: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg240: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg241: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg242: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg243: tensor<5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg244: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg245: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg246: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg247: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg248: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg249: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg250: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg251: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg252: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg253: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg254: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg255: tensor<5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg256: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg257: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg258: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg259: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg260: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg261: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg262: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg263: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg264: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg265: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg266: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg267: tensor<5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg268: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg269: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg270: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg271: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg272: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg273: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg274: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg275: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg276: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg277: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg278: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg279: tensor<5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg280: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg281: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg282: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg283: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg284: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg285: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg286: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg287: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg288: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg289: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg290: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg291: tensor<5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg292: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg293: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg294: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg295: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg296: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg297: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg298: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg299: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg300: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg301: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg302: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg303: tensor<5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg304: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg305: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg306: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg307: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg308: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg309: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg310: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg311: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg312: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg313: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg314: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg315: tensor<5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg316: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg317: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg318: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg319: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg320: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg321: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg322: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg323: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg324: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg325: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg326: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg327: tensor<5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg328: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg329: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg330: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg331: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg332: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg333: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg334: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg335: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg336: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg337: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg338: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg339: tensor<5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg340: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg341: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg342: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg343: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg344: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg345: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg346: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg347: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg348: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg349: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg350: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg351: tensor<5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg352: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg353: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg354: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg355: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg356: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg357: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg358: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg359: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg360: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg361: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg362: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg363: tensor<5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg364: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg365: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg366: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg367: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg368: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg369: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg370: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg371: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg372: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg373: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg374: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg375: tensor<5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg376: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg377: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg378: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg379: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg380: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg381: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg382: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg383: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg384: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg385: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg386: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg387: tensor<1x257xi64> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg388: tensor<257x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg389: tensor<1280x3x14x14xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg390: tensor<1x3x224x224xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg391: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg392: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg393: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg394: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg395: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg396: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg397: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg398: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg399: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg400: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg401: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg402: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg403: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg404: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg405: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg406: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg407: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg408: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg409: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg410: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg411: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg412: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg413: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg414: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg415: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg416: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg417: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg418: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg419: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg420: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg421: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg422: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg423: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg424: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg425: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg426: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg427: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg428: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg429: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg430: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg431: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg432: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg433: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg434: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg435: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg436: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg437: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg438: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg439: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg440: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg441: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg442: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg443: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg444: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg445: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg446: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg447: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg448: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg449: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg450: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg451: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg452: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg453: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg454: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg455: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg456: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg457: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg458: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg459: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg460: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg461: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg462: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg463: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg464: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg465: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg466: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg467: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg468: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg469: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg470: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg471: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg472: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg473: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg474: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg475: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg476: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg477: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg478: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg479: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg480: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg481: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg482: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg483: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg484: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg485: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg486: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg487: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg488: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg489: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg490: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg491: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg492: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg493: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg494: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg495: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg496: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg497: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg498: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg499: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg500: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg501: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg502: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg503: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg504: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg505: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg506: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg507: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg508: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg509: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg510: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg511: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg512: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg513: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg514: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg515: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg516: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg517: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg518: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg519: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg520: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg521: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg522: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg523: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg524: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg525: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg526: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg527: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg528: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg529: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg530: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg531: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg532: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg533: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg534: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg535: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg536: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg537: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg538: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg539: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg540: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg541: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg542: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg543: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg544: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg545: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg546: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg547: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg548: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg549: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg550: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg551: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg552: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg553: tensor<1280x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg554: tensor<1280x5120xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg555: tensor<5120x1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg556: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg557: tensor<1280xbf16> loc("-1|unknown|unknown|-1|unknownxla__device_data")) -> tensor<1x16x2048xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x16x1280xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<1x20x16x273xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0xFFF0000000000000> : tensor<1x20x16x273xf64> loc(#loc)
    %cst_2 = stablehlo.constant dense<0.353553385> : tensor<1x20x64x273xf32> loc(#loc)
    %cst_3 = stablehlo.constant dense<0.000000e+00> : tensor<1x16x257x257xf32> loc(#loc)
    %cst_4 = stablehlo.constant dense<0xFFF0000000000000> : tensor<1x16x257x257xf64> loc(#loc)
    %cst_5 = stablehlo.constant dense<0.334370166> : tensor<1x16x80x257xf32> loc(#loc)
    %cst_6 = stablehlo.constant dense<0.334370166> : tensor<1x16x257x80xf32> loc(#loc)
    %cst_7 = stablehlo.constant dense<0.353553385> : tensor<1x20x16x64xf32> loc(#loc)
    %cst_8 = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)
    %c = stablehlo.constant dense<true> : tensor<i1> loc(#loc)
    %c_9 = stablehlo.constant dense<false> : tensor<i1> loc(#loc)
    %cst_10 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.custom_call @tt.mark_argument(%arg4) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_latents"}} : (tensor<1x16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc2)
    %1 = stablehlo.reshape %arg8 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2 = stablehlo.custom_call @tt.mark_argument(%1) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_0_ln1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3 = stablehlo.reshape %2 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %4 = stablehlo.reshape %arg7 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %5 = stablehlo.custom_call @tt.mark_argument(%4) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_0_ln1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %6 = stablehlo.reshape %5 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %7 = stablehlo.composite "tenstorrent.layer_norm" %0, %3, %6 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_54} : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc4)
    %8 = stablehlo.reshape %7 : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc5)
    %9 = stablehlo.reshape %arg517 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %10 = stablehlo.custom_call @tt.mark_argument(%9) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_0_attn_to_q_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %11 = stablehlo.reshape %10 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %12 = stablehlo.transpose %11, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc6)
    %13 = stablehlo.dot_general %8, %12, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc7)
    %14 = stablehlo.reshape %13 : (tensor<16x1280xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc8)
    %15 = stablehlo.transpose %14, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,16,64]{3,1,2,0}"} : (tensor<1x16x20x64xbf16>) -> tensor<1x20x16x64xbf16> loc(#loc9)
    %16 = stablehlo.convert %15 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,16,64]{3,1,2,0}"} : (tensor<1x20x16x64xbf16>) -> tensor<1x20x16x64xf32> loc(#loc10)
    %17 = stablehlo.multiply %16, %cst_7 : tensor<1x20x16x64xf32> loc(#loc11)
    %18 = stablehlo.reshape %arg391 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %19 = stablehlo.custom_call @tt.mark_argument(%18) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_embeddings_class_embedding"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %20 = stablehlo.custom_call @tt.mark_argument(%arg390) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_0"}} : (tensor<1x3x224x224xbf16>) -> tensor<1x3x224x224xbf16> loc(#loc2)
    %21 = stablehlo.custom_call @tt.mark_argument(%arg389) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_embeddings_patch_embedding_weight"}} : (tensor<1280x3x14x14xbf16>) -> tensor<1280x3x14x14xbf16> loc(#loc2)
    %22 = stablehlo.convolution(%20, %21) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [14, 14]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x3x224x224xbf16>, tensor<1280x3x14x14xbf16>) -> tensor<1x1280x16x16xbf16> loc(#loc12)
    %23 = stablehlo.reshape %22 : (tensor<1x1280x16x16xbf16>) -> tensor<1x1280x256xbf16> loc(#loc13)
    %24 = stablehlo.transpose %23, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "bf16[1,256,1280]{1,2,0}"} : (tensor<1x1280x256xbf16>) -> tensor<1x256x1280xbf16> loc(#loc14)
    %25 = stablehlo.concatenate %19, %24, dim = 1 : (tensor<1x1x1280xbf16>, tensor<1x256x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc15)
    %26 = stablehlo.reshape %arg388 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc3)
    %27 = stablehlo.custom_call @tt.mark_argument(%26) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_embeddings_position_embedding_weight"}} : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2)
    %28 = stablehlo.reshape %27 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc3)
    %29 = stablehlo.reshape %arg387 : (tensor<1x257xi64>) -> tensor<1x1x257xi64> loc(#loc3)
    %30 = stablehlo.custom_call @tt.mark_argument(%29) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___image_encoder_vision_model_embeddings_position_ids"}} : (tensor<1x1x257xi64>) -> tensor<1x1x257xi64> loc(#loc2)
    %31 = stablehlo.reshape %30 : (tensor<1x1x257xi64>) -> tensor<257xi64> loc(#loc16)
    %32 = stablehlo.convert %31 : (tensor<257xi64>) -> tensor<257xui32> loc(#loc17)
    %33 = "stablehlo.gather"(%28, %32) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 1280>}> : (tensor<257x1280xbf16>, tensor<257xui32>) -> tensor<257x1280xbf16> loc(#loc17)
    %34 = stablehlo.reshape %33 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc16)
    %35 = stablehlo.add %25, %34 : tensor<1x257x1280xbf16> loc(#loc18)
    %36 = stablehlo.reshape %arg386 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %37 = stablehlo.custom_call @tt.mark_argument(%36) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_pre_layrnorm_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %38 = stablehlo.reshape %37 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %39 = stablehlo.reshape %arg385 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %40 = stablehlo.custom_call @tt.mark_argument(%39) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_pre_layrnorm_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %41 = stablehlo.reshape %40 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %42 = stablehlo.composite "tenstorrent.layer_norm" %35, %38, %41 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_37} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc19)
    %43 = stablehlo.reshape %arg384 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %44 = stablehlo.custom_call @tt.mark_argument(%43) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %45 = stablehlo.reshape %44 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %46 = stablehlo.reshape %arg383 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %47 = stablehlo.custom_call @tt.mark_argument(%46) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %48 = stablehlo.reshape %47 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %49 = stablehlo.composite "tenstorrent.layer_norm" %42, %45, %48 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_34} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc20)
    %50 = stablehlo.reshape %49 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc21)
    %51 = stablehlo.reshape %arg395 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %52 = stablehlo.custom_call @tt.mark_argument(%51) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %53 = stablehlo.reshape %52 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %54 = stablehlo.transpose %53, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc22)
    %55 = stablehlo.dot_general %50, %54, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc23)
    %56 = stablehlo.reshape %55 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc21)
    %57 = stablehlo.reshape %arg394 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %58 = stablehlo.custom_call @tt.mark_argument(%57) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %59 = stablehlo.reshape %58 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %60 = stablehlo.broadcast_in_dim %59, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc24)
    %61 = stablehlo.add %56, %60 : tensor<1x257x1280xbf16> loc(#loc24)
    %62 = stablehlo.reshape %61 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc25)
    %63 = stablehlo.transpose %62, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc26)
    %64 = stablehlo.convert %63 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc27)
    %65 = stablehlo.multiply %64, %cst_6 : tensor<1x16x257x80xf32> loc(#loc28)
    %66 = stablehlo.reshape %arg393 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %67 = stablehlo.custom_call @tt.mark_argument(%66) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %68 = stablehlo.reshape %67 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %69 = stablehlo.transpose %68, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc29)
    %70 = stablehlo.dot_general %50, %69, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc30)
    %71 = stablehlo.reshape %70 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc31)
    %72 = stablehlo.reshape %arg392 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %73 = stablehlo.custom_call @tt.mark_argument(%72) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %74 = stablehlo.reshape %73 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %75 = stablehlo.broadcast_in_dim %74, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc32)
    %76 = stablehlo.add %71, %75 : tensor<1x257x1280xbf16> loc(#loc32)
    %77 = stablehlo.reshape %76 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc33)
    %78 = stablehlo.transpose %77, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc34)
    %79 = stablehlo.convert %78 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc35)
    %80 = stablehlo.transpose %79, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc36)
    %81 = stablehlo.multiply %80, %cst_5 : tensor<1x16x80x257xf32> loc(#loc37)
    %82 = stablehlo.dot_general %65, %81, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc38)
    %83 = stablehlo.convert %82 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc39)
    %84 = stablehlo.compare  EQ, %83, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc39)
    %85 = stablehlo.not %84 : tensor<1x16x257x257xi1> loc(#loc40)
    %86 = stablehlo.reduce(%85 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("616|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_1aten__any"), %arg559: tensor<i1> loc("616|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_1aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc42)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc43)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc41)
    %87 = stablehlo.reshape %86 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc41)
    %88 = stablehlo.not %87 : tensor<1x16x257x1xi1> loc(#loc44)
    %89 = stablehlo.reshape %88 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc45)
    %90 = stablehlo.broadcast_in_dim %89, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc45)
    %91 = stablehlo.reduce(%82 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc46)
    %92 = stablehlo.broadcast_in_dim %91, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc46)
    %93 = stablehlo.subtract %82, %92 : tensor<1x16x257x257xf32> loc(#loc46)
    %94 = stablehlo.exponential %93 : tensor<1x16x257x257xf32> loc(#loc46)
    %95 = stablehlo.reduce(%94 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc46)
    %96 = stablehlo.broadcast_in_dim %95, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc46)
    %97 = stablehlo.divide %94, %96 : tensor<1x16x257x257xf32> loc(#loc46)
    %98 = stablehlo.select %90, %cst_3, %97 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc47)
    %99 = stablehlo.reshape %arg382 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %100 = stablehlo.custom_call @tt.mark_argument(%99) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %101 = stablehlo.reshape %100 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %102 = stablehlo.transpose %101, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc48)
    %103 = stablehlo.dot_general %50, %102, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc49)
    %104 = stablehlo.reshape %103 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc50)
    %105 = stablehlo.reshape %arg381 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %106 = stablehlo.custom_call @tt.mark_argument(%105) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %107 = stablehlo.reshape %106 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %108 = stablehlo.broadcast_in_dim %107, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc51)
    %109 = stablehlo.add %104, %108 : tensor<1x257x1280xbf16> loc(#loc51)
    %110 = stablehlo.reshape %109 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc52)
    %111 = stablehlo.transpose %110, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc53)
    %112 = stablehlo.convert %111 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc54)
    %113 = stablehlo.dot_general %98, %112, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc55)
    %114 = stablehlo.convert %113 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc56)
    %115 = stablehlo.transpose %114, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc57)
    %116 = stablehlo.reshape %115 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc58)
    %117 = stablehlo.reshape %arg380 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %118 = stablehlo.custom_call @tt.mark_argument(%117) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %119 = stablehlo.reshape %118 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %120 = stablehlo.transpose %119, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc59)
    %121 = stablehlo.dot_general %116, %120, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc60)
    %122 = stablehlo.reshape %121 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc58)
    %123 = stablehlo.reshape %arg379 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %124 = stablehlo.custom_call @tt.mark_argument(%123) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %125 = stablehlo.reshape %124 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %126 = stablehlo.broadcast_in_dim %125, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc61)
    %127 = stablehlo.add %122, %126 : tensor<1x257x1280xbf16> loc(#loc61)
    %128 = stablehlo.add %42, %127 : tensor<1x257x1280xbf16> loc(#loc62)
    %129 = stablehlo.reshape %arg378 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %130 = stablehlo.custom_call @tt.mark_argument(%129) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %131 = stablehlo.reshape %130 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %132 = stablehlo.reshape %arg377 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %133 = stablehlo.custom_call @tt.mark_argument(%132) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %134 = stablehlo.reshape %133 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %135 = stablehlo.composite "tenstorrent.layer_norm" %128, %131, %134 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_35} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc63)
    %136 = stablehlo.reshape %135 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc64)
    %137 = stablehlo.reshape %arg376 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %138 = stablehlo.custom_call @tt.mark_argument(%137) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %139 = stablehlo.reshape %138 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %140 = stablehlo.transpose %139, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc65)
    %141 = stablehlo.dot_general %136, %140, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc66)
    %142 = stablehlo.reshape %141 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc64)
    %143 = stablehlo.reshape %arg375 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3)
    %144 = stablehlo.custom_call @tt.mark_argument(%143) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %145 = stablehlo.reshape %144 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3)
    %146 = stablehlo.broadcast_in_dim %145, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc67)
    %147 = stablehlo.add %142, %146 : tensor<1x257x5120xbf16> loc(#loc67)
    %148 = stablehlo.composite "tenstorrent.gelu" %147 {decomposition = @tenstorrent.gelu.impl_16} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc68)
    %149 = stablehlo.reshape %148 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc69)
    %150 = stablehlo.reshape %arg374 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %151 = stablehlo.custom_call @tt.mark_argument(%150) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %152 = stablehlo.reshape %151 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %153 = stablehlo.transpose %152, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc70)
    %154 = stablehlo.dot_general %149, %153, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc71)
    %155 = stablehlo.reshape %154 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc69)
    %156 = stablehlo.reshape %arg373 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %157 = stablehlo.custom_call @tt.mark_argument(%156) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %158 = stablehlo.reshape %157 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %159 = stablehlo.broadcast_in_dim %158, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc72)
    %160 = stablehlo.add %155, %159 : tensor<1x257x1280xbf16> loc(#loc72)
    %161 = stablehlo.add %128, %160 : tensor<1x257x1280xbf16> loc(#loc73)
    %162 = stablehlo.reshape %arg372 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %163 = stablehlo.custom_call @tt.mark_argument(%162) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %164 = stablehlo.reshape %163 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %165 = stablehlo.reshape %arg371 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %166 = stablehlo.custom_call @tt.mark_argument(%165) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %167 = stablehlo.reshape %166 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %168 = stablehlo.composite "tenstorrent.layer_norm" %161, %164, %167 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_39} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc74)
    %169 = stablehlo.reshape %168 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc75)
    %170 = stablehlo.reshape %arg399 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %171 = stablehlo.custom_call @tt.mark_argument(%170) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %172 = stablehlo.reshape %171 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %173 = stablehlo.transpose %172, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc76)
    %174 = stablehlo.dot_general %169, %173, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc77)
    %175 = stablehlo.reshape %174 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc75)
    %176 = stablehlo.reshape %arg398 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %177 = stablehlo.custom_call @tt.mark_argument(%176) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %178 = stablehlo.reshape %177 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %179 = stablehlo.broadcast_in_dim %178, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc78)
    %180 = stablehlo.add %175, %179 : tensor<1x257x1280xbf16> loc(#loc78)
    %181 = stablehlo.reshape %180 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc79)
    %182 = stablehlo.transpose %181, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc80)
    %183 = stablehlo.convert %182 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc81)
    %184 = stablehlo.multiply %183, %cst_6 : tensor<1x16x257x80xf32> loc(#loc82)
    %185 = stablehlo.reshape %arg397 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %186 = stablehlo.custom_call @tt.mark_argument(%185) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %187 = stablehlo.reshape %186 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %188 = stablehlo.transpose %187, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc83)
    %189 = stablehlo.dot_general %169, %188, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc84)
    %190 = stablehlo.reshape %189 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc85)
    %191 = stablehlo.reshape %arg396 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %192 = stablehlo.custom_call @tt.mark_argument(%191) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %193 = stablehlo.reshape %192 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %194 = stablehlo.broadcast_in_dim %193, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc86)
    %195 = stablehlo.add %190, %194 : tensor<1x257x1280xbf16> loc(#loc86)
    %196 = stablehlo.reshape %195 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc87)
    %197 = stablehlo.transpose %196, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc88)
    %198 = stablehlo.convert %197 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc89)
    %199 = stablehlo.transpose %198, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc90)
    %200 = stablehlo.multiply %199, %cst_5 : tensor<1x16x80x257xf32> loc(#loc91)
    %201 = stablehlo.dot_general %184, %200, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc92)
    %202 = stablehlo.convert %201 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc93)
    %203 = stablehlo.compare  EQ, %202, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc93)
    %204 = stablehlo.not %203 : tensor<1x16x257x257xi1> loc(#loc94)
    %205 = stablehlo.reduce(%204 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("690|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_2aten__any"), %arg559: tensor<i1> loc("690|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_2aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc96)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc97)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc95)
    %206 = stablehlo.reshape %205 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc95)
    %207 = stablehlo.not %206 : tensor<1x16x257x1xi1> loc(#loc98)
    %208 = stablehlo.reshape %207 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc99)
    %209 = stablehlo.broadcast_in_dim %208, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc99)
    %210 = stablehlo.reduce(%201 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc100)
    %211 = stablehlo.broadcast_in_dim %210, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc100)
    %212 = stablehlo.subtract %201, %211 : tensor<1x16x257x257xf32> loc(#loc100)
    %213 = stablehlo.exponential %212 : tensor<1x16x257x257xf32> loc(#loc100)
    %214 = stablehlo.reduce(%213 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc100)
    %215 = stablehlo.broadcast_in_dim %214, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc100)
    %216 = stablehlo.divide %213, %215 : tensor<1x16x257x257xf32> loc(#loc100)
    %217 = stablehlo.select %209, %cst_3, %216 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc101)
    %218 = stablehlo.reshape %arg370 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %219 = stablehlo.custom_call @tt.mark_argument(%218) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %220 = stablehlo.reshape %219 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %221 = stablehlo.transpose %220, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc102)
    %222 = stablehlo.dot_general %169, %221, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc103)
    %223 = stablehlo.reshape %222 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc104)
    %224 = stablehlo.reshape %arg369 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %225 = stablehlo.custom_call @tt.mark_argument(%224) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %226 = stablehlo.reshape %225 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %227 = stablehlo.broadcast_in_dim %226, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc105)
    %228 = stablehlo.add %223, %227 : tensor<1x257x1280xbf16> loc(#loc105)
    %229 = stablehlo.reshape %228 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc106)
    %230 = stablehlo.transpose %229, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc107)
    %231 = stablehlo.convert %230 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc108)
    %232 = stablehlo.dot_general %217, %231, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc109)
    %233 = stablehlo.convert %232 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc110)
    %234 = stablehlo.transpose %233, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc111)
    %235 = stablehlo.reshape %234 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc112)
    %236 = stablehlo.reshape %arg368 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %237 = stablehlo.custom_call @tt.mark_argument(%236) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %238 = stablehlo.reshape %237 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %239 = stablehlo.transpose %238, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc113)
    %240 = stablehlo.dot_general %235, %239, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc114)
    %241 = stablehlo.reshape %240 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc112)
    %242 = stablehlo.reshape %arg367 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %243 = stablehlo.custom_call @tt.mark_argument(%242) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %244 = stablehlo.reshape %243 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %245 = stablehlo.broadcast_in_dim %244, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc115)
    %246 = stablehlo.add %241, %245 : tensor<1x257x1280xbf16> loc(#loc115)
    %247 = stablehlo.add %161, %246 : tensor<1x257x1280xbf16> loc(#loc116)
    %248 = stablehlo.reshape %arg366 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %249 = stablehlo.custom_call @tt.mark_argument(%248) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %250 = stablehlo.reshape %249 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %251 = stablehlo.reshape %arg365 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %252 = stablehlo.custom_call @tt.mark_argument(%251) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %253 = stablehlo.reshape %252 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %254 = stablehlo.composite "tenstorrent.layer_norm" %247, %250, %253 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_36} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc117)
    %255 = stablehlo.reshape %254 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc118)
    %256 = stablehlo.reshape %arg364 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %257 = stablehlo.custom_call @tt.mark_argument(%256) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %258 = stablehlo.reshape %257 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %259 = stablehlo.transpose %258, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc119)
    %260 = stablehlo.dot_general %255, %259, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc120)
    %261 = stablehlo.reshape %260 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc118)
    %262 = stablehlo.reshape %arg363 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3)
    %263 = stablehlo.custom_call @tt.mark_argument(%262) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %264 = stablehlo.reshape %263 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3)
    %265 = stablehlo.broadcast_in_dim %264, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc121)
    %266 = stablehlo.add %261, %265 : tensor<1x257x5120xbf16> loc(#loc121)
    %267 = stablehlo.composite "tenstorrent.gelu" %266 {decomposition = @tenstorrent.gelu.impl_19} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc122)
    %268 = stablehlo.reshape %267 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc123)
    %269 = stablehlo.reshape %arg362 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %270 = stablehlo.custom_call @tt.mark_argument(%269) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %271 = stablehlo.reshape %270 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %272 = stablehlo.transpose %271, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc124)
    %273 = stablehlo.dot_general %268, %272, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc125)
    %274 = stablehlo.reshape %273 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc123)
    %275 = stablehlo.reshape %arg361 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %276 = stablehlo.custom_call @tt.mark_argument(%275) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %277 = stablehlo.reshape %276 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %278 = stablehlo.broadcast_in_dim %277, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc126)
    %279 = stablehlo.add %274, %278 : tensor<1x257x1280xbf16> loc(#loc126)
    %280 = stablehlo.add %247, %279 : tensor<1x257x1280xbf16> loc(#loc127)
    %281 = stablehlo.reshape %arg360 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %282 = stablehlo.custom_call @tt.mark_argument(%281) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %283 = stablehlo.reshape %282 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %284 = stablehlo.reshape %arg359 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %285 = stablehlo.custom_call @tt.mark_argument(%284) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %286 = stablehlo.reshape %285 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %287 = stablehlo.composite "tenstorrent.layer_norm" %280, %283, %286 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_28} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc128)
    %288 = stablehlo.reshape %287 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc129)
    %289 = stablehlo.reshape %arg403 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %290 = stablehlo.custom_call @tt.mark_argument(%289) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %291 = stablehlo.reshape %290 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %292 = stablehlo.transpose %291, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc130)
    %293 = stablehlo.dot_general %288, %292, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc131)
    %294 = stablehlo.reshape %293 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc129)
    %295 = stablehlo.reshape %arg402 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %296 = stablehlo.custom_call @tt.mark_argument(%295) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %297 = stablehlo.reshape %296 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %298 = stablehlo.broadcast_in_dim %297, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc132)
    %299 = stablehlo.add %294, %298 : tensor<1x257x1280xbf16> loc(#loc132)
    %300 = stablehlo.reshape %299 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc133)
    %301 = stablehlo.transpose %300, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc134)
    %302 = stablehlo.convert %301 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc135)
    %303 = stablehlo.multiply %302, %cst_6 : tensor<1x16x257x80xf32> loc(#loc136)
    %304 = stablehlo.reshape %arg401 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %305 = stablehlo.custom_call @tt.mark_argument(%304) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %306 = stablehlo.reshape %305 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %307 = stablehlo.transpose %306, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc137)
    %308 = stablehlo.dot_general %288, %307, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc138)
    %309 = stablehlo.reshape %308 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc139)
    %310 = stablehlo.reshape %arg400 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %311 = stablehlo.custom_call @tt.mark_argument(%310) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %312 = stablehlo.reshape %311 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %313 = stablehlo.broadcast_in_dim %312, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc140)
    %314 = stablehlo.add %309, %313 : tensor<1x257x1280xbf16> loc(#loc140)
    %315 = stablehlo.reshape %314 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc141)
    %316 = stablehlo.transpose %315, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc142)
    %317 = stablehlo.convert %316 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc143)
    %318 = stablehlo.transpose %317, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc144)
    %319 = stablehlo.multiply %318, %cst_5 : tensor<1x16x80x257xf32> loc(#loc145)
    %320 = stablehlo.dot_general %303, %319, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc146)
    %321 = stablehlo.convert %320 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc147)
    %322 = stablehlo.compare  EQ, %321, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc147)
    %323 = stablehlo.not %322 : tensor<1x16x257x257xi1> loc(#loc148)
    %324 = stablehlo.reduce(%323 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("764|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_3aten__any"), %arg559: tensor<i1> loc("764|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_3aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc150)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc151)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc149)
    %325 = stablehlo.reshape %324 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc149)
    %326 = stablehlo.not %325 : tensor<1x16x257x1xi1> loc(#loc152)
    %327 = stablehlo.reshape %326 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc153)
    %328 = stablehlo.broadcast_in_dim %327, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc153)
    %329 = stablehlo.reduce(%320 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc154)
    %330 = stablehlo.broadcast_in_dim %329, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc154)
    %331 = stablehlo.subtract %320, %330 : tensor<1x16x257x257xf32> loc(#loc154)
    %332 = stablehlo.exponential %331 : tensor<1x16x257x257xf32> loc(#loc154)
    %333 = stablehlo.reduce(%332 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc154)
    %334 = stablehlo.broadcast_in_dim %333, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc154)
    %335 = stablehlo.divide %332, %334 : tensor<1x16x257x257xf32> loc(#loc154)
    %336 = stablehlo.select %328, %cst_3, %335 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc155)
    %337 = stablehlo.reshape %arg358 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %338 = stablehlo.custom_call @tt.mark_argument(%337) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %339 = stablehlo.reshape %338 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %340 = stablehlo.transpose %339, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc156)
    %341 = stablehlo.dot_general %288, %340, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc157)
    %342 = stablehlo.reshape %341 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc158)
    %343 = stablehlo.reshape %arg357 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %344 = stablehlo.custom_call @tt.mark_argument(%343) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %345 = stablehlo.reshape %344 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %346 = stablehlo.broadcast_in_dim %345, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc159)
    %347 = stablehlo.add %342, %346 : tensor<1x257x1280xbf16> loc(#loc159)
    %348 = stablehlo.reshape %347 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc160)
    %349 = stablehlo.transpose %348, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc161)
    %350 = stablehlo.convert %349 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc162)
    %351 = stablehlo.dot_general %336, %350, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc163)
    %352 = stablehlo.convert %351 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc164)
    %353 = stablehlo.transpose %352, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc165)
    %354 = stablehlo.reshape %353 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc166)
    %355 = stablehlo.reshape %arg356 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %356 = stablehlo.custom_call @tt.mark_argument(%355) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %357 = stablehlo.reshape %356 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %358 = stablehlo.transpose %357, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc167)
    %359 = stablehlo.dot_general %354, %358, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc168)
    %360 = stablehlo.reshape %359 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc166)
    %361 = stablehlo.reshape %arg355 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %362 = stablehlo.custom_call @tt.mark_argument(%361) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %363 = stablehlo.reshape %362 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %364 = stablehlo.broadcast_in_dim %363, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc169)
    %365 = stablehlo.add %360, %364 : tensor<1x257x1280xbf16> loc(#loc169)
    %366 = stablehlo.add %280, %365 : tensor<1x257x1280xbf16> loc(#loc170)
    %367 = stablehlo.reshape %arg354 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %368 = stablehlo.custom_call @tt.mark_argument(%367) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %369 = stablehlo.reshape %368 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %370 = stablehlo.reshape %arg353 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %371 = stablehlo.custom_call @tt.mark_argument(%370) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %372 = stablehlo.reshape %371 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %373 = stablehlo.composite "tenstorrent.layer_norm" %366, %369, %372 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_32} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc171)
    %374 = stablehlo.reshape %373 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc172)
    %375 = stablehlo.reshape %arg352 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %376 = stablehlo.custom_call @tt.mark_argument(%375) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %377 = stablehlo.reshape %376 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %378 = stablehlo.transpose %377, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc173)
    %379 = stablehlo.dot_general %374, %378, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc174)
    %380 = stablehlo.reshape %379 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc172)
    %381 = stablehlo.reshape %arg351 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3)
    %382 = stablehlo.custom_call @tt.mark_argument(%381) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %383 = stablehlo.reshape %382 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3)
    %384 = stablehlo.broadcast_in_dim %383, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc175)
    %385 = stablehlo.add %380, %384 : tensor<1x257x5120xbf16> loc(#loc175)
    %386 = stablehlo.composite "tenstorrent.gelu" %385 {decomposition = @tenstorrent.gelu.impl_21} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc176)
    %387 = stablehlo.reshape %386 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc177)
    %388 = stablehlo.reshape %arg350 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %389 = stablehlo.custom_call @tt.mark_argument(%388) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %390 = stablehlo.reshape %389 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %391 = stablehlo.transpose %390, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc178)
    %392 = stablehlo.dot_general %387, %391, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc179)
    %393 = stablehlo.reshape %392 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc177)
    %394 = stablehlo.reshape %arg349 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %395 = stablehlo.custom_call @tt.mark_argument(%394) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %396 = stablehlo.reshape %395 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %397 = stablehlo.broadcast_in_dim %396, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc180)
    %398 = stablehlo.add %393, %397 : tensor<1x257x1280xbf16> loc(#loc180)
    %399 = stablehlo.add %366, %398 : tensor<1x257x1280xbf16> loc(#loc181)
    %400 = stablehlo.reshape %arg348 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %401 = stablehlo.custom_call @tt.mark_argument(%400) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %402 = stablehlo.reshape %401 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %403 = stablehlo.reshape %arg347 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %404 = stablehlo.custom_call @tt.mark_argument(%403) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %405 = stablehlo.reshape %404 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %406 = stablehlo.composite "tenstorrent.layer_norm" %399, %402, %405 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_43} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc182)
    %407 = stablehlo.reshape %406 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc183)
    %408 = stablehlo.reshape %arg407 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %409 = stablehlo.custom_call @tt.mark_argument(%408) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %410 = stablehlo.reshape %409 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %411 = stablehlo.transpose %410, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc184)
    %412 = stablehlo.dot_general %407, %411, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc185)
    %413 = stablehlo.reshape %412 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc183)
    %414 = stablehlo.reshape %arg406 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %415 = stablehlo.custom_call @tt.mark_argument(%414) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %416 = stablehlo.reshape %415 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %417 = stablehlo.broadcast_in_dim %416, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc186)
    %418 = stablehlo.add %413, %417 : tensor<1x257x1280xbf16> loc(#loc186)
    %419 = stablehlo.reshape %418 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc187)
    %420 = stablehlo.transpose %419, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc188)
    %421 = stablehlo.convert %420 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc189)
    %422 = stablehlo.multiply %421, %cst_6 : tensor<1x16x257x80xf32> loc(#loc190)
    %423 = stablehlo.reshape %arg405 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %424 = stablehlo.custom_call @tt.mark_argument(%423) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %425 = stablehlo.reshape %424 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %426 = stablehlo.transpose %425, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc191)
    %427 = stablehlo.dot_general %407, %426, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc192)
    %428 = stablehlo.reshape %427 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc193)
    %429 = stablehlo.reshape %arg404 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %430 = stablehlo.custom_call @tt.mark_argument(%429) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %431 = stablehlo.reshape %430 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %432 = stablehlo.broadcast_in_dim %431, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc194)
    %433 = stablehlo.add %428, %432 : tensor<1x257x1280xbf16> loc(#loc194)
    %434 = stablehlo.reshape %433 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc195)
    %435 = stablehlo.transpose %434, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc196)
    %436 = stablehlo.convert %435 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc197)
    %437 = stablehlo.transpose %436, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc198)
    %438 = stablehlo.multiply %437, %cst_5 : tensor<1x16x80x257xf32> loc(#loc199)
    %439 = stablehlo.dot_general %422, %438, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc200)
    %440 = stablehlo.convert %439 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc201)
    %441 = stablehlo.compare  EQ, %440, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc201)
    %442 = stablehlo.not %441 : tensor<1x16x257x257xi1> loc(#loc202)
    %443 = stablehlo.reduce(%442 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("838|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_4aten__any"), %arg559: tensor<i1> loc("838|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_4aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc204)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc205)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc203)
    %444 = stablehlo.reshape %443 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc203)
    %445 = stablehlo.not %444 : tensor<1x16x257x1xi1> loc(#loc206)
    %446 = stablehlo.reshape %445 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc207)
    %447 = stablehlo.broadcast_in_dim %446, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc207)
    %448 = stablehlo.reduce(%439 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc208)
    %449 = stablehlo.broadcast_in_dim %448, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc208)
    %450 = stablehlo.subtract %439, %449 : tensor<1x16x257x257xf32> loc(#loc208)
    %451 = stablehlo.exponential %450 : tensor<1x16x257x257xf32> loc(#loc208)
    %452 = stablehlo.reduce(%451 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc208)
    %453 = stablehlo.broadcast_in_dim %452, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc208)
    %454 = stablehlo.divide %451, %453 : tensor<1x16x257x257xf32> loc(#loc208)
    %455 = stablehlo.select %447, %cst_3, %454 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc209)
    %456 = stablehlo.reshape %arg346 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %457 = stablehlo.custom_call @tt.mark_argument(%456) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %458 = stablehlo.reshape %457 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %459 = stablehlo.transpose %458, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc210)
    %460 = stablehlo.dot_general %407, %459, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc211)
    %461 = stablehlo.reshape %460 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc212)
    %462 = stablehlo.reshape %arg345 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %463 = stablehlo.custom_call @tt.mark_argument(%462) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %464 = stablehlo.reshape %463 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %465 = stablehlo.broadcast_in_dim %464, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc213)
    %466 = stablehlo.add %461, %465 : tensor<1x257x1280xbf16> loc(#loc213)
    %467 = stablehlo.reshape %466 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc214)
    %468 = stablehlo.transpose %467, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc215)
    %469 = stablehlo.convert %468 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc216)
    %470 = stablehlo.dot_general %455, %469, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc217)
    %471 = stablehlo.convert %470 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc218)
    %472 = stablehlo.transpose %471, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc219)
    %473 = stablehlo.reshape %472 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc220)
    %474 = stablehlo.reshape %arg344 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %475 = stablehlo.custom_call @tt.mark_argument(%474) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %476 = stablehlo.reshape %475 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %477 = stablehlo.transpose %476, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc221)
    %478 = stablehlo.dot_general %473, %477, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc222)
    %479 = stablehlo.reshape %478 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc220)
    %480 = stablehlo.reshape %arg343 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %481 = stablehlo.custom_call @tt.mark_argument(%480) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %482 = stablehlo.reshape %481 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %483 = stablehlo.broadcast_in_dim %482, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc223)
    %484 = stablehlo.add %479, %483 : tensor<1x257x1280xbf16> loc(#loc223)
    %485 = stablehlo.add %399, %484 : tensor<1x257x1280xbf16> loc(#loc224)
    %486 = stablehlo.reshape %arg342 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %487 = stablehlo.custom_call @tt.mark_argument(%486) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %488 = stablehlo.reshape %487 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %489 = stablehlo.reshape %arg341 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %490 = stablehlo.custom_call @tt.mark_argument(%489) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %491 = stablehlo.reshape %490 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %492 = stablehlo.composite "tenstorrent.layer_norm" %485, %488, %491 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_52} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc225)
    %493 = stablehlo.reshape %492 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc226)
    %494 = stablehlo.reshape %arg340 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %495 = stablehlo.custom_call @tt.mark_argument(%494) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %496 = stablehlo.reshape %495 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %497 = stablehlo.transpose %496, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc227)
    %498 = stablehlo.dot_general %493, %497, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc228)
    %499 = stablehlo.reshape %498 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc226)
    %500 = stablehlo.reshape %arg339 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3)
    %501 = stablehlo.custom_call @tt.mark_argument(%500) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %502 = stablehlo.reshape %501 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3)
    %503 = stablehlo.broadcast_in_dim %502, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc229)
    %504 = stablehlo.add %499, %503 : tensor<1x257x5120xbf16> loc(#loc229)
    %505 = stablehlo.composite "tenstorrent.gelu" %504 {decomposition = @tenstorrent.gelu.impl_20} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc230)
    %506 = stablehlo.reshape %505 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc231)
    %507 = stablehlo.reshape %arg338 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %508 = stablehlo.custom_call @tt.mark_argument(%507) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %509 = stablehlo.reshape %508 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %510 = stablehlo.transpose %509, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc232)
    %511 = stablehlo.dot_general %506, %510, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc233)
    %512 = stablehlo.reshape %511 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc231)
    %513 = stablehlo.reshape %arg337 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %514 = stablehlo.custom_call @tt.mark_argument(%513) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %515 = stablehlo.reshape %514 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %516 = stablehlo.broadcast_in_dim %515, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc234)
    %517 = stablehlo.add %512, %516 : tensor<1x257x1280xbf16> loc(#loc234)
    %518 = stablehlo.add %485, %517 : tensor<1x257x1280xbf16> loc(#loc235)
    %519 = stablehlo.reshape %arg336 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %520 = stablehlo.custom_call @tt.mark_argument(%519) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %521 = stablehlo.reshape %520 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %522 = stablehlo.reshape %arg335 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %523 = stablehlo.custom_call @tt.mark_argument(%522) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %524 = stablehlo.reshape %523 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %525 = stablehlo.composite "tenstorrent.layer_norm" %518, %521, %524 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_68} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc236)
    %526 = stablehlo.reshape %525 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc237)
    %527 = stablehlo.reshape %arg411 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %528 = stablehlo.custom_call @tt.mark_argument(%527) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %529 = stablehlo.reshape %528 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %530 = stablehlo.transpose %529, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc238)
    %531 = stablehlo.dot_general %526, %530, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc239)
    %532 = stablehlo.reshape %531 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc237)
    %533 = stablehlo.reshape %arg410 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %534 = stablehlo.custom_call @tt.mark_argument(%533) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %535 = stablehlo.reshape %534 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %536 = stablehlo.broadcast_in_dim %535, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc240)
    %537 = stablehlo.add %532, %536 : tensor<1x257x1280xbf16> loc(#loc240)
    %538 = stablehlo.reshape %537 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc241)
    %539 = stablehlo.transpose %538, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc242)
    %540 = stablehlo.convert %539 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc243)
    %541 = stablehlo.multiply %540, %cst_6 : tensor<1x16x257x80xf32> loc(#loc244)
    %542 = stablehlo.reshape %arg409 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %543 = stablehlo.custom_call @tt.mark_argument(%542) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %544 = stablehlo.reshape %543 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %545 = stablehlo.transpose %544, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc245)
    %546 = stablehlo.dot_general %526, %545, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc246)
    %547 = stablehlo.reshape %546 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc247)
    %548 = stablehlo.reshape %arg408 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %549 = stablehlo.custom_call @tt.mark_argument(%548) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %550 = stablehlo.reshape %549 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %551 = stablehlo.broadcast_in_dim %550, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc248)
    %552 = stablehlo.add %547, %551 : tensor<1x257x1280xbf16> loc(#loc248)
    %553 = stablehlo.reshape %552 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc249)
    %554 = stablehlo.transpose %553, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc250)
    %555 = stablehlo.convert %554 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc251)
    %556 = stablehlo.transpose %555, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc252)
    %557 = stablehlo.multiply %556, %cst_5 : tensor<1x16x80x257xf32> loc(#loc253)
    %558 = stablehlo.dot_general %541, %557, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc254)
    %559 = stablehlo.convert %558 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc255)
    %560 = stablehlo.compare  EQ, %559, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc255)
    %561 = stablehlo.not %560 : tensor<1x16x257x257xi1> loc(#loc256)
    %562 = stablehlo.reduce(%561 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("912|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_5aten__any"), %arg559: tensor<i1> loc("912|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_5aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc258)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc259)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc257)
    %563 = stablehlo.reshape %562 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc257)
    %564 = stablehlo.not %563 : tensor<1x16x257x1xi1> loc(#loc260)
    %565 = stablehlo.reshape %564 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc261)
    %566 = stablehlo.broadcast_in_dim %565, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc261)
    %567 = stablehlo.reduce(%558 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc262)
    %568 = stablehlo.broadcast_in_dim %567, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc262)
    %569 = stablehlo.subtract %558, %568 : tensor<1x16x257x257xf32> loc(#loc262)
    %570 = stablehlo.exponential %569 : tensor<1x16x257x257xf32> loc(#loc262)
    %571 = stablehlo.reduce(%570 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc262)
    %572 = stablehlo.broadcast_in_dim %571, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc262)
    %573 = stablehlo.divide %570, %572 : tensor<1x16x257x257xf32> loc(#loc262)
    %574 = stablehlo.select %566, %cst_3, %573 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc263)
    %575 = stablehlo.reshape %arg334 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %576 = stablehlo.custom_call @tt.mark_argument(%575) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %577 = stablehlo.reshape %576 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %578 = stablehlo.transpose %577, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc264)
    %579 = stablehlo.dot_general %526, %578, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc265)
    %580 = stablehlo.reshape %579 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc266)
    %581 = stablehlo.reshape %arg333 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %582 = stablehlo.custom_call @tt.mark_argument(%581) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %583 = stablehlo.reshape %582 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %584 = stablehlo.broadcast_in_dim %583, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc267)
    %585 = stablehlo.add %580, %584 : tensor<1x257x1280xbf16> loc(#loc267)
    %586 = stablehlo.reshape %585 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc268)
    %587 = stablehlo.transpose %586, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc269)
    %588 = stablehlo.convert %587 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc270)
    %589 = stablehlo.dot_general %574, %588, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc271)
    %590 = stablehlo.convert %589 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc272)
    %591 = stablehlo.transpose %590, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc273)
    %592 = stablehlo.reshape %591 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc274)
    %593 = stablehlo.reshape %arg332 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %594 = stablehlo.custom_call @tt.mark_argument(%593) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %595 = stablehlo.reshape %594 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %596 = stablehlo.transpose %595, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc275)
    %597 = stablehlo.dot_general %592, %596, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc276)
    %598 = stablehlo.reshape %597 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc274)
    %599 = stablehlo.reshape %arg331 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %600 = stablehlo.custom_call @tt.mark_argument(%599) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %601 = stablehlo.reshape %600 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %602 = stablehlo.broadcast_in_dim %601, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc277)
    %603 = stablehlo.add %598, %602 : tensor<1x257x1280xbf16> loc(#loc277)
    %604 = stablehlo.add %518, %603 : tensor<1x257x1280xbf16> loc(#loc278)
    %605 = stablehlo.reshape %arg330 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %606 = stablehlo.custom_call @tt.mark_argument(%605) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %607 = stablehlo.reshape %606 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %608 = stablehlo.reshape %arg329 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %609 = stablehlo.custom_call @tt.mark_argument(%608) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %610 = stablehlo.reshape %609 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %611 = stablehlo.composite "tenstorrent.layer_norm" %604, %607, %610 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_38} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc279)
    %612 = stablehlo.reshape %611 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc280)
    %613 = stablehlo.reshape %arg328 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %614 = stablehlo.custom_call @tt.mark_argument(%613) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %615 = stablehlo.reshape %614 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %616 = stablehlo.transpose %615, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc281)
    %617 = stablehlo.dot_general %612, %616, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc282)
    %618 = stablehlo.reshape %617 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc280)
    %619 = stablehlo.reshape %arg327 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3)
    %620 = stablehlo.custom_call @tt.mark_argument(%619) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %621 = stablehlo.reshape %620 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3)
    %622 = stablehlo.broadcast_in_dim %621, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc283)
    %623 = stablehlo.add %618, %622 : tensor<1x257x5120xbf16> loc(#loc283)
    %624 = stablehlo.composite "tenstorrent.gelu" %623 {decomposition = @tenstorrent.gelu.impl_17} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc284)
    %625 = stablehlo.reshape %624 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc285)
    %626 = stablehlo.reshape %arg326 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %627 = stablehlo.custom_call @tt.mark_argument(%626) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %628 = stablehlo.reshape %627 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %629 = stablehlo.transpose %628, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc286)
    %630 = stablehlo.dot_general %625, %629, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc287)
    %631 = stablehlo.reshape %630 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc285)
    %632 = stablehlo.reshape %arg325 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %633 = stablehlo.custom_call @tt.mark_argument(%632) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %634 = stablehlo.reshape %633 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %635 = stablehlo.broadcast_in_dim %634, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc288)
    %636 = stablehlo.add %631, %635 : tensor<1x257x1280xbf16> loc(#loc288)
    %637 = stablehlo.add %604, %636 : tensor<1x257x1280xbf16> loc(#loc289)
    %638 = stablehlo.reshape %arg324 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %639 = stablehlo.custom_call @tt.mark_argument(%638) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %640 = stablehlo.reshape %639 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %641 = stablehlo.reshape %arg323 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %642 = stablehlo.custom_call @tt.mark_argument(%641) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %643 = stablehlo.reshape %642 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %644 = stablehlo.composite "tenstorrent.layer_norm" %637, %640, %643 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_42} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc290)
    %645 = stablehlo.reshape %644 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc291)
    %646 = stablehlo.reshape %arg415 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %647 = stablehlo.custom_call @tt.mark_argument(%646) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %648 = stablehlo.reshape %647 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %649 = stablehlo.transpose %648, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc292)
    %650 = stablehlo.dot_general %645, %649, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc293)
    %651 = stablehlo.reshape %650 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc291)
    %652 = stablehlo.reshape %arg414 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %653 = stablehlo.custom_call @tt.mark_argument(%652) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %654 = stablehlo.reshape %653 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %655 = stablehlo.broadcast_in_dim %654, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc294)
    %656 = stablehlo.add %651, %655 : tensor<1x257x1280xbf16> loc(#loc294)
    %657 = stablehlo.reshape %656 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc295)
    %658 = stablehlo.transpose %657, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc296)
    %659 = stablehlo.convert %658 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc297)
    %660 = stablehlo.multiply %659, %cst_6 : tensor<1x16x257x80xf32> loc(#loc298)
    %661 = stablehlo.reshape %arg413 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %662 = stablehlo.custom_call @tt.mark_argument(%661) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %663 = stablehlo.reshape %662 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %664 = stablehlo.transpose %663, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc299)
    %665 = stablehlo.dot_general %645, %664, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc300)
    %666 = stablehlo.reshape %665 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc301)
    %667 = stablehlo.reshape %arg412 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %668 = stablehlo.custom_call @tt.mark_argument(%667) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %669 = stablehlo.reshape %668 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %670 = stablehlo.broadcast_in_dim %669, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc302)
    %671 = stablehlo.add %666, %670 : tensor<1x257x1280xbf16> loc(#loc302)
    %672 = stablehlo.reshape %671 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc303)
    %673 = stablehlo.transpose %672, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc304)
    %674 = stablehlo.convert %673 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc305)
    %675 = stablehlo.transpose %674, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc306)
    %676 = stablehlo.multiply %675, %cst_5 : tensor<1x16x80x257xf32> loc(#loc307)
    %677 = stablehlo.dot_general %660, %676, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc308)
    %678 = stablehlo.convert %677 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc309)
    %679 = stablehlo.compare  EQ, %678, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc309)
    %680 = stablehlo.not %679 : tensor<1x16x257x257xi1> loc(#loc310)
    %681 = stablehlo.reduce(%680 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("986|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_6aten__any"), %arg559: tensor<i1> loc("986|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_6aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc312)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc313)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc311)
    %682 = stablehlo.reshape %681 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc311)
    %683 = stablehlo.not %682 : tensor<1x16x257x1xi1> loc(#loc314)
    %684 = stablehlo.reshape %683 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc315)
    %685 = stablehlo.broadcast_in_dim %684, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc315)
    %686 = stablehlo.reduce(%677 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc316)
    %687 = stablehlo.broadcast_in_dim %686, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc316)
    %688 = stablehlo.subtract %677, %687 : tensor<1x16x257x257xf32> loc(#loc316)
    %689 = stablehlo.exponential %688 : tensor<1x16x257x257xf32> loc(#loc316)
    %690 = stablehlo.reduce(%689 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc316)
    %691 = stablehlo.broadcast_in_dim %690, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc316)
    %692 = stablehlo.divide %689, %691 : tensor<1x16x257x257xf32> loc(#loc316)
    %693 = stablehlo.select %685, %cst_3, %692 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc317)
    %694 = stablehlo.reshape %arg322 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %695 = stablehlo.custom_call @tt.mark_argument(%694) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %696 = stablehlo.reshape %695 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %697 = stablehlo.transpose %696, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc318)
    %698 = stablehlo.dot_general %645, %697, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc319)
    %699 = stablehlo.reshape %698 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc320)
    %700 = stablehlo.reshape %arg321 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %701 = stablehlo.custom_call @tt.mark_argument(%700) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %702 = stablehlo.reshape %701 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %703 = stablehlo.broadcast_in_dim %702, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc321)
    %704 = stablehlo.add %699, %703 : tensor<1x257x1280xbf16> loc(#loc321)
    %705 = stablehlo.reshape %704 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc322)
    %706 = stablehlo.transpose %705, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc323)
    %707 = stablehlo.convert %706 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc324)
    %708 = stablehlo.dot_general %693, %707, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc325)
    %709 = stablehlo.convert %708 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc326)
    %710 = stablehlo.transpose %709, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc327)
    %711 = stablehlo.reshape %710 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc328)
    %712 = stablehlo.reshape %arg320 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %713 = stablehlo.custom_call @tt.mark_argument(%712) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %714 = stablehlo.reshape %713 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %715 = stablehlo.transpose %714, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc329)
    %716 = stablehlo.dot_general %711, %715, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc330)
    %717 = stablehlo.reshape %716 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc328)
    %718 = stablehlo.reshape %arg319 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %719 = stablehlo.custom_call @tt.mark_argument(%718) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %720 = stablehlo.reshape %719 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %721 = stablehlo.broadcast_in_dim %720, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc331)
    %722 = stablehlo.add %717, %721 : tensor<1x257x1280xbf16> loc(#loc331)
    %723 = stablehlo.add %637, %722 : tensor<1x257x1280xbf16> loc(#loc332)
    %724 = stablehlo.reshape %arg318 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %725 = stablehlo.custom_call @tt.mark_argument(%724) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %726 = stablehlo.reshape %725 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %727 = stablehlo.reshape %arg317 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %728 = stablehlo.custom_call @tt.mark_argument(%727) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %729 = stablehlo.reshape %728 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %730 = stablehlo.composite "tenstorrent.layer_norm" %723, %726, %729 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_41} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc333)
    %731 = stablehlo.reshape %730 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc334)
    %732 = stablehlo.reshape %arg316 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %733 = stablehlo.custom_call @tt.mark_argument(%732) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %734 = stablehlo.reshape %733 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %735 = stablehlo.transpose %734, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc335)
    %736 = stablehlo.dot_general %731, %735, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc336)
    %737 = stablehlo.reshape %736 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc334)
    %738 = stablehlo.reshape %arg315 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3)
    %739 = stablehlo.custom_call @tt.mark_argument(%738) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %740 = stablehlo.reshape %739 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3)
    %741 = stablehlo.broadcast_in_dim %740, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc337)
    %742 = stablehlo.add %737, %741 : tensor<1x257x5120xbf16> loc(#loc337)
    %743 = stablehlo.composite "tenstorrent.gelu" %742 {decomposition = @tenstorrent.gelu.impl_26} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc338)
    %744 = stablehlo.reshape %743 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc339)
    %745 = stablehlo.reshape %arg314 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %746 = stablehlo.custom_call @tt.mark_argument(%745) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %747 = stablehlo.reshape %746 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %748 = stablehlo.transpose %747, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc340)
    %749 = stablehlo.dot_general %744, %748, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc341)
    %750 = stablehlo.reshape %749 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc339)
    %751 = stablehlo.reshape %arg313 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %752 = stablehlo.custom_call @tt.mark_argument(%751) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %753 = stablehlo.reshape %752 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %754 = stablehlo.broadcast_in_dim %753, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc342)
    %755 = stablehlo.add %750, %754 : tensor<1x257x1280xbf16> loc(#loc342)
    %756 = stablehlo.add %723, %755 : tensor<1x257x1280xbf16> loc(#loc343)
    %757 = stablehlo.reshape %arg312 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %758 = stablehlo.custom_call @tt.mark_argument(%757) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %759 = stablehlo.reshape %758 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %760 = stablehlo.reshape %arg311 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %761 = stablehlo.custom_call @tt.mark_argument(%760) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %762 = stablehlo.reshape %761 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %763 = stablehlo.composite "tenstorrent.layer_norm" %756, %759, %762 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_64} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc344)
    %764 = stablehlo.reshape %763 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc345)
    %765 = stablehlo.reshape %arg419 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %766 = stablehlo.custom_call @tt.mark_argument(%765) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %767 = stablehlo.reshape %766 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %768 = stablehlo.transpose %767, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc346)
    %769 = stablehlo.dot_general %764, %768, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc347)
    %770 = stablehlo.reshape %769 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc345)
    %771 = stablehlo.reshape %arg418 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %772 = stablehlo.custom_call @tt.mark_argument(%771) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %773 = stablehlo.reshape %772 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %774 = stablehlo.broadcast_in_dim %773, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc348)
    %775 = stablehlo.add %770, %774 : tensor<1x257x1280xbf16> loc(#loc348)
    %776 = stablehlo.reshape %775 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc349)
    %777 = stablehlo.transpose %776, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc350)
    %778 = stablehlo.convert %777 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc351)
    %779 = stablehlo.multiply %778, %cst_6 : tensor<1x16x257x80xf32> loc(#loc352)
    %780 = stablehlo.reshape %arg417 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %781 = stablehlo.custom_call @tt.mark_argument(%780) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %782 = stablehlo.reshape %781 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %783 = stablehlo.transpose %782, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc353)
    %784 = stablehlo.dot_general %764, %783, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc354)
    %785 = stablehlo.reshape %784 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc355)
    %786 = stablehlo.reshape %arg416 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %787 = stablehlo.custom_call @tt.mark_argument(%786) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %788 = stablehlo.reshape %787 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %789 = stablehlo.broadcast_in_dim %788, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc356)
    %790 = stablehlo.add %785, %789 : tensor<1x257x1280xbf16> loc(#loc356)
    %791 = stablehlo.reshape %790 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc357)
    %792 = stablehlo.transpose %791, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc358)
    %793 = stablehlo.convert %792 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc359)
    %794 = stablehlo.transpose %793, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc360)
    %795 = stablehlo.multiply %794, %cst_5 : tensor<1x16x80x257xf32> loc(#loc361)
    %796 = stablehlo.dot_general %779, %795, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc362)
    %797 = stablehlo.convert %796 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc363)
    %798 = stablehlo.compare  EQ, %797, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc363)
    %799 = stablehlo.not %798 : tensor<1x16x257x257xi1> loc(#loc364)
    %800 = stablehlo.reduce(%799 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("1060|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_7aten__any"), %arg559: tensor<i1> loc("1060|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_7aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc366)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc367)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc365)
    %801 = stablehlo.reshape %800 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc365)
    %802 = stablehlo.not %801 : tensor<1x16x257x1xi1> loc(#loc368)
    %803 = stablehlo.reshape %802 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc369)
    %804 = stablehlo.broadcast_in_dim %803, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc369)
    %805 = stablehlo.reduce(%796 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc370)
    %806 = stablehlo.broadcast_in_dim %805, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc370)
    %807 = stablehlo.subtract %796, %806 : tensor<1x16x257x257xf32> loc(#loc370)
    %808 = stablehlo.exponential %807 : tensor<1x16x257x257xf32> loc(#loc370)
    %809 = stablehlo.reduce(%808 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc370)
    %810 = stablehlo.broadcast_in_dim %809, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc370)
    %811 = stablehlo.divide %808, %810 : tensor<1x16x257x257xf32> loc(#loc370)
    %812 = stablehlo.select %804, %cst_3, %811 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc371)
    %813 = stablehlo.reshape %arg310 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %814 = stablehlo.custom_call @tt.mark_argument(%813) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %815 = stablehlo.reshape %814 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %816 = stablehlo.transpose %815, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc372)
    %817 = stablehlo.dot_general %764, %816, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc373)
    %818 = stablehlo.reshape %817 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc374)
    %819 = stablehlo.reshape %arg309 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %820 = stablehlo.custom_call @tt.mark_argument(%819) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %821 = stablehlo.reshape %820 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %822 = stablehlo.broadcast_in_dim %821, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc375)
    %823 = stablehlo.add %818, %822 : tensor<1x257x1280xbf16> loc(#loc375)
    %824 = stablehlo.reshape %823 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc376)
    %825 = stablehlo.transpose %824, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc377)
    %826 = stablehlo.convert %825 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc378)
    %827 = stablehlo.dot_general %812, %826, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc379)
    %828 = stablehlo.convert %827 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc380)
    %829 = stablehlo.transpose %828, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc381)
    %830 = stablehlo.reshape %829 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc382)
    %831 = stablehlo.reshape %arg308 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %832 = stablehlo.custom_call @tt.mark_argument(%831) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %833 = stablehlo.reshape %832 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %834 = stablehlo.transpose %833, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc383)
    %835 = stablehlo.dot_general %830, %834, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc384)
    %836 = stablehlo.reshape %835 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc382)
    %837 = stablehlo.reshape %arg307 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %838 = stablehlo.custom_call @tt.mark_argument(%837) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %839 = stablehlo.reshape %838 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %840 = stablehlo.broadcast_in_dim %839, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc385)
    %841 = stablehlo.add %836, %840 : tensor<1x257x1280xbf16> loc(#loc385)
    %842 = stablehlo.add %756, %841 : tensor<1x257x1280xbf16> loc(#loc386)
    %843 = stablehlo.reshape %arg306 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %844 = stablehlo.custom_call @tt.mark_argument(%843) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %845 = stablehlo.reshape %844 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %846 = stablehlo.reshape %arg305 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %847 = stablehlo.custom_call @tt.mark_argument(%846) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %848 = stablehlo.reshape %847 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %849 = stablehlo.composite "tenstorrent.layer_norm" %842, %845, %848 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_25} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc387)
    %850 = stablehlo.reshape %849 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc388)
    %851 = stablehlo.reshape %arg304 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %852 = stablehlo.custom_call @tt.mark_argument(%851) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %853 = stablehlo.reshape %852 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %854 = stablehlo.transpose %853, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc389)
    %855 = stablehlo.dot_general %850, %854, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc390)
    %856 = stablehlo.reshape %855 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc388)
    %857 = stablehlo.reshape %arg303 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3)
    %858 = stablehlo.custom_call @tt.mark_argument(%857) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %859 = stablehlo.reshape %858 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3)
    %860 = stablehlo.broadcast_in_dim %859, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc391)
    %861 = stablehlo.add %856, %860 : tensor<1x257x5120xbf16> loc(#loc391)
    %862 = stablehlo.composite "tenstorrent.gelu" %861 {decomposition = @tenstorrent.gelu.impl_15} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc392)
    %863 = stablehlo.reshape %862 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc393)
    %864 = stablehlo.reshape %arg302 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %865 = stablehlo.custom_call @tt.mark_argument(%864) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %866 = stablehlo.reshape %865 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %867 = stablehlo.transpose %866, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc394)
    %868 = stablehlo.dot_general %863, %867, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc395)
    %869 = stablehlo.reshape %868 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc393)
    %870 = stablehlo.reshape %arg301 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %871 = stablehlo.custom_call @tt.mark_argument(%870) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %872 = stablehlo.reshape %871 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %873 = stablehlo.broadcast_in_dim %872, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc396)
    %874 = stablehlo.add %869, %873 : tensor<1x257x1280xbf16> loc(#loc396)
    %875 = stablehlo.add %842, %874 : tensor<1x257x1280xbf16> loc(#loc397)
    %876 = stablehlo.reshape %arg300 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %877 = stablehlo.custom_call @tt.mark_argument(%876) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %878 = stablehlo.reshape %877 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %879 = stablehlo.reshape %arg299 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %880 = stablehlo.custom_call @tt.mark_argument(%879) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %881 = stablehlo.reshape %880 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %882 = stablehlo.composite "tenstorrent.layer_norm" %875, %878, %881 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_65} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc398)
    %883 = stablehlo.reshape %882 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc399)
    %884 = stablehlo.reshape %arg423 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %885 = stablehlo.custom_call @tt.mark_argument(%884) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %886 = stablehlo.reshape %885 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %887 = stablehlo.transpose %886, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc400)
    %888 = stablehlo.dot_general %883, %887, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc401)
    %889 = stablehlo.reshape %888 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc399)
    %890 = stablehlo.reshape %arg422 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %891 = stablehlo.custom_call @tt.mark_argument(%890) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %892 = stablehlo.reshape %891 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %893 = stablehlo.broadcast_in_dim %892, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc402)
    %894 = stablehlo.add %889, %893 : tensor<1x257x1280xbf16> loc(#loc402)
    %895 = stablehlo.reshape %894 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc403)
    %896 = stablehlo.transpose %895, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc404)
    %897 = stablehlo.convert %896 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc405)
    %898 = stablehlo.multiply %897, %cst_6 : tensor<1x16x257x80xf32> loc(#loc406)
    %899 = stablehlo.reshape %arg421 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %900 = stablehlo.custom_call @tt.mark_argument(%899) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %901 = stablehlo.reshape %900 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %902 = stablehlo.transpose %901, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc407)
    %903 = stablehlo.dot_general %883, %902, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc408)
    %904 = stablehlo.reshape %903 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc409)
    %905 = stablehlo.reshape %arg420 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %906 = stablehlo.custom_call @tt.mark_argument(%905) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %907 = stablehlo.reshape %906 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %908 = stablehlo.broadcast_in_dim %907, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc410)
    %909 = stablehlo.add %904, %908 : tensor<1x257x1280xbf16> loc(#loc410)
    %910 = stablehlo.reshape %909 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc411)
    %911 = stablehlo.transpose %910, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc412)
    %912 = stablehlo.convert %911 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc413)
    %913 = stablehlo.transpose %912, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc414)
    %914 = stablehlo.multiply %913, %cst_5 : tensor<1x16x80x257xf32> loc(#loc415)
    %915 = stablehlo.dot_general %898, %914, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc416)
    %916 = stablehlo.convert %915 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc417)
    %917 = stablehlo.compare  EQ, %916, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc417)
    %918 = stablehlo.not %917 : tensor<1x16x257x257xi1> loc(#loc418)
    %919 = stablehlo.reduce(%918 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("1134|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_8aten__any"), %arg559: tensor<i1> loc("1134|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_8aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc420)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc421)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc419)
    %920 = stablehlo.reshape %919 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc419)
    %921 = stablehlo.not %920 : tensor<1x16x257x1xi1> loc(#loc422)
    %922 = stablehlo.reshape %921 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc423)
    %923 = stablehlo.broadcast_in_dim %922, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc423)
    %924 = stablehlo.reduce(%915 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc424)
    %925 = stablehlo.broadcast_in_dim %924, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc424)
    %926 = stablehlo.subtract %915, %925 : tensor<1x16x257x257xf32> loc(#loc424)
    %927 = stablehlo.exponential %926 : tensor<1x16x257x257xf32> loc(#loc424)
    %928 = stablehlo.reduce(%927 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc424)
    %929 = stablehlo.broadcast_in_dim %928, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc424)
    %930 = stablehlo.divide %927, %929 : tensor<1x16x257x257xf32> loc(#loc424)
    %931 = stablehlo.select %923, %cst_3, %930 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc425)
    %932 = stablehlo.reshape %arg298 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %933 = stablehlo.custom_call @tt.mark_argument(%932) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %934 = stablehlo.reshape %933 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %935 = stablehlo.transpose %934, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc426)
    %936 = stablehlo.dot_general %883, %935, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc427)
    %937 = stablehlo.reshape %936 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc428)
    %938 = stablehlo.reshape %arg297 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %939 = stablehlo.custom_call @tt.mark_argument(%938) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %940 = stablehlo.reshape %939 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %941 = stablehlo.broadcast_in_dim %940, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc429)
    %942 = stablehlo.add %937, %941 : tensor<1x257x1280xbf16> loc(#loc429)
    %943 = stablehlo.reshape %942 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc430)
    %944 = stablehlo.transpose %943, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc431)
    %945 = stablehlo.convert %944 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc432)
    %946 = stablehlo.dot_general %931, %945, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc433)
    %947 = stablehlo.convert %946 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc434)
    %948 = stablehlo.transpose %947, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc435)
    %949 = stablehlo.reshape %948 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc436)
    %950 = stablehlo.reshape %arg296 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %951 = stablehlo.custom_call @tt.mark_argument(%950) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %952 = stablehlo.reshape %951 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %953 = stablehlo.transpose %952, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc437)
    %954 = stablehlo.dot_general %949, %953, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc438)
    %955 = stablehlo.reshape %954 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc436)
    %956 = stablehlo.reshape %arg295 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %957 = stablehlo.custom_call @tt.mark_argument(%956) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %958 = stablehlo.reshape %957 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %959 = stablehlo.broadcast_in_dim %958, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc439)
    %960 = stablehlo.add %955, %959 : tensor<1x257x1280xbf16> loc(#loc439)
    %961 = stablehlo.add %875, %960 : tensor<1x257x1280xbf16> loc(#loc440)
    %962 = stablehlo.reshape %arg294 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %963 = stablehlo.custom_call @tt.mark_argument(%962) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %964 = stablehlo.reshape %963 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %965 = stablehlo.reshape %arg293 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %966 = stablehlo.custom_call @tt.mark_argument(%965) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %967 = stablehlo.reshape %966 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %968 = stablehlo.composite "tenstorrent.layer_norm" %961, %964, %967 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_29} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc441)
    %969 = stablehlo.reshape %968 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc442)
    %970 = stablehlo.reshape %arg292 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %971 = stablehlo.custom_call @tt.mark_argument(%970) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %972 = stablehlo.reshape %971 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %973 = stablehlo.transpose %972, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc443)
    %974 = stablehlo.dot_general %969, %973, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc444)
    %975 = stablehlo.reshape %974 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc442)
    %976 = stablehlo.reshape %arg291 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3)
    %977 = stablehlo.custom_call @tt.mark_argument(%976) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %978 = stablehlo.reshape %977 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3)
    %979 = stablehlo.broadcast_in_dim %978, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc445)
    %980 = stablehlo.add %975, %979 : tensor<1x257x5120xbf16> loc(#loc445)
    %981 = stablehlo.composite "tenstorrent.gelu" %980 {decomposition = @tenstorrent.gelu.impl_13} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc446)
    %982 = stablehlo.reshape %981 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc447)
    %983 = stablehlo.reshape %arg290 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %984 = stablehlo.custom_call @tt.mark_argument(%983) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %985 = stablehlo.reshape %984 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %986 = stablehlo.transpose %985, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc448)
    %987 = stablehlo.dot_general %982, %986, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc449)
    %988 = stablehlo.reshape %987 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc447)
    %989 = stablehlo.reshape %arg289 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %990 = stablehlo.custom_call @tt.mark_argument(%989) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %991 = stablehlo.reshape %990 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %992 = stablehlo.broadcast_in_dim %991, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc450)
    %993 = stablehlo.add %988, %992 : tensor<1x257x1280xbf16> loc(#loc450)
    %994 = stablehlo.add %961, %993 : tensor<1x257x1280xbf16> loc(#loc451)
    %995 = stablehlo.reshape %arg288 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %996 = stablehlo.custom_call @tt.mark_argument(%995) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %997 = stablehlo.reshape %996 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %998 = stablehlo.reshape %arg287 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %999 = stablehlo.custom_call @tt.mark_argument(%998) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1000 = stablehlo.reshape %999 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1001 = stablehlo.composite "tenstorrent.layer_norm" %994, %997, %1000 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_22} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc452)
    %1002 = stablehlo.reshape %1001 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc453)
    %1003 = stablehlo.reshape %arg427 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1004 = stablehlo.custom_call @tt.mark_argument(%1003) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1005 = stablehlo.reshape %1004 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1006 = stablehlo.transpose %1005, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc454)
    %1007 = stablehlo.dot_general %1002, %1006, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc455)
    %1008 = stablehlo.reshape %1007 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc453)
    %1009 = stablehlo.reshape %arg426 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1010 = stablehlo.custom_call @tt.mark_argument(%1009) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1011 = stablehlo.reshape %1010 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1012 = stablehlo.broadcast_in_dim %1011, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc456)
    %1013 = stablehlo.add %1008, %1012 : tensor<1x257x1280xbf16> loc(#loc456)
    %1014 = stablehlo.reshape %1013 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc457)
    %1015 = stablehlo.transpose %1014, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc458)
    %1016 = stablehlo.convert %1015 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc459)
    %1017 = stablehlo.multiply %1016, %cst_6 : tensor<1x16x257x80xf32> loc(#loc460)
    %1018 = stablehlo.reshape %arg425 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1019 = stablehlo.custom_call @tt.mark_argument(%1018) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1020 = stablehlo.reshape %1019 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1021 = stablehlo.transpose %1020, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc461)
    %1022 = stablehlo.dot_general %1002, %1021, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc462)
    %1023 = stablehlo.reshape %1022 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc463)
    %1024 = stablehlo.reshape %arg424 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1025 = stablehlo.custom_call @tt.mark_argument(%1024) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1026 = stablehlo.reshape %1025 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1027 = stablehlo.broadcast_in_dim %1026, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc464)
    %1028 = stablehlo.add %1023, %1027 : tensor<1x257x1280xbf16> loc(#loc464)
    %1029 = stablehlo.reshape %1028 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc465)
    %1030 = stablehlo.transpose %1029, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc466)
    %1031 = stablehlo.convert %1030 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc467)
    %1032 = stablehlo.transpose %1031, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc468)
    %1033 = stablehlo.multiply %1032, %cst_5 : tensor<1x16x80x257xf32> loc(#loc469)
    %1034 = stablehlo.dot_general %1017, %1033, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc470)
    %1035 = stablehlo.convert %1034 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc471)
    %1036 = stablehlo.compare  EQ, %1035, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc471)
    %1037 = stablehlo.not %1036 : tensor<1x16x257x257xi1> loc(#loc472)
    %1038 = stablehlo.reduce(%1037 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("1208|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_9aten__any"), %arg559: tensor<i1> loc("1208|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_9aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc474)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc475)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc473)
    %1039 = stablehlo.reshape %1038 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc473)
    %1040 = stablehlo.not %1039 : tensor<1x16x257x1xi1> loc(#loc476)
    %1041 = stablehlo.reshape %1040 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc477)
    %1042 = stablehlo.broadcast_in_dim %1041, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc477)
    %1043 = stablehlo.reduce(%1034 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc478)
    %1044 = stablehlo.broadcast_in_dim %1043, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc478)
    %1045 = stablehlo.subtract %1034, %1044 : tensor<1x16x257x257xf32> loc(#loc478)
    %1046 = stablehlo.exponential %1045 : tensor<1x16x257x257xf32> loc(#loc478)
    %1047 = stablehlo.reduce(%1046 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc478)
    %1048 = stablehlo.broadcast_in_dim %1047, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc478)
    %1049 = stablehlo.divide %1046, %1048 : tensor<1x16x257x257xf32> loc(#loc478)
    %1050 = stablehlo.select %1042, %cst_3, %1049 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc479)
    %1051 = stablehlo.reshape %arg286 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1052 = stablehlo.custom_call @tt.mark_argument(%1051) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1053 = stablehlo.reshape %1052 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1054 = stablehlo.transpose %1053, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc480)
    %1055 = stablehlo.dot_general %1002, %1054, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc481)
    %1056 = stablehlo.reshape %1055 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc482)
    %1057 = stablehlo.reshape %arg285 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1058 = stablehlo.custom_call @tt.mark_argument(%1057) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1059 = stablehlo.reshape %1058 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1060 = stablehlo.broadcast_in_dim %1059, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc483)
    %1061 = stablehlo.add %1056, %1060 : tensor<1x257x1280xbf16> loc(#loc483)
    %1062 = stablehlo.reshape %1061 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc484)
    %1063 = stablehlo.transpose %1062, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc485)
    %1064 = stablehlo.convert %1063 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc486)
    %1065 = stablehlo.dot_general %1050, %1064, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc487)
    %1066 = stablehlo.convert %1065 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc488)
    %1067 = stablehlo.transpose %1066, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc489)
    %1068 = stablehlo.reshape %1067 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc490)
    %1069 = stablehlo.reshape %arg284 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1070 = stablehlo.custom_call @tt.mark_argument(%1069) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1071 = stablehlo.reshape %1070 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1072 = stablehlo.transpose %1071, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc491)
    %1073 = stablehlo.dot_general %1068, %1072, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc492)
    %1074 = stablehlo.reshape %1073 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc490)
    %1075 = stablehlo.reshape %arg283 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1076 = stablehlo.custom_call @tt.mark_argument(%1075) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1077 = stablehlo.reshape %1076 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1078 = stablehlo.broadcast_in_dim %1077, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc493)
    %1079 = stablehlo.add %1074, %1078 : tensor<1x257x1280xbf16> loc(#loc493)
    %1080 = stablehlo.add %994, %1079 : tensor<1x257x1280xbf16> loc(#loc494)
    %1081 = stablehlo.reshape %arg282 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1082 = stablehlo.custom_call @tt.mark_argument(%1081) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1083 = stablehlo.reshape %1082 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1084 = stablehlo.reshape %arg281 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1085 = stablehlo.custom_call @tt.mark_argument(%1084) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1086 = stablehlo.reshape %1085 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1087 = stablehlo.composite "tenstorrent.layer_norm" %1080, %1083, %1086 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_27} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc495)
    %1088 = stablehlo.reshape %1087 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc496)
    %1089 = stablehlo.reshape %arg280 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %1090 = stablehlo.custom_call @tt.mark_argument(%1089) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %1091 = stablehlo.reshape %1090 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %1092 = stablehlo.transpose %1091, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc497)
    %1093 = stablehlo.dot_general %1088, %1092, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc498)
    %1094 = stablehlo.reshape %1093 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc496)
    %1095 = stablehlo.reshape %arg279 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3)
    %1096 = stablehlo.custom_call @tt.mark_argument(%1095) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %1097 = stablehlo.reshape %1096 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3)
    %1098 = stablehlo.broadcast_in_dim %1097, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc499)
    %1099 = stablehlo.add %1094, %1098 : tensor<1x257x5120xbf16> loc(#loc499)
    %1100 = stablehlo.composite "tenstorrent.gelu" %1099 {decomposition = @tenstorrent.gelu.impl_11} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc500)
    %1101 = stablehlo.reshape %1100 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc501)
    %1102 = stablehlo.reshape %arg278 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %1103 = stablehlo.custom_call @tt.mark_argument(%1102) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %1104 = stablehlo.reshape %1103 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %1105 = stablehlo.transpose %1104, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc502)
    %1106 = stablehlo.dot_general %1101, %1105, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc503)
    %1107 = stablehlo.reshape %1106 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc501)
    %1108 = stablehlo.reshape %arg277 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1109 = stablehlo.custom_call @tt.mark_argument(%1108) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1110 = stablehlo.reshape %1109 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1111 = stablehlo.broadcast_in_dim %1110, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc504)
    %1112 = stablehlo.add %1107, %1111 : tensor<1x257x1280xbf16> loc(#loc504)
    %1113 = stablehlo.add %1080, %1112 : tensor<1x257x1280xbf16> loc(#loc505)
    %1114 = stablehlo.reshape %arg276 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1115 = stablehlo.custom_call @tt.mark_argument(%1114) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1116 = stablehlo.reshape %1115 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1117 = stablehlo.reshape %arg275 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1118 = stablehlo.custom_call @tt.mark_argument(%1117) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1119 = stablehlo.reshape %1118 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1120 = stablehlo.composite "tenstorrent.layer_norm" %1113, %1116, %1119 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_59} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc506)
    %1121 = stablehlo.reshape %1120 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc507)
    %1122 = stablehlo.reshape %arg431 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1123 = stablehlo.custom_call @tt.mark_argument(%1122) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1124 = stablehlo.reshape %1123 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1125 = stablehlo.transpose %1124, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc508)
    %1126 = stablehlo.dot_general %1121, %1125, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc509)
    %1127 = stablehlo.reshape %1126 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc507)
    %1128 = stablehlo.reshape %arg430 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1129 = stablehlo.custom_call @tt.mark_argument(%1128) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1130 = stablehlo.reshape %1129 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1131 = stablehlo.broadcast_in_dim %1130, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc510)
    %1132 = stablehlo.add %1127, %1131 : tensor<1x257x1280xbf16> loc(#loc510)
    %1133 = stablehlo.reshape %1132 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc511)
    %1134 = stablehlo.transpose %1133, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc512)
    %1135 = stablehlo.convert %1134 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc513)
    %1136 = stablehlo.multiply %1135, %cst_6 : tensor<1x16x257x80xf32> loc(#loc514)
    %1137 = stablehlo.reshape %arg429 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1138 = stablehlo.custom_call @tt.mark_argument(%1137) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1139 = stablehlo.reshape %1138 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1140 = stablehlo.transpose %1139, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc515)
    %1141 = stablehlo.dot_general %1121, %1140, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc516)
    %1142 = stablehlo.reshape %1141 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc517)
    %1143 = stablehlo.reshape %arg428 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1144 = stablehlo.custom_call @tt.mark_argument(%1143) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1145 = stablehlo.reshape %1144 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1146 = stablehlo.broadcast_in_dim %1145, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc518)
    %1147 = stablehlo.add %1142, %1146 : tensor<1x257x1280xbf16> loc(#loc518)
    %1148 = stablehlo.reshape %1147 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc519)
    %1149 = stablehlo.transpose %1148, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc520)
    %1150 = stablehlo.convert %1149 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc521)
    %1151 = stablehlo.transpose %1150, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc522)
    %1152 = stablehlo.multiply %1151, %cst_5 : tensor<1x16x80x257xf32> loc(#loc523)
    %1153 = stablehlo.dot_general %1136, %1152, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc524)
    %1154 = stablehlo.convert %1153 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc525)
    %1155 = stablehlo.compare  EQ, %1154, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc525)
    %1156 = stablehlo.not %1155 : tensor<1x16x257x257xi1> loc(#loc526)
    %1157 = stablehlo.reduce(%1156 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("1282|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_10aten__any"), %arg559: tensor<i1> loc("1282|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_10aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc528)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc529)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc527)
    %1158 = stablehlo.reshape %1157 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc527)
    %1159 = stablehlo.not %1158 : tensor<1x16x257x1xi1> loc(#loc530)
    %1160 = stablehlo.reshape %1159 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc531)
    %1161 = stablehlo.broadcast_in_dim %1160, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc531)
    %1162 = stablehlo.reduce(%1153 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc532)
    %1163 = stablehlo.broadcast_in_dim %1162, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc532)
    %1164 = stablehlo.subtract %1153, %1163 : tensor<1x16x257x257xf32> loc(#loc532)
    %1165 = stablehlo.exponential %1164 : tensor<1x16x257x257xf32> loc(#loc532)
    %1166 = stablehlo.reduce(%1165 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc532)
    %1167 = stablehlo.broadcast_in_dim %1166, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc532)
    %1168 = stablehlo.divide %1165, %1167 : tensor<1x16x257x257xf32> loc(#loc532)
    %1169 = stablehlo.select %1161, %cst_3, %1168 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc533)
    %1170 = stablehlo.reshape %arg274 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1171 = stablehlo.custom_call @tt.mark_argument(%1170) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1172 = stablehlo.reshape %1171 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1173 = stablehlo.transpose %1172, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc534)
    %1174 = stablehlo.dot_general %1121, %1173, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc535)
    %1175 = stablehlo.reshape %1174 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc536)
    %1176 = stablehlo.reshape %arg273 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1177 = stablehlo.custom_call @tt.mark_argument(%1176) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1178 = stablehlo.reshape %1177 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1179 = stablehlo.broadcast_in_dim %1178, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc537)
    %1180 = stablehlo.add %1175, %1179 : tensor<1x257x1280xbf16> loc(#loc537)
    %1181 = stablehlo.reshape %1180 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc538)
    %1182 = stablehlo.transpose %1181, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc539)
    %1183 = stablehlo.convert %1182 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc540)
    %1184 = stablehlo.dot_general %1169, %1183, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc541)
    %1185 = stablehlo.convert %1184 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc542)
    %1186 = stablehlo.transpose %1185, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc543)
    %1187 = stablehlo.reshape %1186 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc544)
    %1188 = stablehlo.reshape %arg272 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1189 = stablehlo.custom_call @tt.mark_argument(%1188) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1190 = stablehlo.reshape %1189 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1191 = stablehlo.transpose %1190, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc545)
    %1192 = stablehlo.dot_general %1187, %1191, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc546)
    %1193 = stablehlo.reshape %1192 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc544)
    %1194 = stablehlo.reshape %arg271 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1195 = stablehlo.custom_call @tt.mark_argument(%1194) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1196 = stablehlo.reshape %1195 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1197 = stablehlo.broadcast_in_dim %1196, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc547)
    %1198 = stablehlo.add %1193, %1197 : tensor<1x257x1280xbf16> loc(#loc547)
    %1199 = stablehlo.add %1113, %1198 : tensor<1x257x1280xbf16> loc(#loc548)
    %1200 = stablehlo.reshape %arg270 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1201 = stablehlo.custom_call @tt.mark_argument(%1200) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1202 = stablehlo.reshape %1201 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1203 = stablehlo.reshape %arg269 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1204 = stablehlo.custom_call @tt.mark_argument(%1203) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1205 = stablehlo.reshape %1204 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1206 = stablehlo.composite "tenstorrent.layer_norm" %1199, %1202, %1205 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_45} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc549)
    %1207 = stablehlo.reshape %1206 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc550)
    %1208 = stablehlo.reshape %arg268 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %1209 = stablehlo.custom_call @tt.mark_argument(%1208) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %1210 = stablehlo.reshape %1209 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %1211 = stablehlo.transpose %1210, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc551)
    %1212 = stablehlo.dot_general %1207, %1211, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc552)
    %1213 = stablehlo.reshape %1212 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc550)
    %1214 = stablehlo.reshape %arg267 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3)
    %1215 = stablehlo.custom_call @tt.mark_argument(%1214) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %1216 = stablehlo.reshape %1215 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3)
    %1217 = stablehlo.broadcast_in_dim %1216, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc553)
    %1218 = stablehlo.add %1213, %1217 : tensor<1x257x5120xbf16> loc(#loc553)
    %1219 = stablehlo.composite "tenstorrent.gelu" %1218 {decomposition = @tenstorrent.gelu.impl_22} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc554)
    %1220 = stablehlo.reshape %1219 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc555)
    %1221 = stablehlo.reshape %arg266 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %1222 = stablehlo.custom_call @tt.mark_argument(%1221) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %1223 = stablehlo.reshape %1222 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %1224 = stablehlo.transpose %1223, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc556)
    %1225 = stablehlo.dot_general %1220, %1224, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc557)
    %1226 = stablehlo.reshape %1225 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc555)
    %1227 = stablehlo.reshape %arg265 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1228 = stablehlo.custom_call @tt.mark_argument(%1227) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1229 = stablehlo.reshape %1228 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1230 = stablehlo.broadcast_in_dim %1229, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc558)
    %1231 = stablehlo.add %1226, %1230 : tensor<1x257x1280xbf16> loc(#loc558)
    %1232 = stablehlo.add %1199, %1231 : tensor<1x257x1280xbf16> loc(#loc559)
    %1233 = stablehlo.reshape %arg264 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1234 = stablehlo.custom_call @tt.mark_argument(%1233) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1235 = stablehlo.reshape %1234 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1236 = stablehlo.reshape %arg263 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1237 = stablehlo.custom_call @tt.mark_argument(%1236) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1238 = stablehlo.reshape %1237 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1239 = stablehlo.composite "tenstorrent.layer_norm" %1232, %1235, %1238 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_21} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc560)
    %1240 = stablehlo.reshape %1239 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc561)
    %1241 = stablehlo.reshape %arg435 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1242 = stablehlo.custom_call @tt.mark_argument(%1241) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1243 = stablehlo.reshape %1242 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1244 = stablehlo.transpose %1243, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc562)
    %1245 = stablehlo.dot_general %1240, %1244, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc563)
    %1246 = stablehlo.reshape %1245 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc561)
    %1247 = stablehlo.reshape %arg434 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1248 = stablehlo.custom_call @tt.mark_argument(%1247) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1249 = stablehlo.reshape %1248 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1250 = stablehlo.broadcast_in_dim %1249, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc564)
    %1251 = stablehlo.add %1246, %1250 : tensor<1x257x1280xbf16> loc(#loc564)
    %1252 = stablehlo.reshape %1251 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc565)
    %1253 = stablehlo.transpose %1252, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc566)
    %1254 = stablehlo.convert %1253 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc567)
    %1255 = stablehlo.multiply %1254, %cst_6 : tensor<1x16x257x80xf32> loc(#loc568)
    %1256 = stablehlo.reshape %arg433 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1257 = stablehlo.custom_call @tt.mark_argument(%1256) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1258 = stablehlo.reshape %1257 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1259 = stablehlo.transpose %1258, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc569)
    %1260 = stablehlo.dot_general %1240, %1259, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc570)
    %1261 = stablehlo.reshape %1260 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc571)
    %1262 = stablehlo.reshape %arg432 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1263 = stablehlo.custom_call @tt.mark_argument(%1262) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1264 = stablehlo.reshape %1263 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1265 = stablehlo.broadcast_in_dim %1264, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc572)
    %1266 = stablehlo.add %1261, %1265 : tensor<1x257x1280xbf16> loc(#loc572)
    %1267 = stablehlo.reshape %1266 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc573)
    %1268 = stablehlo.transpose %1267, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc574)
    %1269 = stablehlo.convert %1268 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc575)
    %1270 = stablehlo.transpose %1269, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc576)
    %1271 = stablehlo.multiply %1270, %cst_5 : tensor<1x16x80x257xf32> loc(#loc577)
    %1272 = stablehlo.dot_general %1255, %1271, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc578)
    %1273 = stablehlo.convert %1272 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc579)
    %1274 = stablehlo.compare  EQ, %1273, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc579)
    %1275 = stablehlo.not %1274 : tensor<1x16x257x257xi1> loc(#loc580)
    %1276 = stablehlo.reduce(%1275 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("1356|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_11aten__any"), %arg559: tensor<i1> loc("1356|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_11aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc582)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc583)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc581)
    %1277 = stablehlo.reshape %1276 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc581)
    %1278 = stablehlo.not %1277 : tensor<1x16x257x1xi1> loc(#loc584)
    %1279 = stablehlo.reshape %1278 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc585)
    %1280 = stablehlo.broadcast_in_dim %1279, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc585)
    %1281 = stablehlo.reduce(%1272 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc586)
    %1282 = stablehlo.broadcast_in_dim %1281, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc586)
    %1283 = stablehlo.subtract %1272, %1282 : tensor<1x16x257x257xf32> loc(#loc586)
    %1284 = stablehlo.exponential %1283 : tensor<1x16x257x257xf32> loc(#loc586)
    %1285 = stablehlo.reduce(%1284 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc586)
    %1286 = stablehlo.broadcast_in_dim %1285, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc586)
    %1287 = stablehlo.divide %1284, %1286 : tensor<1x16x257x257xf32> loc(#loc586)
    %1288 = stablehlo.select %1280, %cst_3, %1287 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc587)
    %1289 = stablehlo.reshape %arg262 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1290 = stablehlo.custom_call @tt.mark_argument(%1289) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1291 = stablehlo.reshape %1290 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1292 = stablehlo.transpose %1291, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc588)
    %1293 = stablehlo.dot_general %1240, %1292, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc589)
    %1294 = stablehlo.reshape %1293 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc590)
    %1295 = stablehlo.reshape %arg261 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1296 = stablehlo.custom_call @tt.mark_argument(%1295) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1297 = stablehlo.reshape %1296 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1298 = stablehlo.broadcast_in_dim %1297, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc591)
    %1299 = stablehlo.add %1294, %1298 : tensor<1x257x1280xbf16> loc(#loc591)
    %1300 = stablehlo.reshape %1299 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc592)
    %1301 = stablehlo.transpose %1300, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc593)
    %1302 = stablehlo.convert %1301 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc594)
    %1303 = stablehlo.dot_general %1288, %1302, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc595)
    %1304 = stablehlo.convert %1303 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc596)
    %1305 = stablehlo.transpose %1304, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc597)
    %1306 = stablehlo.reshape %1305 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc598)
    %1307 = stablehlo.reshape %arg260 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1308 = stablehlo.custom_call @tt.mark_argument(%1307) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1309 = stablehlo.reshape %1308 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1310 = stablehlo.transpose %1309, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc599)
    %1311 = stablehlo.dot_general %1306, %1310, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc600)
    %1312 = stablehlo.reshape %1311 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc598)
    %1313 = stablehlo.reshape %arg259 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1314 = stablehlo.custom_call @tt.mark_argument(%1313) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1315 = stablehlo.reshape %1314 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1316 = stablehlo.broadcast_in_dim %1315, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc601)
    %1317 = stablehlo.add %1312, %1316 : tensor<1x257x1280xbf16> loc(#loc601)
    %1318 = stablehlo.add %1232, %1317 : tensor<1x257x1280xbf16> loc(#loc602)
    %1319 = stablehlo.reshape %arg258 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1320 = stablehlo.custom_call @tt.mark_argument(%1319) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1321 = stablehlo.reshape %1320 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1322 = stablehlo.reshape %arg257 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1323 = stablehlo.custom_call @tt.mark_argument(%1322) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1324 = stablehlo.reshape %1323 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1325 = stablehlo.composite "tenstorrent.layer_norm" %1318, %1321, %1324 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_53} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc603)
    %1326 = stablehlo.reshape %1325 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc604)
    %1327 = stablehlo.reshape %arg256 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %1328 = stablehlo.custom_call @tt.mark_argument(%1327) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %1329 = stablehlo.reshape %1328 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %1330 = stablehlo.transpose %1329, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc605)
    %1331 = stablehlo.dot_general %1326, %1330, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc606)
    %1332 = stablehlo.reshape %1331 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc604)
    %1333 = stablehlo.reshape %arg255 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3)
    %1334 = stablehlo.custom_call @tt.mark_argument(%1333) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %1335 = stablehlo.reshape %1334 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3)
    %1336 = stablehlo.broadcast_in_dim %1335, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc607)
    %1337 = stablehlo.add %1332, %1336 : tensor<1x257x5120xbf16> loc(#loc607)
    %1338 = stablehlo.composite "tenstorrent.gelu" %1337 {decomposition = @tenstorrent.gelu.impl_25} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc608)
    %1339 = stablehlo.reshape %1338 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc609)
    %1340 = stablehlo.reshape %arg254 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %1341 = stablehlo.custom_call @tt.mark_argument(%1340) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %1342 = stablehlo.reshape %1341 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %1343 = stablehlo.transpose %1342, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc610)
    %1344 = stablehlo.dot_general %1339, %1343, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc611)
    %1345 = stablehlo.reshape %1344 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc609)
    %1346 = stablehlo.reshape %arg253 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1347 = stablehlo.custom_call @tt.mark_argument(%1346) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1348 = stablehlo.reshape %1347 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1349 = stablehlo.broadcast_in_dim %1348, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc612)
    %1350 = stablehlo.add %1345, %1349 : tensor<1x257x1280xbf16> loc(#loc612)
    %1351 = stablehlo.add %1318, %1350 : tensor<1x257x1280xbf16> loc(#loc613)
    %1352 = stablehlo.reshape %arg252 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1353 = stablehlo.custom_call @tt.mark_argument(%1352) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1354 = stablehlo.reshape %1353 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1355 = stablehlo.reshape %arg251 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1356 = stablehlo.custom_call @tt.mark_argument(%1355) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1357 = stablehlo.reshape %1356 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1358 = stablehlo.composite "tenstorrent.layer_norm" %1351, %1354, %1357 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_56} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc614)
    %1359 = stablehlo.reshape %1358 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc615)
    %1360 = stablehlo.reshape %arg439 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1361 = stablehlo.custom_call @tt.mark_argument(%1360) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1362 = stablehlo.reshape %1361 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1363 = stablehlo.transpose %1362, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc616)
    %1364 = stablehlo.dot_general %1359, %1363, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc617)
    %1365 = stablehlo.reshape %1364 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc615)
    %1366 = stablehlo.reshape %arg438 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1367 = stablehlo.custom_call @tt.mark_argument(%1366) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1368 = stablehlo.reshape %1367 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1369 = stablehlo.broadcast_in_dim %1368, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc618)
    %1370 = stablehlo.add %1365, %1369 : tensor<1x257x1280xbf16> loc(#loc618)
    %1371 = stablehlo.reshape %1370 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc619)
    %1372 = stablehlo.transpose %1371, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc620)
    %1373 = stablehlo.convert %1372 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc621)
    %1374 = stablehlo.multiply %1373, %cst_6 : tensor<1x16x257x80xf32> loc(#loc622)
    %1375 = stablehlo.reshape %arg437 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1376 = stablehlo.custom_call @tt.mark_argument(%1375) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1377 = stablehlo.reshape %1376 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1378 = stablehlo.transpose %1377, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc623)
    %1379 = stablehlo.dot_general %1359, %1378, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc624)
    %1380 = stablehlo.reshape %1379 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc625)
    %1381 = stablehlo.reshape %arg436 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1382 = stablehlo.custom_call @tt.mark_argument(%1381) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1383 = stablehlo.reshape %1382 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1384 = stablehlo.broadcast_in_dim %1383, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc626)
    %1385 = stablehlo.add %1380, %1384 : tensor<1x257x1280xbf16> loc(#loc626)
    %1386 = stablehlo.reshape %1385 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc627)
    %1387 = stablehlo.transpose %1386, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc628)
    %1388 = stablehlo.convert %1387 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc629)
    %1389 = stablehlo.transpose %1388, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc630)
    %1390 = stablehlo.multiply %1389, %cst_5 : tensor<1x16x80x257xf32> loc(#loc631)
    %1391 = stablehlo.dot_general %1374, %1390, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc632)
    %1392 = stablehlo.convert %1391 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc633)
    %1393 = stablehlo.compare  EQ, %1392, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc633)
    %1394 = stablehlo.not %1393 : tensor<1x16x257x257xi1> loc(#loc634)
    %1395 = stablehlo.reduce(%1394 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("1430|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_12aten__any"), %arg559: tensor<i1> loc("1430|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_12aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc636)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc637)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc635)
    %1396 = stablehlo.reshape %1395 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc635)
    %1397 = stablehlo.not %1396 : tensor<1x16x257x1xi1> loc(#loc638)
    %1398 = stablehlo.reshape %1397 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc639)
    %1399 = stablehlo.broadcast_in_dim %1398, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc639)
    %1400 = stablehlo.reduce(%1391 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc640)
    %1401 = stablehlo.broadcast_in_dim %1400, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc640)
    %1402 = stablehlo.subtract %1391, %1401 : tensor<1x16x257x257xf32> loc(#loc640)
    %1403 = stablehlo.exponential %1402 : tensor<1x16x257x257xf32> loc(#loc640)
    %1404 = stablehlo.reduce(%1403 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc640)
    %1405 = stablehlo.broadcast_in_dim %1404, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc640)
    %1406 = stablehlo.divide %1403, %1405 : tensor<1x16x257x257xf32> loc(#loc640)
    %1407 = stablehlo.select %1399, %cst_3, %1406 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc641)
    %1408 = stablehlo.reshape %arg250 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1409 = stablehlo.custom_call @tt.mark_argument(%1408) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1410 = stablehlo.reshape %1409 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1411 = stablehlo.transpose %1410, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc642)
    %1412 = stablehlo.dot_general %1359, %1411, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc643)
    %1413 = stablehlo.reshape %1412 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc644)
    %1414 = stablehlo.reshape %arg249 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1415 = stablehlo.custom_call @tt.mark_argument(%1414) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1416 = stablehlo.reshape %1415 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1417 = stablehlo.broadcast_in_dim %1416, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc645)
    %1418 = stablehlo.add %1413, %1417 : tensor<1x257x1280xbf16> loc(#loc645)
    %1419 = stablehlo.reshape %1418 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc646)
    %1420 = stablehlo.transpose %1419, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc647)
    %1421 = stablehlo.convert %1420 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc648)
    %1422 = stablehlo.dot_general %1407, %1421, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc649)
    %1423 = stablehlo.convert %1422 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc650)
    %1424 = stablehlo.transpose %1423, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc651)
    %1425 = stablehlo.reshape %1424 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc652)
    %1426 = stablehlo.reshape %arg248 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1427 = stablehlo.custom_call @tt.mark_argument(%1426) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1428 = stablehlo.reshape %1427 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1429 = stablehlo.transpose %1428, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc653)
    %1430 = stablehlo.dot_general %1425, %1429, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc654)
    %1431 = stablehlo.reshape %1430 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc652)
    %1432 = stablehlo.reshape %arg247 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1433 = stablehlo.custom_call @tt.mark_argument(%1432) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1434 = stablehlo.reshape %1433 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1435 = stablehlo.broadcast_in_dim %1434, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc655)
    %1436 = stablehlo.add %1431, %1435 : tensor<1x257x1280xbf16> loc(#loc655)
    %1437 = stablehlo.add %1351, %1436 : tensor<1x257x1280xbf16> loc(#loc656)
    %1438 = stablehlo.reshape %arg246 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1439 = stablehlo.custom_call @tt.mark_argument(%1438) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1440 = stablehlo.reshape %1439 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1441 = stablehlo.reshape %arg245 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1442 = stablehlo.custom_call @tt.mark_argument(%1441) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1443 = stablehlo.reshape %1442 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1444 = stablehlo.composite "tenstorrent.layer_norm" %1437, %1440, %1443 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_57} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc657)
    %1445 = stablehlo.reshape %1444 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc658)
    %1446 = stablehlo.reshape %arg244 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %1447 = stablehlo.custom_call @tt.mark_argument(%1446) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %1448 = stablehlo.reshape %1447 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %1449 = stablehlo.transpose %1448, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc659)
    %1450 = stablehlo.dot_general %1445, %1449, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc660)
    %1451 = stablehlo.reshape %1450 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc658)
    %1452 = stablehlo.reshape %arg243 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3)
    %1453 = stablehlo.custom_call @tt.mark_argument(%1452) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %1454 = stablehlo.reshape %1453 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3)
    %1455 = stablehlo.broadcast_in_dim %1454, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc661)
    %1456 = stablehlo.add %1451, %1455 : tensor<1x257x5120xbf16> loc(#loc661)
    %1457 = stablehlo.composite "tenstorrent.gelu" %1456 {decomposition = @tenstorrent.gelu.impl_30} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc662)
    %1458 = stablehlo.reshape %1457 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc663)
    %1459 = stablehlo.reshape %arg242 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %1460 = stablehlo.custom_call @tt.mark_argument(%1459) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %1461 = stablehlo.reshape %1460 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %1462 = stablehlo.transpose %1461, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc664)
    %1463 = stablehlo.dot_general %1458, %1462, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc665)
    %1464 = stablehlo.reshape %1463 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc663)
    %1465 = stablehlo.reshape %arg241 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1466 = stablehlo.custom_call @tt.mark_argument(%1465) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1467 = stablehlo.reshape %1466 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1468 = stablehlo.broadcast_in_dim %1467, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc666)
    %1469 = stablehlo.add %1464, %1468 : tensor<1x257x1280xbf16> loc(#loc666)
    %1470 = stablehlo.add %1437, %1469 : tensor<1x257x1280xbf16> loc(#loc667)
    %1471 = stablehlo.reshape %arg240 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1472 = stablehlo.custom_call @tt.mark_argument(%1471) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1473 = stablehlo.reshape %1472 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1474 = stablehlo.reshape %arg239 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1475 = stablehlo.custom_call @tt.mark_argument(%1474) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1476 = stablehlo.reshape %1475 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1477 = stablehlo.composite "tenstorrent.layer_norm" %1470, %1473, %1476 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_69} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc668)
    %1478 = stablehlo.reshape %1477 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc669)
    %1479 = stablehlo.reshape %arg443 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1480 = stablehlo.custom_call @tt.mark_argument(%1479) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1481 = stablehlo.reshape %1480 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1482 = stablehlo.transpose %1481, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc670)
    %1483 = stablehlo.dot_general %1478, %1482, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc671)
    %1484 = stablehlo.reshape %1483 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc669)
    %1485 = stablehlo.reshape %arg442 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1486 = stablehlo.custom_call @tt.mark_argument(%1485) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1487 = stablehlo.reshape %1486 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1488 = stablehlo.broadcast_in_dim %1487, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc672)
    %1489 = stablehlo.add %1484, %1488 : tensor<1x257x1280xbf16> loc(#loc672)
    %1490 = stablehlo.reshape %1489 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc673)
    %1491 = stablehlo.transpose %1490, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc674)
    %1492 = stablehlo.convert %1491 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc675)
    %1493 = stablehlo.multiply %1492, %cst_6 : tensor<1x16x257x80xf32> loc(#loc676)
    %1494 = stablehlo.reshape %arg441 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1495 = stablehlo.custom_call @tt.mark_argument(%1494) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1496 = stablehlo.reshape %1495 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1497 = stablehlo.transpose %1496, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc677)
    %1498 = stablehlo.dot_general %1478, %1497, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc678)
    %1499 = stablehlo.reshape %1498 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc679)
    %1500 = stablehlo.reshape %arg440 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1501 = stablehlo.custom_call @tt.mark_argument(%1500) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1502 = stablehlo.reshape %1501 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1503 = stablehlo.broadcast_in_dim %1502, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc680)
    %1504 = stablehlo.add %1499, %1503 : tensor<1x257x1280xbf16> loc(#loc680)
    %1505 = stablehlo.reshape %1504 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc681)
    %1506 = stablehlo.transpose %1505, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc682)
    %1507 = stablehlo.convert %1506 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc683)
    %1508 = stablehlo.transpose %1507, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc684)
    %1509 = stablehlo.multiply %1508, %cst_5 : tensor<1x16x80x257xf32> loc(#loc685)
    %1510 = stablehlo.dot_general %1493, %1509, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc686)
    %1511 = stablehlo.convert %1510 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc687)
    %1512 = stablehlo.compare  EQ, %1511, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc687)
    %1513 = stablehlo.not %1512 : tensor<1x16x257x257xi1> loc(#loc688)
    %1514 = stablehlo.reduce(%1513 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("1504|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_13aten__any"), %arg559: tensor<i1> loc("1504|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_13aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc690)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc691)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc689)
    %1515 = stablehlo.reshape %1514 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc689)
    %1516 = stablehlo.not %1515 : tensor<1x16x257x1xi1> loc(#loc692)
    %1517 = stablehlo.reshape %1516 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc693)
    %1518 = stablehlo.broadcast_in_dim %1517, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc693)
    %1519 = stablehlo.reduce(%1510 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc694)
    %1520 = stablehlo.broadcast_in_dim %1519, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc694)
    %1521 = stablehlo.subtract %1510, %1520 : tensor<1x16x257x257xf32> loc(#loc694)
    %1522 = stablehlo.exponential %1521 : tensor<1x16x257x257xf32> loc(#loc694)
    %1523 = stablehlo.reduce(%1522 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc694)
    %1524 = stablehlo.broadcast_in_dim %1523, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc694)
    %1525 = stablehlo.divide %1522, %1524 : tensor<1x16x257x257xf32> loc(#loc694)
    %1526 = stablehlo.select %1518, %cst_3, %1525 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc695)
    %1527 = stablehlo.reshape %arg238 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1528 = stablehlo.custom_call @tt.mark_argument(%1527) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1529 = stablehlo.reshape %1528 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1530 = stablehlo.transpose %1529, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc696)
    %1531 = stablehlo.dot_general %1478, %1530, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc697)
    %1532 = stablehlo.reshape %1531 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc698)
    %1533 = stablehlo.reshape %arg237 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1534 = stablehlo.custom_call @tt.mark_argument(%1533) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1535 = stablehlo.reshape %1534 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1536 = stablehlo.broadcast_in_dim %1535, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc699)
    %1537 = stablehlo.add %1532, %1536 : tensor<1x257x1280xbf16> loc(#loc699)
    %1538 = stablehlo.reshape %1537 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc700)
    %1539 = stablehlo.transpose %1538, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc701)
    %1540 = stablehlo.convert %1539 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc702)
    %1541 = stablehlo.dot_general %1526, %1540, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc703)
    %1542 = stablehlo.convert %1541 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc704)
    %1543 = stablehlo.transpose %1542, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc705)
    %1544 = stablehlo.reshape %1543 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc706)
    %1545 = stablehlo.reshape %arg236 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1546 = stablehlo.custom_call @tt.mark_argument(%1545) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1547 = stablehlo.reshape %1546 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1548 = stablehlo.transpose %1547, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc707)
    %1549 = stablehlo.dot_general %1544, %1548, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc708)
    %1550 = stablehlo.reshape %1549 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc706)
    %1551 = stablehlo.reshape %arg235 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1552 = stablehlo.custom_call @tt.mark_argument(%1551) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1553 = stablehlo.reshape %1552 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1554 = stablehlo.broadcast_in_dim %1553, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc709)
    %1555 = stablehlo.add %1550, %1554 : tensor<1x257x1280xbf16> loc(#loc709)
    %1556 = stablehlo.add %1470, %1555 : tensor<1x257x1280xbf16> loc(#loc710)
    %1557 = stablehlo.reshape %arg234 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1558 = stablehlo.custom_call @tt.mark_argument(%1557) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1559 = stablehlo.reshape %1558 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1560 = stablehlo.reshape %arg233 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1561 = stablehlo.custom_call @tt.mark_argument(%1560) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1562 = stablehlo.reshape %1561 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1563 = stablehlo.composite "tenstorrent.layer_norm" %1556, %1559, %1562 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_60} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc711)
    %1564 = stablehlo.reshape %1563 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc712)
    %1565 = stablehlo.reshape %arg232 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %1566 = stablehlo.custom_call @tt.mark_argument(%1565) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %1567 = stablehlo.reshape %1566 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %1568 = stablehlo.transpose %1567, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc713)
    %1569 = stablehlo.dot_general %1564, %1568, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc714)
    %1570 = stablehlo.reshape %1569 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc712)
    %1571 = stablehlo.reshape %arg231 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3)
    %1572 = stablehlo.custom_call @tt.mark_argument(%1571) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %1573 = stablehlo.reshape %1572 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3)
    %1574 = stablehlo.broadcast_in_dim %1573, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc715)
    %1575 = stablehlo.add %1570, %1574 : tensor<1x257x5120xbf16> loc(#loc715)
    %1576 = stablehlo.composite "tenstorrent.gelu" %1575 {decomposition = @tenstorrent.gelu.impl_18} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc716)
    %1577 = stablehlo.reshape %1576 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc717)
    %1578 = stablehlo.reshape %arg230 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %1579 = stablehlo.custom_call @tt.mark_argument(%1578) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %1580 = stablehlo.reshape %1579 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %1581 = stablehlo.transpose %1580, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc718)
    %1582 = stablehlo.dot_general %1577, %1581, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc719)
    %1583 = stablehlo.reshape %1582 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc717)
    %1584 = stablehlo.reshape %arg229 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1585 = stablehlo.custom_call @tt.mark_argument(%1584) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1586 = stablehlo.reshape %1585 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1587 = stablehlo.broadcast_in_dim %1586, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc720)
    %1588 = stablehlo.add %1583, %1587 : tensor<1x257x1280xbf16> loc(#loc720)
    %1589 = stablehlo.add %1556, %1588 : tensor<1x257x1280xbf16> loc(#loc721)
    %1590 = stablehlo.reshape %arg228 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1591 = stablehlo.custom_call @tt.mark_argument(%1590) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1592 = stablehlo.reshape %1591 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1593 = stablehlo.reshape %arg227 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1594 = stablehlo.custom_call @tt.mark_argument(%1593) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1595 = stablehlo.reshape %1594 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1596 = stablehlo.composite "tenstorrent.layer_norm" %1589, %1592, %1595 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_73} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc722)
    %1597 = stablehlo.reshape %1596 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc723)
    %1598 = stablehlo.reshape %arg447 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1599 = stablehlo.custom_call @tt.mark_argument(%1598) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1600 = stablehlo.reshape %1599 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1601 = stablehlo.transpose %1600, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc724)
    %1602 = stablehlo.dot_general %1597, %1601, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc725)
    %1603 = stablehlo.reshape %1602 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc723)
    %1604 = stablehlo.reshape %arg446 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1605 = stablehlo.custom_call @tt.mark_argument(%1604) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1606 = stablehlo.reshape %1605 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1607 = stablehlo.broadcast_in_dim %1606, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc726)
    %1608 = stablehlo.add %1603, %1607 : tensor<1x257x1280xbf16> loc(#loc726)
    %1609 = stablehlo.reshape %1608 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc727)
    %1610 = stablehlo.transpose %1609, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc728)
    %1611 = stablehlo.convert %1610 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc729)
    %1612 = stablehlo.multiply %1611, %cst_6 : tensor<1x16x257x80xf32> loc(#loc730)
    %1613 = stablehlo.reshape %arg445 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1614 = stablehlo.custom_call @tt.mark_argument(%1613) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1615 = stablehlo.reshape %1614 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1616 = stablehlo.transpose %1615, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc731)
    %1617 = stablehlo.dot_general %1597, %1616, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc732)
    %1618 = stablehlo.reshape %1617 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc733)
    %1619 = stablehlo.reshape %arg444 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1620 = stablehlo.custom_call @tt.mark_argument(%1619) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1621 = stablehlo.reshape %1620 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1622 = stablehlo.broadcast_in_dim %1621, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc734)
    %1623 = stablehlo.add %1618, %1622 : tensor<1x257x1280xbf16> loc(#loc734)
    %1624 = stablehlo.reshape %1623 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc735)
    %1625 = stablehlo.transpose %1624, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc736)
    %1626 = stablehlo.convert %1625 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc737)
    %1627 = stablehlo.transpose %1626, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc738)
    %1628 = stablehlo.multiply %1627, %cst_5 : tensor<1x16x80x257xf32> loc(#loc739)
    %1629 = stablehlo.dot_general %1612, %1628, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc740)
    %1630 = stablehlo.convert %1629 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc741)
    %1631 = stablehlo.compare  EQ, %1630, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc741)
    %1632 = stablehlo.not %1631 : tensor<1x16x257x257xi1> loc(#loc742)
    %1633 = stablehlo.reduce(%1632 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("1578|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_14aten__any"), %arg559: tensor<i1> loc("1578|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_14aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc744)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc745)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc743)
    %1634 = stablehlo.reshape %1633 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc743)
    %1635 = stablehlo.not %1634 : tensor<1x16x257x1xi1> loc(#loc746)
    %1636 = stablehlo.reshape %1635 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc747)
    %1637 = stablehlo.broadcast_in_dim %1636, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc747)
    %1638 = stablehlo.reduce(%1629 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc748)
    %1639 = stablehlo.broadcast_in_dim %1638, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc748)
    %1640 = stablehlo.subtract %1629, %1639 : tensor<1x16x257x257xf32> loc(#loc748)
    %1641 = stablehlo.exponential %1640 : tensor<1x16x257x257xf32> loc(#loc748)
    %1642 = stablehlo.reduce(%1641 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc748)
    %1643 = stablehlo.broadcast_in_dim %1642, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc748)
    %1644 = stablehlo.divide %1641, %1643 : tensor<1x16x257x257xf32> loc(#loc748)
    %1645 = stablehlo.select %1637, %cst_3, %1644 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc749)
    %1646 = stablehlo.reshape %arg226 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1647 = stablehlo.custom_call @tt.mark_argument(%1646) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1648 = stablehlo.reshape %1647 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1649 = stablehlo.transpose %1648, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc750)
    %1650 = stablehlo.dot_general %1597, %1649, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc751)
    %1651 = stablehlo.reshape %1650 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc752)
    %1652 = stablehlo.reshape %arg225 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1653 = stablehlo.custom_call @tt.mark_argument(%1652) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1654 = stablehlo.reshape %1653 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1655 = stablehlo.broadcast_in_dim %1654, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc753)
    %1656 = stablehlo.add %1651, %1655 : tensor<1x257x1280xbf16> loc(#loc753)
    %1657 = stablehlo.reshape %1656 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc754)
    %1658 = stablehlo.transpose %1657, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc755)
    %1659 = stablehlo.convert %1658 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc756)
    %1660 = stablehlo.dot_general %1645, %1659, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc757)
    %1661 = stablehlo.convert %1660 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc758)
    %1662 = stablehlo.transpose %1661, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc759)
    %1663 = stablehlo.reshape %1662 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc760)
    %1664 = stablehlo.reshape %arg224 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1665 = stablehlo.custom_call @tt.mark_argument(%1664) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1666 = stablehlo.reshape %1665 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1667 = stablehlo.transpose %1666, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc761)
    %1668 = stablehlo.dot_general %1663, %1667, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc762)
    %1669 = stablehlo.reshape %1668 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc760)
    %1670 = stablehlo.reshape %arg223 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1671 = stablehlo.custom_call @tt.mark_argument(%1670) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1672 = stablehlo.reshape %1671 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1673 = stablehlo.broadcast_in_dim %1672, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc763)
    %1674 = stablehlo.add %1669, %1673 : tensor<1x257x1280xbf16> loc(#loc763)
    %1675 = stablehlo.add %1589, %1674 : tensor<1x257x1280xbf16> loc(#loc764)
    %1676 = stablehlo.reshape %arg222 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1677 = stablehlo.custom_call @tt.mark_argument(%1676) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1678 = stablehlo.reshape %1677 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1679 = stablehlo.reshape %arg221 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1680 = stablehlo.custom_call @tt.mark_argument(%1679) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1681 = stablehlo.reshape %1680 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1682 = stablehlo.composite "tenstorrent.layer_norm" %1675, %1678, %1681 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_62} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc765)
    %1683 = stablehlo.reshape %1682 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc766)
    %1684 = stablehlo.reshape %arg220 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %1685 = stablehlo.custom_call @tt.mark_argument(%1684) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %1686 = stablehlo.reshape %1685 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %1687 = stablehlo.transpose %1686, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc767)
    %1688 = stablehlo.dot_general %1683, %1687, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc768)
    %1689 = stablehlo.reshape %1688 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc766)
    %1690 = stablehlo.reshape %arg219 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3)
    %1691 = stablehlo.custom_call @tt.mark_argument(%1690) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %1692 = stablehlo.reshape %1691 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3)
    %1693 = stablehlo.broadcast_in_dim %1692, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc769)
    %1694 = stablehlo.add %1689, %1693 : tensor<1x257x5120xbf16> loc(#loc769)
    %1695 = stablehlo.composite "tenstorrent.gelu" %1694 {decomposition = @tenstorrent.gelu.impl_28} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc770)
    %1696 = stablehlo.reshape %1695 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc771)
    %1697 = stablehlo.reshape %arg218 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %1698 = stablehlo.custom_call @tt.mark_argument(%1697) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %1699 = stablehlo.reshape %1698 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %1700 = stablehlo.transpose %1699, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc772)
    %1701 = stablehlo.dot_general %1696, %1700, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc773)
    %1702 = stablehlo.reshape %1701 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc771)
    %1703 = stablehlo.reshape %arg217 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1704 = stablehlo.custom_call @tt.mark_argument(%1703) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1705 = stablehlo.reshape %1704 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1706 = stablehlo.broadcast_in_dim %1705, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc774)
    %1707 = stablehlo.add %1702, %1706 : tensor<1x257x1280xbf16> loc(#loc774)
    %1708 = stablehlo.add %1675, %1707 : tensor<1x257x1280xbf16> loc(#loc775)
    %1709 = stablehlo.reshape %arg216 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1710 = stablehlo.custom_call @tt.mark_argument(%1709) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1711 = stablehlo.reshape %1710 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1712 = stablehlo.reshape %arg215 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1713 = stablehlo.custom_call @tt.mark_argument(%1712) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1714 = stablehlo.reshape %1713 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1715 = stablehlo.composite "tenstorrent.layer_norm" %1708, %1711, %1714 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_66} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc776)
    %1716 = stablehlo.reshape %1715 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc777)
    %1717 = stablehlo.reshape %arg451 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1718 = stablehlo.custom_call @tt.mark_argument(%1717) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1719 = stablehlo.reshape %1718 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1720 = stablehlo.transpose %1719, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc778)
    %1721 = stablehlo.dot_general %1716, %1720, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc779)
    %1722 = stablehlo.reshape %1721 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc777)
    %1723 = stablehlo.reshape %arg450 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1724 = stablehlo.custom_call @tt.mark_argument(%1723) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1725 = stablehlo.reshape %1724 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1726 = stablehlo.broadcast_in_dim %1725, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc780)
    %1727 = stablehlo.add %1722, %1726 : tensor<1x257x1280xbf16> loc(#loc780)
    %1728 = stablehlo.reshape %1727 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc781)
    %1729 = stablehlo.transpose %1728, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc782)
    %1730 = stablehlo.convert %1729 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc783)
    %1731 = stablehlo.multiply %1730, %cst_6 : tensor<1x16x257x80xf32> loc(#loc784)
    %1732 = stablehlo.reshape %arg449 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1733 = stablehlo.custom_call @tt.mark_argument(%1732) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1734 = stablehlo.reshape %1733 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1735 = stablehlo.transpose %1734, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc785)
    %1736 = stablehlo.dot_general %1716, %1735, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc786)
    %1737 = stablehlo.reshape %1736 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc787)
    %1738 = stablehlo.reshape %arg448 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1739 = stablehlo.custom_call @tt.mark_argument(%1738) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1740 = stablehlo.reshape %1739 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1741 = stablehlo.broadcast_in_dim %1740, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc788)
    %1742 = stablehlo.add %1737, %1741 : tensor<1x257x1280xbf16> loc(#loc788)
    %1743 = stablehlo.reshape %1742 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc789)
    %1744 = stablehlo.transpose %1743, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc790)
    %1745 = stablehlo.convert %1744 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc791)
    %1746 = stablehlo.transpose %1745, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc792)
    %1747 = stablehlo.multiply %1746, %cst_5 : tensor<1x16x80x257xf32> loc(#loc793)
    %1748 = stablehlo.dot_general %1731, %1747, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc794)
    %1749 = stablehlo.convert %1748 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc795)
    %1750 = stablehlo.compare  EQ, %1749, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc795)
    %1751 = stablehlo.not %1750 : tensor<1x16x257x257xi1> loc(#loc796)
    %1752 = stablehlo.reduce(%1751 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("1652|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_15aten__any"), %arg559: tensor<i1> loc("1652|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_15aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc798)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc799)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc797)
    %1753 = stablehlo.reshape %1752 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc797)
    %1754 = stablehlo.not %1753 : tensor<1x16x257x1xi1> loc(#loc800)
    %1755 = stablehlo.reshape %1754 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc801)
    %1756 = stablehlo.broadcast_in_dim %1755, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc801)
    %1757 = stablehlo.reduce(%1748 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc802)
    %1758 = stablehlo.broadcast_in_dim %1757, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc802)
    %1759 = stablehlo.subtract %1748, %1758 : tensor<1x16x257x257xf32> loc(#loc802)
    %1760 = stablehlo.exponential %1759 : tensor<1x16x257x257xf32> loc(#loc802)
    %1761 = stablehlo.reduce(%1760 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc802)
    %1762 = stablehlo.broadcast_in_dim %1761, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc802)
    %1763 = stablehlo.divide %1760, %1762 : tensor<1x16x257x257xf32> loc(#loc802)
    %1764 = stablehlo.select %1756, %cst_3, %1763 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc803)
    %1765 = stablehlo.reshape %arg214 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1766 = stablehlo.custom_call @tt.mark_argument(%1765) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1767 = stablehlo.reshape %1766 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1768 = stablehlo.transpose %1767, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc804)
    %1769 = stablehlo.dot_general %1716, %1768, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc805)
    %1770 = stablehlo.reshape %1769 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc806)
    %1771 = stablehlo.reshape %arg213 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1772 = stablehlo.custom_call @tt.mark_argument(%1771) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1773 = stablehlo.reshape %1772 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1774 = stablehlo.broadcast_in_dim %1773, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc807)
    %1775 = stablehlo.add %1770, %1774 : tensor<1x257x1280xbf16> loc(#loc807)
    %1776 = stablehlo.reshape %1775 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc808)
    %1777 = stablehlo.transpose %1776, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc809)
    %1778 = stablehlo.convert %1777 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc810)
    %1779 = stablehlo.dot_general %1764, %1778, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc811)
    %1780 = stablehlo.convert %1779 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc812)
    %1781 = stablehlo.transpose %1780, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc813)
    %1782 = stablehlo.reshape %1781 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc814)
    %1783 = stablehlo.reshape %arg212 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1784 = stablehlo.custom_call @tt.mark_argument(%1783) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1785 = stablehlo.reshape %1784 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1786 = stablehlo.transpose %1785, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc815)
    %1787 = stablehlo.dot_general %1782, %1786, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc816)
    %1788 = stablehlo.reshape %1787 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc814)
    %1789 = stablehlo.reshape %arg211 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1790 = stablehlo.custom_call @tt.mark_argument(%1789) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1791 = stablehlo.reshape %1790 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1792 = stablehlo.broadcast_in_dim %1791, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc817)
    %1793 = stablehlo.add %1788, %1792 : tensor<1x257x1280xbf16> loc(#loc817)
    %1794 = stablehlo.add %1708, %1793 : tensor<1x257x1280xbf16> loc(#loc818)
    %1795 = stablehlo.reshape %arg210 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1796 = stablehlo.custom_call @tt.mark_argument(%1795) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1797 = stablehlo.reshape %1796 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1798 = stablehlo.reshape %arg209 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1799 = stablehlo.custom_call @tt.mark_argument(%1798) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1800 = stablehlo.reshape %1799 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1801 = stablehlo.composite "tenstorrent.layer_norm" %1794, %1797, %1800 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_33} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc819)
    %1802 = stablehlo.reshape %1801 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc820)
    %1803 = stablehlo.reshape %arg208 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %1804 = stablehlo.custom_call @tt.mark_argument(%1803) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %1805 = stablehlo.reshape %1804 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %1806 = stablehlo.transpose %1805, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc821)
    %1807 = stablehlo.dot_general %1802, %1806, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc822)
    %1808 = stablehlo.reshape %1807 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc820)
    %1809 = stablehlo.reshape %arg207 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3)
    %1810 = stablehlo.custom_call @tt.mark_argument(%1809) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %1811 = stablehlo.reshape %1810 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3)
    %1812 = stablehlo.broadcast_in_dim %1811, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc823)
    %1813 = stablehlo.add %1808, %1812 : tensor<1x257x5120xbf16> loc(#loc823)
    %1814 = stablehlo.composite "tenstorrent.gelu" %1813 {decomposition = @tenstorrent.gelu.impl_31} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc824)
    %1815 = stablehlo.reshape %1814 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc825)
    %1816 = stablehlo.reshape %arg206 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %1817 = stablehlo.custom_call @tt.mark_argument(%1816) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %1818 = stablehlo.reshape %1817 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %1819 = stablehlo.transpose %1818, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc826)
    %1820 = stablehlo.dot_general %1815, %1819, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc827)
    %1821 = stablehlo.reshape %1820 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc825)
    %1822 = stablehlo.reshape %arg205 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1823 = stablehlo.custom_call @tt.mark_argument(%1822) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1824 = stablehlo.reshape %1823 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1825 = stablehlo.broadcast_in_dim %1824, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc828)
    %1826 = stablehlo.add %1821, %1825 : tensor<1x257x1280xbf16> loc(#loc828)
    %1827 = stablehlo.add %1794, %1826 : tensor<1x257x1280xbf16> loc(#loc829)
    %1828 = stablehlo.reshape %arg204 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1829 = stablehlo.custom_call @tt.mark_argument(%1828) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1830 = stablehlo.reshape %1829 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1831 = stablehlo.reshape %arg203 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1832 = stablehlo.custom_call @tt.mark_argument(%1831) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1833 = stablehlo.reshape %1832 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1834 = stablehlo.composite "tenstorrent.layer_norm" %1827, %1830, %1833 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_40} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc830)
    %1835 = stablehlo.reshape %1834 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc831)
    %1836 = stablehlo.reshape %arg455 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1837 = stablehlo.custom_call @tt.mark_argument(%1836) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1838 = stablehlo.reshape %1837 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1839 = stablehlo.transpose %1838, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc832)
    %1840 = stablehlo.dot_general %1835, %1839, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc833)
    %1841 = stablehlo.reshape %1840 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc831)
    %1842 = stablehlo.reshape %arg454 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1843 = stablehlo.custom_call @tt.mark_argument(%1842) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1844 = stablehlo.reshape %1843 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1845 = stablehlo.broadcast_in_dim %1844, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc834)
    %1846 = stablehlo.add %1841, %1845 : tensor<1x257x1280xbf16> loc(#loc834)
    %1847 = stablehlo.reshape %1846 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc835)
    %1848 = stablehlo.transpose %1847, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc836)
    %1849 = stablehlo.convert %1848 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc837)
    %1850 = stablehlo.multiply %1849, %cst_6 : tensor<1x16x257x80xf32> loc(#loc838)
    %1851 = stablehlo.reshape %arg453 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1852 = stablehlo.custom_call @tt.mark_argument(%1851) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1853 = stablehlo.reshape %1852 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1854 = stablehlo.transpose %1853, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc839)
    %1855 = stablehlo.dot_general %1835, %1854, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc840)
    %1856 = stablehlo.reshape %1855 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc841)
    %1857 = stablehlo.reshape %arg452 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1858 = stablehlo.custom_call @tt.mark_argument(%1857) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1859 = stablehlo.reshape %1858 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1860 = stablehlo.broadcast_in_dim %1859, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc842)
    %1861 = stablehlo.add %1856, %1860 : tensor<1x257x1280xbf16> loc(#loc842)
    %1862 = stablehlo.reshape %1861 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc843)
    %1863 = stablehlo.transpose %1862, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc844)
    %1864 = stablehlo.convert %1863 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc845)
    %1865 = stablehlo.transpose %1864, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc846)
    %1866 = stablehlo.multiply %1865, %cst_5 : tensor<1x16x80x257xf32> loc(#loc847)
    %1867 = stablehlo.dot_general %1850, %1866, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc848)
    %1868 = stablehlo.convert %1867 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc849)
    %1869 = stablehlo.compare  EQ, %1868, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc849)
    %1870 = stablehlo.not %1869 : tensor<1x16x257x257xi1> loc(#loc850)
    %1871 = stablehlo.reduce(%1870 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("1726|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_16aten__any"), %arg559: tensor<i1> loc("1726|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_16aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc852)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc853)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc851)
    %1872 = stablehlo.reshape %1871 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc851)
    %1873 = stablehlo.not %1872 : tensor<1x16x257x1xi1> loc(#loc854)
    %1874 = stablehlo.reshape %1873 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc855)
    %1875 = stablehlo.broadcast_in_dim %1874, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc855)
    %1876 = stablehlo.reduce(%1867 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc856)
    %1877 = stablehlo.broadcast_in_dim %1876, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc856)
    %1878 = stablehlo.subtract %1867, %1877 : tensor<1x16x257x257xf32> loc(#loc856)
    %1879 = stablehlo.exponential %1878 : tensor<1x16x257x257xf32> loc(#loc856)
    %1880 = stablehlo.reduce(%1879 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc856)
    %1881 = stablehlo.broadcast_in_dim %1880, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc856)
    %1882 = stablehlo.divide %1879, %1881 : tensor<1x16x257x257xf32> loc(#loc856)
    %1883 = stablehlo.select %1875, %cst_3, %1882 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc857)
    %1884 = stablehlo.reshape %arg202 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1885 = stablehlo.custom_call @tt.mark_argument(%1884) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1886 = stablehlo.reshape %1885 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1887 = stablehlo.transpose %1886, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc858)
    %1888 = stablehlo.dot_general %1835, %1887, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc859)
    %1889 = stablehlo.reshape %1888 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc860)
    %1890 = stablehlo.reshape %arg201 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1891 = stablehlo.custom_call @tt.mark_argument(%1890) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1892 = stablehlo.reshape %1891 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1893 = stablehlo.broadcast_in_dim %1892, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc861)
    %1894 = stablehlo.add %1889, %1893 : tensor<1x257x1280xbf16> loc(#loc861)
    %1895 = stablehlo.reshape %1894 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc862)
    %1896 = stablehlo.transpose %1895, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc863)
    %1897 = stablehlo.convert %1896 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc864)
    %1898 = stablehlo.dot_general %1883, %1897, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc865)
    %1899 = stablehlo.convert %1898 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc866)
    %1900 = stablehlo.transpose %1899, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc867)
    %1901 = stablehlo.reshape %1900 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc868)
    %1902 = stablehlo.reshape %arg200 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1903 = stablehlo.custom_call @tt.mark_argument(%1902) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1904 = stablehlo.reshape %1903 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1905 = stablehlo.transpose %1904, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc869)
    %1906 = stablehlo.dot_general %1901, %1905, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc870)
    %1907 = stablehlo.reshape %1906 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc868)
    %1908 = stablehlo.reshape %arg199 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1909 = stablehlo.custom_call @tt.mark_argument(%1908) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1910 = stablehlo.reshape %1909 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1911 = stablehlo.broadcast_in_dim %1910, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc871)
    %1912 = stablehlo.add %1907, %1911 : tensor<1x257x1280xbf16> loc(#loc871)
    %1913 = stablehlo.add %1827, %1912 : tensor<1x257x1280xbf16> loc(#loc872)
    %1914 = stablehlo.reshape %arg198 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1915 = stablehlo.custom_call @tt.mark_argument(%1914) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1916 = stablehlo.reshape %1915 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1917 = stablehlo.reshape %arg197 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1918 = stablehlo.custom_call @tt.mark_argument(%1917) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1919 = stablehlo.reshape %1918 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1920 = stablehlo.composite "tenstorrent.layer_norm" %1913, %1916, %1919 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_48} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc873)
    %1921 = stablehlo.reshape %1920 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc874)
    %1922 = stablehlo.reshape %arg196 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %1923 = stablehlo.custom_call @tt.mark_argument(%1922) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %1924 = stablehlo.reshape %1923 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %1925 = stablehlo.transpose %1924, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc875)
    %1926 = stablehlo.dot_general %1921, %1925, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc876)
    %1927 = stablehlo.reshape %1926 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc874)
    %1928 = stablehlo.reshape %arg195 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3)
    %1929 = stablehlo.custom_call @tt.mark_argument(%1928) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %1930 = stablehlo.reshape %1929 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3)
    %1931 = stablehlo.broadcast_in_dim %1930, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc877)
    %1932 = stablehlo.add %1927, %1931 : tensor<1x257x5120xbf16> loc(#loc877)
    %1933 = stablehlo.composite "tenstorrent.gelu" %1932 {decomposition = @tenstorrent.gelu.impl_27} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc878)
    %1934 = stablehlo.reshape %1933 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc879)
    %1935 = stablehlo.reshape %arg194 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %1936 = stablehlo.custom_call @tt.mark_argument(%1935) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %1937 = stablehlo.reshape %1936 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %1938 = stablehlo.transpose %1937, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc880)
    %1939 = stablehlo.dot_general %1934, %1938, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc881)
    %1940 = stablehlo.reshape %1939 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc879)
    %1941 = stablehlo.reshape %arg193 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1942 = stablehlo.custom_call @tt.mark_argument(%1941) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1943 = stablehlo.reshape %1942 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1944 = stablehlo.broadcast_in_dim %1943, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc882)
    %1945 = stablehlo.add %1940, %1944 : tensor<1x257x1280xbf16> loc(#loc882)
    %1946 = stablehlo.add %1913, %1945 : tensor<1x257x1280xbf16> loc(#loc883)
    %1947 = stablehlo.reshape %arg192 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1948 = stablehlo.custom_call @tt.mark_argument(%1947) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1949 = stablehlo.reshape %1948 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1950 = stablehlo.reshape %arg191 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1951 = stablehlo.custom_call @tt.mark_argument(%1950) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1952 = stablehlo.reshape %1951 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1953 = stablehlo.composite "tenstorrent.layer_norm" %1946, %1949, %1952 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_44} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc884)
    %1954 = stablehlo.reshape %1953 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc885)
    %1955 = stablehlo.reshape %arg459 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1956 = stablehlo.custom_call @tt.mark_argument(%1955) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1957 = stablehlo.reshape %1956 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1958 = stablehlo.transpose %1957, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc886)
    %1959 = stablehlo.dot_general %1954, %1958, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc887)
    %1960 = stablehlo.reshape %1959 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc885)
    %1961 = stablehlo.reshape %arg458 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1962 = stablehlo.custom_call @tt.mark_argument(%1961) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1963 = stablehlo.reshape %1962 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1964 = stablehlo.broadcast_in_dim %1963, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc888)
    %1965 = stablehlo.add %1960, %1964 : tensor<1x257x1280xbf16> loc(#loc888)
    %1966 = stablehlo.reshape %1965 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc889)
    %1967 = stablehlo.transpose %1966, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc890)
    %1968 = stablehlo.convert %1967 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc891)
    %1969 = stablehlo.multiply %1968, %cst_6 : tensor<1x16x257x80xf32> loc(#loc892)
    %1970 = stablehlo.reshape %arg457 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %1971 = stablehlo.custom_call @tt.mark_argument(%1970) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1972 = stablehlo.reshape %1971 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %1973 = stablehlo.transpose %1972, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc893)
    %1974 = stablehlo.dot_general %1954, %1973, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc894)
    %1975 = stablehlo.reshape %1974 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc895)
    %1976 = stablehlo.reshape %arg456 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %1977 = stablehlo.custom_call @tt.mark_argument(%1976) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1978 = stablehlo.reshape %1977 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %1979 = stablehlo.broadcast_in_dim %1978, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc896)
    %1980 = stablehlo.add %1975, %1979 : tensor<1x257x1280xbf16> loc(#loc896)
    %1981 = stablehlo.reshape %1980 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc897)
    %1982 = stablehlo.transpose %1981, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc898)
    %1983 = stablehlo.convert %1982 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc899)
    %1984 = stablehlo.transpose %1983, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc900)
    %1985 = stablehlo.multiply %1984, %cst_5 : tensor<1x16x80x257xf32> loc(#loc901)
    %1986 = stablehlo.dot_general %1969, %1985, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc902)
    %1987 = stablehlo.convert %1986 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc903)
    %1988 = stablehlo.compare  EQ, %1987, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc903)
    %1989 = stablehlo.not %1988 : tensor<1x16x257x257xi1> loc(#loc904)
    %1990 = stablehlo.reduce(%1989 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("1800|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_17aten__any"), %arg559: tensor<i1> loc("1800|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_17aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc906)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc907)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc905)
    %1991 = stablehlo.reshape %1990 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc905)
    %1992 = stablehlo.not %1991 : tensor<1x16x257x1xi1> loc(#loc908)
    %1993 = stablehlo.reshape %1992 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc909)
    %1994 = stablehlo.broadcast_in_dim %1993, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc909)
    %1995 = stablehlo.reduce(%1986 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc910)
    %1996 = stablehlo.broadcast_in_dim %1995, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc910)
    %1997 = stablehlo.subtract %1986, %1996 : tensor<1x16x257x257xf32> loc(#loc910)
    %1998 = stablehlo.exponential %1997 : tensor<1x16x257x257xf32> loc(#loc910)
    %1999 = stablehlo.reduce(%1998 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc910)
    %2000 = stablehlo.broadcast_in_dim %1999, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc910)
    %2001 = stablehlo.divide %1998, %2000 : tensor<1x16x257x257xf32> loc(#loc910)
    %2002 = stablehlo.select %1994, %cst_3, %2001 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc911)
    %2003 = stablehlo.reshape %arg190 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2004 = stablehlo.custom_call @tt.mark_argument(%2003) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2005 = stablehlo.reshape %2004 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2006 = stablehlo.transpose %2005, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc912)
    %2007 = stablehlo.dot_general %1954, %2006, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc913)
    %2008 = stablehlo.reshape %2007 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc914)
    %2009 = stablehlo.reshape %arg189 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2010 = stablehlo.custom_call @tt.mark_argument(%2009) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2011 = stablehlo.reshape %2010 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2012 = stablehlo.broadcast_in_dim %2011, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc915)
    %2013 = stablehlo.add %2008, %2012 : tensor<1x257x1280xbf16> loc(#loc915)
    %2014 = stablehlo.reshape %2013 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc916)
    %2015 = stablehlo.transpose %2014, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc917)
    %2016 = stablehlo.convert %2015 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc918)
    %2017 = stablehlo.dot_general %2002, %2016, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc919)
    %2018 = stablehlo.convert %2017 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc920)
    %2019 = stablehlo.transpose %2018, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc921)
    %2020 = stablehlo.reshape %2019 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc922)
    %2021 = stablehlo.reshape %arg188 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2022 = stablehlo.custom_call @tt.mark_argument(%2021) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2023 = stablehlo.reshape %2022 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2024 = stablehlo.transpose %2023, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc923)
    %2025 = stablehlo.dot_general %2020, %2024, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc924)
    %2026 = stablehlo.reshape %2025 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc922)
    %2027 = stablehlo.reshape %arg187 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2028 = stablehlo.custom_call @tt.mark_argument(%2027) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2029 = stablehlo.reshape %2028 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2030 = stablehlo.broadcast_in_dim %2029, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc925)
    %2031 = stablehlo.add %2026, %2030 : tensor<1x257x1280xbf16> loc(#loc925)
    %2032 = stablehlo.add %1946, %2031 : tensor<1x257x1280xbf16> loc(#loc926)
    %2033 = stablehlo.reshape %arg186 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2034 = stablehlo.custom_call @tt.mark_argument(%2033) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2035 = stablehlo.reshape %2034 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2036 = stablehlo.reshape %arg185 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2037 = stablehlo.custom_call @tt.mark_argument(%2036) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2038 = stablehlo.reshape %2037 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2039 = stablehlo.composite "tenstorrent.layer_norm" %2032, %2035, %2038 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_50} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc927)
    %2040 = stablehlo.reshape %2039 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc928)
    %2041 = stablehlo.reshape %arg184 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %2042 = stablehlo.custom_call @tt.mark_argument(%2041) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %2043 = stablehlo.reshape %2042 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %2044 = stablehlo.transpose %2043, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc929)
    %2045 = stablehlo.dot_general %2040, %2044, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc930)
    %2046 = stablehlo.reshape %2045 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc928)
    %2047 = stablehlo.reshape %arg183 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3)
    %2048 = stablehlo.custom_call @tt.mark_argument(%2047) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %2049 = stablehlo.reshape %2048 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3)
    %2050 = stablehlo.broadcast_in_dim %2049, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc931)
    %2051 = stablehlo.add %2046, %2050 : tensor<1x257x5120xbf16> loc(#loc931)
    %2052 = stablehlo.composite "tenstorrent.gelu" %2051 {decomposition = @tenstorrent.gelu.impl_32} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc932)
    %2053 = stablehlo.reshape %2052 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc933)
    %2054 = stablehlo.reshape %arg182 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %2055 = stablehlo.custom_call @tt.mark_argument(%2054) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %2056 = stablehlo.reshape %2055 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %2057 = stablehlo.transpose %2056, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc934)
    %2058 = stablehlo.dot_general %2053, %2057, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc935)
    %2059 = stablehlo.reshape %2058 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc933)
    %2060 = stablehlo.reshape %arg181 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2061 = stablehlo.custom_call @tt.mark_argument(%2060) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2062 = stablehlo.reshape %2061 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2063 = stablehlo.broadcast_in_dim %2062, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc936)
    %2064 = stablehlo.add %2059, %2063 : tensor<1x257x1280xbf16> loc(#loc936)
    %2065 = stablehlo.add %2032, %2064 : tensor<1x257x1280xbf16> loc(#loc937)
    %2066 = stablehlo.reshape %arg180 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2067 = stablehlo.custom_call @tt.mark_argument(%2066) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2068 = stablehlo.reshape %2067 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2069 = stablehlo.reshape %arg179 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2070 = stablehlo.custom_call @tt.mark_argument(%2069) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2071 = stablehlo.reshape %2070 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2072 = stablehlo.composite "tenstorrent.layer_norm" %2065, %2068, %2071 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_30} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc938)
    %2073 = stablehlo.reshape %2072 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc939)
    %2074 = stablehlo.reshape %arg463 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2075 = stablehlo.custom_call @tt.mark_argument(%2074) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2076 = stablehlo.reshape %2075 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2077 = stablehlo.transpose %2076, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc940)
    %2078 = stablehlo.dot_general %2073, %2077, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc941)
    %2079 = stablehlo.reshape %2078 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc939)
    %2080 = stablehlo.reshape %arg462 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2081 = stablehlo.custom_call @tt.mark_argument(%2080) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2082 = stablehlo.reshape %2081 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2083 = stablehlo.broadcast_in_dim %2082, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc942)
    %2084 = stablehlo.add %2079, %2083 : tensor<1x257x1280xbf16> loc(#loc942)
    %2085 = stablehlo.reshape %2084 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc943)
    %2086 = stablehlo.transpose %2085, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc944)
    %2087 = stablehlo.convert %2086 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc945)
    %2088 = stablehlo.multiply %2087, %cst_6 : tensor<1x16x257x80xf32> loc(#loc946)
    %2089 = stablehlo.reshape %arg461 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2090 = stablehlo.custom_call @tt.mark_argument(%2089) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2091 = stablehlo.reshape %2090 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2092 = stablehlo.transpose %2091, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc947)
    %2093 = stablehlo.dot_general %2073, %2092, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc948)
    %2094 = stablehlo.reshape %2093 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc949)
    %2095 = stablehlo.reshape %arg460 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2096 = stablehlo.custom_call @tt.mark_argument(%2095) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2097 = stablehlo.reshape %2096 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2098 = stablehlo.broadcast_in_dim %2097, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc950)
    %2099 = stablehlo.add %2094, %2098 : tensor<1x257x1280xbf16> loc(#loc950)
    %2100 = stablehlo.reshape %2099 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc951)
    %2101 = stablehlo.transpose %2100, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc952)
    %2102 = stablehlo.convert %2101 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc953)
    %2103 = stablehlo.transpose %2102, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc954)
    %2104 = stablehlo.multiply %2103, %cst_5 : tensor<1x16x80x257xf32> loc(#loc955)
    %2105 = stablehlo.dot_general %2088, %2104, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc956)
    %2106 = stablehlo.convert %2105 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc957)
    %2107 = stablehlo.compare  EQ, %2106, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc957)
    %2108 = stablehlo.not %2107 : tensor<1x16x257x257xi1> loc(#loc958)
    %2109 = stablehlo.reduce(%2108 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("1874|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_18aten__any"), %arg559: tensor<i1> loc("1874|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_18aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc960)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc961)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc959)
    %2110 = stablehlo.reshape %2109 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc959)
    %2111 = stablehlo.not %2110 : tensor<1x16x257x1xi1> loc(#loc962)
    %2112 = stablehlo.reshape %2111 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc963)
    %2113 = stablehlo.broadcast_in_dim %2112, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc963)
    %2114 = stablehlo.reduce(%2105 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc964)
    %2115 = stablehlo.broadcast_in_dim %2114, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc964)
    %2116 = stablehlo.subtract %2105, %2115 : tensor<1x16x257x257xf32> loc(#loc964)
    %2117 = stablehlo.exponential %2116 : tensor<1x16x257x257xf32> loc(#loc964)
    %2118 = stablehlo.reduce(%2117 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc964)
    %2119 = stablehlo.broadcast_in_dim %2118, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc964)
    %2120 = stablehlo.divide %2117, %2119 : tensor<1x16x257x257xf32> loc(#loc964)
    %2121 = stablehlo.select %2113, %cst_3, %2120 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc965)
    %2122 = stablehlo.reshape %arg178 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2123 = stablehlo.custom_call @tt.mark_argument(%2122) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2124 = stablehlo.reshape %2123 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2125 = stablehlo.transpose %2124, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc966)
    %2126 = stablehlo.dot_general %2073, %2125, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc967)
    %2127 = stablehlo.reshape %2126 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc968)
    %2128 = stablehlo.reshape %arg177 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2129 = stablehlo.custom_call @tt.mark_argument(%2128) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2130 = stablehlo.reshape %2129 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2131 = stablehlo.broadcast_in_dim %2130, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc969)
    %2132 = stablehlo.add %2127, %2131 : tensor<1x257x1280xbf16> loc(#loc969)
    %2133 = stablehlo.reshape %2132 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc970)
    %2134 = stablehlo.transpose %2133, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc971)
    %2135 = stablehlo.convert %2134 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc972)
    %2136 = stablehlo.dot_general %2121, %2135, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc973)
    %2137 = stablehlo.convert %2136 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc974)
    %2138 = stablehlo.transpose %2137, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc975)
    %2139 = stablehlo.reshape %2138 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc976)
    %2140 = stablehlo.reshape %arg176 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2141 = stablehlo.custom_call @tt.mark_argument(%2140) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2142 = stablehlo.reshape %2141 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2143 = stablehlo.transpose %2142, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc977)
    %2144 = stablehlo.dot_general %2139, %2143, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc978)
    %2145 = stablehlo.reshape %2144 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc976)
    %2146 = stablehlo.reshape %arg175 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2147 = stablehlo.custom_call @tt.mark_argument(%2146) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2148 = stablehlo.reshape %2147 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2149 = stablehlo.broadcast_in_dim %2148, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc979)
    %2150 = stablehlo.add %2145, %2149 : tensor<1x257x1280xbf16> loc(#loc979)
    %2151 = stablehlo.add %2065, %2150 : tensor<1x257x1280xbf16> loc(#loc980)
    %2152 = stablehlo.reshape %arg174 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2153 = stablehlo.custom_call @tt.mark_argument(%2152) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2154 = stablehlo.reshape %2153 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2155 = stablehlo.reshape %arg173 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2156 = stablehlo.custom_call @tt.mark_argument(%2155) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2157 = stablehlo.reshape %2156 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2158 = stablehlo.composite "tenstorrent.layer_norm" %2151, %2154, %2157 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_70} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc981)
    %2159 = stablehlo.reshape %2158 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc982)
    %2160 = stablehlo.reshape %arg172 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %2161 = stablehlo.custom_call @tt.mark_argument(%2160) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %2162 = stablehlo.reshape %2161 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %2163 = stablehlo.transpose %2162, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc983)
    %2164 = stablehlo.dot_general %2159, %2163, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc984)
    %2165 = stablehlo.reshape %2164 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc982)
    %2166 = stablehlo.reshape %arg171 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3)
    %2167 = stablehlo.custom_call @tt.mark_argument(%2166) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %2168 = stablehlo.reshape %2167 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3)
    %2169 = stablehlo.broadcast_in_dim %2168, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc985)
    %2170 = stablehlo.add %2165, %2169 : tensor<1x257x5120xbf16> loc(#loc985)
    %2171 = stablehlo.composite "tenstorrent.gelu" %2170 {decomposition = @tenstorrent.gelu.impl_33} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc986)
    %2172 = stablehlo.reshape %2171 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc987)
    %2173 = stablehlo.reshape %arg170 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %2174 = stablehlo.custom_call @tt.mark_argument(%2173) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %2175 = stablehlo.reshape %2174 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %2176 = stablehlo.transpose %2175, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc988)
    %2177 = stablehlo.dot_general %2172, %2176, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc989)
    %2178 = stablehlo.reshape %2177 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc987)
    %2179 = stablehlo.reshape %arg169 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2180 = stablehlo.custom_call @tt.mark_argument(%2179) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2181 = stablehlo.reshape %2180 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2182 = stablehlo.broadcast_in_dim %2181, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc990)
    %2183 = stablehlo.add %2178, %2182 : tensor<1x257x1280xbf16> loc(#loc990)
    %2184 = stablehlo.add %2151, %2183 : tensor<1x257x1280xbf16> loc(#loc991)
    %2185 = stablehlo.reshape %arg168 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2186 = stablehlo.custom_call @tt.mark_argument(%2185) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2187 = stablehlo.reshape %2186 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2188 = stablehlo.reshape %arg167 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2189 = stablehlo.custom_call @tt.mark_argument(%2188) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2190 = stablehlo.reshape %2189 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2191 = stablehlo.composite "tenstorrent.layer_norm" %2184, %2187, %2190 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_74} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc992)
    %2192 = stablehlo.reshape %2191 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc993)
    %2193 = stablehlo.reshape %arg467 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2194 = stablehlo.custom_call @tt.mark_argument(%2193) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2195 = stablehlo.reshape %2194 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2196 = stablehlo.transpose %2195, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc994)
    %2197 = stablehlo.dot_general %2192, %2196, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc995)
    %2198 = stablehlo.reshape %2197 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc993)
    %2199 = stablehlo.reshape %arg466 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2200 = stablehlo.custom_call @tt.mark_argument(%2199) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2201 = stablehlo.reshape %2200 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2202 = stablehlo.broadcast_in_dim %2201, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc996)
    %2203 = stablehlo.add %2198, %2202 : tensor<1x257x1280xbf16> loc(#loc996)
    %2204 = stablehlo.reshape %2203 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc997)
    %2205 = stablehlo.transpose %2204, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc998)
    %2206 = stablehlo.convert %2205 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc999)
    %2207 = stablehlo.multiply %2206, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1000)
    %2208 = stablehlo.reshape %arg465 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2209 = stablehlo.custom_call @tt.mark_argument(%2208) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2210 = stablehlo.reshape %2209 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2211 = stablehlo.transpose %2210, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1001)
    %2212 = stablehlo.dot_general %2192, %2211, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1002)
    %2213 = stablehlo.reshape %2212 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1003)
    %2214 = stablehlo.reshape %arg464 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2215 = stablehlo.custom_call @tt.mark_argument(%2214) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2216 = stablehlo.reshape %2215 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2217 = stablehlo.broadcast_in_dim %2216, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1004)
    %2218 = stablehlo.add %2213, %2217 : tensor<1x257x1280xbf16> loc(#loc1004)
    %2219 = stablehlo.reshape %2218 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1005)
    %2220 = stablehlo.transpose %2219, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1006)
    %2221 = stablehlo.convert %2220 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1007)
    %2222 = stablehlo.transpose %2221, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1008)
    %2223 = stablehlo.multiply %2222, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1009)
    %2224 = stablehlo.dot_general %2207, %2223, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1010)
    %2225 = stablehlo.convert %2224 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1011)
    %2226 = stablehlo.compare  EQ, %2225, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1011)
    %2227 = stablehlo.not %2226 : tensor<1x16x257x257xi1> loc(#loc1012)
    %2228 = stablehlo.reduce(%2227 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("1948|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_19aten__any"), %arg559: tensor<i1> loc("1948|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_19aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1014)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1015)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc1013)
    %2229 = stablehlo.reshape %2228 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1013)
    %2230 = stablehlo.not %2229 : tensor<1x16x257x1xi1> loc(#loc1016)
    %2231 = stablehlo.reshape %2230 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1017)
    %2232 = stablehlo.broadcast_in_dim %2231, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1017)
    %2233 = stablehlo.reduce(%2224 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1018)
    %2234 = stablehlo.broadcast_in_dim %2233, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1018)
    %2235 = stablehlo.subtract %2224, %2234 : tensor<1x16x257x257xf32> loc(#loc1018)
    %2236 = stablehlo.exponential %2235 : tensor<1x16x257x257xf32> loc(#loc1018)
    %2237 = stablehlo.reduce(%2236 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1018)
    %2238 = stablehlo.broadcast_in_dim %2237, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1018)
    %2239 = stablehlo.divide %2236, %2238 : tensor<1x16x257x257xf32> loc(#loc1018)
    %2240 = stablehlo.select %2232, %cst_3, %2239 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1019)
    %2241 = stablehlo.reshape %arg166 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2242 = stablehlo.custom_call @tt.mark_argument(%2241) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2243 = stablehlo.reshape %2242 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2244 = stablehlo.transpose %2243, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1020)
    %2245 = stablehlo.dot_general %2192, %2244, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1021)
    %2246 = stablehlo.reshape %2245 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1022)
    %2247 = stablehlo.reshape %arg165 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2248 = stablehlo.custom_call @tt.mark_argument(%2247) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2249 = stablehlo.reshape %2248 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2250 = stablehlo.broadcast_in_dim %2249, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1023)
    %2251 = stablehlo.add %2246, %2250 : tensor<1x257x1280xbf16> loc(#loc1023)
    %2252 = stablehlo.reshape %2251 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1024)
    %2253 = stablehlo.transpose %2252, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1025)
    %2254 = stablehlo.convert %2253 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1026)
    %2255 = stablehlo.dot_general %2240, %2254, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1027)
    %2256 = stablehlo.convert %2255 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1028)
    %2257 = stablehlo.transpose %2256, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1029)
    %2258 = stablehlo.reshape %2257 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1030)
    %2259 = stablehlo.reshape %arg164 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2260 = stablehlo.custom_call @tt.mark_argument(%2259) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2261 = stablehlo.reshape %2260 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2262 = stablehlo.transpose %2261, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1031)
    %2263 = stablehlo.dot_general %2258, %2262, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1032)
    %2264 = stablehlo.reshape %2263 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1030)
    %2265 = stablehlo.reshape %arg163 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2266 = stablehlo.custom_call @tt.mark_argument(%2265) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2267 = stablehlo.reshape %2266 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2268 = stablehlo.broadcast_in_dim %2267, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1033)
    %2269 = stablehlo.add %2264, %2268 : tensor<1x257x1280xbf16> loc(#loc1033)
    %2270 = stablehlo.add %2184, %2269 : tensor<1x257x1280xbf16> loc(#loc1034)
    %2271 = stablehlo.reshape %arg162 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2272 = stablehlo.custom_call @tt.mark_argument(%2271) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2273 = stablehlo.reshape %2272 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2274 = stablehlo.reshape %arg161 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2275 = stablehlo.custom_call @tt.mark_argument(%2274) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2276 = stablehlo.reshape %2275 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2277 = stablehlo.composite "tenstorrent.layer_norm" %2270, %2273, %2276 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_61} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1035)
    %2278 = stablehlo.reshape %2277 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1036)
    %2279 = stablehlo.reshape %arg160 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %2280 = stablehlo.custom_call @tt.mark_argument(%2279) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %2281 = stablehlo.reshape %2280 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %2282 = stablehlo.transpose %2281, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1037)
    %2283 = stablehlo.dot_general %2278, %2282, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1038)
    %2284 = stablehlo.reshape %2283 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1036)
    %2285 = stablehlo.reshape %arg159 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3)
    %2286 = stablehlo.custom_call @tt.mark_argument(%2285) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %2287 = stablehlo.reshape %2286 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3)
    %2288 = stablehlo.broadcast_in_dim %2287, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1039)
    %2289 = stablehlo.add %2284, %2288 : tensor<1x257x5120xbf16> loc(#loc1039)
    %2290 = stablehlo.composite "tenstorrent.gelu" %2289 {decomposition = @tenstorrent.gelu.impl_23} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1040)
    %2291 = stablehlo.reshape %2290 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1041)
    %2292 = stablehlo.reshape %arg158 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %2293 = stablehlo.custom_call @tt.mark_argument(%2292) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %2294 = stablehlo.reshape %2293 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %2295 = stablehlo.transpose %2294, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1042)
    %2296 = stablehlo.dot_general %2291, %2295, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1043)
    %2297 = stablehlo.reshape %2296 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1041)
    %2298 = stablehlo.reshape %arg157 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2299 = stablehlo.custom_call @tt.mark_argument(%2298) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2300 = stablehlo.reshape %2299 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2301 = stablehlo.broadcast_in_dim %2300, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1044)
    %2302 = stablehlo.add %2297, %2301 : tensor<1x257x1280xbf16> loc(#loc1044)
    %2303 = stablehlo.add %2270, %2302 : tensor<1x257x1280xbf16> loc(#loc1045)
    %2304 = stablehlo.reshape %arg156 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2305 = stablehlo.custom_call @tt.mark_argument(%2304) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2306 = stablehlo.reshape %2305 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2307 = stablehlo.reshape %arg155 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2308 = stablehlo.custom_call @tt.mark_argument(%2307) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2309 = stablehlo.reshape %2308 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2310 = stablehlo.composite "tenstorrent.layer_norm" %2303, %2306, %2309 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_20} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1046)
    %2311 = stablehlo.reshape %2310 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1047)
    %2312 = stablehlo.reshape %arg471 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2313 = stablehlo.custom_call @tt.mark_argument(%2312) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2314 = stablehlo.reshape %2313 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2315 = stablehlo.transpose %2314, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1048)
    %2316 = stablehlo.dot_general %2311, %2315, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1049)
    %2317 = stablehlo.reshape %2316 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1047)
    %2318 = stablehlo.reshape %arg470 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2319 = stablehlo.custom_call @tt.mark_argument(%2318) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2320 = stablehlo.reshape %2319 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2321 = stablehlo.broadcast_in_dim %2320, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1050)
    %2322 = stablehlo.add %2317, %2321 : tensor<1x257x1280xbf16> loc(#loc1050)
    %2323 = stablehlo.reshape %2322 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1051)
    %2324 = stablehlo.transpose %2323, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1052)
    %2325 = stablehlo.convert %2324 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1053)
    %2326 = stablehlo.multiply %2325, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1054)
    %2327 = stablehlo.reshape %arg469 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2328 = stablehlo.custom_call @tt.mark_argument(%2327) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2329 = stablehlo.reshape %2328 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2330 = stablehlo.transpose %2329, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1055)
    %2331 = stablehlo.dot_general %2311, %2330, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1056)
    %2332 = stablehlo.reshape %2331 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1057)
    %2333 = stablehlo.reshape %arg468 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2334 = stablehlo.custom_call @tt.mark_argument(%2333) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2335 = stablehlo.reshape %2334 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2336 = stablehlo.broadcast_in_dim %2335, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1058)
    %2337 = stablehlo.add %2332, %2336 : tensor<1x257x1280xbf16> loc(#loc1058)
    %2338 = stablehlo.reshape %2337 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1059)
    %2339 = stablehlo.transpose %2338, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1060)
    %2340 = stablehlo.convert %2339 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1061)
    %2341 = stablehlo.transpose %2340, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1062)
    %2342 = stablehlo.multiply %2341, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1063)
    %2343 = stablehlo.dot_general %2326, %2342, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1064)
    %2344 = stablehlo.convert %2343 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1065)
    %2345 = stablehlo.compare  EQ, %2344, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1065)
    %2346 = stablehlo.not %2345 : tensor<1x16x257x257xi1> loc(#loc1066)
    %2347 = stablehlo.reduce(%2346 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("2022|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_20aten__any"), %arg559: tensor<i1> loc("2022|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_20aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1068)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1069)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc1067)
    %2348 = stablehlo.reshape %2347 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1067)
    %2349 = stablehlo.not %2348 : tensor<1x16x257x1xi1> loc(#loc1070)
    %2350 = stablehlo.reshape %2349 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1071)
    %2351 = stablehlo.broadcast_in_dim %2350, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1071)
    %2352 = stablehlo.reduce(%2343 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1072)
    %2353 = stablehlo.broadcast_in_dim %2352, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1072)
    %2354 = stablehlo.subtract %2343, %2353 : tensor<1x16x257x257xf32> loc(#loc1072)
    %2355 = stablehlo.exponential %2354 : tensor<1x16x257x257xf32> loc(#loc1072)
    %2356 = stablehlo.reduce(%2355 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1072)
    %2357 = stablehlo.broadcast_in_dim %2356, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1072)
    %2358 = stablehlo.divide %2355, %2357 : tensor<1x16x257x257xf32> loc(#loc1072)
    %2359 = stablehlo.select %2351, %cst_3, %2358 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1073)
    %2360 = stablehlo.reshape %arg154 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2361 = stablehlo.custom_call @tt.mark_argument(%2360) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2362 = stablehlo.reshape %2361 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2363 = stablehlo.transpose %2362, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1074)
    %2364 = stablehlo.dot_general %2311, %2363, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1075)
    %2365 = stablehlo.reshape %2364 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1076)
    %2366 = stablehlo.reshape %arg153 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2367 = stablehlo.custom_call @tt.mark_argument(%2366) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2368 = stablehlo.reshape %2367 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2369 = stablehlo.broadcast_in_dim %2368, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1077)
    %2370 = stablehlo.add %2365, %2369 : tensor<1x257x1280xbf16> loc(#loc1077)
    %2371 = stablehlo.reshape %2370 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1078)
    %2372 = stablehlo.transpose %2371, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1079)
    %2373 = stablehlo.convert %2372 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1080)
    %2374 = stablehlo.dot_general %2359, %2373, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1081)
    %2375 = stablehlo.convert %2374 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1082)
    %2376 = stablehlo.transpose %2375, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1083)
    %2377 = stablehlo.reshape %2376 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1084)
    %2378 = stablehlo.reshape %arg152 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2379 = stablehlo.custom_call @tt.mark_argument(%2378) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2380 = stablehlo.reshape %2379 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2381 = stablehlo.transpose %2380, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1085)
    %2382 = stablehlo.dot_general %2377, %2381, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1086)
    %2383 = stablehlo.reshape %2382 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1084)
    %2384 = stablehlo.reshape %arg151 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2385 = stablehlo.custom_call @tt.mark_argument(%2384) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2386 = stablehlo.reshape %2385 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2387 = stablehlo.broadcast_in_dim %2386, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1087)
    %2388 = stablehlo.add %2383, %2387 : tensor<1x257x1280xbf16> loc(#loc1087)
    %2389 = stablehlo.add %2303, %2388 : tensor<1x257x1280xbf16> loc(#loc1088)
    %2390 = stablehlo.reshape %arg150 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2391 = stablehlo.custom_call @tt.mark_argument(%2390) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2392 = stablehlo.reshape %2391 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2393 = stablehlo.reshape %arg149 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2394 = stablehlo.custom_call @tt.mark_argument(%2393) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2395 = stablehlo.reshape %2394 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2396 = stablehlo.composite "tenstorrent.layer_norm" %2389, %2392, %2395 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_51} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1089)
    %2397 = stablehlo.reshape %2396 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1090)
    %2398 = stablehlo.reshape %arg148 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %2399 = stablehlo.custom_call @tt.mark_argument(%2398) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %2400 = stablehlo.reshape %2399 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %2401 = stablehlo.transpose %2400, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1091)
    %2402 = stablehlo.dot_general %2397, %2401, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1092)
    %2403 = stablehlo.reshape %2402 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1090)
    %2404 = stablehlo.reshape %arg147 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3)
    %2405 = stablehlo.custom_call @tt.mark_argument(%2404) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %2406 = stablehlo.reshape %2405 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3)
    %2407 = stablehlo.broadcast_in_dim %2406, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1093)
    %2408 = stablehlo.add %2403, %2407 : tensor<1x257x5120xbf16> loc(#loc1093)
    %2409 = stablehlo.composite "tenstorrent.gelu" %2408 {decomposition = @tenstorrent.gelu.impl_10} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1094)
    %2410 = stablehlo.reshape %2409 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1095)
    %2411 = stablehlo.reshape %arg146 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %2412 = stablehlo.custom_call @tt.mark_argument(%2411) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %2413 = stablehlo.reshape %2412 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %2414 = stablehlo.transpose %2413, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1096)
    %2415 = stablehlo.dot_general %2410, %2414, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1097)
    %2416 = stablehlo.reshape %2415 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1095)
    %2417 = stablehlo.reshape %arg145 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2418 = stablehlo.custom_call @tt.mark_argument(%2417) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2419 = stablehlo.reshape %2418 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2420 = stablehlo.broadcast_in_dim %2419, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1098)
    %2421 = stablehlo.add %2416, %2420 : tensor<1x257x1280xbf16> loc(#loc1098)
    %2422 = stablehlo.add %2389, %2421 : tensor<1x257x1280xbf16> loc(#loc1099)
    %2423 = stablehlo.reshape %arg144 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2424 = stablehlo.custom_call @tt.mark_argument(%2423) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2425 = stablehlo.reshape %2424 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2426 = stablehlo.reshape %arg143 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2427 = stablehlo.custom_call @tt.mark_argument(%2426) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2428 = stablehlo.reshape %2427 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2429 = stablehlo.composite "tenstorrent.layer_norm" %2422, %2425, %2428 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_19} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1100)
    %2430 = stablehlo.reshape %2429 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1101)
    %2431 = stablehlo.reshape %arg475 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2432 = stablehlo.custom_call @tt.mark_argument(%2431) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2433 = stablehlo.reshape %2432 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2434 = stablehlo.transpose %2433, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1102)
    %2435 = stablehlo.dot_general %2430, %2434, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1103)
    %2436 = stablehlo.reshape %2435 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1101)
    %2437 = stablehlo.reshape %arg474 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2438 = stablehlo.custom_call @tt.mark_argument(%2437) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2439 = stablehlo.reshape %2438 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2440 = stablehlo.broadcast_in_dim %2439, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1104)
    %2441 = stablehlo.add %2436, %2440 : tensor<1x257x1280xbf16> loc(#loc1104)
    %2442 = stablehlo.reshape %2441 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1105)
    %2443 = stablehlo.transpose %2442, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1106)
    %2444 = stablehlo.convert %2443 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1107)
    %2445 = stablehlo.multiply %2444, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1108)
    %2446 = stablehlo.reshape %arg473 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2447 = stablehlo.custom_call @tt.mark_argument(%2446) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2448 = stablehlo.reshape %2447 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2449 = stablehlo.transpose %2448, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1109)
    %2450 = stablehlo.dot_general %2430, %2449, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1110)
    %2451 = stablehlo.reshape %2450 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1111)
    %2452 = stablehlo.reshape %arg472 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2453 = stablehlo.custom_call @tt.mark_argument(%2452) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2454 = stablehlo.reshape %2453 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2455 = stablehlo.broadcast_in_dim %2454, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1112)
    %2456 = stablehlo.add %2451, %2455 : tensor<1x257x1280xbf16> loc(#loc1112)
    %2457 = stablehlo.reshape %2456 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1113)
    %2458 = stablehlo.transpose %2457, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1114)
    %2459 = stablehlo.convert %2458 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1115)
    %2460 = stablehlo.transpose %2459, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1116)
    %2461 = stablehlo.multiply %2460, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1117)
    %2462 = stablehlo.dot_general %2445, %2461, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1118)
    %2463 = stablehlo.convert %2462 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1119)
    %2464 = stablehlo.compare  EQ, %2463, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1119)
    %2465 = stablehlo.not %2464 : tensor<1x16x257x257xi1> loc(#loc1120)
    %2466 = stablehlo.reduce(%2465 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("2096|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_21aten__any"), %arg559: tensor<i1> loc("2096|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_21aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1122)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1123)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc1121)
    %2467 = stablehlo.reshape %2466 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1121)
    %2468 = stablehlo.not %2467 : tensor<1x16x257x1xi1> loc(#loc1124)
    %2469 = stablehlo.reshape %2468 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1125)
    %2470 = stablehlo.broadcast_in_dim %2469, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1125)
    %2471 = stablehlo.reduce(%2462 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1126)
    %2472 = stablehlo.broadcast_in_dim %2471, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1126)
    %2473 = stablehlo.subtract %2462, %2472 : tensor<1x16x257x257xf32> loc(#loc1126)
    %2474 = stablehlo.exponential %2473 : tensor<1x16x257x257xf32> loc(#loc1126)
    %2475 = stablehlo.reduce(%2474 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1126)
    %2476 = stablehlo.broadcast_in_dim %2475, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1126)
    %2477 = stablehlo.divide %2474, %2476 : tensor<1x16x257x257xf32> loc(#loc1126)
    %2478 = stablehlo.select %2470, %cst_3, %2477 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1127)
    %2479 = stablehlo.reshape %arg142 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2480 = stablehlo.custom_call @tt.mark_argument(%2479) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2481 = stablehlo.reshape %2480 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2482 = stablehlo.transpose %2481, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1128)
    %2483 = stablehlo.dot_general %2430, %2482, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1129)
    %2484 = stablehlo.reshape %2483 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1130)
    %2485 = stablehlo.reshape %arg141 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2486 = stablehlo.custom_call @tt.mark_argument(%2485) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2487 = stablehlo.reshape %2486 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2488 = stablehlo.broadcast_in_dim %2487, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1131)
    %2489 = stablehlo.add %2484, %2488 : tensor<1x257x1280xbf16> loc(#loc1131)
    %2490 = stablehlo.reshape %2489 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1132)
    %2491 = stablehlo.transpose %2490, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1133)
    %2492 = stablehlo.convert %2491 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1134)
    %2493 = stablehlo.dot_general %2478, %2492, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1135)
    %2494 = stablehlo.convert %2493 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1136)
    %2495 = stablehlo.transpose %2494, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1137)
    %2496 = stablehlo.reshape %2495 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1138)
    %2497 = stablehlo.reshape %arg140 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2498 = stablehlo.custom_call @tt.mark_argument(%2497) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2499 = stablehlo.reshape %2498 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2500 = stablehlo.transpose %2499, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1139)
    %2501 = stablehlo.dot_general %2496, %2500, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1140)
    %2502 = stablehlo.reshape %2501 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1138)
    %2503 = stablehlo.reshape %arg139 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2504 = stablehlo.custom_call @tt.mark_argument(%2503) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2505 = stablehlo.reshape %2504 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2506 = stablehlo.broadcast_in_dim %2505, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1141)
    %2507 = stablehlo.add %2502, %2506 : tensor<1x257x1280xbf16> loc(#loc1141)
    %2508 = stablehlo.add %2422, %2507 : tensor<1x257x1280xbf16> loc(#loc1142)
    %2509 = stablehlo.reshape %arg138 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2510 = stablehlo.custom_call @tt.mark_argument(%2509) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2511 = stablehlo.reshape %2510 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2512 = stablehlo.reshape %arg137 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2513 = stablehlo.custom_call @tt.mark_argument(%2512) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2514 = stablehlo.reshape %2513 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2515 = stablehlo.composite "tenstorrent.layer_norm" %2508, %2511, %2514 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_47} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1143)
    %2516 = stablehlo.reshape %2515 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1144)
    %2517 = stablehlo.reshape %arg136 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %2518 = stablehlo.custom_call @tt.mark_argument(%2517) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %2519 = stablehlo.reshape %2518 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %2520 = stablehlo.transpose %2519, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1145)
    %2521 = stablehlo.dot_general %2516, %2520, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1146)
    %2522 = stablehlo.reshape %2521 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1144)
    %2523 = stablehlo.reshape %arg135 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3)
    %2524 = stablehlo.custom_call @tt.mark_argument(%2523) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %2525 = stablehlo.reshape %2524 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3)
    %2526 = stablehlo.broadcast_in_dim %2525, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1147)
    %2527 = stablehlo.add %2522, %2526 : tensor<1x257x5120xbf16> loc(#loc1147)
    %2528 = stablehlo.composite "tenstorrent.gelu" %2527 {decomposition = @tenstorrent.gelu.impl_8} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1148)
    %2529 = stablehlo.reshape %2528 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1149)
    %2530 = stablehlo.reshape %arg134 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %2531 = stablehlo.custom_call @tt.mark_argument(%2530) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %2532 = stablehlo.reshape %2531 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %2533 = stablehlo.transpose %2532, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1150)
    %2534 = stablehlo.dot_general %2529, %2533, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1151)
    %2535 = stablehlo.reshape %2534 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1149)
    %2536 = stablehlo.reshape %arg133 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2537 = stablehlo.custom_call @tt.mark_argument(%2536) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2538 = stablehlo.reshape %2537 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2539 = stablehlo.broadcast_in_dim %2538, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1152)
    %2540 = stablehlo.add %2535, %2539 : tensor<1x257x1280xbf16> loc(#loc1152)
    %2541 = stablehlo.add %2508, %2540 : tensor<1x257x1280xbf16> loc(#loc1153)
    %2542 = stablehlo.reshape %arg132 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2543 = stablehlo.custom_call @tt.mark_argument(%2542) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2544 = stablehlo.reshape %2543 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2545 = stablehlo.reshape %arg131 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2546 = stablehlo.custom_call @tt.mark_argument(%2545) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2547 = stablehlo.reshape %2546 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2548 = stablehlo.composite "tenstorrent.layer_norm" %2541, %2544, %2547 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_55} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1154)
    %2549 = stablehlo.reshape %2548 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1155)
    %2550 = stablehlo.reshape %arg479 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2551 = stablehlo.custom_call @tt.mark_argument(%2550) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2552 = stablehlo.reshape %2551 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2553 = stablehlo.transpose %2552, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1156)
    %2554 = stablehlo.dot_general %2549, %2553, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1157)
    %2555 = stablehlo.reshape %2554 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1155)
    %2556 = stablehlo.reshape %arg478 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2557 = stablehlo.custom_call @tt.mark_argument(%2556) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2558 = stablehlo.reshape %2557 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2559 = stablehlo.broadcast_in_dim %2558, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1158)
    %2560 = stablehlo.add %2555, %2559 : tensor<1x257x1280xbf16> loc(#loc1158)
    %2561 = stablehlo.reshape %2560 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1159)
    %2562 = stablehlo.transpose %2561, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1160)
    %2563 = stablehlo.convert %2562 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1161)
    %2564 = stablehlo.multiply %2563, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1162)
    %2565 = stablehlo.reshape %arg477 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2566 = stablehlo.custom_call @tt.mark_argument(%2565) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2567 = stablehlo.reshape %2566 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2568 = stablehlo.transpose %2567, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1163)
    %2569 = stablehlo.dot_general %2549, %2568, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1164)
    %2570 = stablehlo.reshape %2569 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1165)
    %2571 = stablehlo.reshape %arg476 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2572 = stablehlo.custom_call @tt.mark_argument(%2571) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2573 = stablehlo.reshape %2572 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2574 = stablehlo.broadcast_in_dim %2573, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1166)
    %2575 = stablehlo.add %2570, %2574 : tensor<1x257x1280xbf16> loc(#loc1166)
    %2576 = stablehlo.reshape %2575 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1167)
    %2577 = stablehlo.transpose %2576, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1168)
    %2578 = stablehlo.convert %2577 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1169)
    %2579 = stablehlo.transpose %2578, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1170)
    %2580 = stablehlo.multiply %2579, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1171)
    %2581 = stablehlo.dot_general %2564, %2580, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1172)
    %2582 = stablehlo.convert %2581 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1173)
    %2583 = stablehlo.compare  EQ, %2582, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1173)
    %2584 = stablehlo.not %2583 : tensor<1x16x257x257xi1> loc(#loc1174)
    %2585 = stablehlo.reduce(%2584 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("2170|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_22aten__any"), %arg559: tensor<i1> loc("2170|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_22aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1176)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1177)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc1175)
    %2586 = stablehlo.reshape %2585 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1175)
    %2587 = stablehlo.not %2586 : tensor<1x16x257x1xi1> loc(#loc1178)
    %2588 = stablehlo.reshape %2587 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1179)
    %2589 = stablehlo.broadcast_in_dim %2588, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1179)
    %2590 = stablehlo.reduce(%2581 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1180)
    %2591 = stablehlo.broadcast_in_dim %2590, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1180)
    %2592 = stablehlo.subtract %2581, %2591 : tensor<1x16x257x257xf32> loc(#loc1180)
    %2593 = stablehlo.exponential %2592 : tensor<1x16x257x257xf32> loc(#loc1180)
    %2594 = stablehlo.reduce(%2593 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1180)
    %2595 = stablehlo.broadcast_in_dim %2594, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1180)
    %2596 = stablehlo.divide %2593, %2595 : tensor<1x16x257x257xf32> loc(#loc1180)
    %2597 = stablehlo.select %2589, %cst_3, %2596 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1181)
    %2598 = stablehlo.reshape %arg130 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2599 = stablehlo.custom_call @tt.mark_argument(%2598) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2600 = stablehlo.reshape %2599 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2601 = stablehlo.transpose %2600, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1182)
    %2602 = stablehlo.dot_general %2549, %2601, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1183)
    %2603 = stablehlo.reshape %2602 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1184)
    %2604 = stablehlo.reshape %arg129 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2605 = stablehlo.custom_call @tt.mark_argument(%2604) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2606 = stablehlo.reshape %2605 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2607 = stablehlo.broadcast_in_dim %2606, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1185)
    %2608 = stablehlo.add %2603, %2607 : tensor<1x257x1280xbf16> loc(#loc1185)
    %2609 = stablehlo.reshape %2608 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1186)
    %2610 = stablehlo.transpose %2609, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1187)
    %2611 = stablehlo.convert %2610 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1188)
    %2612 = stablehlo.dot_general %2597, %2611, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1189)
    %2613 = stablehlo.convert %2612 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1190)
    %2614 = stablehlo.transpose %2613, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1191)
    %2615 = stablehlo.reshape %2614 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1192)
    %2616 = stablehlo.reshape %arg128 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2617 = stablehlo.custom_call @tt.mark_argument(%2616) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2618 = stablehlo.reshape %2617 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2619 = stablehlo.transpose %2618, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1193)
    %2620 = stablehlo.dot_general %2615, %2619, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1194)
    %2621 = stablehlo.reshape %2620 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1192)
    %2622 = stablehlo.reshape %arg127 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2623 = stablehlo.custom_call @tt.mark_argument(%2622) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2624 = stablehlo.reshape %2623 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2625 = stablehlo.broadcast_in_dim %2624, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1195)
    %2626 = stablehlo.add %2621, %2625 : tensor<1x257x1280xbf16> loc(#loc1195)
    %2627 = stablehlo.add %2541, %2626 : tensor<1x257x1280xbf16> loc(#loc1196)
    %2628 = stablehlo.reshape %arg126 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2629 = stablehlo.custom_call @tt.mark_argument(%2628) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2630 = stablehlo.reshape %2629 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2631 = stablehlo.reshape %arg125 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2632 = stablehlo.custom_call @tt.mark_argument(%2631) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2633 = stablehlo.reshape %2632 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2634 = stablehlo.composite "tenstorrent.layer_norm" %2627, %2630, %2633 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_18} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1197)
    %2635 = stablehlo.reshape %2634 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1198)
    %2636 = stablehlo.reshape %arg124 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %2637 = stablehlo.custom_call @tt.mark_argument(%2636) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %2638 = stablehlo.reshape %2637 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %2639 = stablehlo.transpose %2638, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1199)
    %2640 = stablehlo.dot_general %2635, %2639, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1200)
    %2641 = stablehlo.reshape %2640 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1198)
    %2642 = stablehlo.reshape %arg123 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3)
    %2643 = stablehlo.custom_call @tt.mark_argument(%2642) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %2644 = stablehlo.reshape %2643 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3)
    %2645 = stablehlo.broadcast_in_dim %2644, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1201)
    %2646 = stablehlo.add %2641, %2645 : tensor<1x257x5120xbf16> loc(#loc1201)
    %2647 = stablehlo.composite "tenstorrent.gelu" %2646 {decomposition = @tenstorrent.gelu.impl_7} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1202)
    %2648 = stablehlo.reshape %2647 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1203)
    %2649 = stablehlo.reshape %arg122 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %2650 = stablehlo.custom_call @tt.mark_argument(%2649) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %2651 = stablehlo.reshape %2650 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %2652 = stablehlo.transpose %2651, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1204)
    %2653 = stablehlo.dot_general %2648, %2652, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1205)
    %2654 = stablehlo.reshape %2653 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1203)
    %2655 = stablehlo.reshape %arg121 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2656 = stablehlo.custom_call @tt.mark_argument(%2655) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2657 = stablehlo.reshape %2656 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2658 = stablehlo.broadcast_in_dim %2657, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1206)
    %2659 = stablehlo.add %2654, %2658 : tensor<1x257x1280xbf16> loc(#loc1206)
    %2660 = stablehlo.add %2627, %2659 : tensor<1x257x1280xbf16> loc(#loc1207)
    %2661 = stablehlo.reshape %arg120 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2662 = stablehlo.custom_call @tt.mark_argument(%2661) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2663 = stablehlo.reshape %2662 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2664 = stablehlo.reshape %arg119 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2665 = stablehlo.custom_call @tt.mark_argument(%2664) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2666 = stablehlo.reshape %2665 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2667 = stablehlo.composite "tenstorrent.layer_norm" %2660, %2663, %2666 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_16} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1208)
    %2668 = stablehlo.reshape %2667 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1209)
    %2669 = stablehlo.reshape %arg483 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2670 = stablehlo.custom_call @tt.mark_argument(%2669) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2671 = stablehlo.reshape %2670 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2672 = stablehlo.transpose %2671, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1210)
    %2673 = stablehlo.dot_general %2668, %2672, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1211)
    %2674 = stablehlo.reshape %2673 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1209)
    %2675 = stablehlo.reshape %arg482 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2676 = stablehlo.custom_call @tt.mark_argument(%2675) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2677 = stablehlo.reshape %2676 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2678 = stablehlo.broadcast_in_dim %2677, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1212)
    %2679 = stablehlo.add %2674, %2678 : tensor<1x257x1280xbf16> loc(#loc1212)
    %2680 = stablehlo.reshape %2679 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1213)
    %2681 = stablehlo.transpose %2680, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1214)
    %2682 = stablehlo.convert %2681 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1215)
    %2683 = stablehlo.multiply %2682, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1216)
    %2684 = stablehlo.reshape %arg481 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2685 = stablehlo.custom_call @tt.mark_argument(%2684) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2686 = stablehlo.reshape %2685 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2687 = stablehlo.transpose %2686, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1217)
    %2688 = stablehlo.dot_general %2668, %2687, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1218)
    %2689 = stablehlo.reshape %2688 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1219)
    %2690 = stablehlo.reshape %arg480 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2691 = stablehlo.custom_call @tt.mark_argument(%2690) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2692 = stablehlo.reshape %2691 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2693 = stablehlo.broadcast_in_dim %2692, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1220)
    %2694 = stablehlo.add %2689, %2693 : tensor<1x257x1280xbf16> loc(#loc1220)
    %2695 = stablehlo.reshape %2694 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1221)
    %2696 = stablehlo.transpose %2695, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1222)
    %2697 = stablehlo.convert %2696 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1223)
    %2698 = stablehlo.transpose %2697, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1224)
    %2699 = stablehlo.multiply %2698, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1225)
    %2700 = stablehlo.dot_general %2683, %2699, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1226)
    %2701 = stablehlo.convert %2700 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1227)
    %2702 = stablehlo.compare  EQ, %2701, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1227)
    %2703 = stablehlo.not %2702 : tensor<1x16x257x257xi1> loc(#loc1228)
    %2704 = stablehlo.reduce(%2703 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("2244|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_23aten__any"), %arg559: tensor<i1> loc("2244|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_23aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1230)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1231)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc1229)
    %2705 = stablehlo.reshape %2704 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1229)
    %2706 = stablehlo.not %2705 : tensor<1x16x257x1xi1> loc(#loc1232)
    %2707 = stablehlo.reshape %2706 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1233)
    %2708 = stablehlo.broadcast_in_dim %2707, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1233)
    %2709 = stablehlo.reduce(%2700 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1234)
    %2710 = stablehlo.broadcast_in_dim %2709, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1234)
    %2711 = stablehlo.subtract %2700, %2710 : tensor<1x16x257x257xf32> loc(#loc1234)
    %2712 = stablehlo.exponential %2711 : tensor<1x16x257x257xf32> loc(#loc1234)
    %2713 = stablehlo.reduce(%2712 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1234)
    %2714 = stablehlo.broadcast_in_dim %2713, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1234)
    %2715 = stablehlo.divide %2712, %2714 : tensor<1x16x257x257xf32> loc(#loc1234)
    %2716 = stablehlo.select %2708, %cst_3, %2715 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1235)
    %2717 = stablehlo.reshape %arg118 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2718 = stablehlo.custom_call @tt.mark_argument(%2717) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2719 = stablehlo.reshape %2718 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2720 = stablehlo.transpose %2719, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1236)
    %2721 = stablehlo.dot_general %2668, %2720, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1237)
    %2722 = stablehlo.reshape %2721 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1238)
    %2723 = stablehlo.reshape %arg117 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2724 = stablehlo.custom_call @tt.mark_argument(%2723) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2725 = stablehlo.reshape %2724 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2726 = stablehlo.broadcast_in_dim %2725, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1239)
    %2727 = stablehlo.add %2722, %2726 : tensor<1x257x1280xbf16> loc(#loc1239)
    %2728 = stablehlo.reshape %2727 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1240)
    %2729 = stablehlo.transpose %2728, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1241)
    %2730 = stablehlo.convert %2729 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1242)
    %2731 = stablehlo.dot_general %2716, %2730, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1243)
    %2732 = stablehlo.convert %2731 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1244)
    %2733 = stablehlo.transpose %2732, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1245)
    %2734 = stablehlo.reshape %2733 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1246)
    %2735 = stablehlo.reshape %arg116 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2736 = stablehlo.custom_call @tt.mark_argument(%2735) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2737 = stablehlo.reshape %2736 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2738 = stablehlo.transpose %2737, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1247)
    %2739 = stablehlo.dot_general %2734, %2738, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1248)
    %2740 = stablehlo.reshape %2739 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1246)
    %2741 = stablehlo.reshape %arg115 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2742 = stablehlo.custom_call @tt.mark_argument(%2741) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2743 = stablehlo.reshape %2742 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2744 = stablehlo.broadcast_in_dim %2743, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1249)
    %2745 = stablehlo.add %2740, %2744 : tensor<1x257x1280xbf16> loc(#loc1249)
    %2746 = stablehlo.add %2660, %2745 : tensor<1x257x1280xbf16> loc(#loc1250)
    %2747 = stablehlo.reshape %arg114 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2748 = stablehlo.custom_call @tt.mark_argument(%2747) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2749 = stablehlo.reshape %2748 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2750 = stablehlo.reshape %arg113 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2751 = stablehlo.custom_call @tt.mark_argument(%2750) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2752 = stablehlo.reshape %2751 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2753 = stablehlo.composite "tenstorrent.layer_norm" %2746, %2749, %2752 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_24} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1251)
    %2754 = stablehlo.reshape %2753 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1252)
    %2755 = stablehlo.reshape %arg112 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %2756 = stablehlo.custom_call @tt.mark_argument(%2755) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %2757 = stablehlo.reshape %2756 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %2758 = stablehlo.transpose %2757, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1253)
    %2759 = stablehlo.dot_general %2754, %2758, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1254)
    %2760 = stablehlo.reshape %2759 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1252)
    %2761 = stablehlo.reshape %arg111 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3)
    %2762 = stablehlo.custom_call @tt.mark_argument(%2761) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %2763 = stablehlo.reshape %2762 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3)
    %2764 = stablehlo.broadcast_in_dim %2763, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1255)
    %2765 = stablehlo.add %2760, %2764 : tensor<1x257x5120xbf16> loc(#loc1255)
    %2766 = stablehlo.composite "tenstorrent.gelu" %2765 {decomposition = @tenstorrent.gelu.impl_14} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1256)
    %2767 = stablehlo.reshape %2766 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1257)
    %2768 = stablehlo.reshape %arg110 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %2769 = stablehlo.custom_call @tt.mark_argument(%2768) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %2770 = stablehlo.reshape %2769 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %2771 = stablehlo.transpose %2770, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1258)
    %2772 = stablehlo.dot_general %2767, %2771, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1259)
    %2773 = stablehlo.reshape %2772 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1257)
    %2774 = stablehlo.reshape %arg109 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2775 = stablehlo.custom_call @tt.mark_argument(%2774) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2776 = stablehlo.reshape %2775 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2777 = stablehlo.broadcast_in_dim %2776, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1260)
    %2778 = stablehlo.add %2773, %2777 : tensor<1x257x1280xbf16> loc(#loc1260)
    %2779 = stablehlo.add %2746, %2778 : tensor<1x257x1280xbf16> loc(#loc1261)
    %2780 = stablehlo.reshape %arg108 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2781 = stablehlo.custom_call @tt.mark_argument(%2780) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2782 = stablehlo.reshape %2781 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2783 = stablehlo.reshape %arg107 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2784 = stablehlo.custom_call @tt.mark_argument(%2783) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2785 = stablehlo.reshape %2784 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2786 = stablehlo.composite "tenstorrent.layer_norm" %2779, %2782, %2785 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_58} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1262)
    %2787 = stablehlo.reshape %2786 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1263)
    %2788 = stablehlo.reshape %arg487 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2789 = stablehlo.custom_call @tt.mark_argument(%2788) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2790 = stablehlo.reshape %2789 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2791 = stablehlo.transpose %2790, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1264)
    %2792 = stablehlo.dot_general %2787, %2791, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1265)
    %2793 = stablehlo.reshape %2792 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1263)
    %2794 = stablehlo.reshape %arg486 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2795 = stablehlo.custom_call @tt.mark_argument(%2794) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2796 = stablehlo.reshape %2795 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2797 = stablehlo.broadcast_in_dim %2796, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1266)
    %2798 = stablehlo.add %2793, %2797 : tensor<1x257x1280xbf16> loc(#loc1266)
    %2799 = stablehlo.reshape %2798 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1267)
    %2800 = stablehlo.transpose %2799, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1268)
    %2801 = stablehlo.convert %2800 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1269)
    %2802 = stablehlo.multiply %2801, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1270)
    %2803 = stablehlo.reshape %arg485 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2804 = stablehlo.custom_call @tt.mark_argument(%2803) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2805 = stablehlo.reshape %2804 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2806 = stablehlo.transpose %2805, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1271)
    %2807 = stablehlo.dot_general %2787, %2806, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1272)
    %2808 = stablehlo.reshape %2807 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1273)
    %2809 = stablehlo.reshape %arg484 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2810 = stablehlo.custom_call @tt.mark_argument(%2809) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2811 = stablehlo.reshape %2810 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2812 = stablehlo.broadcast_in_dim %2811, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1274)
    %2813 = stablehlo.add %2808, %2812 : tensor<1x257x1280xbf16> loc(#loc1274)
    %2814 = stablehlo.reshape %2813 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1275)
    %2815 = stablehlo.transpose %2814, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1276)
    %2816 = stablehlo.convert %2815 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1277)
    %2817 = stablehlo.transpose %2816, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1278)
    %2818 = stablehlo.multiply %2817, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1279)
    %2819 = stablehlo.dot_general %2802, %2818, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1280)
    %2820 = stablehlo.convert %2819 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1281)
    %2821 = stablehlo.compare  EQ, %2820, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1281)
    %2822 = stablehlo.not %2821 : tensor<1x16x257x257xi1> loc(#loc1282)
    %2823 = stablehlo.reduce(%2822 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("2318|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_24aten__any"), %arg559: tensor<i1> loc("2318|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_24aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1284)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1285)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc1283)
    %2824 = stablehlo.reshape %2823 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1283)
    %2825 = stablehlo.not %2824 : tensor<1x16x257x1xi1> loc(#loc1286)
    %2826 = stablehlo.reshape %2825 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1287)
    %2827 = stablehlo.broadcast_in_dim %2826, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1287)
    %2828 = stablehlo.reduce(%2819 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1288)
    %2829 = stablehlo.broadcast_in_dim %2828, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1288)
    %2830 = stablehlo.subtract %2819, %2829 : tensor<1x16x257x257xf32> loc(#loc1288)
    %2831 = stablehlo.exponential %2830 : tensor<1x16x257x257xf32> loc(#loc1288)
    %2832 = stablehlo.reduce(%2831 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1288)
    %2833 = stablehlo.broadcast_in_dim %2832, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1288)
    %2834 = stablehlo.divide %2831, %2833 : tensor<1x16x257x257xf32> loc(#loc1288)
    %2835 = stablehlo.select %2827, %cst_3, %2834 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1289)
    %2836 = stablehlo.reshape %arg106 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2837 = stablehlo.custom_call @tt.mark_argument(%2836) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2838 = stablehlo.reshape %2837 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2839 = stablehlo.transpose %2838, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1290)
    %2840 = stablehlo.dot_general %2787, %2839, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1291)
    %2841 = stablehlo.reshape %2840 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1292)
    %2842 = stablehlo.reshape %arg105 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2843 = stablehlo.custom_call @tt.mark_argument(%2842) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2844 = stablehlo.reshape %2843 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2845 = stablehlo.broadcast_in_dim %2844, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1293)
    %2846 = stablehlo.add %2841, %2845 : tensor<1x257x1280xbf16> loc(#loc1293)
    %2847 = stablehlo.reshape %2846 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1294)
    %2848 = stablehlo.transpose %2847, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1295)
    %2849 = stablehlo.convert %2848 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1296)
    %2850 = stablehlo.dot_general %2835, %2849, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1297)
    %2851 = stablehlo.convert %2850 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1298)
    %2852 = stablehlo.transpose %2851, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1299)
    %2853 = stablehlo.reshape %2852 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1300)
    %2854 = stablehlo.reshape %arg104 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2855 = stablehlo.custom_call @tt.mark_argument(%2854) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2856 = stablehlo.reshape %2855 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2857 = stablehlo.transpose %2856, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1301)
    %2858 = stablehlo.dot_general %2853, %2857, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1302)
    %2859 = stablehlo.reshape %2858 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1300)
    %2860 = stablehlo.reshape %arg103 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2861 = stablehlo.custom_call @tt.mark_argument(%2860) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2862 = stablehlo.reshape %2861 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2863 = stablehlo.broadcast_in_dim %2862, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1303)
    %2864 = stablehlo.add %2859, %2863 : tensor<1x257x1280xbf16> loc(#loc1303)
    %2865 = stablehlo.add %2779, %2864 : tensor<1x257x1280xbf16> loc(#loc1304)
    %2866 = stablehlo.reshape %arg102 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2867 = stablehlo.custom_call @tt.mark_argument(%2866) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2868 = stablehlo.reshape %2867 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2869 = stablehlo.reshape %arg101 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2870 = stablehlo.custom_call @tt.mark_argument(%2869) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2871 = stablehlo.reshape %2870 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2872 = stablehlo.composite "tenstorrent.layer_norm" %2865, %2868, %2871 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_15} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1305)
    %2873 = stablehlo.reshape %2872 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1306)
    %2874 = stablehlo.reshape %arg100 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %2875 = stablehlo.custom_call @tt.mark_argument(%2874) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %2876 = stablehlo.reshape %2875 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %2877 = stablehlo.transpose %2876, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1307)
    %2878 = stablehlo.dot_general %2873, %2877, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1308)
    %2879 = stablehlo.reshape %2878 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1306)
    %2880 = stablehlo.reshape %arg99 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3)
    %2881 = stablehlo.custom_call @tt.mark_argument(%2880) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %2882 = stablehlo.reshape %2881 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3)
    %2883 = stablehlo.broadcast_in_dim %2882, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1309)
    %2884 = stablehlo.add %2879, %2883 : tensor<1x257x5120xbf16> loc(#loc1309)
    %2885 = stablehlo.composite "tenstorrent.gelu" %2884 {decomposition = @tenstorrent.gelu.impl_6} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1310)
    %2886 = stablehlo.reshape %2885 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1311)
    %2887 = stablehlo.reshape %arg98 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %2888 = stablehlo.custom_call @tt.mark_argument(%2887) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %2889 = stablehlo.reshape %2888 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %2890 = stablehlo.transpose %2889, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1312)
    %2891 = stablehlo.dot_general %2886, %2890, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1313)
    %2892 = stablehlo.reshape %2891 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1311)
    %2893 = stablehlo.reshape %arg97 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2894 = stablehlo.custom_call @tt.mark_argument(%2893) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2895 = stablehlo.reshape %2894 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2896 = stablehlo.broadcast_in_dim %2895, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1314)
    %2897 = stablehlo.add %2892, %2896 : tensor<1x257x1280xbf16> loc(#loc1314)
    %2898 = stablehlo.add %2865, %2897 : tensor<1x257x1280xbf16> loc(#loc1315)
    %2899 = stablehlo.reshape %arg96 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2900 = stablehlo.custom_call @tt.mark_argument(%2899) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2901 = stablehlo.reshape %2900 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2902 = stablehlo.reshape %arg95 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2903 = stablehlo.custom_call @tt.mark_argument(%2902) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2904 = stablehlo.reshape %2903 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2905 = stablehlo.composite "tenstorrent.layer_norm" %2898, %2901, %2904 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_14} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1316)
    %2906 = stablehlo.reshape %2905 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1317)
    %2907 = stablehlo.reshape %arg491 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2908 = stablehlo.custom_call @tt.mark_argument(%2907) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2909 = stablehlo.reshape %2908 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2910 = stablehlo.transpose %2909, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1318)
    %2911 = stablehlo.dot_general %2906, %2910, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1319)
    %2912 = stablehlo.reshape %2911 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1317)
    %2913 = stablehlo.reshape %arg490 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2914 = stablehlo.custom_call @tt.mark_argument(%2913) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2915 = stablehlo.reshape %2914 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2916 = stablehlo.broadcast_in_dim %2915, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1320)
    %2917 = stablehlo.add %2912, %2916 : tensor<1x257x1280xbf16> loc(#loc1320)
    %2918 = stablehlo.reshape %2917 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1321)
    %2919 = stablehlo.transpose %2918, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1322)
    %2920 = stablehlo.convert %2919 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1323)
    %2921 = stablehlo.multiply %2920, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1324)
    %2922 = stablehlo.reshape %arg489 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2923 = stablehlo.custom_call @tt.mark_argument(%2922) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2924 = stablehlo.reshape %2923 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2925 = stablehlo.transpose %2924, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1325)
    %2926 = stablehlo.dot_general %2906, %2925, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1326)
    %2927 = stablehlo.reshape %2926 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1327)
    %2928 = stablehlo.reshape %arg488 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2929 = stablehlo.custom_call @tt.mark_argument(%2928) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2930 = stablehlo.reshape %2929 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2931 = stablehlo.broadcast_in_dim %2930, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1328)
    %2932 = stablehlo.add %2927, %2931 : tensor<1x257x1280xbf16> loc(#loc1328)
    %2933 = stablehlo.reshape %2932 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1329)
    %2934 = stablehlo.transpose %2933, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1330)
    %2935 = stablehlo.convert %2934 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1331)
    %2936 = stablehlo.transpose %2935, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1332)
    %2937 = stablehlo.multiply %2936, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1333)
    %2938 = stablehlo.dot_general %2921, %2937, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1334)
    %2939 = stablehlo.convert %2938 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1335)
    %2940 = stablehlo.compare  EQ, %2939, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1335)
    %2941 = stablehlo.not %2940 : tensor<1x16x257x257xi1> loc(#loc1336)
    %2942 = stablehlo.reduce(%2941 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("2392|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_25aten__any"), %arg559: tensor<i1> loc("2392|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_25aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1338)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1339)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc1337)
    %2943 = stablehlo.reshape %2942 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1337)
    %2944 = stablehlo.not %2943 : tensor<1x16x257x1xi1> loc(#loc1340)
    %2945 = stablehlo.reshape %2944 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1341)
    %2946 = stablehlo.broadcast_in_dim %2945, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1341)
    %2947 = stablehlo.reduce(%2938 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1342)
    %2948 = stablehlo.broadcast_in_dim %2947, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1342)
    %2949 = stablehlo.subtract %2938, %2948 : tensor<1x16x257x257xf32> loc(#loc1342)
    %2950 = stablehlo.exponential %2949 : tensor<1x16x257x257xf32> loc(#loc1342)
    %2951 = stablehlo.reduce(%2950 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1342)
    %2952 = stablehlo.broadcast_in_dim %2951, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1342)
    %2953 = stablehlo.divide %2950, %2952 : tensor<1x16x257x257xf32> loc(#loc1342)
    %2954 = stablehlo.select %2946, %cst_3, %2953 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1343)
    %2955 = stablehlo.reshape %arg94 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2956 = stablehlo.custom_call @tt.mark_argument(%2955) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2957 = stablehlo.reshape %2956 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2958 = stablehlo.transpose %2957, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1344)
    %2959 = stablehlo.dot_general %2906, %2958, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1345)
    %2960 = stablehlo.reshape %2959 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1346)
    %2961 = stablehlo.reshape %arg93 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2962 = stablehlo.custom_call @tt.mark_argument(%2961) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2963 = stablehlo.reshape %2962 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2964 = stablehlo.broadcast_in_dim %2963, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1347)
    %2965 = stablehlo.add %2960, %2964 : tensor<1x257x1280xbf16> loc(#loc1347)
    %2966 = stablehlo.reshape %2965 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1348)
    %2967 = stablehlo.transpose %2966, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1349)
    %2968 = stablehlo.convert %2967 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1350)
    %2969 = stablehlo.dot_general %2954, %2968, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1351)
    %2970 = stablehlo.convert %2969 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1352)
    %2971 = stablehlo.transpose %2970, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1353)
    %2972 = stablehlo.reshape %2971 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1354)
    %2973 = stablehlo.reshape %arg92 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %2974 = stablehlo.custom_call @tt.mark_argument(%2973) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2975 = stablehlo.reshape %2974 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %2976 = stablehlo.transpose %2975, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1355)
    %2977 = stablehlo.dot_general %2972, %2976, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1356)
    %2978 = stablehlo.reshape %2977 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1354)
    %2979 = stablehlo.reshape %arg91 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2980 = stablehlo.custom_call @tt.mark_argument(%2979) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2981 = stablehlo.reshape %2980 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2982 = stablehlo.broadcast_in_dim %2981, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1357)
    %2983 = stablehlo.add %2978, %2982 : tensor<1x257x1280xbf16> loc(#loc1357)
    %2984 = stablehlo.add %2898, %2983 : tensor<1x257x1280xbf16> loc(#loc1358)
    %2985 = stablehlo.reshape %arg90 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2986 = stablehlo.custom_call @tt.mark_argument(%2985) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2987 = stablehlo.reshape %2986 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2988 = stablehlo.reshape %arg89 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %2989 = stablehlo.custom_call @tt.mark_argument(%2988) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2990 = stablehlo.reshape %2989 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %2991 = stablehlo.composite "tenstorrent.layer_norm" %2984, %2987, %2990 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_13} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1359)
    %2992 = stablehlo.reshape %2991 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1360)
    %2993 = stablehlo.reshape %arg88 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %2994 = stablehlo.custom_call @tt.mark_argument(%2993) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %2995 = stablehlo.reshape %2994 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %2996 = stablehlo.transpose %2995, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1361)
    %2997 = stablehlo.dot_general %2992, %2996, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1362)
    %2998 = stablehlo.reshape %2997 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1360)
    %2999 = stablehlo.reshape %arg87 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3)
    %3000 = stablehlo.custom_call @tt.mark_argument(%2999) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %3001 = stablehlo.reshape %3000 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3)
    %3002 = stablehlo.broadcast_in_dim %3001, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1363)
    %3003 = stablehlo.add %2998, %3002 : tensor<1x257x5120xbf16> loc(#loc1363)
    %3004 = stablehlo.composite "tenstorrent.gelu" %3003 {decomposition = @tenstorrent.gelu.impl_5} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1364)
    %3005 = stablehlo.reshape %3004 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1365)
    %3006 = stablehlo.reshape %arg86 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %3007 = stablehlo.custom_call @tt.mark_argument(%3006) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %3008 = stablehlo.reshape %3007 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %3009 = stablehlo.transpose %3008, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1366)
    %3010 = stablehlo.dot_general %3005, %3009, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1367)
    %3011 = stablehlo.reshape %3010 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1365)
    %3012 = stablehlo.reshape %arg85 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3013 = stablehlo.custom_call @tt.mark_argument(%3012) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3014 = stablehlo.reshape %3013 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3015 = stablehlo.broadcast_in_dim %3014, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1368)
    %3016 = stablehlo.add %3011, %3015 : tensor<1x257x1280xbf16> loc(#loc1368)
    %3017 = stablehlo.add %2984, %3016 : tensor<1x257x1280xbf16> loc(#loc1369)
    %3018 = stablehlo.reshape %arg84 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3019 = stablehlo.custom_call @tt.mark_argument(%3018) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3020 = stablehlo.reshape %3019 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3021 = stablehlo.reshape %arg83 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3022 = stablehlo.custom_call @tt.mark_argument(%3021) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3023 = stablehlo.reshape %3022 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3024 = stablehlo.composite "tenstorrent.layer_norm" %3017, %3020, %3023 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_12} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1370)
    %3025 = stablehlo.reshape %3024 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1371)
    %3026 = stablehlo.reshape %arg495 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3027 = stablehlo.custom_call @tt.mark_argument(%3026) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3028 = stablehlo.reshape %3027 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3029 = stablehlo.transpose %3028, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1372)
    %3030 = stablehlo.dot_general %3025, %3029, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1373)
    %3031 = stablehlo.reshape %3030 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1371)
    %3032 = stablehlo.reshape %arg494 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3033 = stablehlo.custom_call @tt.mark_argument(%3032) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3034 = stablehlo.reshape %3033 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3035 = stablehlo.broadcast_in_dim %3034, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1374)
    %3036 = stablehlo.add %3031, %3035 : tensor<1x257x1280xbf16> loc(#loc1374)
    %3037 = stablehlo.reshape %3036 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1375)
    %3038 = stablehlo.transpose %3037, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1376)
    %3039 = stablehlo.convert %3038 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1377)
    %3040 = stablehlo.multiply %3039, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1378)
    %3041 = stablehlo.reshape %arg493 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3042 = stablehlo.custom_call @tt.mark_argument(%3041) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3043 = stablehlo.reshape %3042 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3044 = stablehlo.transpose %3043, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1379)
    %3045 = stablehlo.dot_general %3025, %3044, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1380)
    %3046 = stablehlo.reshape %3045 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1381)
    %3047 = stablehlo.reshape %arg492 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3048 = stablehlo.custom_call @tt.mark_argument(%3047) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3049 = stablehlo.reshape %3048 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3050 = stablehlo.broadcast_in_dim %3049, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1382)
    %3051 = stablehlo.add %3046, %3050 : tensor<1x257x1280xbf16> loc(#loc1382)
    %3052 = stablehlo.reshape %3051 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1383)
    %3053 = stablehlo.transpose %3052, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1384)
    %3054 = stablehlo.convert %3053 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1385)
    %3055 = stablehlo.transpose %3054, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1386)
    %3056 = stablehlo.multiply %3055, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1387)
    %3057 = stablehlo.dot_general %3040, %3056, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1388)
    %3058 = stablehlo.convert %3057 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1389)
    %3059 = stablehlo.compare  EQ, %3058, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1389)
    %3060 = stablehlo.not %3059 : tensor<1x16x257x257xi1> loc(#loc1390)
    %3061 = stablehlo.reduce(%3060 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("2466|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_26aten__any"), %arg559: tensor<i1> loc("2466|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_26aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1392)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1393)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc1391)
    %3062 = stablehlo.reshape %3061 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1391)
    %3063 = stablehlo.not %3062 : tensor<1x16x257x1xi1> loc(#loc1394)
    %3064 = stablehlo.reshape %3063 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1395)
    %3065 = stablehlo.broadcast_in_dim %3064, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1395)
    %3066 = stablehlo.reduce(%3057 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1396)
    %3067 = stablehlo.broadcast_in_dim %3066, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1396)
    %3068 = stablehlo.subtract %3057, %3067 : tensor<1x16x257x257xf32> loc(#loc1396)
    %3069 = stablehlo.exponential %3068 : tensor<1x16x257x257xf32> loc(#loc1396)
    %3070 = stablehlo.reduce(%3069 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1396)
    %3071 = stablehlo.broadcast_in_dim %3070, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1396)
    %3072 = stablehlo.divide %3069, %3071 : tensor<1x16x257x257xf32> loc(#loc1396)
    %3073 = stablehlo.select %3065, %cst_3, %3072 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1397)
    %3074 = stablehlo.reshape %arg82 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3075 = stablehlo.custom_call @tt.mark_argument(%3074) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3076 = stablehlo.reshape %3075 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3077 = stablehlo.transpose %3076, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1398)
    %3078 = stablehlo.dot_general %3025, %3077, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1399)
    %3079 = stablehlo.reshape %3078 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1400)
    %3080 = stablehlo.reshape %arg81 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3081 = stablehlo.custom_call @tt.mark_argument(%3080) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3082 = stablehlo.reshape %3081 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3083 = stablehlo.broadcast_in_dim %3082, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1401)
    %3084 = stablehlo.add %3079, %3083 : tensor<1x257x1280xbf16> loc(#loc1401)
    %3085 = stablehlo.reshape %3084 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1402)
    %3086 = stablehlo.transpose %3085, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1403)
    %3087 = stablehlo.convert %3086 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1404)
    %3088 = stablehlo.dot_general %3073, %3087, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1405)
    %3089 = stablehlo.convert %3088 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1406)
    %3090 = stablehlo.transpose %3089, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1407)
    %3091 = stablehlo.reshape %3090 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1408)
    %3092 = stablehlo.reshape %arg80 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3093 = stablehlo.custom_call @tt.mark_argument(%3092) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3094 = stablehlo.reshape %3093 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3095 = stablehlo.transpose %3094, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1409)
    %3096 = stablehlo.dot_general %3091, %3095, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1410)
    %3097 = stablehlo.reshape %3096 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1408)
    %3098 = stablehlo.reshape %arg79 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3099 = stablehlo.custom_call @tt.mark_argument(%3098) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3100 = stablehlo.reshape %3099 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3101 = stablehlo.broadcast_in_dim %3100, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1411)
    %3102 = stablehlo.add %3097, %3101 : tensor<1x257x1280xbf16> loc(#loc1411)
    %3103 = stablehlo.add %3017, %3102 : tensor<1x257x1280xbf16> loc(#loc1412)
    %3104 = stablehlo.reshape %arg78 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3105 = stablehlo.custom_call @tt.mark_argument(%3104) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3106 = stablehlo.reshape %3105 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3107 = stablehlo.reshape %arg77 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3108 = stablehlo.custom_call @tt.mark_argument(%3107) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3109 = stablehlo.reshape %3108 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3110 = stablehlo.composite "tenstorrent.layer_norm" %3103, %3106, %3109 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_11} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1413)
    %3111 = stablehlo.reshape %3110 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1414)
    %3112 = stablehlo.reshape %arg76 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %3113 = stablehlo.custom_call @tt.mark_argument(%3112) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %3114 = stablehlo.reshape %3113 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %3115 = stablehlo.transpose %3114, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1415)
    %3116 = stablehlo.dot_general %3111, %3115, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1416)
    %3117 = stablehlo.reshape %3116 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1414)
    %3118 = stablehlo.reshape %arg75 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3)
    %3119 = stablehlo.custom_call @tt.mark_argument(%3118) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %3120 = stablehlo.reshape %3119 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3)
    %3121 = stablehlo.broadcast_in_dim %3120, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1417)
    %3122 = stablehlo.add %3117, %3121 : tensor<1x257x5120xbf16> loc(#loc1417)
    %3123 = stablehlo.composite "tenstorrent.gelu" %3122 {decomposition = @tenstorrent.gelu.impl_4} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1418)
    %3124 = stablehlo.reshape %3123 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1419)
    %3125 = stablehlo.reshape %arg74 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %3126 = stablehlo.custom_call @tt.mark_argument(%3125) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %3127 = stablehlo.reshape %3126 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %3128 = stablehlo.transpose %3127, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1420)
    %3129 = stablehlo.dot_general %3124, %3128, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1421)
    %3130 = stablehlo.reshape %3129 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1419)
    %3131 = stablehlo.reshape %arg73 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3132 = stablehlo.custom_call @tt.mark_argument(%3131) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3133 = stablehlo.reshape %3132 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3134 = stablehlo.broadcast_in_dim %3133, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1422)
    %3135 = stablehlo.add %3130, %3134 : tensor<1x257x1280xbf16> loc(#loc1422)
    %3136 = stablehlo.add %3103, %3135 : tensor<1x257x1280xbf16> loc(#loc1423)
    %3137 = stablehlo.reshape %arg72 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3138 = stablehlo.custom_call @tt.mark_argument(%3137) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3139 = stablehlo.reshape %3138 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3140 = stablehlo.reshape %arg71 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3141 = stablehlo.custom_call @tt.mark_argument(%3140) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3142 = stablehlo.reshape %3141 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3143 = stablehlo.composite "tenstorrent.layer_norm" %3136, %3139, %3142 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_10} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1424)
    %3144 = stablehlo.reshape %3143 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1425)
    %3145 = stablehlo.reshape %arg499 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3146 = stablehlo.custom_call @tt.mark_argument(%3145) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3147 = stablehlo.reshape %3146 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3148 = stablehlo.transpose %3147, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1426)
    %3149 = stablehlo.dot_general %3144, %3148, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1427)
    %3150 = stablehlo.reshape %3149 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1425)
    %3151 = stablehlo.reshape %arg498 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3152 = stablehlo.custom_call @tt.mark_argument(%3151) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3153 = stablehlo.reshape %3152 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3154 = stablehlo.broadcast_in_dim %3153, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1428)
    %3155 = stablehlo.add %3150, %3154 : tensor<1x257x1280xbf16> loc(#loc1428)
    %3156 = stablehlo.reshape %3155 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1429)
    %3157 = stablehlo.transpose %3156, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1430)
    %3158 = stablehlo.convert %3157 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1431)
    %3159 = stablehlo.multiply %3158, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1432)
    %3160 = stablehlo.reshape %arg497 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3161 = stablehlo.custom_call @tt.mark_argument(%3160) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3162 = stablehlo.reshape %3161 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3163 = stablehlo.transpose %3162, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1433)
    %3164 = stablehlo.dot_general %3144, %3163, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1434)
    %3165 = stablehlo.reshape %3164 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1435)
    %3166 = stablehlo.reshape %arg496 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3167 = stablehlo.custom_call @tt.mark_argument(%3166) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3168 = stablehlo.reshape %3167 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3169 = stablehlo.broadcast_in_dim %3168, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1436)
    %3170 = stablehlo.add %3165, %3169 : tensor<1x257x1280xbf16> loc(#loc1436)
    %3171 = stablehlo.reshape %3170 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1437)
    %3172 = stablehlo.transpose %3171, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1438)
    %3173 = stablehlo.convert %3172 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1439)
    %3174 = stablehlo.transpose %3173, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1440)
    %3175 = stablehlo.multiply %3174, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1441)
    %3176 = stablehlo.dot_general %3159, %3175, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1442)
    %3177 = stablehlo.convert %3176 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1443)
    %3178 = stablehlo.compare  EQ, %3177, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1443)
    %3179 = stablehlo.not %3178 : tensor<1x16x257x257xi1> loc(#loc1444)
    %3180 = stablehlo.reduce(%3179 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("2540|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_27aten__any"), %arg559: tensor<i1> loc("2540|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_27aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1446)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1447)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc1445)
    %3181 = stablehlo.reshape %3180 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1445)
    %3182 = stablehlo.not %3181 : tensor<1x16x257x1xi1> loc(#loc1448)
    %3183 = stablehlo.reshape %3182 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1449)
    %3184 = stablehlo.broadcast_in_dim %3183, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1449)
    %3185 = stablehlo.reduce(%3176 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1450)
    %3186 = stablehlo.broadcast_in_dim %3185, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1450)
    %3187 = stablehlo.subtract %3176, %3186 : tensor<1x16x257x257xf32> loc(#loc1450)
    %3188 = stablehlo.exponential %3187 : tensor<1x16x257x257xf32> loc(#loc1450)
    %3189 = stablehlo.reduce(%3188 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1450)
    %3190 = stablehlo.broadcast_in_dim %3189, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1450)
    %3191 = stablehlo.divide %3188, %3190 : tensor<1x16x257x257xf32> loc(#loc1450)
    %3192 = stablehlo.select %3184, %cst_3, %3191 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1451)
    %3193 = stablehlo.reshape %arg70 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3194 = stablehlo.custom_call @tt.mark_argument(%3193) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3195 = stablehlo.reshape %3194 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3196 = stablehlo.transpose %3195, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1452)
    %3197 = stablehlo.dot_general %3144, %3196, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1453)
    %3198 = stablehlo.reshape %3197 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1454)
    %3199 = stablehlo.reshape %arg69 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3200 = stablehlo.custom_call @tt.mark_argument(%3199) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3201 = stablehlo.reshape %3200 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3202 = stablehlo.broadcast_in_dim %3201, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1455)
    %3203 = stablehlo.add %3198, %3202 : tensor<1x257x1280xbf16> loc(#loc1455)
    %3204 = stablehlo.reshape %3203 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1456)
    %3205 = stablehlo.transpose %3204, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1457)
    %3206 = stablehlo.convert %3205 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1458)
    %3207 = stablehlo.dot_general %3192, %3206, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1459)
    %3208 = stablehlo.convert %3207 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1460)
    %3209 = stablehlo.transpose %3208, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1461)
    %3210 = stablehlo.reshape %3209 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1462)
    %3211 = stablehlo.reshape %arg68 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3212 = stablehlo.custom_call @tt.mark_argument(%3211) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3213 = stablehlo.reshape %3212 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3214 = stablehlo.transpose %3213, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1463)
    %3215 = stablehlo.dot_general %3210, %3214, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1464)
    %3216 = stablehlo.reshape %3215 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1462)
    %3217 = stablehlo.reshape %arg67 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3218 = stablehlo.custom_call @tt.mark_argument(%3217) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3219 = stablehlo.reshape %3218 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3220 = stablehlo.broadcast_in_dim %3219, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1465)
    %3221 = stablehlo.add %3216, %3220 : tensor<1x257x1280xbf16> loc(#loc1465)
    %3222 = stablehlo.add %3136, %3221 : tensor<1x257x1280xbf16> loc(#loc1466)
    %3223 = stablehlo.reshape %arg66 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3224 = stablehlo.custom_call @tt.mark_argument(%3223) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3225 = stablehlo.reshape %3224 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3226 = stablehlo.reshape %arg65 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3227 = stablehlo.custom_call @tt.mark_argument(%3226) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3228 = stablehlo.reshape %3227 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3229 = stablehlo.composite "tenstorrent.layer_norm" %3222, %3225, %3228 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_9} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1467)
    %3230 = stablehlo.reshape %3229 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1468)
    %3231 = stablehlo.reshape %arg64 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %3232 = stablehlo.custom_call @tt.mark_argument(%3231) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %3233 = stablehlo.reshape %3232 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %3234 = stablehlo.transpose %3233, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1469)
    %3235 = stablehlo.dot_general %3230, %3234, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1470)
    %3236 = stablehlo.reshape %3235 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1468)
    %3237 = stablehlo.reshape %arg63 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3)
    %3238 = stablehlo.custom_call @tt.mark_argument(%3237) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %3239 = stablehlo.reshape %3238 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3)
    %3240 = stablehlo.broadcast_in_dim %3239, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1471)
    %3241 = stablehlo.add %3236, %3240 : tensor<1x257x5120xbf16> loc(#loc1471)
    %3242 = stablehlo.composite "tenstorrent.gelu" %3241 {decomposition = @tenstorrent.gelu.impl_9} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1472)
    %3243 = stablehlo.reshape %3242 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1473)
    %3244 = stablehlo.reshape %arg62 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %3245 = stablehlo.custom_call @tt.mark_argument(%3244) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %3246 = stablehlo.reshape %3245 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %3247 = stablehlo.transpose %3246, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1474)
    %3248 = stablehlo.dot_general %3243, %3247, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1475)
    %3249 = stablehlo.reshape %3248 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1473)
    %3250 = stablehlo.reshape %arg61 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3251 = stablehlo.custom_call @tt.mark_argument(%3250) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3252 = stablehlo.reshape %3251 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3253 = stablehlo.broadcast_in_dim %3252, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1476)
    %3254 = stablehlo.add %3249, %3253 : tensor<1x257x1280xbf16> loc(#loc1476)
    %3255 = stablehlo.add %3222, %3254 : tensor<1x257x1280xbf16> loc(#loc1477)
    %3256 = stablehlo.reshape %arg60 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3257 = stablehlo.custom_call @tt.mark_argument(%3256) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3258 = stablehlo.reshape %3257 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3259 = stablehlo.reshape %arg59 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3260 = stablehlo.custom_call @tt.mark_argument(%3259) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3261 = stablehlo.reshape %3260 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3262 = stablehlo.composite "tenstorrent.layer_norm" %3255, %3258, %3261 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_8} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1478)
    %3263 = stablehlo.reshape %3262 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1479)
    %3264 = stablehlo.reshape %arg503 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3265 = stablehlo.custom_call @tt.mark_argument(%3264) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3266 = stablehlo.reshape %3265 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3267 = stablehlo.transpose %3266, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1480)
    %3268 = stablehlo.dot_general %3263, %3267, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1481)
    %3269 = stablehlo.reshape %3268 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1479)
    %3270 = stablehlo.reshape %arg502 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3271 = stablehlo.custom_call @tt.mark_argument(%3270) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3272 = stablehlo.reshape %3271 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3273 = stablehlo.broadcast_in_dim %3272, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1482)
    %3274 = stablehlo.add %3269, %3273 : tensor<1x257x1280xbf16> loc(#loc1482)
    %3275 = stablehlo.reshape %3274 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1483)
    %3276 = stablehlo.transpose %3275, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1484)
    %3277 = stablehlo.convert %3276 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1485)
    %3278 = stablehlo.multiply %3277, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1486)
    %3279 = stablehlo.reshape %arg501 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3280 = stablehlo.custom_call @tt.mark_argument(%3279) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3281 = stablehlo.reshape %3280 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3282 = stablehlo.transpose %3281, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1487)
    %3283 = stablehlo.dot_general %3263, %3282, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1488)
    %3284 = stablehlo.reshape %3283 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1489)
    %3285 = stablehlo.reshape %arg500 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3286 = stablehlo.custom_call @tt.mark_argument(%3285) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3287 = stablehlo.reshape %3286 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3288 = stablehlo.broadcast_in_dim %3287, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1490)
    %3289 = stablehlo.add %3284, %3288 : tensor<1x257x1280xbf16> loc(#loc1490)
    %3290 = stablehlo.reshape %3289 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1491)
    %3291 = stablehlo.transpose %3290, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1492)
    %3292 = stablehlo.convert %3291 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1493)
    %3293 = stablehlo.transpose %3292, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1494)
    %3294 = stablehlo.multiply %3293, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1495)
    %3295 = stablehlo.dot_general %3278, %3294, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1496)
    %3296 = stablehlo.convert %3295 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1497)
    %3297 = stablehlo.compare  EQ, %3296, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1497)
    %3298 = stablehlo.not %3297 : tensor<1x16x257x257xi1> loc(#loc1498)
    %3299 = stablehlo.reduce(%3298 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("2614|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_28aten__any"), %arg559: tensor<i1> loc("2614|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_28aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1500)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1501)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc1499)
    %3300 = stablehlo.reshape %3299 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1499)
    %3301 = stablehlo.not %3300 : tensor<1x16x257x1xi1> loc(#loc1502)
    %3302 = stablehlo.reshape %3301 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1503)
    %3303 = stablehlo.broadcast_in_dim %3302, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1503)
    %3304 = stablehlo.reduce(%3295 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1504)
    %3305 = stablehlo.broadcast_in_dim %3304, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1504)
    %3306 = stablehlo.subtract %3295, %3305 : tensor<1x16x257x257xf32> loc(#loc1504)
    %3307 = stablehlo.exponential %3306 : tensor<1x16x257x257xf32> loc(#loc1504)
    %3308 = stablehlo.reduce(%3307 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1504)
    %3309 = stablehlo.broadcast_in_dim %3308, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1504)
    %3310 = stablehlo.divide %3307, %3309 : tensor<1x16x257x257xf32> loc(#loc1504)
    %3311 = stablehlo.select %3303, %cst_3, %3310 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1505)
    %3312 = stablehlo.reshape %arg58 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3313 = stablehlo.custom_call @tt.mark_argument(%3312) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3314 = stablehlo.reshape %3313 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3315 = stablehlo.transpose %3314, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1506)
    %3316 = stablehlo.dot_general %3263, %3315, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1507)
    %3317 = stablehlo.reshape %3316 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1508)
    %3318 = stablehlo.reshape %arg57 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3319 = stablehlo.custom_call @tt.mark_argument(%3318) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3320 = stablehlo.reshape %3319 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3321 = stablehlo.broadcast_in_dim %3320, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1509)
    %3322 = stablehlo.add %3317, %3321 : tensor<1x257x1280xbf16> loc(#loc1509)
    %3323 = stablehlo.reshape %3322 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1510)
    %3324 = stablehlo.transpose %3323, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1511)
    %3325 = stablehlo.convert %3324 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1512)
    %3326 = stablehlo.dot_general %3311, %3325, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1513)
    %3327 = stablehlo.convert %3326 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1514)
    %3328 = stablehlo.transpose %3327, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1515)
    %3329 = stablehlo.reshape %3328 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1516)
    %3330 = stablehlo.reshape %arg56 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3331 = stablehlo.custom_call @tt.mark_argument(%3330) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3332 = stablehlo.reshape %3331 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3333 = stablehlo.transpose %3332, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1517)
    %3334 = stablehlo.dot_general %3329, %3333, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1518)
    %3335 = stablehlo.reshape %3334 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1516)
    %3336 = stablehlo.reshape %arg55 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3337 = stablehlo.custom_call @tt.mark_argument(%3336) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3338 = stablehlo.reshape %3337 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3339 = stablehlo.broadcast_in_dim %3338, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1519)
    %3340 = stablehlo.add %3335, %3339 : tensor<1x257x1280xbf16> loc(#loc1519)
    %3341 = stablehlo.add %3255, %3340 : tensor<1x257x1280xbf16> loc(#loc1520)
    %3342 = stablehlo.reshape %arg54 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3343 = stablehlo.custom_call @tt.mark_argument(%3342) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3344 = stablehlo.reshape %3343 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3345 = stablehlo.reshape %arg53 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3346 = stablehlo.custom_call @tt.mark_argument(%3345) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3347 = stablehlo.reshape %3346 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3348 = stablehlo.composite "tenstorrent.layer_norm" %3341, %3344, %3347 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_46} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1521)
    %3349 = stablehlo.reshape %3348 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1522)
    %3350 = stablehlo.reshape %arg52 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %3351 = stablehlo.custom_call @tt.mark_argument(%3350) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %3352 = stablehlo.reshape %3351 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %3353 = stablehlo.transpose %3352, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1523)
    %3354 = stablehlo.dot_general %3349, %3353, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1524)
    %3355 = stablehlo.reshape %3354 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1522)
    %3356 = stablehlo.reshape %arg51 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3)
    %3357 = stablehlo.custom_call @tt.mark_argument(%3356) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %3358 = stablehlo.reshape %3357 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3)
    %3359 = stablehlo.broadcast_in_dim %3358, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1525)
    %3360 = stablehlo.add %3355, %3359 : tensor<1x257x5120xbf16> loc(#loc1525)
    %3361 = stablehlo.composite "tenstorrent.gelu" %3360 {decomposition = @tenstorrent.gelu.impl_29} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1526)
    %3362 = stablehlo.reshape %3361 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1527)
    %3363 = stablehlo.reshape %arg50 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %3364 = stablehlo.custom_call @tt.mark_argument(%3363) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %3365 = stablehlo.reshape %3364 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %3366 = stablehlo.transpose %3365, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1528)
    %3367 = stablehlo.dot_general %3362, %3366, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1529)
    %3368 = stablehlo.reshape %3367 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1527)
    %3369 = stablehlo.reshape %arg49 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3370 = stablehlo.custom_call @tt.mark_argument(%3369) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3371 = stablehlo.reshape %3370 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3372 = stablehlo.broadcast_in_dim %3371, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1530)
    %3373 = stablehlo.add %3368, %3372 : tensor<1x257x1280xbf16> loc(#loc1530)
    %3374 = stablehlo.add %3341, %3373 : tensor<1x257x1280xbf16> loc(#loc1531)
    %3375 = stablehlo.reshape %arg48 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3376 = stablehlo.custom_call @tt.mark_argument(%3375) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3377 = stablehlo.reshape %3376 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3378 = stablehlo.reshape %arg47 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3379 = stablehlo.custom_call @tt.mark_argument(%3378) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3380 = stablehlo.reshape %3379 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3381 = stablehlo.composite "tenstorrent.layer_norm" %3374, %3377, %3380 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_7} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1532)
    %3382 = stablehlo.reshape %3381 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1533)
    %3383 = stablehlo.reshape %arg507 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3384 = stablehlo.custom_call @tt.mark_argument(%3383) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3385 = stablehlo.reshape %3384 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3386 = stablehlo.transpose %3385, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1534)
    %3387 = stablehlo.dot_general %3382, %3386, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1535)
    %3388 = stablehlo.reshape %3387 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1533)
    %3389 = stablehlo.reshape %arg506 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3390 = stablehlo.custom_call @tt.mark_argument(%3389) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3391 = stablehlo.reshape %3390 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3392 = stablehlo.broadcast_in_dim %3391, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1536)
    %3393 = stablehlo.add %3388, %3392 : tensor<1x257x1280xbf16> loc(#loc1536)
    %3394 = stablehlo.reshape %3393 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1537)
    %3395 = stablehlo.transpose %3394, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1538)
    %3396 = stablehlo.convert %3395 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1539)
    %3397 = stablehlo.multiply %3396, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1540)
    %3398 = stablehlo.reshape %arg505 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3399 = stablehlo.custom_call @tt.mark_argument(%3398) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3400 = stablehlo.reshape %3399 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3401 = stablehlo.transpose %3400, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1541)
    %3402 = stablehlo.dot_general %3382, %3401, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1542)
    %3403 = stablehlo.reshape %3402 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1543)
    %3404 = stablehlo.reshape %arg504 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3405 = stablehlo.custom_call @tt.mark_argument(%3404) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3406 = stablehlo.reshape %3405 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3407 = stablehlo.broadcast_in_dim %3406, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1544)
    %3408 = stablehlo.add %3403, %3407 : tensor<1x257x1280xbf16> loc(#loc1544)
    %3409 = stablehlo.reshape %3408 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1545)
    %3410 = stablehlo.transpose %3409, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1546)
    %3411 = stablehlo.convert %3410 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1547)
    %3412 = stablehlo.transpose %3411, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1548)
    %3413 = stablehlo.multiply %3412, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1549)
    %3414 = stablehlo.dot_general %3397, %3413, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1550)
    %3415 = stablehlo.convert %3414 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1551)
    %3416 = stablehlo.compare  EQ, %3415, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1551)
    %3417 = stablehlo.not %3416 : tensor<1x16x257x257xi1> loc(#loc1552)
    %3418 = stablehlo.reduce(%3417 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("2688|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_29aten__any"), %arg559: tensor<i1> loc("2688|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_29aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1554)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1555)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc1553)
    %3419 = stablehlo.reshape %3418 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1553)
    %3420 = stablehlo.not %3419 : tensor<1x16x257x1xi1> loc(#loc1556)
    %3421 = stablehlo.reshape %3420 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1557)
    %3422 = stablehlo.broadcast_in_dim %3421, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1557)
    %3423 = stablehlo.reduce(%3414 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1558)
    %3424 = stablehlo.broadcast_in_dim %3423, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1558)
    %3425 = stablehlo.subtract %3414, %3424 : tensor<1x16x257x257xf32> loc(#loc1558)
    %3426 = stablehlo.exponential %3425 : tensor<1x16x257x257xf32> loc(#loc1558)
    %3427 = stablehlo.reduce(%3426 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1558)
    %3428 = stablehlo.broadcast_in_dim %3427, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1558)
    %3429 = stablehlo.divide %3426, %3428 : tensor<1x16x257x257xf32> loc(#loc1558)
    %3430 = stablehlo.select %3422, %cst_3, %3429 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1559)
    %3431 = stablehlo.reshape %arg46 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3432 = stablehlo.custom_call @tt.mark_argument(%3431) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3433 = stablehlo.reshape %3432 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3434 = stablehlo.transpose %3433, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1560)
    %3435 = stablehlo.dot_general %3382, %3434, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1561)
    %3436 = stablehlo.reshape %3435 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1562)
    %3437 = stablehlo.reshape %arg45 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3438 = stablehlo.custom_call @tt.mark_argument(%3437) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3439 = stablehlo.reshape %3438 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3440 = stablehlo.broadcast_in_dim %3439, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1563)
    %3441 = stablehlo.add %3436, %3440 : tensor<1x257x1280xbf16> loc(#loc1563)
    %3442 = stablehlo.reshape %3441 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1564)
    %3443 = stablehlo.transpose %3442, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1565)
    %3444 = stablehlo.convert %3443 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1566)
    %3445 = stablehlo.dot_general %3430, %3444, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1567)
    %3446 = stablehlo.convert %3445 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1568)
    %3447 = stablehlo.transpose %3446, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1569)
    %3448 = stablehlo.reshape %3447 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1570)
    %3449 = stablehlo.reshape %arg44 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3450 = stablehlo.custom_call @tt.mark_argument(%3449) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3451 = stablehlo.reshape %3450 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3452 = stablehlo.transpose %3451, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1571)
    %3453 = stablehlo.dot_general %3448, %3452, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1572)
    %3454 = stablehlo.reshape %3453 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1570)
    %3455 = stablehlo.reshape %arg43 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3456 = stablehlo.custom_call @tt.mark_argument(%3455) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3457 = stablehlo.reshape %3456 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3458 = stablehlo.broadcast_in_dim %3457, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1573)
    %3459 = stablehlo.add %3454, %3458 : tensor<1x257x1280xbf16> loc(#loc1573)
    %3460 = stablehlo.add %3374, %3459 : tensor<1x257x1280xbf16> loc(#loc1574)
    %3461 = stablehlo.reshape %arg42 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3462 = stablehlo.custom_call @tt.mark_argument(%3461) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3463 = stablehlo.reshape %3462 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3464 = stablehlo.reshape %arg41 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3465 = stablehlo.custom_call @tt.mark_argument(%3464) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3466 = stablehlo.reshape %3465 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3467 = stablehlo.composite "tenstorrent.layer_norm" %3460, %3463, %3466 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_6} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1575)
    %3468 = stablehlo.reshape %3467 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1576)
    %3469 = stablehlo.reshape %arg40 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %3470 = stablehlo.custom_call @tt.mark_argument(%3469) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %3471 = stablehlo.reshape %3470 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %3472 = stablehlo.transpose %3471, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1577)
    %3473 = stablehlo.dot_general %3468, %3472, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1578)
    %3474 = stablehlo.reshape %3473 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1576)
    %3475 = stablehlo.reshape %arg39 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3)
    %3476 = stablehlo.custom_call @tt.mark_argument(%3475) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %3477 = stablehlo.reshape %3476 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3)
    %3478 = stablehlo.broadcast_in_dim %3477, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1579)
    %3479 = stablehlo.add %3474, %3478 : tensor<1x257x5120xbf16> loc(#loc1579)
    %3480 = stablehlo.composite "tenstorrent.gelu" %3479 {decomposition = @tenstorrent.gelu.impl_2} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1580)
    %3481 = stablehlo.reshape %3480 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1581)
    %3482 = stablehlo.reshape %arg38 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %3483 = stablehlo.custom_call @tt.mark_argument(%3482) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %3484 = stablehlo.reshape %3483 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %3485 = stablehlo.transpose %3484, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1582)
    %3486 = stablehlo.dot_general %3481, %3485, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1583)
    %3487 = stablehlo.reshape %3486 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1581)
    %3488 = stablehlo.reshape %arg37 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3489 = stablehlo.custom_call @tt.mark_argument(%3488) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3490 = stablehlo.reshape %3489 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3491 = stablehlo.broadcast_in_dim %3490, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1584)
    %3492 = stablehlo.add %3487, %3491 : tensor<1x257x1280xbf16> loc(#loc1584)
    %3493 = stablehlo.add %3460, %3492 : tensor<1x257x1280xbf16> loc(#loc1585)
    %3494 = stablehlo.reshape %arg36 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3495 = stablehlo.custom_call @tt.mark_argument(%3494) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3496 = stablehlo.reshape %3495 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3497 = stablehlo.reshape %arg35 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3498 = stablehlo.custom_call @tt.mark_argument(%3497) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3499 = stablehlo.reshape %3498 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3500 = stablehlo.composite "tenstorrent.layer_norm" %3493, %3496, %3499 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_4} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1586)
    %3501 = stablehlo.reshape %3500 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1587)
    %3502 = stablehlo.reshape %arg511 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3503 = stablehlo.custom_call @tt.mark_argument(%3502) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3504 = stablehlo.reshape %3503 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3505 = stablehlo.transpose %3504, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1588)
    %3506 = stablehlo.dot_general %3501, %3505, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1589)
    %3507 = stablehlo.reshape %3506 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1587)
    %3508 = stablehlo.reshape %arg510 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3509 = stablehlo.custom_call @tt.mark_argument(%3508) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3510 = stablehlo.reshape %3509 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3511 = stablehlo.broadcast_in_dim %3510, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1590)
    %3512 = stablehlo.add %3507, %3511 : tensor<1x257x1280xbf16> loc(#loc1590)
    %3513 = stablehlo.reshape %3512 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1591)
    %3514 = stablehlo.transpose %3513, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1592)
    %3515 = stablehlo.convert %3514 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1593)
    %3516 = stablehlo.multiply %3515, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1594)
    %3517 = stablehlo.reshape %arg509 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3518 = stablehlo.custom_call @tt.mark_argument(%3517) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3519 = stablehlo.reshape %3518 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3520 = stablehlo.transpose %3519, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1595)
    %3521 = stablehlo.dot_general %3501, %3520, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1596)
    %3522 = stablehlo.reshape %3521 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1597)
    %3523 = stablehlo.reshape %arg508 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3524 = stablehlo.custom_call @tt.mark_argument(%3523) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3525 = stablehlo.reshape %3524 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3526 = stablehlo.broadcast_in_dim %3525, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1598)
    %3527 = stablehlo.add %3522, %3526 : tensor<1x257x1280xbf16> loc(#loc1598)
    %3528 = stablehlo.reshape %3527 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1599)
    %3529 = stablehlo.transpose %3528, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1600)
    %3530 = stablehlo.convert %3529 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1601)
    %3531 = stablehlo.transpose %3530, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1602)
    %3532 = stablehlo.multiply %3531, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1603)
    %3533 = stablehlo.dot_general %3516, %3532, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1604)
    %3534 = stablehlo.convert %3533 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1605)
    %3535 = stablehlo.compare  EQ, %3534, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1605)
    %3536 = stablehlo.not %3535 : tensor<1x16x257x257xi1> loc(#loc1606)
    %3537 = stablehlo.reduce(%3536 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("2762|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_30aten__any"), %arg559: tensor<i1> loc("2762|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_30aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1608)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1609)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc1607)
    %3538 = stablehlo.reshape %3537 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1607)
    %3539 = stablehlo.not %3538 : tensor<1x16x257x1xi1> loc(#loc1610)
    %3540 = stablehlo.reshape %3539 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1611)
    %3541 = stablehlo.broadcast_in_dim %3540, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1611)
    %3542 = stablehlo.reduce(%3533 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1612)
    %3543 = stablehlo.broadcast_in_dim %3542, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1612)
    %3544 = stablehlo.subtract %3533, %3543 : tensor<1x16x257x257xf32> loc(#loc1612)
    %3545 = stablehlo.exponential %3544 : tensor<1x16x257x257xf32> loc(#loc1612)
    %3546 = stablehlo.reduce(%3545 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1612)
    %3547 = stablehlo.broadcast_in_dim %3546, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1612)
    %3548 = stablehlo.divide %3545, %3547 : tensor<1x16x257x257xf32> loc(#loc1612)
    %3549 = stablehlo.select %3541, %cst_3, %3548 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1613)
    %3550 = stablehlo.reshape %arg34 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3551 = stablehlo.custom_call @tt.mark_argument(%3550) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3552 = stablehlo.reshape %3551 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3553 = stablehlo.transpose %3552, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1614)
    %3554 = stablehlo.dot_general %3501, %3553, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1615)
    %3555 = stablehlo.reshape %3554 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1616)
    %3556 = stablehlo.reshape %arg33 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3557 = stablehlo.custom_call @tt.mark_argument(%3556) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3558 = stablehlo.reshape %3557 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3559 = stablehlo.broadcast_in_dim %3558, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1617)
    %3560 = stablehlo.add %3555, %3559 : tensor<1x257x1280xbf16> loc(#loc1617)
    %3561 = stablehlo.reshape %3560 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1618)
    %3562 = stablehlo.transpose %3561, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1619)
    %3563 = stablehlo.convert %3562 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1620)
    %3564 = stablehlo.dot_general %3549, %3563, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1621)
    %3565 = stablehlo.convert %3564 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1622)
    %3566 = stablehlo.transpose %3565, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1623)
    %3567 = stablehlo.reshape %3566 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1624)
    %3568 = stablehlo.reshape %arg32 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3569 = stablehlo.custom_call @tt.mark_argument(%3568) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3570 = stablehlo.reshape %3569 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3571 = stablehlo.transpose %3570, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1625)
    %3572 = stablehlo.dot_general %3567, %3571, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1626)
    %3573 = stablehlo.reshape %3572 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1624)
    %3574 = stablehlo.reshape %arg31 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3575 = stablehlo.custom_call @tt.mark_argument(%3574) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3576 = stablehlo.reshape %3575 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3577 = stablehlo.broadcast_in_dim %3576, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1627)
    %3578 = stablehlo.add %3573, %3577 : tensor<1x257x1280xbf16> loc(#loc1627)
    %3579 = stablehlo.add %3493, %3578 : tensor<1x257x1280xbf16> loc(#loc1628)
    %3580 = stablehlo.reshape %arg30 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3581 = stablehlo.custom_call @tt.mark_argument(%3580) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3582 = stablehlo.reshape %3581 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3583 = stablehlo.reshape %arg29 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3584 = stablehlo.custom_call @tt.mark_argument(%3583) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3585 = stablehlo.reshape %3584 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3586 = stablehlo.composite "tenstorrent.layer_norm" %3579, %3582, %3585 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_72} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1629)
    %3587 = stablehlo.reshape %3586 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1630)
    %3588 = stablehlo.reshape %arg28 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %3589 = stablehlo.custom_call @tt.mark_argument(%3588) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %3590 = stablehlo.reshape %3589 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %3591 = stablehlo.transpose %3590, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1631)
    %3592 = stablehlo.dot_general %3587, %3591, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1632)
    %3593 = stablehlo.reshape %3592 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1630)
    %3594 = stablehlo.reshape %arg27 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3)
    %3595 = stablehlo.custom_call @tt.mark_argument(%3594) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %3596 = stablehlo.reshape %3595 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3)
    %3597 = stablehlo.broadcast_in_dim %3596, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1633)
    %3598 = stablehlo.add %3593, %3597 : tensor<1x257x5120xbf16> loc(#loc1633)
    %3599 = stablehlo.composite "tenstorrent.gelu" %3598 {decomposition = @tenstorrent.gelu.impl_1} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1634)
    %3600 = stablehlo.reshape %3599 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1635)
    %3601 = stablehlo.reshape %arg26 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %3602 = stablehlo.custom_call @tt.mark_argument(%3601) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %3603 = stablehlo.reshape %3602 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %3604 = stablehlo.transpose %3603, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1636)
    %3605 = stablehlo.dot_general %3600, %3604, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1637)
    %3606 = stablehlo.reshape %3605 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1635)
    %3607 = stablehlo.reshape %arg25 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3608 = stablehlo.custom_call @tt.mark_argument(%3607) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3609 = stablehlo.reshape %3608 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3610 = stablehlo.broadcast_in_dim %3609, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1638)
    %3611 = stablehlo.add %3606, %3610 : tensor<1x257x1280xbf16> loc(#loc1638)
    %3612 = stablehlo.add %3579, %3611 : tensor<1x257x1280xbf16> loc(#loc1639)
    %3613 = stablehlo.reshape %arg24 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3614 = stablehlo.custom_call @tt.mark_argument(%3613) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_layer_norm1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3615 = stablehlo.reshape %3614 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3616 = stablehlo.reshape %arg23 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3617 = stablehlo.custom_call @tt.mark_argument(%3616) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_layer_norm1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3618 = stablehlo.reshape %3617 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3619 = stablehlo.composite "tenstorrent.layer_norm" %3612, %3615, %3618 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_31} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1640)
    %3620 = stablehlo.reshape %3619 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1641)
    %3621 = stablehlo.reshape %arg515 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3622 = stablehlo.custom_call @tt.mark_argument(%3621) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_q_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3623 = stablehlo.reshape %3622 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3624 = stablehlo.transpose %3623, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1642)
    %3625 = stablehlo.dot_general %3620, %3624, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1643)
    %3626 = stablehlo.reshape %3625 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1641)
    %3627 = stablehlo.reshape %arg514 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3628 = stablehlo.custom_call @tt.mark_argument(%3627) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_q_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3629 = stablehlo.reshape %3628 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3630 = stablehlo.broadcast_in_dim %3629, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1644)
    %3631 = stablehlo.add %3626, %3630 : tensor<1x257x1280xbf16> loc(#loc1644)
    %3632 = stablehlo.reshape %3631 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1645)
    %3633 = stablehlo.transpose %3632, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1646)
    %3634 = stablehlo.convert %3633 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1647)
    %3635 = stablehlo.multiply %3634, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1648)
    %3636 = stablehlo.reshape %arg513 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3637 = stablehlo.custom_call @tt.mark_argument(%3636) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_k_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3638 = stablehlo.reshape %3637 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3639 = stablehlo.transpose %3638, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1649)
    %3640 = stablehlo.dot_general %3620, %3639, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1650)
    %3641 = stablehlo.reshape %3640 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1651)
    %3642 = stablehlo.reshape %arg512 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3643 = stablehlo.custom_call @tt.mark_argument(%3642) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_k_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3644 = stablehlo.reshape %3643 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3645 = stablehlo.broadcast_in_dim %3644, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1652)
    %3646 = stablehlo.add %3641, %3645 : tensor<1x257x1280xbf16> loc(#loc1652)
    %3647 = stablehlo.reshape %3646 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1653)
    %3648 = stablehlo.transpose %3647, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1654)
    %3649 = stablehlo.convert %3648 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1655)
    %3650 = stablehlo.transpose %3649, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1656)
    %3651 = stablehlo.multiply %3650, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1657)
    %3652 = stablehlo.dot_general %3635, %3651, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1658)
    %3653 = stablehlo.convert %3652 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1659)
    %3654 = stablehlo.compare  EQ, %3653, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1659)
    %3655 = stablehlo.not %3654 : tensor<1x16x257x257xi1> loc(#loc1660)
    %3656 = stablehlo.reduce(%3655 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("2836|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_31aten__any"), %arg559: tensor<i1> loc("2836|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_31aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1662)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1663)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc1661)
    %3657 = stablehlo.reshape %3656 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1661)
    %3658 = stablehlo.not %3657 : tensor<1x16x257x1xi1> loc(#loc1664)
    %3659 = stablehlo.reshape %3658 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1665)
    %3660 = stablehlo.broadcast_in_dim %3659, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1665)
    %3661 = stablehlo.reduce(%3652 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1666)
    %3662 = stablehlo.broadcast_in_dim %3661, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1666)
    %3663 = stablehlo.subtract %3652, %3662 : tensor<1x16x257x257xf32> loc(#loc1666)
    %3664 = stablehlo.exponential %3663 : tensor<1x16x257x257xf32> loc(#loc1666)
    %3665 = stablehlo.reduce(%3664 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1666)
    %3666 = stablehlo.broadcast_in_dim %3665, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1666)
    %3667 = stablehlo.divide %3664, %3666 : tensor<1x16x257x257xf32> loc(#loc1666)
    %3668 = stablehlo.select %3660, %cst_3, %3667 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1667)
    %3669 = stablehlo.reshape %arg22 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3670 = stablehlo.custom_call @tt.mark_argument(%3669) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_v_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3671 = stablehlo.reshape %3670 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3672 = stablehlo.transpose %3671, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1668)
    %3673 = stablehlo.dot_general %3620, %3672, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1669)
    %3674 = stablehlo.reshape %3673 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1670)
    %3675 = stablehlo.reshape %arg21 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3676 = stablehlo.custom_call @tt.mark_argument(%3675) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_v_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3677 = stablehlo.reshape %3676 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3678 = stablehlo.broadcast_in_dim %3677, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1671)
    %3679 = stablehlo.add %3674, %3678 : tensor<1x257x1280xbf16> loc(#loc1671)
    %3680 = stablehlo.reshape %3679 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1672)
    %3681 = stablehlo.transpose %3680, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1673)
    %3682 = stablehlo.convert %3681 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1674)
    %3683 = stablehlo.dot_general %3668, %3682, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1675)
    %3684 = stablehlo.convert %3683 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1676)
    %3685 = stablehlo.transpose %3684, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1677)
    %3686 = stablehlo.reshape %3685 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1678)
    %3687 = stablehlo.reshape %arg20 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3688 = stablehlo.custom_call @tt.mark_argument(%3687) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_out_proj_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3689 = stablehlo.reshape %3688 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3690 = stablehlo.transpose %3689, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1679)
    %3691 = stablehlo.dot_general %3686, %3690, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1680)
    %3692 = stablehlo.reshape %3691 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1678)
    %3693 = stablehlo.reshape %arg19 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3694 = stablehlo.custom_call @tt.mark_argument(%3693) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_out_proj_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3695 = stablehlo.reshape %3694 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3696 = stablehlo.broadcast_in_dim %3695, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1681)
    %3697 = stablehlo.add %3692, %3696 : tensor<1x257x1280xbf16> loc(#loc1681)
    %3698 = stablehlo.add %3612, %3697 : tensor<1x257x1280xbf16> loc(#loc1682)
    %3699 = stablehlo.reshape %arg18 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3700 = stablehlo.custom_call @tt.mark_argument(%3699) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_layer_norm2_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3701 = stablehlo.reshape %3700 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3702 = stablehlo.reshape %arg17 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3703 = stablehlo.custom_call @tt.mark_argument(%3702) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_layer_norm2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3704 = stablehlo.reshape %3703 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3705 = stablehlo.composite "tenstorrent.layer_norm" %3698, %3701, %3704 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_23} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1683)
    %3706 = stablehlo.reshape %3705 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1684)
    %3707 = stablehlo.reshape %arg16 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %3708 = stablehlo.custom_call @tt.mark_argument(%3707) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_mlp_fc1_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %3709 = stablehlo.reshape %3708 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %3710 = stablehlo.transpose %3709, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1685)
    %3711 = stablehlo.dot_general %3706, %3710, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1686)
    %3712 = stablehlo.reshape %3711 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1684)
    %3713 = stablehlo.reshape %arg15 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3)
    %3714 = stablehlo.custom_call @tt.mark_argument(%3713) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_mlp_fc1_bias"}} : (tensor<1x1x5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %3715 = stablehlo.reshape %3714 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3)
    %3716 = stablehlo.broadcast_in_dim %3715, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1687)
    %3717 = stablehlo.add %3712, %3716 : tensor<1x257x5120xbf16> loc(#loc1687)
    %3718 = stablehlo.composite "tenstorrent.gelu" %3717 {decomposition = @tenstorrent.gelu.impl_12} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1688)
    %3719 = stablehlo.reshape %3718 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1689)
    %3720 = stablehlo.reshape %arg14 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %3721 = stablehlo.custom_call @tt.mark_argument(%3720) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_mlp_fc2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %3722 = stablehlo.reshape %3721 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %3723 = stablehlo.transpose %3722, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1690)
    %3724 = stablehlo.dot_general %3719, %3723, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1691)
    %3725 = stablehlo.reshape %3724 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1689)
    %3726 = stablehlo.reshape %arg13 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3727 = stablehlo.custom_call @tt.mark_argument(%3726) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_mlp_fc2_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3728 = stablehlo.reshape %3727 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3729 = stablehlo.broadcast_in_dim %3728, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1692)
    %3730 = stablehlo.add %3725, %3729 : tensor<1x257x1280xbf16> loc(#loc1692)
    %3731 = stablehlo.add %3698, %3730 : tensor<1x257x1280xbf16> loc(#loc1693)
    %3732 = stablehlo.reshape %3731 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1694)
    %3733 = stablehlo.reshape %arg12 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3734 = stablehlo.custom_call @tt.mark_argument(%3733) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_proj_in_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3735 = stablehlo.reshape %3734 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3736 = stablehlo.transpose %3735, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1695)
    %3737 = stablehlo.dot_general %3732, %3736, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1696)
    %3738 = stablehlo.reshape %3737 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1694)
    %3739 = stablehlo.reshape %arg11 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3740 = stablehlo.custom_call @tt.mark_argument(%3739) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_proj_in_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3741 = stablehlo.reshape %3740 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3742 = stablehlo.broadcast_in_dim %3741, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1697)
    %3743 = stablehlo.add %3738, %3742 : tensor<1x257x1280xbf16> loc(#loc1697)
    %3744 = stablehlo.reshape %arg10 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3745 = stablehlo.custom_call @tt.mark_argument(%3744) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_0_ln0_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3746 = stablehlo.reshape %3745 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3747 = stablehlo.reshape %arg9 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3748 = stablehlo.custom_call @tt.mark_argument(%3747) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_0_ln0_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3749 = stablehlo.reshape %3748 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3750 = stablehlo.composite "tenstorrent.layer_norm" %3743, %3746, %3749 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_63} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1698)
    %3751 = stablehlo.concatenate %3750, %7, dim = 1 : (tensor<1x257x1280xbf16>, tensor<1x16x1280xbf16>) -> tensor<1x273x1280xbf16> loc(#loc1699)
    %3752 = stablehlo.reshape %3751 : (tensor<1x273x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1700)
    %3753 = stablehlo.reshape %arg516 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3754 = stablehlo.custom_call @tt.mark_argument(%3753) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_0_attn_to_k_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3755 = stablehlo.reshape %3754 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3756 = stablehlo.transpose %3755, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1701)
    %3757 = stablehlo.dot_general %3752, %3756, contracting_dims = [1] x [0] : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1702)
    %3758 = stablehlo.reshape %3757 : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc1703)
    %3759 = stablehlo.transpose %3758, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,273,64]{3,1,2,0}"} : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc1704)
    %3760 = stablehlo.convert %3759 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,273,64]{3,1,2,0}"} : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc1705)
    %3761 = stablehlo.transpose %3760, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,64,273]{2,1,3,0}"} : (tensor<1x20x273x64xf32>) -> tensor<1x20x64x273xf32> loc(#loc1706)
    %3762 = stablehlo.multiply %3761, %cst_2 : tensor<1x20x64x273xf32> loc(#loc1707)
    %3763 = stablehlo.dot_general %17, %3762, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x20x16x64xf32>, tensor<1x20x64x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc1708)
    %3764 = stablehlo.convert %3763 : (tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf64> loc(#loc1709)
    %3765 = stablehlo.compare  EQ, %3764, %cst_1 : (tensor<1x20x16x273xf64>, tensor<1x20x16x273xf64>) -> tensor<1x20x16x273xi1> loc(#loc1709)
    %3766 = stablehlo.not %3765 : tensor<1x20x16x273xi1> loc(#loc1710)
    %3767 = stablehlo.reduce(%3766 init: %c_9) across dimensions = [3] : (tensor<1x20x16x273xi1>, tensor<i1>) -> tensor<1x20x16xi1>
     reducer(%arg558: tensor<i1> loc("2925|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_33aten__any"), %arg559: tensor<i1> loc("2925|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_33aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1712)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1713)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc1711)
    %3768 = stablehlo.reshape %3767 : (tensor<1x20x16xi1>) -> tensor<1x20x16x1xi1> loc(#loc1711)
    %3769 = stablehlo.not %3768 : tensor<1x20x16x1xi1> loc(#loc1714)
    %3770 = stablehlo.reshape %3769 : (tensor<1x20x16x1xi1>) -> tensor<1x20x16xi1> loc(#loc1715)
    %3771 = stablehlo.broadcast_in_dim %3770, dims = [0, 1, 2] : (tensor<1x20x16xi1>) -> tensor<1x20x16x273xi1> loc(#loc1715)
    %3772 = stablehlo.reduce(%3763 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x20x16x273xf32>, tensor<f32>) -> tensor<1x20x16xf32> loc(#loc1716)
    %3773 = stablehlo.broadcast_in_dim %3772, dims = [0, 1, 2] : (tensor<1x20x16xf32>) -> tensor<1x20x16x273xf32> loc(#loc1716)
    %3774 = stablehlo.subtract %3763, %3773 : tensor<1x20x16x273xf32> loc(#loc1716)
    %3775 = stablehlo.exponential %3774 : tensor<1x20x16x273xf32> loc(#loc1716)
    %3776 = stablehlo.reduce(%3775 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x20x16x273xf32>, tensor<f32>) -> tensor<1x20x16xf32> loc(#loc1716)
    %3777 = stablehlo.broadcast_in_dim %3776, dims = [0, 1, 2] : (tensor<1x20x16xf32>) -> tensor<1x20x16x273xf32> loc(#loc1716)
    %3778 = stablehlo.divide %3775, %3777 : tensor<1x20x16x273xf32> loc(#loc1716)
    %3779 = stablehlo.select %3771, %cst_0, %3778 : tensor<1x20x16x273xi1>, tensor<1x20x16x273xf32> loc(#loc1717)
    %3780 = stablehlo.reshape %arg6 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3781 = stablehlo.custom_call @tt.mark_argument(%3780) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_0_attn_to_v_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3782 = stablehlo.reshape %3781 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3783 = stablehlo.transpose %3782, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1718)
    %3784 = stablehlo.dot_general %3752, %3783, contracting_dims = [1] x [0] : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1719)
    %3785 = stablehlo.reshape %3784 : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc1720)
    %3786 = stablehlo.transpose %3785, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,273,64]{3,1,2,0}"} : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc1721)
    %3787 = stablehlo.convert %3786 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,273,64]{3,1,2,0}"} : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc1722)
    %3788 = stablehlo.dot_general %3779, %3787, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x20x16x273xf32>, tensor<1x20x273x64xf32>) -> tensor<1x20x16x64xf32> loc(#loc1723)
    %3789 = stablehlo.convert %3788 : (tensor<1x20x16x64xf32>) -> tensor<1x20x16x64xbf16> loc(#loc1724)
    %3790 = stablehlo.transpose %3789, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,20,64]{3,1,2,0}"} : (tensor<1x20x16x64xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc1725)
    %3791 = stablehlo.reshape %3790 : (tensor<1x16x20x64xbf16>) -> tensor<16x1280xbf16> loc(#loc1726)
    %3792 = stablehlo.reshape %arg5 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3793 = stablehlo.custom_call @tt.mark_argument(%3792) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_0_attn_to_out_0_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3794 = stablehlo.reshape %3793 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3795 = stablehlo.transpose %3794, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1727)
    %3796 = stablehlo.dot_general %3791, %3795, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1728)
    %3797 = stablehlo.reshape %3796 : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1729)
    %3798 = stablehlo.divide %3797, %cst : tensor<1x16x1280xbf16> loc(#loc1730)
    %3799 = stablehlo.add %3798, %0 : tensor<1x16x1280xbf16> loc(#loc1731)
    %3800 = stablehlo.reshape %arg521 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3801 = stablehlo.custom_call @tt.mark_argument(%3800) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_0_ff_0_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3802 = stablehlo.reshape %3801 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3803 = stablehlo.reshape %arg520 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3804 = stablehlo.custom_call @tt.mark_argument(%3803) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_0_ff_0_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3805 = stablehlo.reshape %3804 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3806 = stablehlo.composite "tenstorrent.layer_norm" %3799, %3802, %3805 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_3} : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1732)
    %3807 = stablehlo.reshape %3806 : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1733)
    %3808 = stablehlo.reshape %arg519 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %3809 = stablehlo.custom_call @tt.mark_argument(%3808) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "getattr_l__self___resampler_layers_0_ff___1___net_0_proj_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %3810 = stablehlo.reshape %3809 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %3811 = stablehlo.transpose %3810, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1734)
    %3812 = stablehlo.dot_general %3807, %3811, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc1735)
    %3813 = stablehlo.reshape %3812 : (tensor<16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc1733)
    %3814 = stablehlo.composite "tenstorrent.gelu" %3813 {decomposition = @tenstorrent.gelu.impl_0} : (tensor<1x16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc1736)
    %3815 = stablehlo.reshape %3814 : (tensor<1x16x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc1737)
    %3816 = stablehlo.reshape %arg518 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %3817 = stablehlo.custom_call @tt.mark_argument(%3816) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "getattr_l__self___resampler_layers_0_ff___1___net_2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %3818 = stablehlo.reshape %3817 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %3819 = stablehlo.transpose %3818, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1738)
    %3820 = stablehlo.dot_general %3815, %3819, contracting_dims = [1] x [0] : (tensor<16x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1739)
    %3821 = stablehlo.reshape %3820 : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1737)
    %3822 = stablehlo.add %3821, %3799 : tensor<1x16x1280xbf16> loc(#loc1740)
    %3823 = stablehlo.reshape %arg525 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3824 = stablehlo.custom_call @tt.mark_argument(%3823) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_1_ln1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3825 = stablehlo.reshape %3824 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3826 = stablehlo.reshape %arg524 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3827 = stablehlo.custom_call @tt.mark_argument(%3826) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_1_ln1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3828 = stablehlo.reshape %3827 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3829 = stablehlo.composite "tenstorrent.layer_norm" %3822, %3825, %3828 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_5} : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1741)
    %3830 = stablehlo.reshape %3829 : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1742)
    %3831 = stablehlo.reshape %arg529 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3832 = stablehlo.custom_call @tt.mark_argument(%3831) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_1_attn_to_q_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3833 = stablehlo.reshape %3832 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3834 = stablehlo.transpose %3833, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1743)
    %3835 = stablehlo.dot_general %3830, %3834, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1744)
    %3836 = stablehlo.reshape %3835 : (tensor<16x1280xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc1745)
    %3837 = stablehlo.transpose %3836, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,16,64]{3,1,2,0}"} : (tensor<1x16x20x64xbf16>) -> tensor<1x20x16x64xbf16> loc(#loc1746)
    %3838 = stablehlo.convert %3837 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,16,64]{3,1,2,0}"} : (tensor<1x20x16x64xbf16>) -> tensor<1x20x16x64xf32> loc(#loc1747)
    %3839 = stablehlo.multiply %3838, %cst_7 : tensor<1x20x16x64xf32> loc(#loc1748)
    %3840 = stablehlo.reshape %arg527 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3841 = stablehlo.custom_call @tt.mark_argument(%3840) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_1_ln0_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3842 = stablehlo.reshape %3841 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3843 = stablehlo.reshape %arg526 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3844 = stablehlo.custom_call @tt.mark_argument(%3843) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_1_ln0_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3845 = stablehlo.reshape %3844 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3846 = stablehlo.composite "tenstorrent.layer_norm" %3743, %3842, %3845 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_67} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1749)
    %3847 = stablehlo.concatenate %3846, %3829, dim = 1 : (tensor<1x257x1280xbf16>, tensor<1x16x1280xbf16>) -> tensor<1x273x1280xbf16> loc(#loc1750)
    %3848 = stablehlo.reshape %3847 : (tensor<1x273x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1751)
    %3849 = stablehlo.reshape %arg528 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3850 = stablehlo.custom_call @tt.mark_argument(%3849) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_1_attn_to_k_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3851 = stablehlo.reshape %3850 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3852 = stablehlo.transpose %3851, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1752)
    %3853 = stablehlo.dot_general %3848, %3852, contracting_dims = [1] x [0] : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1753)
    %3854 = stablehlo.reshape %3853 : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc1754)
    %3855 = stablehlo.transpose %3854, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,273,64]{3,1,2,0}"} : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc1755)
    %3856 = stablehlo.convert %3855 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,273,64]{3,1,2,0}"} : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc1756)
    %3857 = stablehlo.transpose %3856, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,64,273]{2,1,3,0}"} : (tensor<1x20x273x64xf32>) -> tensor<1x20x64x273xf32> loc(#loc1757)
    %3858 = stablehlo.multiply %3857, %cst_2 : tensor<1x20x64x273xf32> loc(#loc1758)
    %3859 = stablehlo.dot_general %3839, %3858, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x20x16x64xf32>, tensor<1x20x64x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc1759)
    %3860 = stablehlo.convert %3859 : (tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf64> loc(#loc1760)
    %3861 = stablehlo.compare  EQ, %3860, %cst_1 : (tensor<1x20x16x273xf64>, tensor<1x20x16x273xf64>) -> tensor<1x20x16x273xi1> loc(#loc1760)
    %3862 = stablehlo.not %3861 : tensor<1x20x16x273xi1> loc(#loc1761)
    %3863 = stablehlo.reduce(%3862 init: %c_9) across dimensions = [3] : (tensor<1x20x16x273xi1>, tensor<i1>) -> tensor<1x20x16xi1>
     reducer(%arg558: tensor<i1> loc("3010|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_34aten__any"), %arg559: tensor<i1> loc("3010|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_34aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1763)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1764)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc1762)
    %3864 = stablehlo.reshape %3863 : (tensor<1x20x16xi1>) -> tensor<1x20x16x1xi1> loc(#loc1762)
    %3865 = stablehlo.not %3864 : tensor<1x20x16x1xi1> loc(#loc1765)
    %3866 = stablehlo.reshape %3865 : (tensor<1x20x16x1xi1>) -> tensor<1x20x16xi1> loc(#loc1766)
    %3867 = stablehlo.broadcast_in_dim %3866, dims = [0, 1, 2] : (tensor<1x20x16xi1>) -> tensor<1x20x16x273xi1> loc(#loc1766)
    %3868 = stablehlo.reduce(%3859 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x20x16x273xf32>, tensor<f32>) -> tensor<1x20x16xf32> loc(#loc1767)
    %3869 = stablehlo.broadcast_in_dim %3868, dims = [0, 1, 2] : (tensor<1x20x16xf32>) -> tensor<1x20x16x273xf32> loc(#loc1767)
    %3870 = stablehlo.subtract %3859, %3869 : tensor<1x20x16x273xf32> loc(#loc1767)
    %3871 = stablehlo.exponential %3870 : tensor<1x20x16x273xf32> loc(#loc1767)
    %3872 = stablehlo.reduce(%3871 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x20x16x273xf32>, tensor<f32>) -> tensor<1x20x16xf32> loc(#loc1767)
    %3873 = stablehlo.broadcast_in_dim %3872, dims = [0, 1, 2] : (tensor<1x20x16xf32>) -> tensor<1x20x16x273xf32> loc(#loc1767)
    %3874 = stablehlo.divide %3871, %3873 : tensor<1x20x16x273xf32> loc(#loc1767)
    %3875 = stablehlo.select %3867, %cst_0, %3874 : tensor<1x20x16x273xi1>, tensor<1x20x16x273xf32> loc(#loc1768)
    %3876 = stablehlo.reshape %arg523 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3877 = stablehlo.custom_call @tt.mark_argument(%3876) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_1_attn_to_v_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3878 = stablehlo.reshape %3877 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3879 = stablehlo.transpose %3878, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1769)
    %3880 = stablehlo.dot_general %3848, %3879, contracting_dims = [1] x [0] : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1770)
    %3881 = stablehlo.reshape %3880 : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc1771)
    %3882 = stablehlo.transpose %3881, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,273,64]{3,1,2,0}"} : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc1772)
    %3883 = stablehlo.convert %3882 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,273,64]{3,1,2,0}"} : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc1773)
    %3884 = stablehlo.dot_general %3875, %3883, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x20x16x273xf32>, tensor<1x20x273x64xf32>) -> tensor<1x20x16x64xf32> loc(#loc1774)
    %3885 = stablehlo.convert %3884 : (tensor<1x20x16x64xf32>) -> tensor<1x20x16x64xbf16> loc(#loc1775)
    %3886 = stablehlo.transpose %3885, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,20,64]{3,1,2,0}"} : (tensor<1x20x16x64xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc1776)
    %3887 = stablehlo.reshape %3886 : (tensor<1x16x20x64xbf16>) -> tensor<16x1280xbf16> loc(#loc1777)
    %3888 = stablehlo.reshape %arg522 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3889 = stablehlo.custom_call @tt.mark_argument(%3888) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_1_attn_to_out_0_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3890 = stablehlo.reshape %3889 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3891 = stablehlo.transpose %3890, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1778)
    %3892 = stablehlo.dot_general %3887, %3891, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1779)
    %3893 = stablehlo.reshape %3892 : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1780)
    %3894 = stablehlo.divide %3893, %cst : tensor<1x16x1280xbf16> loc(#loc1781)
    %3895 = stablehlo.add %3894, %3822 : tensor<1x16x1280xbf16> loc(#loc1782)
    %3896 = stablehlo.reshape %arg533 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3897 = stablehlo.custom_call @tt.mark_argument(%3896) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_1_ff_0_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3898 = stablehlo.reshape %3897 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3899 = stablehlo.reshape %arg532 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3900 = stablehlo.custom_call @tt.mark_argument(%3899) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_1_ff_0_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3901 = stablehlo.reshape %3900 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3902 = stablehlo.composite "tenstorrent.layer_norm" %3895, %3898, %3901 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_17} : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1783)
    %3903 = stablehlo.reshape %3902 : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1784)
    %3904 = stablehlo.reshape %arg531 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %3905 = stablehlo.custom_call @tt.mark_argument(%3904) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "getattr_l__self___resampler_layers_1_ff___1___net_0_proj_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %3906 = stablehlo.reshape %3905 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %3907 = stablehlo.transpose %3906, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1785)
    %3908 = stablehlo.dot_general %3903, %3907, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc1786)
    %3909 = stablehlo.reshape %3908 : (tensor<16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc1784)
    %3910 = stablehlo.composite "tenstorrent.gelu" %3909 {decomposition = @tenstorrent.gelu.impl} : (tensor<1x16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc1787)
    %3911 = stablehlo.reshape %3910 : (tensor<1x16x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc1788)
    %3912 = stablehlo.reshape %arg530 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %3913 = stablehlo.custom_call @tt.mark_argument(%3912) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "getattr_l__self___resampler_layers_1_ff___1___net_2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %3914 = stablehlo.reshape %3913 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %3915 = stablehlo.transpose %3914, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1789)
    %3916 = stablehlo.dot_general %3911, %3915, contracting_dims = [1] x [0] : (tensor<16x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1790)
    %3917 = stablehlo.reshape %3916 : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1788)
    %3918 = stablehlo.add %3917, %3895 : tensor<1x16x1280xbf16> loc(#loc1791)
    %3919 = stablehlo.reshape %arg537 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3920 = stablehlo.custom_call @tt.mark_argument(%3919) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_2_ln1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3921 = stablehlo.reshape %3920 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3922 = stablehlo.reshape %arg536 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3923 = stablehlo.custom_call @tt.mark_argument(%3922) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_2_ln1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3924 = stablehlo.reshape %3923 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3925 = stablehlo.composite "tenstorrent.layer_norm" %3918, %3921, %3924 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_49} : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1792)
    %3926 = stablehlo.reshape %3925 : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1793)
    %3927 = stablehlo.reshape %arg541 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3928 = stablehlo.custom_call @tt.mark_argument(%3927) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_2_attn_to_q_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3929 = stablehlo.reshape %3928 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3930 = stablehlo.transpose %3929, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1794)
    %3931 = stablehlo.dot_general %3926, %3930, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1795)
    %3932 = stablehlo.reshape %3931 : (tensor<16x1280xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc1796)
    %3933 = stablehlo.transpose %3932, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,16,64]{3,1,2,0}"} : (tensor<1x16x20x64xbf16>) -> tensor<1x20x16x64xbf16> loc(#loc1797)
    %3934 = stablehlo.convert %3933 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,16,64]{3,1,2,0}"} : (tensor<1x20x16x64xbf16>) -> tensor<1x20x16x64xf32> loc(#loc1798)
    %3935 = stablehlo.multiply %3934, %cst_7 : tensor<1x20x16x64xf32> loc(#loc1799)
    %3936 = stablehlo.reshape %arg539 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3937 = stablehlo.custom_call @tt.mark_argument(%3936) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_2_ln0_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3938 = stablehlo.reshape %3937 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3939 = stablehlo.reshape %arg538 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3940 = stablehlo.custom_call @tt.mark_argument(%3939) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_2_ln0_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3941 = stablehlo.reshape %3940 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3942 = stablehlo.composite "tenstorrent.layer_norm" %3743, %3938, %3941 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_2} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1800)
    %3943 = stablehlo.concatenate %3942, %3925, dim = 1 : (tensor<1x257x1280xbf16>, tensor<1x16x1280xbf16>) -> tensor<1x273x1280xbf16> loc(#loc1801)
    %3944 = stablehlo.reshape %3943 : (tensor<1x273x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1802)
    %3945 = stablehlo.reshape %arg540 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3946 = stablehlo.custom_call @tt.mark_argument(%3945) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_2_attn_to_k_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3947 = stablehlo.reshape %3946 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3948 = stablehlo.transpose %3947, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1803)
    %3949 = stablehlo.dot_general %3944, %3948, contracting_dims = [1] x [0] : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1804)
    %3950 = stablehlo.reshape %3949 : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc1805)
    %3951 = stablehlo.transpose %3950, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,273,64]{3,1,2,0}"} : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc1806)
    %3952 = stablehlo.convert %3951 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,273,64]{3,1,2,0}"} : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc1807)
    %3953 = stablehlo.transpose %3952, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,64,273]{2,1,3,0}"} : (tensor<1x20x273x64xf32>) -> tensor<1x20x64x273xf32> loc(#loc1808)
    %3954 = stablehlo.multiply %3953, %cst_2 : tensor<1x20x64x273xf32> loc(#loc1809)
    %3955 = stablehlo.dot_general %3935, %3954, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x20x16x64xf32>, tensor<1x20x64x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc1810)
    %3956 = stablehlo.convert %3955 : (tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf64> loc(#loc1811)
    %3957 = stablehlo.compare  EQ, %3956, %cst_1 : (tensor<1x20x16x273xf64>, tensor<1x20x16x273xf64>) -> tensor<1x20x16x273xi1> loc(#loc1811)
    %3958 = stablehlo.not %3957 : tensor<1x20x16x273xi1> loc(#loc1812)
    %3959 = stablehlo.reduce(%3958 init: %c_9) across dimensions = [3] : (tensor<1x20x16x273xi1>, tensor<i1>) -> tensor<1x20x16xi1>
     reducer(%arg558: tensor<i1> loc("3095|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_35aten__any"), %arg559: tensor<i1> loc("3095|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_35aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1814)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1815)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc1813)
    %3960 = stablehlo.reshape %3959 : (tensor<1x20x16xi1>) -> tensor<1x20x16x1xi1> loc(#loc1813)
    %3961 = stablehlo.not %3960 : tensor<1x20x16x1xi1> loc(#loc1816)
    %3962 = stablehlo.reshape %3961 : (tensor<1x20x16x1xi1>) -> tensor<1x20x16xi1> loc(#loc1817)
    %3963 = stablehlo.broadcast_in_dim %3962, dims = [0, 1, 2] : (tensor<1x20x16xi1>) -> tensor<1x20x16x273xi1> loc(#loc1817)
    %3964 = stablehlo.reduce(%3955 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x20x16x273xf32>, tensor<f32>) -> tensor<1x20x16xf32> loc(#loc1818)
    %3965 = stablehlo.broadcast_in_dim %3964, dims = [0, 1, 2] : (tensor<1x20x16xf32>) -> tensor<1x20x16x273xf32> loc(#loc1818)
    %3966 = stablehlo.subtract %3955, %3965 : tensor<1x20x16x273xf32> loc(#loc1818)
    %3967 = stablehlo.exponential %3966 : tensor<1x20x16x273xf32> loc(#loc1818)
    %3968 = stablehlo.reduce(%3967 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x20x16x273xf32>, tensor<f32>) -> tensor<1x20x16xf32> loc(#loc1818)
    %3969 = stablehlo.broadcast_in_dim %3968, dims = [0, 1, 2] : (tensor<1x20x16xf32>) -> tensor<1x20x16x273xf32> loc(#loc1818)
    %3970 = stablehlo.divide %3967, %3969 : tensor<1x20x16x273xf32> loc(#loc1818)
    %3971 = stablehlo.select %3963, %cst_0, %3970 : tensor<1x20x16x273xi1>, tensor<1x20x16x273xf32> loc(#loc1819)
    %3972 = stablehlo.reshape %arg535 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3973 = stablehlo.custom_call @tt.mark_argument(%3972) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_2_attn_to_v_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3974 = stablehlo.reshape %3973 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3975 = stablehlo.transpose %3974, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1820)
    %3976 = stablehlo.dot_general %3944, %3975, contracting_dims = [1] x [0] : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1821)
    %3977 = stablehlo.reshape %3976 : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc1822)
    %3978 = stablehlo.transpose %3977, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,273,64]{3,1,2,0}"} : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc1823)
    %3979 = stablehlo.convert %3978 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,273,64]{3,1,2,0}"} : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc1824)
    %3980 = stablehlo.dot_general %3971, %3979, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x20x16x273xf32>, tensor<1x20x273x64xf32>) -> tensor<1x20x16x64xf32> loc(#loc1825)
    %3981 = stablehlo.convert %3980 : (tensor<1x20x16x64xf32>) -> tensor<1x20x16x64xbf16> loc(#loc1826)
    %3982 = stablehlo.transpose %3981, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,20,64]{3,1,2,0}"} : (tensor<1x20x16x64xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc1827)
    %3983 = stablehlo.reshape %3982 : (tensor<1x16x20x64xbf16>) -> tensor<16x1280xbf16> loc(#loc1828)
    %3984 = stablehlo.reshape %arg534 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %3985 = stablehlo.custom_call @tt.mark_argument(%3984) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_2_attn_to_out_0_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3986 = stablehlo.reshape %3985 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %3987 = stablehlo.transpose %3986, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1829)
    %3988 = stablehlo.dot_general %3983, %3987, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1830)
    %3989 = stablehlo.reshape %3988 : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1831)
    %3990 = stablehlo.divide %3989, %cst : tensor<1x16x1280xbf16> loc(#loc1832)
    %3991 = stablehlo.add %3990, %3918 : tensor<1x16x1280xbf16> loc(#loc1833)
    %3992 = stablehlo.reshape %arg545 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3993 = stablehlo.custom_call @tt.mark_argument(%3992) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_2_ff_0_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3994 = stablehlo.reshape %3993 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3995 = stablehlo.reshape %arg544 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %3996 = stablehlo.custom_call @tt.mark_argument(%3995) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_2_ff_0_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3997 = stablehlo.reshape %3996 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %3998 = stablehlo.composite "tenstorrent.layer_norm" %3991, %3994, %3997 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_26} : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1834)
    %3999 = stablehlo.reshape %3998 : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1835)
    %4000 = stablehlo.reshape %arg543 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %4001 = stablehlo.custom_call @tt.mark_argument(%4000) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "getattr_l__self___resampler_layers_2_ff___1___net_0_proj_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %4002 = stablehlo.reshape %4001 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %4003 = stablehlo.transpose %4002, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1836)
    %4004 = stablehlo.dot_general %3999, %4003, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc1837)
    %4005 = stablehlo.reshape %4004 : (tensor<16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc1835)
    %4006 = stablehlo.composite "tenstorrent.gelu" %4005 {decomposition = @tenstorrent.gelu.impl_3} : (tensor<1x16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc1838)
    %4007 = stablehlo.reshape %4006 : (tensor<1x16x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc1839)
    %4008 = stablehlo.reshape %arg542 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %4009 = stablehlo.custom_call @tt.mark_argument(%4008) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "getattr_l__self___resampler_layers_2_ff___1___net_2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %4010 = stablehlo.reshape %4009 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %4011 = stablehlo.transpose %4010, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1840)
    %4012 = stablehlo.dot_general %4007, %4011, contracting_dims = [1] x [0] : (tensor<16x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1841)
    %4013 = stablehlo.reshape %4012 : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1839)
    %4014 = stablehlo.add %4013, %3991 : tensor<1x16x1280xbf16> loc(#loc1842)
    %4015 = stablehlo.reshape %arg549 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %4016 = stablehlo.custom_call @tt.mark_argument(%4015) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_3_ln1_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %4017 = stablehlo.reshape %4016 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %4018 = stablehlo.reshape %arg548 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %4019 = stablehlo.custom_call @tt.mark_argument(%4018) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_3_ln1_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %4020 = stablehlo.reshape %4019 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %4021 = stablehlo.composite "tenstorrent.layer_norm" %4014, %4017, %4020 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_71} : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1843)
    %4022 = stablehlo.reshape %4021 : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1844)
    %4023 = stablehlo.reshape %arg553 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %4024 = stablehlo.custom_call @tt.mark_argument(%4023) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_3_attn_to_q_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %4025 = stablehlo.reshape %4024 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %4026 = stablehlo.transpose %4025, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1845)
    %4027 = stablehlo.dot_general %4022, %4026, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1846)
    %4028 = stablehlo.reshape %4027 : (tensor<16x1280xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc1847)
    %4029 = stablehlo.transpose %4028, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,16,64]{3,1,2,0}"} : (tensor<1x16x20x64xbf16>) -> tensor<1x20x16x64xbf16> loc(#loc1848)
    %4030 = stablehlo.convert %4029 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,16,64]{3,1,2,0}"} : (tensor<1x20x16x64xbf16>) -> tensor<1x20x16x64xf32> loc(#loc1849)
    %4031 = stablehlo.multiply %4030, %cst_7 : tensor<1x20x16x64xf32> loc(#loc1850)
    %4032 = stablehlo.reshape %arg551 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %4033 = stablehlo.custom_call @tt.mark_argument(%4032) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_3_ln0_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %4034 = stablehlo.reshape %4033 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %4035 = stablehlo.reshape %arg550 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %4036 = stablehlo.custom_call @tt.mark_argument(%4035) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_3_ln0_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %4037 = stablehlo.reshape %4036 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %4038 = stablehlo.composite "tenstorrent.layer_norm" %3743, %4034, %4037 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_1} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1851)
    %4039 = stablehlo.concatenate %4038, %4021, dim = 1 : (tensor<1x257x1280xbf16>, tensor<1x16x1280xbf16>) -> tensor<1x273x1280xbf16> loc(#loc1852)
    %4040 = stablehlo.reshape %4039 : (tensor<1x273x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1853)
    %4041 = stablehlo.reshape %arg552 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %4042 = stablehlo.custom_call @tt.mark_argument(%4041) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_3_attn_to_k_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %4043 = stablehlo.reshape %4042 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %4044 = stablehlo.transpose %4043, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1854)
    %4045 = stablehlo.dot_general %4040, %4044, contracting_dims = [1] x [0] : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1855)
    %4046 = stablehlo.reshape %4045 : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc1856)
    %4047 = stablehlo.transpose %4046, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,273,64]{3,1,2,0}"} : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc1857)
    %4048 = stablehlo.convert %4047 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,273,64]{3,1,2,0}"} : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc1858)
    %4049 = stablehlo.transpose %4048, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,64,273]{2,1,3,0}"} : (tensor<1x20x273x64xf32>) -> tensor<1x20x64x273xf32> loc(#loc1859)
    %4050 = stablehlo.multiply %4049, %cst_2 : tensor<1x20x64x273xf32> loc(#loc1860)
    %4051 = stablehlo.dot_general %4031, %4050, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x20x16x64xf32>, tensor<1x20x64x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc1861)
    %4052 = stablehlo.convert %4051 : (tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf64> loc(#loc1862)
    %4053 = stablehlo.compare  EQ, %4052, %cst_1 : (tensor<1x20x16x273xf64>, tensor<1x20x16x273xf64>) -> tensor<1x20x16x273xi1> loc(#loc1862)
    %4054 = stablehlo.not %4053 : tensor<1x20x16x273xi1> loc(#loc1863)
    %4055 = stablehlo.reduce(%4054 init: %c_9) across dimensions = [3] : (tensor<1x20x16x273xi1>, tensor<i1>) -> tensor<1x20x16xi1>
     reducer(%arg558: tensor<i1> loc("3180|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_36aten__any"), %arg559: tensor<i1> loc("3180|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_36aten__any"))  {
      %4130 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1865)
      %4131 = stablehlo.select %4130, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1866)
      stablehlo.return %4131 : tensor<i1> loc(#loc)
    } loc(#loc1864)
    %4056 = stablehlo.reshape %4055 : (tensor<1x20x16xi1>) -> tensor<1x20x16x1xi1> loc(#loc1864)
    %4057 = stablehlo.not %4056 : tensor<1x20x16x1xi1> loc(#loc1867)
    %4058 = stablehlo.reshape %4057 : (tensor<1x20x16x1xi1>) -> tensor<1x20x16xi1> loc(#loc1868)
    %4059 = stablehlo.broadcast_in_dim %4058, dims = [0, 1, 2] : (tensor<1x20x16xi1>) -> tensor<1x20x16x273xi1> loc(#loc1868)
    %4060 = stablehlo.reduce(%4051 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x20x16x273xf32>, tensor<f32>) -> tensor<1x20x16xf32> loc(#loc1869)
    %4061 = stablehlo.broadcast_in_dim %4060, dims = [0, 1, 2] : (tensor<1x20x16xf32>) -> tensor<1x20x16x273xf32> loc(#loc1869)
    %4062 = stablehlo.subtract %4051, %4061 : tensor<1x20x16x273xf32> loc(#loc1869)
    %4063 = stablehlo.exponential %4062 : tensor<1x20x16x273xf32> loc(#loc1869)
    %4064 = stablehlo.reduce(%4063 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x20x16x273xf32>, tensor<f32>) -> tensor<1x20x16xf32> loc(#loc1869)
    %4065 = stablehlo.broadcast_in_dim %4064, dims = [0, 1, 2] : (tensor<1x20x16xf32>) -> tensor<1x20x16x273xf32> loc(#loc1869)
    %4066 = stablehlo.divide %4063, %4065 : tensor<1x20x16x273xf32> loc(#loc1869)
    %4067 = stablehlo.select %4059, %cst_0, %4066 : tensor<1x20x16x273xi1>, tensor<1x20x16x273xf32> loc(#loc1870)
    %4068 = stablehlo.reshape %arg547 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %4069 = stablehlo.custom_call @tt.mark_argument(%4068) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_3_attn_to_v_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %4070 = stablehlo.reshape %4069 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %4071 = stablehlo.transpose %4070, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1871)
    %4072 = stablehlo.dot_general %4040, %4071, contracting_dims = [1] x [0] : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1872)
    %4073 = stablehlo.reshape %4072 : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc1873)
    %4074 = stablehlo.transpose %4073, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,273,64]{3,1,2,0}"} : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc1874)
    %4075 = stablehlo.convert %4074 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,273,64]{3,1,2,0}"} : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc1875)
    %4076 = stablehlo.dot_general %4067, %4075, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x20x16x273xf32>, tensor<1x20x273x64xf32>) -> tensor<1x20x16x64xf32> loc(#loc1876)
    %4077 = stablehlo.convert %4076 : (tensor<1x20x16x64xf32>) -> tensor<1x20x16x64xbf16> loc(#loc1877)
    %4078 = stablehlo.transpose %4077, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,20,64]{3,1,2,0}"} : (tensor<1x20x16x64xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc1878)
    %4079 = stablehlo.reshape %4078 : (tensor<1x16x20x64xbf16>) -> tensor<16x1280xbf16> loc(#loc1879)
    %4080 = stablehlo.reshape %arg546 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc3)
    %4081 = stablehlo.custom_call @tt.mark_argument(%4080) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_3_attn_to_out_0_weight"}} : (tensor<1x1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %4082 = stablehlo.reshape %4081 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc3)
    %4083 = stablehlo.transpose %4082, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1880)
    %4084 = stablehlo.dot_general %4079, %4083, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1881)
    %4085 = stablehlo.reshape %4084 : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1882)
    %4086 = stablehlo.divide %4085, %cst : tensor<1x16x1280xbf16> loc(#loc1883)
    %4087 = stablehlo.add %4086, %4014 : tensor<1x16x1280xbf16> loc(#loc1884)
    %4088 = stablehlo.reshape %arg557 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %4089 = stablehlo.custom_call @tt.mark_argument(%4088) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_3_ff_0_weight"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %4090 = stablehlo.reshape %4089 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %4091 = stablehlo.reshape %arg556 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc3)
    %4092 = stablehlo.custom_call @tt.mark_argument(%4091) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_layers_3_ff_0_bias"}} : (tensor<1x1x1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %4093 = stablehlo.reshape %4092 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc3)
    %4094 = stablehlo.composite "tenstorrent.layer_norm" %4087, %4090, %4093 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_0} : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1885)
    %4095 = stablehlo.reshape %4094 : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1886)
    %4096 = stablehlo.reshape %arg555 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc3)
    %4097 = stablehlo.custom_call @tt.mark_argument(%4096) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "getattr_l__self___resampler_layers_3_ff___1___net_0_proj_weight"}} : (tensor<1x5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %4098 = stablehlo.reshape %4097 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc3)
    %4099 = stablehlo.transpose %4098, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1887)
    %4100 = stablehlo.dot_general %4095, %4099, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc1888)
    %4101 = stablehlo.reshape %4100 : (tensor<16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc1886)
    %4102 = stablehlo.composite "tenstorrent.gelu" %4101 {decomposition = @tenstorrent.gelu.impl_24} : (tensor<1x16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc1889)
    %4103 = stablehlo.reshape %4102 : (tensor<1x16x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc1890)
    %4104 = stablehlo.reshape %arg554 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc3)
    %4105 = stablehlo.custom_call @tt.mark_argument(%4104) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "getattr_l__self___resampler_layers_3_ff___1___net_2_weight"}} : (tensor<1x1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %4106 = stablehlo.reshape %4105 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc3)
    %4107 = stablehlo.transpose %4106, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1891)
    %4108 = stablehlo.dot_general %4103, %4107, contracting_dims = [1] x [0] : (tensor<16x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1892)
    %4109 = stablehlo.reshape %4108 : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1890)
    %4110 = stablehlo.add %4109, %4087 : tensor<1x16x1280xbf16> loc(#loc1893)
    %4111 = stablehlo.reshape %4110 : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1894)
    %4112 = stablehlo.reshape %arg3 : (tensor<2048x1280xbf16>) -> tensor<1x2048x1280xbf16> loc(#loc3)
    %4113 = stablehlo.custom_call @tt.mark_argument(%4112) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_proj_out_weight"}} : (tensor<1x2048x1280xbf16>) -> tensor<1x2048x1280xbf16> loc(#loc2)
    %4114 = stablehlo.reshape %4113 : (tensor<1x2048x1280xbf16>) -> tensor<2048x1280xbf16> loc(#loc3)
    %4115 = stablehlo.transpose %4114, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,2048]{0,1}"} : (tensor<2048x1280xbf16>) -> tensor<1280x2048xbf16> loc(#loc1895)
    %4116 = stablehlo.dot_general %4111, %4115, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1896)
    %4117 = stablehlo.reshape %4116 : (tensor<16x2048xbf16>) -> tensor<1x16x2048xbf16> loc(#loc1894)
    %4118 = stablehlo.reshape %arg2 : (tensor<2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc3)
    %4119 = stablehlo.custom_call @tt.mark_argument(%4118) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_proj_out_bias"}} : (tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc2)
    %4120 = stablehlo.reshape %4119 : (tensor<1x1x2048xbf16>) -> tensor<2048xbf16> loc(#loc3)
    %4121 = stablehlo.broadcast_in_dim %4120, dims = [2] : (tensor<2048xbf16>) -> tensor<1x16x2048xbf16> loc(#loc1897)
    %4122 = stablehlo.add %4117, %4121 : tensor<1x16x2048xbf16> loc(#loc1897)
    %4123 = stablehlo.reshape %arg1 : (tensor<2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc3)
    %4124 = stablehlo.custom_call @tt.mark_argument(%4123) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_norm_out_weight"}} : (tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc2)
    %4125 = stablehlo.reshape %4124 : (tensor<1x1x2048xbf16>) -> tensor<2048xbf16> loc(#loc3)
    %4126 = stablehlo.reshape %arg0 : (tensor<2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc3)
    %4127 = stablehlo.custom_call @tt.mark_argument(%4126) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___resampler_norm_out_bias"}} : (tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc2)
    %4128 = stablehlo.reshape %4127 : (tensor<1x1x2048xbf16>) -> tensor<2048xbf16> loc(#loc3)
    %4129 = stablehlo.composite "tenstorrent.layer_norm" %4122, %4125, %4128 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<2048> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl} : (tensor<1x16x2048xbf16>, tensor<2048xbf16>, tensor<2048xbf16>) -> tensor<1x16x2048xbf16> loc(#loc1898)
    return %4129 : tensor<1x16x2048xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl(%arg0: tensor<1x16x2048xbf16> loc("3219|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|mark_tensor_384xla__mark_tensor"), %arg1: tensor<2048xbf16> loc("3220|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|mark_tensor_385xla__mark_tensor"), %arg2: tensor<2048xbf16> loc("3221|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|mark_tensor_386xla__mark_tensor")) -> tensor<1x16x2048xbf16> {
    %cst = stablehlo.constant dense<4.8828125E-4> : tensor<1x16xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<9.99999974E-6> : tensor<1x16x1xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x16x2048xbf16>) -> tensor<1x16x2048xf32> loc(#loc1902)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x2048xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc1903)
    %2 = stablehlo.multiply %1, %cst : tensor<1x16xf32> loc(#loc1903)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x2048xf32> loc(#loc1904)
    %4 = stablehlo.subtract %0, %3 : tensor<1x16x2048xf32> loc(#loc1904)
    %5 = stablehlo.multiply %4, %4 : tensor<1x16x2048xf32> loc(#loc1903)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x2048xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc1903)
    %7 = stablehlo.multiply %6, %cst : tensor<1x16xf32> loc(#loc1903)
    %8 = stablehlo.reshape %7 : (tensor<1x16xf32>) -> tensor<1x16x1xf32> loc(#loc1903)
    %9 = stablehlo.add %8, %cst_0 : tensor<1x16x1xf32> loc(#loc1905)
    %10 = stablehlo.rsqrt %9 : tensor<1x16x1xf32> loc(#loc1906)
    %11 = stablehlo.reshape %10 : (tensor<1x16x1xf32>) -> tensor<1x16xf32> loc(#loc1907)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x2048xf32> loc(#loc1907)
    %13 = stablehlo.multiply %4, %12 : tensor<1x16x2048xf32> loc(#loc1907)
    %14 = stablehlo.convert %arg1 : (tensor<2048xbf16>) -> tensor<2048xf32> loc(#loc1908)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<2048xf32>) -> tensor<1x16x2048xf32> loc(#loc1909)
    %16 = stablehlo.multiply %13, %15 : tensor<1x16x2048xf32> loc(#loc1909)
    %17 = stablehlo.convert %arg2 : (tensor<2048xbf16>) -> tensor<2048xf32> loc(#loc1910)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<2048xf32>) -> tensor<1x16x2048xf32> loc(#loc1910)
    %19 = stablehlo.add %16, %18 : tensor<1x16x2048xf32> loc(#loc1910)
    %20 = stablehlo.convert %19 : (tensor<1x16x2048xf32>) -> tensor<1x16x2048xbf16> loc(#loc1911)
    return %20 : tensor<1x16x2048xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_0(%arg0: tensor<1x16x1280xbf16> loc("3194|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_378xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("3195|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_379xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("3196|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_380xla__mark_tensor")) -> tensor<1x16x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x16x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x16xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x16x1280xbf16>) -> tensor<1x16x1280xf32> loc(#loc1915)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc1916)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x16xf32> loc(#loc1916)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc1917)
    %4 = stablehlo.subtract %0, %3 : tensor<1x16x1280xf32> loc(#loc1917)
    %5 = stablehlo.multiply %4, %4 : tensor<1x16x1280xf32> loc(#loc1916)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc1916)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x16xf32> loc(#loc1916)
    %8 = stablehlo.reshape %7 : (tensor<1x16xf32>) -> tensor<1x16x1xf32> loc(#loc1916)
    %9 = stablehlo.add %8, %cst : tensor<1x16x1xf32> loc(#loc1918)
    %10 = stablehlo.rsqrt %9 : tensor<1x16x1xf32> loc(#loc1919)
    %11 = stablehlo.reshape %10 : (tensor<1x16x1xf32>) -> tensor<1x16xf32> loc(#loc1920)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc1920)
    %13 = stablehlo.multiply %4, %12 : tensor<1x16x1280xf32> loc(#loc1920)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc1921)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc1922)
    %16 = stablehlo.multiply %13, %15 : tensor<1x16x1280xf32> loc(#loc1922)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc1923)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc1923)
    %19 = stablehlo.add %16, %18 : tensor<1x16x1280xf32> loc(#loc1923)
    %20 = stablehlo.convert %19 : (tensor<1x16x1280xf32>) -> tensor<1x16x1280xbf16> loc(#loc1924)
    return %20 : tensor<1x16x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_1(%arg0: tensor<1x257x1280xbf16> loc("3131|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_370xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("3132|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_371xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("3133|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_372xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc1928)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc1929)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc1929)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc1930)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc1930)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc1929)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc1929)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc1929)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc1929)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc1931)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc1932)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc1933)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc1933)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc1933)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc1934)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc1935)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc1935)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc1936)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc1936)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc1936)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc1937)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_2(%arg0: tensor<1x257x1280xbf16> loc("3046|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_356xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("3047|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_357xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("3048|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_358xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc1941)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc1942)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc1942)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc1943)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc1943)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc1942)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc1942)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc1942)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc1942)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc1944)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc1945)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc1946)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc1946)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc1946)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc1947)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc1948)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc1948)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc1949)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc1949)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc1949)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc1950)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl(%arg0: tensor<1x16x5120xbf16> loc("3039|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|GELU[getattr(resampler.layers[1].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|mark_tensor_354xla__mark_tensor")) -> tensor<1x16x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x16x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x16x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x16x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x16x5120xbf16> loc(#loc1952)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x16x5120xbf16> loc(#loc1952)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc1952)
    %3 = stablehlo.add %2, %cst : tensor<1x16x5120xbf16> loc(#loc1952)
    %4 = stablehlo.multiply %0, %3 : tensor<1x16x5120xbf16> loc(#loc1952)
    return %4 : tensor<1x16x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_0(%arg0: tensor<1x16x5120xbf16> loc("2954|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|GELU[getattr(resampler.layers[0].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|mark_tensor_340xla__mark_tensor")) -> tensor<1x16x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x16x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x16x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x16x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x16x5120xbf16> loc(#loc1954)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x16x5120xbf16> loc(#loc1954)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc1954)
    %3 = stablehlo.add %2, %cst : tensor<1x16x5120xbf16> loc(#loc1954)
    %4 = stablehlo.multiply %0, %3 : tensor<1x16x5120xbf16> loc(#loc1954)
    return %4 : tensor<1x16x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_3(%arg0: tensor<1x16x1280xbf16> loc("2939|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_336xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("2940|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_337xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("2941|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_338xla__mark_tensor")) -> tensor<1x16x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x16x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x16xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x16x1280xbf16>) -> tensor<1x16x1280xf32> loc(#loc1958)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc1959)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x16xf32> loc(#loc1959)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc1960)
    %4 = stablehlo.subtract %0, %3 : tensor<1x16x1280xf32> loc(#loc1960)
    %5 = stablehlo.multiply %4, %4 : tensor<1x16x1280xf32> loc(#loc1959)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc1959)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x16xf32> loc(#loc1959)
    %8 = stablehlo.reshape %7 : (tensor<1x16xf32>) -> tensor<1x16x1xf32> loc(#loc1959)
    %9 = stablehlo.add %8, %cst : tensor<1x16x1xf32> loc(#loc1961)
    %10 = stablehlo.rsqrt %9 : tensor<1x16x1xf32> loc(#loc1962)
    %11 = stablehlo.reshape %10 : (tensor<1x16x1xf32>) -> tensor<1x16xf32> loc(#loc1963)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc1963)
    %13 = stablehlo.multiply %4, %12 : tensor<1x16x1280xf32> loc(#loc1963)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc1964)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc1965)
    %16 = stablehlo.multiply %13, %15 : tensor<1x16x1280xf32> loc(#loc1965)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc1966)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc1966)
    %19 = stablehlo.add %16, %18 : tensor<1x16x1280xf32> loc(#loc1966)
    %20 = stablehlo.convert %19 : (tensor<1x16x1280xf32>) -> tensor<1x16x1280xbf16> loc(#loc1967)
    return %20 : tensor<1x16x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_1(%arg0: tensor<1x257x5120xbf16> loc("2791|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[29].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_302xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc1969)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc1969)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1969)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc1969)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc1969)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_4(%arg0: tensor<1x257x1280xbf16> loc("2724|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_294xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("2725|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_295xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("2726|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_296xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc1973)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc1974)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc1974)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc1975)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc1975)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc1974)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc1974)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc1974)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc1974)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc1976)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc1977)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc1978)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc1978)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc1978)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc1979)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc1980)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc1980)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc1981)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc1981)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc1981)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc1982)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_2(%arg0: tensor<1x257x5120xbf16> loc("2717|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[28].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_292xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc1984)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc1984)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1984)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc1984)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc1984)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_5(%arg0: tensor<1x16x1280xbf16> loc("2974|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_346xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("2975|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_347xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("2976|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_348xla__mark_tensor")) -> tensor<1x16x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x16x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x16xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x16x1280xbf16>) -> tensor<1x16x1280xf32> loc(#loc1988)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc1989)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x16xf32> loc(#loc1989)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc1990)
    %4 = stablehlo.subtract %0, %3 : tensor<1x16x1280xf32> loc(#loc1990)
    %5 = stablehlo.multiply %4, %4 : tensor<1x16x1280xf32> loc(#loc1989)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc1989)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x16xf32> loc(#loc1989)
    %8 = stablehlo.reshape %7 : (tensor<1x16xf32>) -> tensor<1x16x1xf32> loc(#loc1989)
    %9 = stablehlo.add %8, %cst : tensor<1x16x1xf32> loc(#loc1991)
    %10 = stablehlo.rsqrt %9 : tensor<1x16x1xf32> loc(#loc1992)
    %11 = stablehlo.reshape %10 : (tensor<1x16x1xf32>) -> tensor<1x16xf32> loc(#loc1993)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc1993)
    %13 = stablehlo.multiply %4, %12 : tensor<1x16x1280xf32> loc(#loc1993)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc1994)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc1995)
    %16 = stablehlo.multiply %13, %15 : tensor<1x16x1280xf32> loc(#loc1995)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc1996)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc1996)
    %19 = stablehlo.add %16, %18 : tensor<1x16x1280xf32> loc(#loc1996)
    %20 = stablehlo.convert %19 : (tensor<1x16x1280xf32>) -> tensor<1x16x1280xbf16> loc(#loc1997)
    return %20 : tensor<1x16x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_6(%arg0: tensor<1x257x1280xbf16> loc("2701|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_288xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("2702|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_289xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("2703|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_290xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2001)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2002)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2002)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2003)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2003)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2002)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2002)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2002)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2002)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2004)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2005)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2006)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2006)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2006)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2007)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2008)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2008)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2009)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2009)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2009)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2010)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_7(%arg0: tensor<1x257x1280xbf16> loc("2650|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_284xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("2651|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_285xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("2652|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_286xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2014)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2015)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2015)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2016)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2016)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2015)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2015)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2015)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2015)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2017)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2018)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2019)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2019)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2019)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2020)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2021)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2021)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2022)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2022)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2022)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2023)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_3(%arg0: tensor<1x16x5120xbf16> loc("3124|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|GELU[getattr(resampler.layers[2].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|mark_tensor_368xla__mark_tensor")) -> tensor<1x16x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x16x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x16x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x16x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x16x5120xbf16> loc(#loc2025)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x16x5120xbf16> loc(#loc2025)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc2025)
    %3 = stablehlo.add %2, %cst : tensor<1x16x5120xbf16> loc(#loc2025)
    %4 = stablehlo.multiply %0, %3 : tensor<1x16x5120xbf16> loc(#loc2025)
    return %4 : tensor<1x16x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_8(%arg0: tensor<1x257x1280xbf16> loc("2576|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_274xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("2577|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_275xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("2578|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_276xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2029)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2030)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2030)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2031)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2031)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2030)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2030)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2030)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2030)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2032)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2033)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2034)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2034)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2034)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2035)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2036)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2036)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2037)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2037)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2037)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2038)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_9(%arg0: tensor<1x257x1280xbf16> loc("2553|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_268xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("2554|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_269xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("2555|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_270xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2042)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2043)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2043)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2044)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2044)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2043)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2043)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2043)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2043)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2045)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2046)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2047)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2047)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2047)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2048)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2049)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2049)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2050)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2050)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2050)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2051)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_10(%arg0: tensor<1x257x1280xbf16> loc("2502|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_264xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("2503|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_265xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("2504|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_266xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2055)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2056)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2056)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2057)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2057)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2056)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2056)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2056)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2056)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2058)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2059)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2060)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2060)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2060)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2061)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2062)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2062)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2063)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2063)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2063)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2064)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_4(%arg0: tensor<1x257x5120xbf16> loc("2495|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[25].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_262xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2066)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2066)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2066)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2066)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2066)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_11(%arg0: tensor<1x257x1280xbf16> loc("2479|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_258xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("2480|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_259xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("2481|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_260xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2070)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2071)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2071)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2072)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2072)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2071)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2071)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2071)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2071)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2073)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2074)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2075)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2075)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2075)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2076)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2077)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2077)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2078)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2078)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2078)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2079)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_12(%arg0: tensor<1x257x1280xbf16> loc("2428|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_254xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("2429|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_255xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("2430|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_256xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2083)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2084)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2084)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2085)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2085)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2084)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2084)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2084)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2084)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2086)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2087)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2088)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2088)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2088)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2089)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2090)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2090)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2091)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2091)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2091)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2092)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_5(%arg0: tensor<1x257x5120xbf16> loc("2421|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[24].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_252xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2094)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2094)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2094)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2094)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2094)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_13(%arg0: tensor<1x257x1280xbf16> loc("2405|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_248xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("2406|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_249xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("2407|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_250xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2098)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2099)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2099)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2100)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2100)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2099)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2099)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2099)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2099)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2101)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2102)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2103)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2103)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2103)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2104)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2105)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2105)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2106)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2106)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2106)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2107)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_14(%arg0: tensor<1x257x1280xbf16> loc("2354|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_244xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("2355|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_245xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("2356|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_246xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2111)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2112)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2112)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2113)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2113)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2112)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2112)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2112)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2112)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2114)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2115)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2116)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2116)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2116)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2117)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2118)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2118)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2119)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2119)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2119)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2120)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_6(%arg0: tensor<1x257x5120xbf16> loc("2347|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[23].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_242xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2122)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2122)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2122)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2122)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2122)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_15(%arg0: tensor<1x257x1280xbf16> loc("2331|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_238xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("2332|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_239xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("2333|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_240xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2126)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2127)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2127)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2128)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2128)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2127)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2127)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2127)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2127)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2129)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2130)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2131)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2131)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2131)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2132)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2133)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2133)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2134)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2134)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2134)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2135)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_16(%arg0: tensor<1x257x1280xbf16> loc("2206|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_224xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("2207|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_225xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("2208|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_226xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2139)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2140)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2140)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2141)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2141)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2140)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2140)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2140)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2140)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2142)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2143)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2144)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2144)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2144)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2145)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2146)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2146)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2147)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2147)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2147)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2148)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_7(%arg0: tensor<1x257x5120xbf16> loc("2199|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[21].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_222xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2150)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2150)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2150)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2150)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2150)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_17(%arg0: tensor<1x16x1280xbf16> loc("3024|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_350xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("3025|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_351xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("3026|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_352xla__mark_tensor")) -> tensor<1x16x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x16x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x16xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x16x1280xbf16>) -> tensor<1x16x1280xf32> loc(#loc2154)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc2155)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x16xf32> loc(#loc2155)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc2156)
    %4 = stablehlo.subtract %0, %3 : tensor<1x16x1280xf32> loc(#loc2156)
    %5 = stablehlo.multiply %4, %4 : tensor<1x16x1280xf32> loc(#loc2155)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc2155)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x16xf32> loc(#loc2155)
    %8 = stablehlo.reshape %7 : (tensor<1x16xf32>) -> tensor<1x16x1xf32> loc(#loc2155)
    %9 = stablehlo.add %8, %cst : tensor<1x16x1xf32> loc(#loc2157)
    %10 = stablehlo.rsqrt %9 : tensor<1x16x1xf32> loc(#loc2158)
    %11 = stablehlo.reshape %10 : (tensor<1x16x1xf32>) -> tensor<1x16xf32> loc(#loc2159)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc2159)
    %13 = stablehlo.multiply %4, %12 : tensor<1x16x1280xf32> loc(#loc2159)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2160)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc2161)
    %16 = stablehlo.multiply %13, %15 : tensor<1x16x1280xf32> loc(#loc2161)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2162)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc2162)
    %19 = stablehlo.add %16, %18 : tensor<1x16x1280xf32> loc(#loc2162)
    %20 = stablehlo.convert %19 : (tensor<1x16x1280xf32>) -> tensor<1x16x1280xbf16> loc(#loc2163)
    return %20 : tensor<1x16x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_18(%arg0: tensor<1x257x1280xbf16> loc("2183|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_218xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("2184|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_219xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("2185|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_220xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2167)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2168)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2168)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2169)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2169)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2168)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2168)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2168)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2168)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2170)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2171)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2172)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2172)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2172)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2173)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2174)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2174)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2175)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2175)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2175)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2176)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_8(%arg0: tensor<1x257x5120xbf16> loc("2125|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[20].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_212xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2178)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2178)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2178)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2178)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2178)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_9(%arg0: tensor<1x257x5120xbf16> loc("2569|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[26].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_272xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2180)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2180)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2180)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2180)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2180)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_19(%arg0: tensor<1x257x1280xbf16> loc("2058|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_204xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("2059|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_205xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("2060|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_206xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2184)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2185)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2185)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2186)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2186)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2185)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2185)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2185)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2185)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2187)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2188)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2189)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2189)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2189)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2190)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2191)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2191)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2192)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2192)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2192)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2193)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_10(%arg0: tensor<1x257x5120xbf16> loc("2051|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[19].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_202xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2195)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2195)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2195)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2195)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2195)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_20(%arg0: tensor<1x257x1280xbf16> loc("1984|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_194xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("1985|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_195xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("1986|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_196xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2199)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2200)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2200)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2201)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2201)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2200)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2200)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2200)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2200)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2202)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2203)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2204)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2204)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2204)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2205)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2206)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2206)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2207)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2207)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2207)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2208)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_11(%arg0: tensor<1x257x5120xbf16> loc("1237|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[8].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_92xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2210)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2210)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2210)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2210)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2210)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_21(%arg0: tensor<1x257x1280xbf16> loc("1318|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_104xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("1319|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_105xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("1320|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_106xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2214)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2215)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2215)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2216)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2216)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2215)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2215)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2215)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2215)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2217)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2218)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2219)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2219)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2219)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2220)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2221)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2221)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2222)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2222)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2222)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2223)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_22(%arg0: tensor<1x257x1280xbf16> loc("1170|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_84xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("1171|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_85xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("1172|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_86xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2227)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2228)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2228)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2229)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2229)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2228)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2228)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2228)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2228)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2230)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2231)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2232)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2232)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2232)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2233)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2234)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2234)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2235)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2235)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2235)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2236)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_12(%arg0: tensor<1x257x5120xbf16> loc("2865|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[30].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_312xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2238)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2238)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2238)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2238)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2238)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_13(%arg0: tensor<1x257x5120xbf16> loc("1163|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[7].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_82xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2240)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2240)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2240)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2240)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2240)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_23(%arg0: tensor<1x257x1280xbf16> loc("2849|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_308xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("2850|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_309xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("2851|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_310xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2244)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2245)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2245)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2246)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2246)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2245)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2245)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2245)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2245)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2247)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2248)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2249)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2249)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2249)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2250)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2251)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2251)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2252)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2252)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2252)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2253)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_14(%arg0: tensor<1x257x5120xbf16> loc("2273|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[22].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_232xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2255)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2255)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2255)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2255)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2255)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_24(%arg0: tensor<1x257x1280xbf16> loc("2257|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_228xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("2258|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_229xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("2259|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_230xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2259)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2260)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2260)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2261)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2261)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2260)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2260)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2260)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2260)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2262)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2263)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2264)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2264)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2264)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2265)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2266)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2266)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2267)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2267)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2267)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2268)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_15(%arg0: tensor<1x257x5120xbf16> loc("1089|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[6].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_72xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2270)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2270)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2270)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2270)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2270)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_25(%arg0: tensor<1x257x1280xbf16> loc("1073|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_68xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("1074|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_69xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("1075|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_70xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2274)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2275)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2275)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2276)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2276)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2275)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2275)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2275)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2275)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2277)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2278)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2279)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2279)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2279)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2280)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2281)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2281)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2282)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2282)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2282)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2283)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_16(%arg0: tensor<1x257x5120xbf16> loc("645|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[0].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_12xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2285)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2285)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2285)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2285)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2285)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_26(%arg0: tensor<1x16x1280xbf16> loc("3109|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_364xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("3110|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_365xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("3111|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_366xla__mark_tensor")) -> tensor<1x16x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x16x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x16xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x16x1280xbf16>) -> tensor<1x16x1280xf32> loc(#loc2289)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc2290)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x16xf32> loc(#loc2290)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc2291)
    %4 = stablehlo.subtract %0, %3 : tensor<1x16x1280xf32> loc(#loc2291)
    %5 = stablehlo.multiply %4, %4 : tensor<1x16x1280xf32> loc(#loc2290)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc2290)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x16xf32> loc(#loc2290)
    %8 = stablehlo.reshape %7 : (tensor<1x16xf32>) -> tensor<1x16x1xf32> loc(#loc2290)
    %9 = stablehlo.add %8, %cst : tensor<1x16x1xf32> loc(#loc2292)
    %10 = stablehlo.rsqrt %9 : tensor<1x16x1xf32> loc(#loc2293)
    %11 = stablehlo.reshape %10 : (tensor<1x16x1xf32>) -> tensor<1x16xf32> loc(#loc2294)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc2294)
    %13 = stablehlo.multiply %4, %12 : tensor<1x16x1280xf32> loc(#loc2294)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2295)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc2296)
    %16 = stablehlo.multiply %13, %15 : tensor<1x16x1280xf32> loc(#loc2296)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2297)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc2297)
    %19 = stablehlo.add %16, %18 : tensor<1x16x1280xf32> loc(#loc2297)
    %20 = stablehlo.convert %19 : (tensor<1x16x1280xf32>) -> tensor<1x16x1280xbf16> loc(#loc2298)
    return %20 : tensor<1x16x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_27(%arg0: tensor<1x257x1280xbf16> loc("1221|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_88xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("1222|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_89xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("1223|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_90xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2302)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2303)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2303)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2304)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2304)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2303)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2303)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2303)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2303)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2305)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2306)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2307)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2307)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2307)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2308)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2309)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2309)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2310)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2310)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2310)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2311)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_28(%arg0: tensor<1x257x1280xbf16> loc("726|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_24xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("727|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_25xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("728|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_26xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2315)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2316)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2316)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2317)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2317)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2316)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2316)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2316)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2316)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2318)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2319)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2320)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2320)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2320)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2321)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2322)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2322)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2323)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2323)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2323)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2324)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_29(%arg0: tensor<1x257x1280xbf16> loc("1147|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_78xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("1148|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_79xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("1149|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_80xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2328)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2329)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2329)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2330)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2330)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2329)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2329)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2329)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2329)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2331)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2332)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2333)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2333)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2333)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2334)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2335)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2335)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2336)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2336)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2336)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2337)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_30(%arg0: tensor<1x257x1280xbf16> loc("1836|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_174xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("1837|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_175xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("1838|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_176xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2341)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2342)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2342)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2343)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2343)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2342)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2342)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2342)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2342)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2344)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2345)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2346)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2346)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2346)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2347)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2348)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2348)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2349)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2349)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2349)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2350)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_17(%arg0: tensor<1x257x5120xbf16> loc("941|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[4].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_52xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2352)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2352)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2352)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2352)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2352)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_31(%arg0: tensor<1x257x1280xbf16> loc("2798|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_304xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("2799|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_305xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("2800|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_306xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2356)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2357)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2357)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2358)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2358)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2357)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2357)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2357)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2357)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2359)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2360)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2361)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2361)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2361)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2362)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2363)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2363)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2364)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2364)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2364)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2365)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_32(%arg0: tensor<1x257x1280xbf16> loc("777|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_28xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("778|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_29xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("779|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_30xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2369)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2370)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2370)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2371)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2371)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2370)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2370)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2370)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2370)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2372)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2373)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2374)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2374)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2374)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2375)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2376)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2376)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2377)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2377)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2377)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2378)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_33(%arg0: tensor<1x257x1280xbf16> loc("1665|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_148xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("1666|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_149xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("1667|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_150xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2382)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2383)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2383)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2384)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2384)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2383)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2383)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2383)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2383)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2385)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2386)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2387)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2387)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2387)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2388)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2389)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2389)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2390)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2390)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2390)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2391)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_34(%arg0: tensor<1x257x1280xbf16> loc("578|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_4xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("579|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_5xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("580|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_6xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2395)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2396)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2396)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2397)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2397)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2396)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2396)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2396)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2396)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2398)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2399)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2400)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2400)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2400)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2401)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2402)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2402)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2403)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2403)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2403)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2404)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_18(%arg0: tensor<1x257x5120xbf16> loc("1533|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[12].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_132xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2406)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2406)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2406)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2406)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2406)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_35(%arg0: tensor<1x257x1280xbf16> loc("629|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_8xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("630|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_9xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("631|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_10xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2410)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2411)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2411)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2412)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2412)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2411)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2411)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2411)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2411)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2413)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2414)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2415)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2415)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2415)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2416)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2417)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2417)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2418)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2418)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2418)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2419)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_36(%arg0: tensor<1x257x1280xbf16> loc("703|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_18xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("704|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_19xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("705|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_20xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2423)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2424)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2424)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2425)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2425)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2424)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2424)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2424)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2424)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2426)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2427)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2428)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2428)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2428)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2429)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2430)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2430)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2431)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2431)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2431)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2432)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_19(%arg0: tensor<1x257x5120xbf16> loc("719|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[1].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_22xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2434)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2434)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2434)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2434)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2434)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_37(%arg0: tensor<1x257x1280xbf16> loc("565|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|mark_tensorxla__mark_tensor"), %arg1: tensor<1280xbf16> loc("566|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|mark_tensor_1xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("567|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|mark_tensor_2xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2438)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2439)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2439)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2440)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2440)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2439)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2439)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2439)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2439)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2441)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2442)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2443)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2443)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2443)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2444)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2445)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2445)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2446)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2446)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2446)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2447)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_38(%arg0: tensor<1x257x1280xbf16> loc("925|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_48xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("926|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_49xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("927|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_50xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2451)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2452)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2452)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2453)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2453)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2452)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2452)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2452)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2452)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2454)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2455)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2456)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2456)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2456)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2457)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2458)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2458)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2459)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2459)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2459)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2460)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_39(%arg0: tensor<1x257x1280xbf16> loc("652|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_14xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("653|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_15xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("654|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_16xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2464)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2465)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2465)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2466)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2466)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2465)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2465)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2465)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2465)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2467)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2468)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2469)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2469)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2469)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2470)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2471)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2471)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2472)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2472)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2472)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2473)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_40(%arg0: tensor<1x257x1280xbf16> loc("1688|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_154xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("1689|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_155xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("1690|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_156xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2477)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2478)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2478)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2479)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2479)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2478)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2478)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2478)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2478)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2480)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2481)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2482)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2482)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2482)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2483)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2484)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2484)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2485)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2485)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2485)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2486)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_20(%arg0: tensor<1x257x5120xbf16> loc("867|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[3].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_42xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2488)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2488)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2488)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2488)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2488)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_41(%arg0: tensor<1x257x1280xbf16> loc("999|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_58xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("1000|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_59xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("1001|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_60xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2492)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2493)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2493)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2494)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2494)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2493)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2493)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2493)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2493)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2495)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2496)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2497)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2497)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2497)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2498)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2499)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2499)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2500)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2500)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2500)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2501)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_21(%arg0: tensor<1x257x5120xbf16> loc("793|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[2].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_32xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2503)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2503)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2503)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2503)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2503)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_42(%arg0: tensor<1x257x1280xbf16> loc("948|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_54xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("949|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_55xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("950|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_56xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2507)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2508)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2508)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2509)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2509)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2508)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2508)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2508)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2508)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2510)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2511)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2512)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2512)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2512)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2513)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2514)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2514)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2515)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2515)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2515)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2516)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_43(%arg0: tensor<1x257x1280xbf16> loc("800|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_34xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("801|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_35xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("802|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_36xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2520)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2521)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2521)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2522)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2522)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2521)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2521)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2521)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2521)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2523)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2524)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2525)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2525)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2525)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2526)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2527)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2527)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2528)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2528)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2528)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2529)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_44(%arg0: tensor<1x257x1280xbf16> loc("1762|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_164xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("1763|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_165xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("1764|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_166xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2533)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2534)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2534)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2535)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2535)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2534)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2534)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2534)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2534)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2536)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2537)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2538)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2538)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2538)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2539)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2540)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2540)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2541)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2541)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2541)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2542)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_45(%arg0: tensor<1x257x1280xbf16> loc("1295|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_98xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("1296|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_99xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("1297|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_100xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2546)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2547)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2547)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2548)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2548)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2547)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2547)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2547)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2547)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2549)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2550)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2551)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2551)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2551)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2552)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2553)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2553)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2554)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2554)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2554)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2555)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_46(%arg0: tensor<1x257x1280xbf16> loc("2627|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_278xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("2628|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_279xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("2629|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_280xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2559)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2560)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2560)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2561)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2561)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2560)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2560)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2560)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2560)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2562)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2563)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2564)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2564)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2564)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2565)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2566)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2566)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2567)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2567)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2567)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2568)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_47(%arg0: tensor<1x257x1280xbf16> loc("2109|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_208xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("2110|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_209xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("2111|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_210xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2572)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2573)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2573)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2574)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2574)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2573)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2573)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2573)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2573)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2575)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2576)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2577)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2577)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2577)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2578)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2579)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2579)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2580)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2580)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2580)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2581)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_48(%arg0: tensor<1x257x1280xbf16> loc("1739|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_158xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("1740|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_159xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("1741|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_160xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2585)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2586)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2586)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2587)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2587)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2586)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2586)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2586)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2586)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2588)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2589)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2590)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2590)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2590)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2591)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2592)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2592)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2593)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2593)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2593)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2594)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_22(%arg0: tensor<1x257x5120xbf16> loc("1311|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[9].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_102xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2596)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2596)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2596)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2596)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2596)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_49(%arg0: tensor<1x16x1280xbf16> loc("3059|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_360xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("3060|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_361xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("3061|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_362xla__mark_tensor")) -> tensor<1x16x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x16x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x16xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x16x1280xbf16>) -> tensor<1x16x1280xf32> loc(#loc2600)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc2601)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x16xf32> loc(#loc2601)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc2602)
    %4 = stablehlo.subtract %0, %3 : tensor<1x16x1280xf32> loc(#loc2602)
    %5 = stablehlo.multiply %4, %4 : tensor<1x16x1280xf32> loc(#loc2601)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc2601)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x16xf32> loc(#loc2601)
    %8 = stablehlo.reshape %7 : (tensor<1x16xf32>) -> tensor<1x16x1xf32> loc(#loc2601)
    %9 = stablehlo.add %8, %cst : tensor<1x16x1xf32> loc(#loc2603)
    %10 = stablehlo.rsqrt %9 : tensor<1x16x1xf32> loc(#loc2604)
    %11 = stablehlo.reshape %10 : (tensor<1x16x1xf32>) -> tensor<1x16xf32> loc(#loc2605)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc2605)
    %13 = stablehlo.multiply %4, %12 : tensor<1x16x1280xf32> loc(#loc2605)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2606)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc2607)
    %16 = stablehlo.multiply %13, %15 : tensor<1x16x1280xf32> loc(#loc2607)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2608)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc2608)
    %19 = stablehlo.add %16, %18 : tensor<1x16x1280xf32> loc(#loc2608)
    %20 = stablehlo.convert %19 : (tensor<1x16x1280xf32>) -> tensor<1x16x1280xbf16> loc(#loc2609)
    return %20 : tensor<1x16x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_50(%arg0: tensor<1x257x1280xbf16> loc("1813|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_168xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("1814|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_169xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("1815|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_170xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2613)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2614)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2614)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2615)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2615)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2614)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2614)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2614)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2614)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2616)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2617)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2618)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2618)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2618)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2619)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2620)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2620)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2621)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2621)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2621)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2622)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_51(%arg0: tensor<1x257x1280xbf16> loc("2035|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_198xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("2036|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_199xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("2037|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_200xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2626)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2627)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2627)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2628)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2628)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2627)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2627)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2627)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2627)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2629)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2630)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2631)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2631)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2631)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2632)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2633)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2633)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2634)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2634)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2634)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2635)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_52(%arg0: tensor<1x257x1280xbf16> loc("851|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_38xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("852|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_39xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("853|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_40xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2639)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2640)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2640)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2641)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2641)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2640)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2640)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2640)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2640)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2642)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2643)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2644)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2644)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2644)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2645)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2646)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2646)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2647)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2647)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2647)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2648)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_23(%arg0: tensor<1x257x5120xbf16> loc("1977|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[18].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_192xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2650)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2650)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2650)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2650)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2650)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_24(%arg0: tensor<1x16x5120xbf16> loc("3209|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|GELU[getattr(resampler.layers[3].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|mark_tensor_382xla__mark_tensor")) -> tensor<1x16x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x16x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x16x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x16x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x16x5120xbf16> loc(#loc2652)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x16x5120xbf16> loc(#loc2652)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc2652)
    %3 = stablehlo.add %2, %cst : tensor<1x16x5120xbf16> loc(#loc2652)
    %4 = stablehlo.multiply %0, %3 : tensor<1x16x5120xbf16> loc(#loc2652)
    return %4 : tensor<1x16x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_53(%arg0: tensor<1x257x1280xbf16> loc("1369|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_108xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("1370|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_109xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("1371|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_110xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2656)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2657)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2657)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2658)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2658)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2657)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2657)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2657)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2657)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2659)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2660)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2661)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2661)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2661)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2662)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2663)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2663)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2664)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2664)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2664)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2665)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_54(%arg0: tensor<1x16x1280xbf16> loc("2889|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_332xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("2890|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_333xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("2891|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_334xla__mark_tensor")) -> tensor<1x16x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x16x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x16xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x16x1280xbf16>) -> tensor<1x16x1280xf32> loc(#loc2669)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc2670)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x16xf32> loc(#loc2670)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc2671)
    %4 = stablehlo.subtract %0, %3 : tensor<1x16x1280xf32> loc(#loc2671)
    %5 = stablehlo.multiply %4, %4 : tensor<1x16x1280xf32> loc(#loc2670)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc2670)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x16xf32> loc(#loc2670)
    %8 = stablehlo.reshape %7 : (tensor<1x16xf32>) -> tensor<1x16x1xf32> loc(#loc2670)
    %9 = stablehlo.add %8, %cst : tensor<1x16x1xf32> loc(#loc2672)
    %10 = stablehlo.rsqrt %9 : tensor<1x16x1xf32> loc(#loc2673)
    %11 = stablehlo.reshape %10 : (tensor<1x16x1xf32>) -> tensor<1x16xf32> loc(#loc2674)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc2674)
    %13 = stablehlo.multiply %4, %12 : tensor<1x16x1280xf32> loc(#loc2674)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2675)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc2676)
    %16 = stablehlo.multiply %13, %15 : tensor<1x16x1280xf32> loc(#loc2676)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2677)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc2677)
    %19 = stablehlo.add %16, %18 : tensor<1x16x1280xf32> loc(#loc2677)
    %20 = stablehlo.convert %19 : (tensor<1x16x1280xf32>) -> tensor<1x16x1280xbf16> loc(#loc2678)
    return %20 : tensor<1x16x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_25(%arg0: tensor<1x257x5120xbf16> loc("1385|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[10].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_112xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2680)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2680)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2680)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2680)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2680)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_55(%arg0: tensor<1x257x1280xbf16> loc("2132|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_214xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("2133|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_215xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("2134|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_216xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2684)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2685)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2685)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2686)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2686)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2685)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2685)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2685)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2685)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2687)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2688)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2689)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2689)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2689)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2690)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2691)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2691)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2692)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2692)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2692)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2693)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_56(%arg0: tensor<1x257x1280xbf16> loc("1392|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_114xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("1393|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_115xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("1394|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_116xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2697)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2698)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2698)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2699)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2699)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2698)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2698)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2698)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2698)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2700)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2701)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2702)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2702)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2702)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2703)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2704)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2704)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2705)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2705)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2705)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2706)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_57(%arg0: tensor<1x257x1280xbf16> loc("1443|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_118xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("1444|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_119xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("1445|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_120xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2710)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2711)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2711)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2712)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2712)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2711)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2711)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2711)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2711)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2713)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2714)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2715)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2715)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2715)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2716)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2717)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2717)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2718)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2718)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2718)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2719)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_58(%arg0: tensor<1x257x1280xbf16> loc("2280|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_234xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("2281|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_235xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("2282|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_236xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2723)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2724)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2724)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2725)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2725)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2724)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2724)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2724)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2724)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2726)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2727)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2728)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2728)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2728)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2729)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2730)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2730)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2731)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2731)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2731)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2732)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_59(%arg0: tensor<1x257x1280xbf16> loc("1244|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_94xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("1245|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_95xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("1246|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_96xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2736)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2737)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2737)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2738)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2738)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2737)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2737)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2737)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2737)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2739)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2740)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2741)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2741)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2741)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2742)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2743)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2743)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2744)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2744)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2744)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2745)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_60(%arg0: tensor<1x257x1280xbf16> loc("1517|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_128xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("1518|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_129xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("1519|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_130xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2749)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2750)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2750)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2751)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2751)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2750)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2750)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2750)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2750)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2752)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2753)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2754)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2754)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2754)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2755)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2756)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2756)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2757)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2757)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2757)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2758)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_26(%arg0: tensor<1x257x5120xbf16> loc("1015|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[5].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_62xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2760)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2760)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2760)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2760)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2760)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_61(%arg0: tensor<1x257x1280xbf16> loc("1961|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_188xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("1962|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_189xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("1963|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_190xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2764)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2765)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2765)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2766)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2766)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2765)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2765)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2765)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2765)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2767)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2768)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2769)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2769)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2769)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2770)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2771)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2771)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2772)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2772)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2772)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2773)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_62(%arg0: tensor<1x257x1280xbf16> loc("1591|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_138xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("1592|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_139xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("1593|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_140xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2777)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2778)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2778)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2779)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2779)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2778)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2778)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2778)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2778)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2780)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2781)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2782)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2782)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2782)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2783)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2784)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2784)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2785)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2785)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2785)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2786)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_63(%arg0: tensor<1x257x1280xbf16> loc("2876|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_328xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("2877|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_329xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("2878|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_330xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2790)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2791)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2791)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2792)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2792)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2791)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2791)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2791)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2791)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2793)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2794)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2795)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2795)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2795)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2796)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2797)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2797)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2798)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2798)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2798)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2799)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_64(%arg0: tensor<1x257x1280xbf16> loc("1022|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_64xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("1023|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_65xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("1024|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_66xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2803)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2804)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2804)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2805)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2805)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2804)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2804)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2804)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2804)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2806)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2807)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2808)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2808)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2808)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2809)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2810)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2810)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2811)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2811)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2811)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2812)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_27(%arg0: tensor<1x257x5120xbf16> loc("1755|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[15].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_162xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2814)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2814)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2814)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2814)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2814)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_28(%arg0: tensor<1x257x5120xbf16> loc("1607|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[13].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_142xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2816)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2816)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2816)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2816)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2816)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_29(%arg0: tensor<1x257x5120xbf16> loc("2643|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[27].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_282xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2818)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2818)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2818)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2818)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2818)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_65(%arg0: tensor<1x257x1280xbf16> loc("1096|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_74xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("1097|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_75xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("1098|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_76xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2822)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2823)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2823)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2824)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2824)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2823)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2823)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2823)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2823)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2825)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2826)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2827)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2827)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2827)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2828)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2829)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2829)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2830)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2830)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2830)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2831)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_66(%arg0: tensor<1x257x1280xbf16> loc("1614|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_144xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("1615|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_145xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("1616|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_146xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2835)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2836)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2836)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2837)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2837)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2836)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2836)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2836)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2836)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2838)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2839)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2840)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2840)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2840)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2841)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2842)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2842)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2843)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2843)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2843)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2844)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_67(%arg0: tensor<1x257x1280xbf16> loc("2961|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_342xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("2962|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_343xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("2963|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_344xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2848)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2849)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2849)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2850)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2850)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2849)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2849)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2849)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2849)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2851)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2852)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2853)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2853)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2853)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2854)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2855)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2855)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2856)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2856)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2856)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2857)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_30(%arg0: tensor<1x257x5120xbf16> loc("1459|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[11].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_122xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2859)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2859)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2859)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2859)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2859)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_31(%arg0: tensor<1x257x5120xbf16> loc("1681|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[14].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_152xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2861)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2861)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2861)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2861)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2861)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_68(%arg0: tensor<1x257x1280xbf16> loc("874|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_44xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("875|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_45xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("876|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_46xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2865)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2866)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2866)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2867)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2867)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2866)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2866)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2866)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2866)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2868)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2869)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2870)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2870)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2870)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2871)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2872)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2872)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2873)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2873)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2873)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2874)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_69(%arg0: tensor<1x257x1280xbf16> loc("1466|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_124xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("1467|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_125xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("1468|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_126xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2878)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2879)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2879)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2880)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2880)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2879)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2879)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2879)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2879)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2881)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2882)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2883)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2883)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2883)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2884)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2885)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2885)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2886)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2886)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2886)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2887)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_32(%arg0: tensor<1x257x5120xbf16> loc("1829|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[16].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_172xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2889)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2889)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2889)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2889)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2889)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_70(%arg0: tensor<1x257x1280xbf16> loc("1887|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_178xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("1888|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_179xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("1889|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_180xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2893)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2894)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2894)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2895)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2895)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2894)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2894)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2894)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2894)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2896)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2897)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2898)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2898)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2898)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2899)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2900)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2900)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2901)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2901)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2901)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2902)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_71(%arg0: tensor<1x16x1280xbf16> loc("3144|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_374xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("3145|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_375xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("3146|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_376xla__mark_tensor")) -> tensor<1x16x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x16x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x16xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x16x1280xbf16>) -> tensor<1x16x1280xf32> loc(#loc2906)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc2907)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x16xf32> loc(#loc2907)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc2908)
    %4 = stablehlo.subtract %0, %3 : tensor<1x16x1280xf32> loc(#loc2908)
    %5 = stablehlo.multiply %4, %4 : tensor<1x16x1280xf32> loc(#loc2907)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc2907)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x16xf32> loc(#loc2907)
    %8 = stablehlo.reshape %7 : (tensor<1x16xf32>) -> tensor<1x16x1xf32> loc(#loc2907)
    %9 = stablehlo.add %8, %cst : tensor<1x16x1xf32> loc(#loc2909)
    %10 = stablehlo.rsqrt %9 : tensor<1x16x1xf32> loc(#loc2910)
    %11 = stablehlo.reshape %10 : (tensor<1x16x1xf32>) -> tensor<1x16xf32> loc(#loc2911)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc2911)
    %13 = stablehlo.multiply %4, %12 : tensor<1x16x1280xf32> loc(#loc2911)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2912)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc2913)
    %16 = stablehlo.multiply %13, %15 : tensor<1x16x1280xf32> loc(#loc2913)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2914)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc2914)
    %19 = stablehlo.add %16, %18 : tensor<1x16x1280xf32> loc(#loc2914)
    %20 = stablehlo.convert %19 : (tensor<1x16x1280xf32>) -> tensor<1x16x1280xbf16> loc(#loc2915)
    return %20 : tensor<1x16x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_72(%arg0: tensor<1x257x1280xbf16> loc("2775|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_298xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("2776|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_299xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("2777|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_300xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2919)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2920)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2920)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2921)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2921)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2920)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2920)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2920)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2920)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2922)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2923)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2924)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2924)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2924)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2925)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2926)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2926)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2927)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2927)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2927)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2928)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_73(%arg0: tensor<1x257x1280xbf16> loc("1540|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_134xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("1541|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_135xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("1542|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_136xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2932)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2933)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2933)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2934)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2934)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2933)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2933)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2933)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2933)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2935)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2936)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2937)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2937)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2937)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2938)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2939)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2939)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2940)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2940)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2940)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2941)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_33(%arg0: tensor<1x257x5120xbf16> loc("1903|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[17].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_182xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2943)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2943)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2943)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2943)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2943)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_74(%arg0: tensor<1x257x1280xbf16> loc("1910|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_184xla__mark_tensor"), %arg1: tensor<1280xbf16> loc("1911|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_185xla__mark_tensor"), %arg2: tensor<1280xbf16> loc("1912|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_186xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2947)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2948)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2948)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2949)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2949)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2948)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2948)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2948)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2948)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2950)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2951)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2952)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2952)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2952)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2953)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2954)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2954)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2955)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2955)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2955)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2956)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("-1|unknown|unknown|-1|unknownxla__custom_call")
#loc3 = loc("-1|unknown|unknown|-1|unknownaten__view")
#loc4 = loc("2901|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_335xla__mark_tensor")
#loc5 = loc("2904|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_194aten__view")
#loc6 = loc("2903|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|permute_355aten__permute")
#loc7 = loc("2904|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_194aten__mm")
#loc8 = loc("2909|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|view_129aten__view")
#loc9 = loc("2910|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|permute_358aten__permute")
#loc10 = loc("2915|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_296xla__cast")
#loc11 = loc("2918|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_200aten__mul")
#loc12 = loc("558|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPVisionEmbeddings[image_encoder.vision_model.embeddings]|Conv2d[image_encoder.vision_model.embeddings.patch_embedding]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:195|forward|202|convolutionaten__convolution_overrideable")
#loc13 = loc("559|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPVisionEmbeddings[image_encoder.vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:195|forward|203|viewaten__view")
#loc14 = loc("560|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPVisionEmbeddings[image_encoder.vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:195|forward|203|permuteaten__permute")
#loc15 = loc("562|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPVisionEmbeddings[image_encoder.vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:195|forward|206|cataten__cat")
#loc16 = loc("563|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPVisionEmbeddings[image_encoder.vision_model.embeddings]|Embedding[image_encoder.vision_model.embeddings.position_embedding]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:195|forward|210|embeddingaten__view")
#loc17 = loc("563|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPVisionEmbeddings[image_encoder.vision_model.embeddings]|Embedding[image_encoder.vision_model.embeddings.position_embedding]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:195|forward|210|embeddingaten__index_select")
#loc18 = loc("564|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPVisionEmbeddings[image_encoder.vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:195|forward|210|addaten__add")
#loc19 = loc("577|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|mark_tensor_3xla__mark_tensor")
#loc20 = loc("590|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_7xla__mark_tensor")
#loc21 = loc("592|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmulaten__view")
#loc22 = loc("591|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_1aten__permute")
#loc23 = loc("592|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmulaten__mm")
#loc24 = loc("593|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_5aten__add")
#loc25 = loc("600|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_1aten__view")
#loc26 = loc("601|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_4aten__permute")
#loc27 = loc("606|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_4xla__cast")
#loc28 = loc("609|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_4aten__mul")
#loc29 = loc("594|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_2aten__permute")
#loc30 = loc("595|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_1aten__mm")
#loc31 = loc("595|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_1aten__view")
#loc32 = loc("596|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_6aten__add")
#loc33 = loc("602|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_2aten__view")
#loc34 = loc("603|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_5aten__permute")
#loc35 = loc("607|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_5xla__cast")
#loc36 = loc("610|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_7aten__permute")
#loc37 = loc("611|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_5aten__mul")
#loc38 = loc("613|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmaxaten__einsum")
#loc39 = loc("614|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eqaten__eq")
#loc40 = loc("615|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_notaten__logical_not")
#loc42 = loc("or.2639")
#loc43 = loc("select.2640")
#loc44 = loc("617|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_1aten__logical_not")
#loc45 = loc("619|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|whereaten__expand")
#loc46 = loc("613|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmaxaten__softmax")
#loc47 = loc("619|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|whereaten__where")
#loc48 = loc("597|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_3aten__permute")
#loc49 = loc("598|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_2aten__mm")
#loc50 = loc("598|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_2aten__view")
#loc51 = loc("599|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_7aten__add")
#loc52 = loc("604|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_3aten__view")
#loc53 = loc("605|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_6aten__permute")
#loc54 = loc("608|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_6xla__cast")
#loc55 = loc("621|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_8aten__einsum")
#loc56 = loc("621|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_8xla__cast")
#loc57 = loc("623|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|cloneaten__permute")
#loc58 = loc("626|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_3aten__view")
#loc59 = loc("625|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_9aten__permute")
#loc60 = loc("626|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_3aten__mm")
#loc61 = loc("627|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_8aten__add")
#loc62 = loc("628|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_9aten__add")
#loc63 = loc("641|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_11xla__mark_tensor")
#loc64 = loc("643|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_4aten__view")
#loc65 = loc("642|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_10aten__permute")
#loc66 = loc("643|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_4aten__mm")
#loc67 = loc("644|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_12aten__add")
#loc68 = loc("647|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[0].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_13xla__mark_tensor")
#loc69 = loc("649|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_5aten__view")
#loc70 = loc("648|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_11aten__permute")
#loc71 = loc("649|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_5aten__mm")
#loc72 = loc("650|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_13aten__add")
#loc73 = loc("651|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_14aten__add")
#loc74 = loc("664|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_17xla__mark_tensor")
#loc75 = loc("666|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_6aten__view")
#loc76 = loc("665|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_12aten__permute")
#loc77 = loc("666|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_6aten__mm")
#loc78 = loc("667|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_17aten__add")
#loc79 = loc("674|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_5aten__view")
#loc80 = loc("675|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_15aten__permute")
#loc81 = loc("680|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_13xla__cast")
#loc82 = loc("683|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_10aten__mul")
#loc83 = loc("668|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_13aten__permute")
#loc84 = loc("669|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_7aten__mm")
#loc85 = loc("669|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_7aten__view")
#loc86 = loc("670|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_18aten__add")
#loc87 = loc("676|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_6aten__view")
#loc88 = loc("677|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_16aten__permute")
#loc89 = loc("681|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_14xla__cast")
#loc90 = loc("684|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_18aten__permute")
#loc91 = loc("685|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_11aten__mul")
#loc92 = loc("687|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_1aten__einsum")
#loc93 = loc("688|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_1aten__eq")
#loc94 = loc("689|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_2aten__logical_not")
#loc96 = loc("or.2955")
#loc97 = loc("select.2956")
#loc98 = loc("691|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_3aten__logical_not")
#loc99 = loc("693|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_1aten__expand")
#loc100 = loc("687|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_1aten__softmax")
#loc101 = loc("693|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_1aten__where")
#loc102 = loc("671|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_14aten__permute")
#loc103 = loc("672|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_8aten__mm")
#loc104 = loc("672|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_8aten__view")
#loc105 = loc("673|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_19aten__add")
#loc106 = loc("678|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_7aten__view")
#loc107 = loc("679|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_17aten__permute")
#loc108 = loc("682|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_15xla__cast")
#loc109 = loc("695|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_17aten__einsum")
#loc110 = loc("695|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_17xla__cast")
#loc111 = loc("697|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_1aten__permute")
#loc112 = loc("700|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_9aten__view")
#loc113 = loc("699|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_20aten__permute")
#loc114 = loc("700|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_9aten__mm")
#loc115 = loc("701|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_20aten__add")
#loc116 = loc("702|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_21aten__add")
#loc117 = loc("715|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_21xla__mark_tensor")
#loc118 = loc("717|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_10aten__view")
#loc119 = loc("716|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_21aten__permute")
#loc120 = loc("717|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_10aten__mm")
#loc121 = loc("718|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_24aten__add")
#loc122 = loc("721|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[1].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_23xla__mark_tensor")
#loc123 = loc("723|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_11aten__view")
#loc124 = loc("722|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_22aten__permute")
#loc125 = loc("723|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_11aten__mm")
#loc126 = loc("724|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_25aten__add")
#loc127 = loc("725|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_26aten__add")
#loc128 = loc("738|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_27xla__mark_tensor")
#loc129 = loc("740|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_12aten__view")
#loc130 = loc("739|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_23aten__permute")
#loc131 = loc("740|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_12aten__mm")
#loc132 = loc("741|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_29aten__add")
#loc133 = loc("748|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_9aten__view")
#loc134 = loc("749|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_26aten__permute")
#loc135 = loc("754|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_22xla__cast")
#loc136 = loc("757|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_16aten__mul")
#loc137 = loc("742|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_24aten__permute")
#loc138 = loc("743|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_13aten__mm")
#loc139 = loc("743|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_13aten__view")
#loc140 = loc("744|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_30aten__add")
#loc141 = loc("750|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_10aten__view")
#loc142 = loc("751|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_27aten__permute")
#loc143 = loc("755|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_23xla__cast")
#loc144 = loc("758|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_29aten__permute")
#loc145 = loc("759|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_17aten__mul")
#loc146 = loc("761|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_2aten__einsum")
#loc147 = loc("762|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_2aten__eq")
#loc148 = loc("763|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_4aten__logical_not")
#loc150 = loc("or.3271")
#loc151 = loc("select.3272")
#loc152 = loc("765|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_5aten__logical_not")
#loc153 = loc("767|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_2aten__expand")
#loc154 = loc("761|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_2aten__softmax")
#loc155 = loc("767|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_2aten__where")
#loc156 = loc("745|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_25aten__permute")
#loc157 = loc("746|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_14aten__mm")
#loc158 = loc("746|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_14aten__view")
#loc159 = loc("747|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_31aten__add")
#loc160 = loc("752|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_11aten__view")
#loc161 = loc("753|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_28aten__permute")
#loc162 = loc("756|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_24xla__cast")
#loc163 = loc("769|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_26aten__einsum")
#loc164 = loc("769|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_26xla__cast")
#loc165 = loc("771|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_2aten__permute")
#loc166 = loc("774|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_15aten__view")
#loc167 = loc("773|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_31aten__permute")
#loc168 = loc("774|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_15aten__mm")
#loc169 = loc("775|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_32aten__add")
#loc170 = loc("776|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_33aten__add")
#loc171 = loc("789|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_31xla__mark_tensor")
#loc172 = loc("791|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_16aten__view")
#loc173 = loc("790|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_32aten__permute")
#loc174 = loc("791|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_16aten__mm")
#loc175 = loc("792|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_36aten__add")
#loc176 = loc("795|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[2].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_33xla__mark_tensor")
#loc177 = loc("797|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_17aten__view")
#loc178 = loc("796|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_33aten__permute")
#loc179 = loc("797|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_17aten__mm")
#loc180 = loc("798|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_37aten__add")
#loc181 = loc("799|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_38aten__add")
#loc182 = loc("812|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_37xla__mark_tensor")
#loc183 = loc("814|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_18aten__view")
#loc184 = loc("813|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_34aten__permute")
#loc185 = loc("814|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_18aten__mm")
#loc186 = loc("815|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_41aten__add")
#loc187 = loc("822|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_13aten__view")
#loc188 = loc("823|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_37aten__permute")
#loc189 = loc("828|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_31xla__cast")
#loc190 = loc("831|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_22aten__mul")
#loc191 = loc("816|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_35aten__permute")
#loc192 = loc("817|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_19aten__mm")
#loc193 = loc("817|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_19aten__view")
#loc194 = loc("818|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_42aten__add")
#loc195 = loc("824|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_14aten__view")
#loc196 = loc("825|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_38aten__permute")
#loc197 = loc("829|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_32xla__cast")
#loc198 = loc("832|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_40aten__permute")
#loc199 = loc("833|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_23aten__mul")
#loc200 = loc("835|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_3aten__einsum")
#loc201 = loc("836|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_3aten__eq")
#loc202 = loc("837|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_6aten__logical_not")
#loc204 = loc("or.3587")
#loc205 = loc("select.3588")
#loc206 = loc("839|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_7aten__logical_not")
#loc207 = loc("841|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_3aten__expand")
#loc208 = loc("835|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_3aten__softmax")
#loc209 = loc("841|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_3aten__where")
#loc210 = loc("819|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_36aten__permute")
#loc211 = loc("820|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_20aten__mm")
#loc212 = loc("820|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_20aten__view")
#loc213 = loc("821|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_43aten__add")
#loc214 = loc("826|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_15aten__view")
#loc215 = loc("827|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_39aten__permute")
#loc216 = loc("830|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_33xla__cast")
#loc217 = loc("843|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_35aten__einsum")
#loc218 = loc("843|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_35xla__cast")
#loc219 = loc("845|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_3aten__permute")
#loc220 = loc("848|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_21aten__view")
#loc221 = loc("847|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_42aten__permute")
#loc222 = loc("848|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_21aten__mm")
#loc223 = loc("849|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_44aten__add")
#loc224 = loc("850|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_45aten__add")
#loc225 = loc("863|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_41xla__mark_tensor")
#loc226 = loc("865|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_22aten__view")
#loc227 = loc("864|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_43aten__permute")
#loc228 = loc("865|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_22aten__mm")
#loc229 = loc("866|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_48aten__add")
#loc230 = loc("869|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[3].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_43xla__mark_tensor")
#loc231 = loc("871|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_23aten__view")
#loc232 = loc("870|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_44aten__permute")
#loc233 = loc("871|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_23aten__mm")
#loc234 = loc("872|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_49aten__add")
#loc235 = loc("873|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_50aten__add")
#loc236 = loc("886|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_47xla__mark_tensor")
#loc237 = loc("888|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_24aten__view")
#loc238 = loc("887|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_45aten__permute")
#loc239 = loc("888|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_24aten__mm")
#loc240 = loc("889|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_53aten__add")
#loc241 = loc("896|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_17aten__view")
#loc242 = loc("897|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_48aten__permute")
#loc243 = loc("902|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_40xla__cast")
#loc244 = loc("905|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_28aten__mul")
#loc245 = loc("890|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_46aten__permute")
#loc246 = loc("891|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_25aten__mm")
#loc247 = loc("891|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_25aten__view")
#loc248 = loc("892|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_54aten__add")
#loc249 = loc("898|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_18aten__view")
#loc250 = loc("899|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_49aten__permute")
#loc251 = loc("903|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_41xla__cast")
#loc252 = loc("906|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_51aten__permute")
#loc253 = loc("907|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_29aten__mul")
#loc254 = loc("909|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_4aten__einsum")
#loc255 = loc("910|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_4aten__eq")
#loc256 = loc("911|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_8aten__logical_not")
#loc258 = loc("or.3903")
#loc259 = loc("select.3904")
#loc260 = loc("913|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_9aten__logical_not")
#loc261 = loc("915|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_4aten__expand")
#loc262 = loc("909|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_4aten__softmax")
#loc263 = loc("915|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_4aten__where")
#loc264 = loc("893|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_47aten__permute")
#loc265 = loc("894|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_26aten__mm")
#loc266 = loc("894|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_26aten__view")
#loc267 = loc("895|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_55aten__add")
#loc268 = loc("900|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_19aten__view")
#loc269 = loc("901|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_50aten__permute")
#loc270 = loc("904|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_42xla__cast")
#loc271 = loc("917|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_44aten__einsum")
#loc272 = loc("917|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_44xla__cast")
#loc273 = loc("919|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_4aten__permute")
#loc274 = loc("922|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_27aten__view")
#loc275 = loc("921|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_53aten__permute")
#loc276 = loc("922|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_27aten__mm")
#loc277 = loc("923|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_56aten__add")
#loc278 = loc("924|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_57aten__add")
#loc279 = loc("937|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_51xla__mark_tensor")
#loc280 = loc("939|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_28aten__view")
#loc281 = loc("938|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_54aten__permute")
#loc282 = loc("939|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_28aten__mm")
#loc283 = loc("940|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_60aten__add")
#loc284 = loc("943|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[4].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_53xla__mark_tensor")
#loc285 = loc("945|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_29aten__view")
#loc286 = loc("944|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_55aten__permute")
#loc287 = loc("945|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_29aten__mm")
#loc288 = loc("946|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_61aten__add")
#loc289 = loc("947|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_62aten__add")
#loc290 = loc("960|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_57xla__mark_tensor")
#loc291 = loc("962|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_30aten__view")
#loc292 = loc("961|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_56aten__permute")
#loc293 = loc("962|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_30aten__mm")
#loc294 = loc("963|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_65aten__add")
#loc295 = loc("970|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_21aten__view")
#loc296 = loc("971|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_59aten__permute")
#loc297 = loc("976|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_49xla__cast")
#loc298 = loc("979|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_34aten__mul")
#loc299 = loc("964|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_57aten__permute")
#loc300 = loc("965|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_31aten__mm")
#loc301 = loc("965|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_31aten__view")
#loc302 = loc("966|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_66aten__add")
#loc303 = loc("972|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_22aten__view")
#loc304 = loc("973|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_60aten__permute")
#loc305 = loc("977|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_50xla__cast")
#loc306 = loc("980|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_62aten__permute")
#loc307 = loc("981|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_35aten__mul")
#loc308 = loc("983|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_5aten__einsum")
#loc309 = loc("984|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_5aten__eq")
#loc310 = loc("985|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_10aten__logical_not")
#loc312 = loc("or.4219")
#loc313 = loc("select.4220")
#loc314 = loc("987|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_11aten__logical_not")
#loc315 = loc("989|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_5aten__expand")
#loc316 = loc("983|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_5aten__softmax")
#loc317 = loc("989|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_5aten__where")
#loc318 = loc("967|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_58aten__permute")
#loc319 = loc("968|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_32aten__mm")
#loc320 = loc("968|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_32aten__view")
#loc321 = loc("969|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_67aten__add")
#loc322 = loc("974|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_23aten__view")
#loc323 = loc("975|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_61aten__permute")
#loc324 = loc("978|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_51xla__cast")
#loc325 = loc("991|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_53aten__einsum")
#loc326 = loc("991|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_53xla__cast")
#loc327 = loc("993|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_5aten__permute")
#loc328 = loc("996|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_33aten__view")
#loc329 = loc("995|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_64aten__permute")
#loc330 = loc("996|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_33aten__mm")
#loc331 = loc("997|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_68aten__add")
#loc332 = loc("998|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_69aten__add")
#loc333 = loc("1011|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_61xla__mark_tensor")
#loc334 = loc("1013|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_34aten__view")
#loc335 = loc("1012|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_65aten__permute")
#loc336 = loc("1013|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_34aten__mm")
#loc337 = loc("1014|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_72aten__add")
#loc338 = loc("1017|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[5].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_63xla__mark_tensor")
#loc339 = loc("1019|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_35aten__view")
#loc340 = loc("1018|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_66aten__permute")
#loc341 = loc("1019|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_35aten__mm")
#loc342 = loc("1020|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_73aten__add")
#loc343 = loc("1021|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_74aten__add")
#loc344 = loc("1034|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_67xla__mark_tensor")
#loc345 = loc("1036|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_36aten__view")
#loc346 = loc("1035|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_67aten__permute")
#loc347 = loc("1036|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_36aten__mm")
#loc348 = loc("1037|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_77aten__add")
#loc349 = loc("1044|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_25aten__view")
#loc350 = loc("1045|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_70aten__permute")
#loc351 = loc("1050|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_58xla__cast")
#loc352 = loc("1053|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_40aten__mul")
#loc353 = loc("1038|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_68aten__permute")
#loc354 = loc("1039|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_37aten__mm")
#loc355 = loc("1039|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_37aten__view")
#loc356 = loc("1040|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_78aten__add")
#loc357 = loc("1046|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_26aten__view")
#loc358 = loc("1047|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_71aten__permute")
#loc359 = loc("1051|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_59xla__cast")
#loc360 = loc("1054|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_73aten__permute")
#loc361 = loc("1055|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_41aten__mul")
#loc362 = loc("1057|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_6aten__einsum")
#loc363 = loc("1058|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_6aten__eq")
#loc364 = loc("1059|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_12aten__logical_not")
#loc366 = loc("or.4535")
#loc367 = loc("select.4536")
#loc368 = loc("1061|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_13aten__logical_not")
#loc369 = loc("1063|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_6aten__expand")
#loc370 = loc("1057|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_6aten__softmax")
#loc371 = loc("1063|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_6aten__where")
#loc372 = loc("1041|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_69aten__permute")
#loc373 = loc("1042|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_38aten__mm")
#loc374 = loc("1042|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_38aten__view")
#loc375 = loc("1043|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_79aten__add")
#loc376 = loc("1048|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_27aten__view")
#loc377 = loc("1049|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_72aten__permute")
#loc378 = loc("1052|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_60xla__cast")
#loc379 = loc("1065|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_62aten__einsum")
#loc380 = loc("1065|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_62xla__cast")
#loc381 = loc("1067|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_6aten__permute")
#loc382 = loc("1070|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_39aten__view")
#loc383 = loc("1069|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_75aten__permute")
#loc384 = loc("1070|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_39aten__mm")
#loc385 = loc("1071|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_80aten__add")
#loc386 = loc("1072|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_81aten__add")
#loc387 = loc("1085|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_71xla__mark_tensor")
#loc388 = loc("1087|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_40aten__view")
#loc389 = loc("1086|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_76aten__permute")
#loc390 = loc("1087|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_40aten__mm")
#loc391 = loc("1088|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_84aten__add")
#loc392 = loc("1091|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[6].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_73xla__mark_tensor")
#loc393 = loc("1093|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_41aten__view")
#loc394 = loc("1092|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_77aten__permute")
#loc395 = loc("1093|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_41aten__mm")
#loc396 = loc("1094|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_85aten__add")
#loc397 = loc("1095|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_86aten__add")
#loc398 = loc("1108|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_77xla__mark_tensor")
#loc399 = loc("1110|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_42aten__view")
#loc400 = loc("1109|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_78aten__permute")
#loc401 = loc("1110|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_42aten__mm")
#loc402 = loc("1111|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_89aten__add")
#loc403 = loc("1118|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_29aten__view")
#loc404 = loc("1119|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_81aten__permute")
#loc405 = loc("1124|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_67xla__cast")
#loc406 = loc("1127|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_46aten__mul")
#loc407 = loc("1112|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_79aten__permute")
#loc408 = loc("1113|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_43aten__mm")
#loc409 = loc("1113|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_43aten__view")
#loc410 = loc("1114|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_90aten__add")
#loc411 = loc("1120|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_30aten__view")
#loc412 = loc("1121|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_82aten__permute")
#loc413 = loc("1125|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_68xla__cast")
#loc414 = loc("1128|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_84aten__permute")
#loc415 = loc("1129|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_47aten__mul")
#loc416 = loc("1131|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_7aten__einsum")
#loc417 = loc("1132|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_7aten__eq")
#loc418 = loc("1133|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_14aten__logical_not")
#loc420 = loc("or.4851")
#loc421 = loc("select.4852")
#loc422 = loc("1135|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_15aten__logical_not")
#loc423 = loc("1137|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_7aten__expand")
#loc424 = loc("1131|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_7aten__softmax")
#loc425 = loc("1137|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_7aten__where")
#loc426 = loc("1115|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_80aten__permute")
#loc427 = loc("1116|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_44aten__mm")
#loc428 = loc("1116|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_44aten__view")
#loc429 = loc("1117|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_91aten__add")
#loc430 = loc("1122|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_31aten__view")
#loc431 = loc("1123|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_83aten__permute")
#loc432 = loc("1126|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_69xla__cast")
#loc433 = loc("1139|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_71aten__einsum")
#loc434 = loc("1139|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_71xla__cast")
#loc435 = loc("1141|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_7aten__permute")
#loc436 = loc("1144|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_45aten__view")
#loc437 = loc("1143|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_86aten__permute")
#loc438 = loc("1144|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_45aten__mm")
#loc439 = loc("1145|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_92aten__add")
#loc440 = loc("1146|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_93aten__add")
#loc441 = loc("1159|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_81xla__mark_tensor")
#loc442 = loc("1161|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_46aten__view")
#loc443 = loc("1160|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_87aten__permute")
#loc444 = loc("1161|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_46aten__mm")
#loc445 = loc("1162|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_96aten__add")
#loc446 = loc("1165|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[7].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_83xla__mark_tensor")
#loc447 = loc("1167|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_47aten__view")
#loc448 = loc("1166|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_88aten__permute")
#loc449 = loc("1167|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_47aten__mm")
#loc450 = loc("1168|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_97aten__add")
#loc451 = loc("1169|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_98aten__add")
#loc452 = loc("1182|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_87xla__mark_tensor")
#loc453 = loc("1184|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_48aten__view")
#loc454 = loc("1183|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_89aten__permute")
#loc455 = loc("1184|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_48aten__mm")
#loc456 = loc("1185|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_101aten__add")
#loc457 = loc("1192|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_33aten__view")
#loc458 = loc("1193|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_92aten__permute")
#loc459 = loc("1198|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_76xla__cast")
#loc460 = loc("1201|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_52aten__mul")
#loc461 = loc("1186|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_90aten__permute")
#loc462 = loc("1187|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_49aten__mm")
#loc463 = loc("1187|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_49aten__view")
#loc464 = loc("1188|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_102aten__add")
#loc465 = loc("1194|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_34aten__view")
#loc466 = loc("1195|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_93aten__permute")
#loc467 = loc("1199|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_77xla__cast")
#loc468 = loc("1202|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_95aten__permute")
#loc469 = loc("1203|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_53aten__mul")
#loc470 = loc("1205|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_8aten__einsum")
#loc471 = loc("1206|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_8aten__eq")
#loc472 = loc("1207|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_16aten__logical_not")
#loc474 = loc("or.5167")
#loc475 = loc("select.5168")
#loc476 = loc("1209|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_17aten__logical_not")
#loc477 = loc("1211|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_8aten__expand")
#loc478 = loc("1205|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_8aten__softmax")
#loc479 = loc("1211|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_8aten__where")
#loc480 = loc("1189|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_91aten__permute")
#loc481 = loc("1190|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_50aten__mm")
#loc482 = loc("1190|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_50aten__view")
#loc483 = loc("1191|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_103aten__add")
#loc484 = loc("1196|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_35aten__view")
#loc485 = loc("1197|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_94aten__permute")
#loc486 = loc("1200|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_78xla__cast")
#loc487 = loc("1213|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_80aten__einsum")
#loc488 = loc("1213|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_80xla__cast")
#loc489 = loc("1215|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_8aten__permute")
#loc490 = loc("1218|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_51aten__view")
#loc491 = loc("1217|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_97aten__permute")
#loc492 = loc("1218|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_51aten__mm")
#loc493 = loc("1219|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_104aten__add")
#loc494 = loc("1220|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_105aten__add")
#loc495 = loc("1233|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_91xla__mark_tensor")
#loc496 = loc("1235|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_52aten__view")
#loc497 = loc("1234|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_98aten__permute")
#loc498 = loc("1235|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_52aten__mm")
#loc499 = loc("1236|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_108aten__add")
#loc500 = loc("1239|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[8].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_93xla__mark_tensor")
#loc501 = loc("1241|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_53aten__view")
#loc502 = loc("1240|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_99aten__permute")
#loc503 = loc("1241|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_53aten__mm")
#loc504 = loc("1242|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_109aten__add")
#loc505 = loc("1243|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_110aten__add")
#loc506 = loc("1256|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_97xla__mark_tensor")
#loc507 = loc("1258|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_54aten__view")
#loc508 = loc("1257|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_100aten__permute")
#loc509 = loc("1258|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_54aten__mm")
#loc510 = loc("1259|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_113aten__add")
#loc511 = loc("1266|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_37aten__view")
#loc512 = loc("1267|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_103aten__permute")
#loc513 = loc("1272|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_85xla__cast")
#loc514 = loc("1275|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_58aten__mul")
#loc515 = loc("1260|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_101aten__permute")
#loc516 = loc("1261|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_55aten__mm")
#loc517 = loc("1261|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_55aten__view")
#loc518 = loc("1262|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_114aten__add")
#loc519 = loc("1268|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_38aten__view")
#loc520 = loc("1269|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_104aten__permute")
#loc521 = loc("1273|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_86xla__cast")
#loc522 = loc("1276|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_106aten__permute")
#loc523 = loc("1277|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_59aten__mul")
#loc524 = loc("1279|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_9aten__einsum")
#loc525 = loc("1280|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_9aten__eq")
#loc526 = loc("1281|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_18aten__logical_not")
#loc528 = loc("or.5483")
#loc529 = loc("select.5484")
#loc530 = loc("1283|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_19aten__logical_not")
#loc531 = loc("1285|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_9aten__expand")
#loc532 = loc("1279|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_9aten__softmax")
#loc533 = loc("1285|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_9aten__where")
#loc534 = loc("1263|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_102aten__permute")
#loc535 = loc("1264|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_56aten__mm")
#loc536 = loc("1264|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_56aten__view")
#loc537 = loc("1265|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_115aten__add")
#loc538 = loc("1270|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_39aten__view")
#loc539 = loc("1271|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_105aten__permute")
#loc540 = loc("1274|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_87xla__cast")
#loc541 = loc("1287|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_89aten__einsum")
#loc542 = loc("1287|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_89xla__cast")
#loc543 = loc("1289|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_9aten__permute")
#loc544 = loc("1292|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_57aten__view")
#loc545 = loc("1291|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_108aten__permute")
#loc546 = loc("1292|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_57aten__mm")
#loc547 = loc("1293|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_116aten__add")
#loc548 = loc("1294|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_117aten__add")
#loc549 = loc("1307|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_101xla__mark_tensor")
#loc550 = loc("1309|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_58aten__view")
#loc551 = loc("1308|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_109aten__permute")
#loc552 = loc("1309|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_58aten__mm")
#loc553 = loc("1310|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_120aten__add")
#loc554 = loc("1313|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[9].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_103xla__mark_tensor")
#loc555 = loc("1315|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_59aten__view")
#loc556 = loc("1314|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_110aten__permute")
#loc557 = loc("1315|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_59aten__mm")
#loc558 = loc("1316|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_121aten__add")
#loc559 = loc("1317|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_122aten__add")
#loc560 = loc("1330|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_107xla__mark_tensor")
#loc561 = loc("1332|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_60aten__view")
#loc562 = loc("1331|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_111aten__permute")
#loc563 = loc("1332|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_60aten__mm")
#loc564 = loc("1333|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_125aten__add")
#loc565 = loc("1340|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_41aten__view")
#loc566 = loc("1341|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_114aten__permute")
#loc567 = loc("1346|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_94xla__cast")
#loc568 = loc("1349|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_64aten__mul")
#loc569 = loc("1334|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_112aten__permute")
#loc570 = loc("1335|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_61aten__mm")
#loc571 = loc("1335|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_61aten__view")
#loc572 = loc("1336|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_126aten__add")
#loc573 = loc("1342|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_42aten__view")
#loc574 = loc("1343|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_115aten__permute")
#loc575 = loc("1347|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_95xla__cast")
#loc576 = loc("1350|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_117aten__permute")
#loc577 = loc("1351|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_65aten__mul")
#loc578 = loc("1353|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_10aten__einsum")
#loc579 = loc("1354|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_10aten__eq")
#loc580 = loc("1355|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_20aten__logical_not")
#loc582 = loc("or.5799")
#loc583 = loc("select.5800")
#loc584 = loc("1357|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_21aten__logical_not")
#loc585 = loc("1359|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_10aten__expand")
#loc586 = loc("1353|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_10aten__softmax")
#loc587 = loc("1359|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_10aten__where")
#loc588 = loc("1337|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_113aten__permute")
#loc589 = loc("1338|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_62aten__mm")
#loc590 = loc("1338|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_62aten__view")
#loc591 = loc("1339|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_127aten__add")
#loc592 = loc("1344|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_43aten__view")
#loc593 = loc("1345|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_116aten__permute")
#loc594 = loc("1348|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_96xla__cast")
#loc595 = loc("1361|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_98aten__einsum")
#loc596 = loc("1361|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_98xla__cast")
#loc597 = loc("1363|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_10aten__permute")
#loc598 = loc("1366|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_63aten__view")
#loc599 = loc("1365|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_119aten__permute")
#loc600 = loc("1366|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_63aten__mm")
#loc601 = loc("1367|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_128aten__add")
#loc602 = loc("1368|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_129aten__add")
#loc603 = loc("1381|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_111xla__mark_tensor")
#loc604 = loc("1383|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_64aten__view")
#loc605 = loc("1382|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_120aten__permute")
#loc606 = loc("1383|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_64aten__mm")
#loc607 = loc("1384|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_132aten__add")
#loc608 = loc("1387|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[10].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_113xla__mark_tensor")
#loc609 = loc("1389|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_65aten__view")
#loc610 = loc("1388|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_121aten__permute")
#loc611 = loc("1389|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_65aten__mm")
#loc612 = loc("1390|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_133aten__add")
#loc613 = loc("1391|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_134aten__add")
#loc614 = loc("1404|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_117xla__mark_tensor")
#loc615 = loc("1406|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_66aten__view")
#loc616 = loc("1405|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_122aten__permute")
#loc617 = loc("1406|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_66aten__mm")
#loc618 = loc("1407|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_137aten__add")
#loc619 = loc("1414|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_45aten__view")
#loc620 = loc("1415|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_125aten__permute")
#loc621 = loc("1420|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_103xla__cast")
#loc622 = loc("1423|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_70aten__mul")
#loc623 = loc("1408|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_123aten__permute")
#loc624 = loc("1409|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_67aten__mm")
#loc625 = loc("1409|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_67aten__view")
#loc626 = loc("1410|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_138aten__add")
#loc627 = loc("1416|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_46aten__view")
#loc628 = loc("1417|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_126aten__permute")
#loc629 = loc("1421|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_104xla__cast")
#loc630 = loc("1424|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_128aten__permute")
#loc631 = loc("1425|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_71aten__mul")
#loc632 = loc("1427|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_11aten__einsum")
#loc633 = loc("1428|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_11aten__eq")
#loc634 = loc("1429|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_22aten__logical_not")
#loc636 = loc("or.6115")
#loc637 = loc("select.6116")
#loc638 = loc("1431|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_23aten__logical_not")
#loc639 = loc("1433|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_11aten__expand")
#loc640 = loc("1427|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_11aten__softmax")
#loc641 = loc("1433|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_11aten__where")
#loc642 = loc("1411|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_124aten__permute")
#loc643 = loc("1412|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_68aten__mm")
#loc644 = loc("1412|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_68aten__view")
#loc645 = loc("1413|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_139aten__add")
#loc646 = loc("1418|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_47aten__view")
#loc647 = loc("1419|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_127aten__permute")
#loc648 = loc("1422|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_105xla__cast")
#loc649 = loc("1435|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_107aten__einsum")
#loc650 = loc("1435|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_107xla__cast")
#loc651 = loc("1437|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_11aten__permute")
#loc652 = loc("1440|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_69aten__view")
#loc653 = loc("1439|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_130aten__permute")
#loc654 = loc("1440|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_69aten__mm")
#loc655 = loc("1441|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_140aten__add")
#loc656 = loc("1442|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_141aten__add")
#loc657 = loc("1455|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_121xla__mark_tensor")
#loc658 = loc("1457|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_70aten__view")
#loc659 = loc("1456|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_131aten__permute")
#loc660 = loc("1457|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_70aten__mm")
#loc661 = loc("1458|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_144aten__add")
#loc662 = loc("1461|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[11].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_123xla__mark_tensor")
#loc663 = loc("1463|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_71aten__view")
#loc664 = loc("1462|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_132aten__permute")
#loc665 = loc("1463|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_71aten__mm")
#loc666 = loc("1464|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_145aten__add")
#loc667 = loc("1465|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_146aten__add")
#loc668 = loc("1478|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_127xla__mark_tensor")
#loc669 = loc("1480|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_72aten__view")
#loc670 = loc("1479|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_133aten__permute")
#loc671 = loc("1480|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_72aten__mm")
#loc672 = loc("1481|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_149aten__add")
#loc673 = loc("1488|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_49aten__view")
#loc674 = loc("1489|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_136aten__permute")
#loc675 = loc("1494|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_112xla__cast")
#loc676 = loc("1497|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_76aten__mul")
#loc677 = loc("1482|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_134aten__permute")
#loc678 = loc("1483|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_73aten__mm")
#loc679 = loc("1483|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_73aten__view")
#loc680 = loc("1484|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_150aten__add")
#loc681 = loc("1490|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_50aten__view")
#loc682 = loc("1491|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_137aten__permute")
#loc683 = loc("1495|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_113xla__cast")
#loc684 = loc("1498|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_139aten__permute")
#loc685 = loc("1499|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_77aten__mul")
#loc686 = loc("1501|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_12aten__einsum")
#loc687 = loc("1502|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_12aten__eq")
#loc688 = loc("1503|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_24aten__logical_not")
#loc690 = loc("or.6431")
#loc691 = loc("select.6432")
#loc692 = loc("1505|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_25aten__logical_not")
#loc693 = loc("1507|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_12aten__expand")
#loc694 = loc("1501|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_12aten__softmax")
#loc695 = loc("1507|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_12aten__where")
#loc696 = loc("1485|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_135aten__permute")
#loc697 = loc("1486|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_74aten__mm")
#loc698 = loc("1486|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_74aten__view")
#loc699 = loc("1487|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_151aten__add")
#loc700 = loc("1492|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_51aten__view")
#loc701 = loc("1493|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_138aten__permute")
#loc702 = loc("1496|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_114xla__cast")
#loc703 = loc("1509|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_116aten__einsum")
#loc704 = loc("1509|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_116xla__cast")
#loc705 = loc("1511|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_12aten__permute")
#loc706 = loc("1514|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_75aten__view")
#loc707 = loc("1513|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_141aten__permute")
#loc708 = loc("1514|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_75aten__mm")
#loc709 = loc("1515|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_152aten__add")
#loc710 = loc("1516|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_153aten__add")
#loc711 = loc("1529|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_131xla__mark_tensor")
#loc712 = loc("1531|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_76aten__view")
#loc713 = loc("1530|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_142aten__permute")
#loc714 = loc("1531|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_76aten__mm")
#loc715 = loc("1532|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_156aten__add")
#loc716 = loc("1535|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[12].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_133xla__mark_tensor")
#loc717 = loc("1537|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_77aten__view")
#loc718 = loc("1536|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_143aten__permute")
#loc719 = loc("1537|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_77aten__mm")
#loc720 = loc("1538|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_157aten__add")
#loc721 = loc("1539|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_158aten__add")
#loc722 = loc("1552|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_137xla__mark_tensor")
#loc723 = loc("1554|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_78aten__view")
#loc724 = loc("1553|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_144aten__permute")
#loc725 = loc("1554|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_78aten__mm")
#loc726 = loc("1555|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_161aten__add")
#loc727 = loc("1562|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_53aten__view")
#loc728 = loc("1563|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_147aten__permute")
#loc729 = loc("1568|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_121xla__cast")
#loc730 = loc("1571|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_82aten__mul")
#loc731 = loc("1556|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_145aten__permute")
#loc732 = loc("1557|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_79aten__mm")
#loc733 = loc("1557|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_79aten__view")
#loc734 = loc("1558|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_162aten__add")
#loc735 = loc("1564|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_54aten__view")
#loc736 = loc("1565|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_148aten__permute")
#loc737 = loc("1569|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_122xla__cast")
#loc738 = loc("1572|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_150aten__permute")
#loc739 = loc("1573|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_83aten__mul")
#loc740 = loc("1575|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_13aten__einsum")
#loc741 = loc("1576|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_13aten__eq")
#loc742 = loc("1577|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_26aten__logical_not")
#loc744 = loc("or.6747")
#loc745 = loc("select.6748")
#loc746 = loc("1579|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_27aten__logical_not")
#loc747 = loc("1581|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_13aten__expand")
#loc748 = loc("1575|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_13aten__softmax")
#loc749 = loc("1581|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_13aten__where")
#loc750 = loc("1559|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_146aten__permute")
#loc751 = loc("1560|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_80aten__mm")
#loc752 = loc("1560|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_80aten__view")
#loc753 = loc("1561|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_163aten__add")
#loc754 = loc("1566|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_55aten__view")
#loc755 = loc("1567|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_149aten__permute")
#loc756 = loc("1570|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_123xla__cast")
#loc757 = loc("1583|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_125aten__einsum")
#loc758 = loc("1583|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_125xla__cast")
#loc759 = loc("1585|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_13aten__permute")
#loc760 = loc("1588|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_81aten__view")
#loc761 = loc("1587|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_152aten__permute")
#loc762 = loc("1588|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_81aten__mm")
#loc763 = loc("1589|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_164aten__add")
#loc764 = loc("1590|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_165aten__add")
#loc765 = loc("1603|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_141xla__mark_tensor")
#loc766 = loc("1605|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_82aten__view")
#loc767 = loc("1604|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_153aten__permute")
#loc768 = loc("1605|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_82aten__mm")
#loc769 = loc("1606|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_168aten__add")
#loc770 = loc("1609|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[13].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_143xla__mark_tensor")
#loc771 = loc("1611|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_83aten__view")
#loc772 = loc("1610|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_154aten__permute")
#loc773 = loc("1611|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_83aten__mm")
#loc774 = loc("1612|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_169aten__add")
#loc775 = loc("1613|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_170aten__add")
#loc776 = loc("1626|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_147xla__mark_tensor")
#loc777 = loc("1628|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_84aten__view")
#loc778 = loc("1627|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_155aten__permute")
#loc779 = loc("1628|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_84aten__mm")
#loc780 = loc("1629|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_173aten__add")
#loc781 = loc("1636|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_57aten__view")
#loc782 = loc("1637|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_158aten__permute")
#loc783 = loc("1642|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_130xla__cast")
#loc784 = loc("1645|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_88aten__mul")
#loc785 = loc("1630|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_156aten__permute")
#loc786 = loc("1631|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_85aten__mm")
#loc787 = loc("1631|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_85aten__view")
#loc788 = loc("1632|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_174aten__add")
#loc789 = loc("1638|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_58aten__view")
#loc790 = loc("1639|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_159aten__permute")
#loc791 = loc("1643|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_131xla__cast")
#loc792 = loc("1646|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_161aten__permute")
#loc793 = loc("1647|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_89aten__mul")
#loc794 = loc("1649|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_14aten__einsum")
#loc795 = loc("1650|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_14aten__eq")
#loc796 = loc("1651|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_28aten__logical_not")
#loc798 = loc("or.7063")
#loc799 = loc("select.7064")
#loc800 = loc("1653|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_29aten__logical_not")
#loc801 = loc("1655|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_14aten__expand")
#loc802 = loc("1649|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_14aten__softmax")
#loc803 = loc("1655|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_14aten__where")
#loc804 = loc("1633|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_157aten__permute")
#loc805 = loc("1634|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_86aten__mm")
#loc806 = loc("1634|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_86aten__view")
#loc807 = loc("1635|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_175aten__add")
#loc808 = loc("1640|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_59aten__view")
#loc809 = loc("1641|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_160aten__permute")
#loc810 = loc("1644|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_132xla__cast")
#loc811 = loc("1657|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_134aten__einsum")
#loc812 = loc("1657|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_134xla__cast")
#loc813 = loc("1659|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_14aten__permute")
#loc814 = loc("1662|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_87aten__view")
#loc815 = loc("1661|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_163aten__permute")
#loc816 = loc("1662|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_87aten__mm")
#loc817 = loc("1663|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_176aten__add")
#loc818 = loc("1664|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_177aten__add")
#loc819 = loc("1677|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_151xla__mark_tensor")
#loc820 = loc("1679|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_88aten__view")
#loc821 = loc("1678|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_164aten__permute")
#loc822 = loc("1679|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_88aten__mm")
#loc823 = loc("1680|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_180aten__add")
#loc824 = loc("1683|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[14].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_153xla__mark_tensor")
#loc825 = loc("1685|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_89aten__view")
#loc826 = loc("1684|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_165aten__permute")
#loc827 = loc("1685|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_89aten__mm")
#loc828 = loc("1686|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_181aten__add")
#loc829 = loc("1687|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_182aten__add")
#loc830 = loc("1700|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_157xla__mark_tensor")
#loc831 = loc("1702|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_90aten__view")
#loc832 = loc("1701|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_166aten__permute")
#loc833 = loc("1702|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_90aten__mm")
#loc834 = loc("1703|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_185aten__add")
#loc835 = loc("1710|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_61aten__view")
#loc836 = loc("1711|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_169aten__permute")
#loc837 = loc("1716|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_139xla__cast")
#loc838 = loc("1719|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_94aten__mul")
#loc839 = loc("1704|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_167aten__permute")
#loc840 = loc("1705|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_91aten__mm")
#loc841 = loc("1705|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_91aten__view")
#loc842 = loc("1706|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_186aten__add")
#loc843 = loc("1712|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_62aten__view")
#loc844 = loc("1713|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_170aten__permute")
#loc845 = loc("1717|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_140xla__cast")
#loc846 = loc("1720|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_172aten__permute")
#loc847 = loc("1721|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_95aten__mul")
#loc848 = loc("1723|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_15aten__einsum")
#loc849 = loc("1724|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_15aten__eq")
#loc850 = loc("1725|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_30aten__logical_not")
#loc852 = loc("or.7379")
#loc853 = loc("select.7380")
#loc854 = loc("1727|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_31aten__logical_not")
#loc855 = loc("1729|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_15aten__expand")
#loc856 = loc("1723|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_15aten__softmax")
#loc857 = loc("1729|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_15aten__where")
#loc858 = loc("1707|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_168aten__permute")
#loc859 = loc("1708|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_92aten__mm")
#loc860 = loc("1708|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_92aten__view")
#loc861 = loc("1709|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_187aten__add")
#loc862 = loc("1714|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_63aten__view")
#loc863 = loc("1715|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_171aten__permute")
#loc864 = loc("1718|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_141xla__cast")
#loc865 = loc("1731|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_143aten__einsum")
#loc866 = loc("1731|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_143xla__cast")
#loc867 = loc("1733|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_15aten__permute")
#loc868 = loc("1736|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_93aten__view")
#loc869 = loc("1735|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_174aten__permute")
#loc870 = loc("1736|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_93aten__mm")
#loc871 = loc("1737|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_188aten__add")
#loc872 = loc("1738|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_189aten__add")
#loc873 = loc("1751|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_161xla__mark_tensor")
#loc874 = loc("1753|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_94aten__view")
#loc875 = loc("1752|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_175aten__permute")
#loc876 = loc("1753|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_94aten__mm")
#loc877 = loc("1754|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_192aten__add")
#loc878 = loc("1757|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[15].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_163xla__mark_tensor")
#loc879 = loc("1759|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_95aten__view")
#loc880 = loc("1758|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_176aten__permute")
#loc881 = loc("1759|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_95aten__mm")
#loc882 = loc("1760|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_193aten__add")
#loc883 = loc("1761|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_194aten__add")
#loc884 = loc("1774|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_167xla__mark_tensor")
#loc885 = loc("1776|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_96aten__view")
#loc886 = loc("1775|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_177aten__permute")
#loc887 = loc("1776|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_96aten__mm")
#loc888 = loc("1777|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_197aten__add")
#loc889 = loc("1784|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_65aten__view")
#loc890 = loc("1785|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_180aten__permute")
#loc891 = loc("1790|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_148xla__cast")
#loc892 = loc("1793|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_100aten__mul")
#loc893 = loc("1778|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_178aten__permute")
#loc894 = loc("1779|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_97aten__mm")
#loc895 = loc("1779|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_97aten__view")
#loc896 = loc("1780|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_198aten__add")
#loc897 = loc("1786|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_66aten__view")
#loc898 = loc("1787|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_181aten__permute")
#loc899 = loc("1791|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_149xla__cast")
#loc900 = loc("1794|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_183aten__permute")
#loc901 = loc("1795|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_101aten__mul")
#loc902 = loc("1797|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_16aten__einsum")
#loc903 = loc("1798|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_16aten__eq")
#loc904 = loc("1799|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_32aten__logical_not")
#loc906 = loc("or.7695")
#loc907 = loc("select.7696")
#loc908 = loc("1801|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_33aten__logical_not")
#loc909 = loc("1803|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_16aten__expand")
#loc910 = loc("1797|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_16aten__softmax")
#loc911 = loc("1803|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_16aten__where")
#loc912 = loc("1781|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_179aten__permute")
#loc913 = loc("1782|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_98aten__mm")
#loc914 = loc("1782|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_98aten__view")
#loc915 = loc("1783|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_199aten__add")
#loc916 = loc("1788|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_67aten__view")
#loc917 = loc("1789|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_182aten__permute")
#loc918 = loc("1792|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_150xla__cast")
#loc919 = loc("1805|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_152aten__einsum")
#loc920 = loc("1805|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_152xla__cast")
#loc921 = loc("1807|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_16aten__permute")
#loc922 = loc("1810|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_99aten__view")
#loc923 = loc("1809|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_185aten__permute")
#loc924 = loc("1810|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_99aten__mm")
#loc925 = loc("1811|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_200aten__add")
#loc926 = loc("1812|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_201aten__add")
#loc927 = loc("1825|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_171xla__mark_tensor")
#loc928 = loc("1827|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_100aten__view")
#loc929 = loc("1826|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_186aten__permute")
#loc930 = loc("1827|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_100aten__mm")
#loc931 = loc("1828|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_204aten__add")
#loc932 = loc("1831|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[16].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_173xla__mark_tensor")
#loc933 = loc("1833|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_101aten__view")
#loc934 = loc("1832|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_187aten__permute")
#loc935 = loc("1833|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_101aten__mm")
#loc936 = loc("1834|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_205aten__add")
#loc937 = loc("1835|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_206aten__add")
#loc938 = loc("1848|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_177xla__mark_tensor")
#loc939 = loc("1850|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_102aten__view")
#loc940 = loc("1849|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_188aten__permute")
#loc941 = loc("1850|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_102aten__mm")
#loc942 = loc("1851|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_209aten__add")
#loc943 = loc("1858|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_69aten__view")
#loc944 = loc("1859|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_191aten__permute")
#loc945 = loc("1864|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_157xla__cast")
#loc946 = loc("1867|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_106aten__mul")
#loc947 = loc("1852|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_189aten__permute")
#loc948 = loc("1853|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_103aten__mm")
#loc949 = loc("1853|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_103aten__view")
#loc950 = loc("1854|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_210aten__add")
#loc951 = loc("1860|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_70aten__view")
#loc952 = loc("1861|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_192aten__permute")
#loc953 = loc("1865|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_158xla__cast")
#loc954 = loc("1868|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_194aten__permute")
#loc955 = loc("1869|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_107aten__mul")
#loc956 = loc("1871|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_17aten__einsum")
#loc957 = loc("1872|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_17aten__eq")
#loc958 = loc("1873|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_34aten__logical_not")
#loc960 = loc("or.8011")
#loc961 = loc("select.8012")
#loc962 = loc("1875|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_35aten__logical_not")
#loc963 = loc("1877|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_17aten__expand")
#loc964 = loc("1871|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_17aten__softmax")
#loc965 = loc("1877|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_17aten__where")
#loc966 = loc("1855|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_190aten__permute")
#loc967 = loc("1856|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_104aten__mm")
#loc968 = loc("1856|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_104aten__view")
#loc969 = loc("1857|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_211aten__add")
#loc970 = loc("1862|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_71aten__view")
#loc971 = loc("1863|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_193aten__permute")
#loc972 = loc("1866|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_159xla__cast")
#loc973 = loc("1879|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_161aten__einsum")
#loc974 = loc("1879|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_161xla__cast")
#loc975 = loc("1881|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_17aten__permute")
#loc976 = loc("1884|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_105aten__view")
#loc977 = loc("1883|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_196aten__permute")
#loc978 = loc("1884|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_105aten__mm")
#loc979 = loc("1885|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_212aten__add")
#loc980 = loc("1886|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_213aten__add")
#loc981 = loc("1899|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_181xla__mark_tensor")
#loc982 = loc("1901|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_106aten__view")
#loc983 = loc("1900|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_197aten__permute")
#loc984 = loc("1901|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_106aten__mm")
#loc985 = loc("1902|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_216aten__add")
#loc986 = loc("1905|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[17].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_183xla__mark_tensor")
#loc987 = loc("1907|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_107aten__view")
#loc988 = loc("1906|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_198aten__permute")
#loc989 = loc("1907|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_107aten__mm")
#loc990 = loc("1908|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_217aten__add")
#loc991 = loc("1909|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_218aten__add")
#loc992 = loc("1922|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_187xla__mark_tensor")
#loc993 = loc("1924|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_108aten__view")
#loc994 = loc("1923|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_199aten__permute")
#loc995 = loc("1924|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_108aten__mm")
#loc996 = loc("1925|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_221aten__add")
#loc997 = loc("1932|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_73aten__view")
#loc998 = loc("1933|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_202aten__permute")
#loc999 = loc("1938|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_166xla__cast")
#loc1000 = loc("1941|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_112aten__mul")
#loc1001 = loc("1926|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_200aten__permute")
#loc1002 = loc("1927|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_109aten__mm")
#loc1003 = loc("1927|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_109aten__view")
#loc1004 = loc("1928|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_222aten__add")
#loc1005 = loc("1934|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_74aten__view")
#loc1006 = loc("1935|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_203aten__permute")
#loc1007 = loc("1939|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_167xla__cast")
#loc1008 = loc("1942|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_205aten__permute")
#loc1009 = loc("1943|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_113aten__mul")
#loc1010 = loc("1945|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_18aten__einsum")
#loc1011 = loc("1946|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_18aten__eq")
#loc1012 = loc("1947|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_36aten__logical_not")
#loc1014 = loc("or.8327")
#loc1015 = loc("select.8328")
#loc1016 = loc("1949|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_37aten__logical_not")
#loc1017 = loc("1951|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_18aten__expand")
#loc1018 = loc("1945|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_18aten__softmax")
#loc1019 = loc("1951|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_18aten__where")
#loc1020 = loc("1929|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_201aten__permute")
#loc1021 = loc("1930|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_110aten__mm")
#loc1022 = loc("1930|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_110aten__view")
#loc1023 = loc("1931|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_223aten__add")
#loc1024 = loc("1936|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_75aten__view")
#loc1025 = loc("1937|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_204aten__permute")
#loc1026 = loc("1940|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_168xla__cast")
#loc1027 = loc("1953|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_170aten__einsum")
#loc1028 = loc("1953|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_170xla__cast")
#loc1029 = loc("1955|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_18aten__permute")
#loc1030 = loc("1958|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_111aten__view")
#loc1031 = loc("1957|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_207aten__permute")
#loc1032 = loc("1958|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_111aten__mm")
#loc1033 = loc("1959|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_224aten__add")
#loc1034 = loc("1960|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_225aten__add")
#loc1035 = loc("1973|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_191xla__mark_tensor")
#loc1036 = loc("1975|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_112aten__view")
#loc1037 = loc("1974|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_208aten__permute")
#loc1038 = loc("1975|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_112aten__mm")
#loc1039 = loc("1976|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_228aten__add")
#loc1040 = loc("1979|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[18].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_193xla__mark_tensor")
#loc1041 = loc("1981|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_113aten__view")
#loc1042 = loc("1980|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_209aten__permute")
#loc1043 = loc("1981|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_113aten__mm")
#loc1044 = loc("1982|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_229aten__add")
#loc1045 = loc("1983|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_230aten__add")
#loc1046 = loc("1996|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_197xla__mark_tensor")
#loc1047 = loc("1998|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_114aten__view")
#loc1048 = loc("1997|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_210aten__permute")
#loc1049 = loc("1998|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_114aten__mm")
#loc1050 = loc("1999|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_233aten__add")
#loc1051 = loc("2006|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_77aten__view")
#loc1052 = loc("2007|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_213aten__permute")
#loc1053 = loc("2012|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_175xla__cast")
#loc1054 = loc("2015|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_118aten__mul")
#loc1055 = loc("2000|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_211aten__permute")
#loc1056 = loc("2001|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_115aten__mm")
#loc1057 = loc("2001|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_115aten__view")
#loc1058 = loc("2002|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_234aten__add")
#loc1059 = loc("2008|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_78aten__view")
#loc1060 = loc("2009|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_214aten__permute")
#loc1061 = loc("2013|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_176xla__cast")
#loc1062 = loc("2016|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_216aten__permute")
#loc1063 = loc("2017|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_119aten__mul")
#loc1064 = loc("2019|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_19aten__einsum")
#loc1065 = loc("2020|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_19aten__eq")
#loc1066 = loc("2021|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_38aten__logical_not")
#loc1068 = loc("or.8643")
#loc1069 = loc("select.8644")
#loc1070 = loc("2023|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_39aten__logical_not")
#loc1071 = loc("2025|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_19aten__expand")
#loc1072 = loc("2019|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_19aten__softmax")
#loc1073 = loc("2025|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_19aten__where")
#loc1074 = loc("2003|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_212aten__permute")
#loc1075 = loc("2004|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_116aten__mm")
#loc1076 = loc("2004|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_116aten__view")
#loc1077 = loc("2005|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_235aten__add")
#loc1078 = loc("2010|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_79aten__view")
#loc1079 = loc("2011|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_215aten__permute")
#loc1080 = loc("2014|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_177xla__cast")
#loc1081 = loc("2027|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_179aten__einsum")
#loc1082 = loc("2027|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_179xla__cast")
#loc1083 = loc("2029|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_19aten__permute")
#loc1084 = loc("2032|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_117aten__view")
#loc1085 = loc("2031|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_218aten__permute")
#loc1086 = loc("2032|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_117aten__mm")
#loc1087 = loc("2033|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_236aten__add")
#loc1088 = loc("2034|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_237aten__add")
#loc1089 = loc("2047|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_201xla__mark_tensor")
#loc1090 = loc("2049|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_118aten__view")
#loc1091 = loc("2048|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_219aten__permute")
#loc1092 = loc("2049|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_118aten__mm")
#loc1093 = loc("2050|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_240aten__add")
#loc1094 = loc("2053|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[19].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_203xla__mark_tensor")
#loc1095 = loc("2055|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_119aten__view")
#loc1096 = loc("2054|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_220aten__permute")
#loc1097 = loc("2055|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_119aten__mm")
#loc1098 = loc("2056|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_241aten__add")
#loc1099 = loc("2057|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_242aten__add")
#loc1100 = loc("2070|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_207xla__mark_tensor")
#loc1101 = loc("2072|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_120aten__view")
#loc1102 = loc("2071|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_221aten__permute")
#loc1103 = loc("2072|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_120aten__mm")
#loc1104 = loc("2073|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_245aten__add")
#loc1105 = loc("2080|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_81aten__view")
#loc1106 = loc("2081|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_224aten__permute")
#loc1107 = loc("2086|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_184xla__cast")
#loc1108 = loc("2089|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_124aten__mul")
#loc1109 = loc("2074|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_222aten__permute")
#loc1110 = loc("2075|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_121aten__mm")
#loc1111 = loc("2075|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_121aten__view")
#loc1112 = loc("2076|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_246aten__add")
#loc1113 = loc("2082|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_82aten__view")
#loc1114 = loc("2083|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_225aten__permute")
#loc1115 = loc("2087|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_185xla__cast")
#loc1116 = loc("2090|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_227aten__permute")
#loc1117 = loc("2091|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_125aten__mul")
#loc1118 = loc("2093|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_20aten__einsum")
#loc1119 = loc("2094|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_20aten__eq")
#loc1120 = loc("2095|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_40aten__logical_not")
#loc1122 = loc("or.8959")
#loc1123 = loc("select.8960")
#loc1124 = loc("2097|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_41aten__logical_not")
#loc1125 = loc("2099|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_20aten__expand")
#loc1126 = loc("2093|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_20aten__softmax")
#loc1127 = loc("2099|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_20aten__where")
#loc1128 = loc("2077|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_223aten__permute")
#loc1129 = loc("2078|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_122aten__mm")
#loc1130 = loc("2078|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_122aten__view")
#loc1131 = loc("2079|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_247aten__add")
#loc1132 = loc("2084|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_83aten__view")
#loc1133 = loc("2085|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_226aten__permute")
#loc1134 = loc("2088|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_186xla__cast")
#loc1135 = loc("2101|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_188aten__einsum")
#loc1136 = loc("2101|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_188xla__cast")
#loc1137 = loc("2103|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_20aten__permute")
#loc1138 = loc("2106|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_123aten__view")
#loc1139 = loc("2105|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_229aten__permute")
#loc1140 = loc("2106|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_123aten__mm")
#loc1141 = loc("2107|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_248aten__add")
#loc1142 = loc("2108|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_249aten__add")
#loc1143 = loc("2121|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_211xla__mark_tensor")
#loc1144 = loc("2123|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_124aten__view")
#loc1145 = loc("2122|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_230aten__permute")
#loc1146 = loc("2123|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_124aten__mm")
#loc1147 = loc("2124|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_252aten__add")
#loc1148 = loc("2127|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[20].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_213xla__mark_tensor")
#loc1149 = loc("2129|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_125aten__view")
#loc1150 = loc("2128|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_231aten__permute")
#loc1151 = loc("2129|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_125aten__mm")
#loc1152 = loc("2130|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_253aten__add")
#loc1153 = loc("2131|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_254aten__add")
#loc1154 = loc("2144|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_217xla__mark_tensor")
#loc1155 = loc("2146|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_126aten__view")
#loc1156 = loc("2145|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_232aten__permute")
#loc1157 = loc("2146|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_126aten__mm")
#loc1158 = loc("2147|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_257aten__add")
#loc1159 = loc("2154|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_85aten__view")
#loc1160 = loc("2155|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_235aten__permute")
#loc1161 = loc("2160|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_193xla__cast")
#loc1162 = loc("2163|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_130aten__mul")
#loc1163 = loc("2148|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_233aten__permute")
#loc1164 = loc("2149|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_127aten__mm")
#loc1165 = loc("2149|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_127aten__view")
#loc1166 = loc("2150|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_258aten__add")
#loc1167 = loc("2156|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_86aten__view")
#loc1168 = loc("2157|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_236aten__permute")
#loc1169 = loc("2161|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_194xla__cast")
#loc1170 = loc("2164|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_238aten__permute")
#loc1171 = loc("2165|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_131aten__mul")
#loc1172 = loc("2167|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_21aten__einsum")
#loc1173 = loc("2168|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_21aten__eq")
#loc1174 = loc("2169|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_42aten__logical_not")
#loc1176 = loc("or.9275")
#loc1177 = loc("select.9276")
#loc1178 = loc("2171|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_43aten__logical_not")
#loc1179 = loc("2173|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_21aten__expand")
#loc1180 = loc("2167|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_21aten__softmax")
#loc1181 = loc("2173|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_21aten__where")
#loc1182 = loc("2151|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_234aten__permute")
#loc1183 = loc("2152|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_128aten__mm")
#loc1184 = loc("2152|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_128aten__view")
#loc1185 = loc("2153|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_259aten__add")
#loc1186 = loc("2158|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_87aten__view")
#loc1187 = loc("2159|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_237aten__permute")
#loc1188 = loc("2162|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_195xla__cast")
#loc1189 = loc("2175|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_197aten__einsum")
#loc1190 = loc("2175|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_197xla__cast")
#loc1191 = loc("2177|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_21aten__permute")
#loc1192 = loc("2180|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_129aten__view")
#loc1193 = loc("2179|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_240aten__permute")
#loc1194 = loc("2180|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_129aten__mm")
#loc1195 = loc("2181|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_260aten__add")
#loc1196 = loc("2182|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_261aten__add")
#loc1197 = loc("2195|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_221xla__mark_tensor")
#loc1198 = loc("2197|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_130aten__view")
#loc1199 = loc("2196|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_241aten__permute")
#loc1200 = loc("2197|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_130aten__mm")
#loc1201 = loc("2198|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_264aten__add")
#loc1202 = loc("2201|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[21].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_223xla__mark_tensor")
#loc1203 = loc("2203|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_131aten__view")
#loc1204 = loc("2202|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_242aten__permute")
#loc1205 = loc("2203|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_131aten__mm")
#loc1206 = loc("2204|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_265aten__add")
#loc1207 = loc("2205|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_266aten__add")
#loc1208 = loc("2218|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_227xla__mark_tensor")
#loc1209 = loc("2220|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_132aten__view")
#loc1210 = loc("2219|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_243aten__permute")
#loc1211 = loc("2220|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_132aten__mm")
#loc1212 = loc("2221|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_269aten__add")
#loc1213 = loc("2228|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_89aten__view")
#loc1214 = loc("2229|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_246aten__permute")
#loc1215 = loc("2234|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_202xla__cast")
#loc1216 = loc("2237|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_136aten__mul")
#loc1217 = loc("2222|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_244aten__permute")
#loc1218 = loc("2223|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_133aten__mm")
#loc1219 = loc("2223|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_133aten__view")
#loc1220 = loc("2224|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_270aten__add")
#loc1221 = loc("2230|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_90aten__view")
#loc1222 = loc("2231|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_247aten__permute")
#loc1223 = loc("2235|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_203xla__cast")
#loc1224 = loc("2238|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_249aten__permute")
#loc1225 = loc("2239|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_137aten__mul")
#loc1226 = loc("2241|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_22aten__einsum")
#loc1227 = loc("2242|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_22aten__eq")
#loc1228 = loc("2243|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_44aten__logical_not")
#loc1230 = loc("or.9591")
#loc1231 = loc("select.9592")
#loc1232 = loc("2245|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_45aten__logical_not")
#loc1233 = loc("2247|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_22aten__expand")
#loc1234 = loc("2241|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_22aten__softmax")
#loc1235 = loc("2247|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_22aten__where")
#loc1236 = loc("2225|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_245aten__permute")
#loc1237 = loc("2226|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_134aten__mm")
#loc1238 = loc("2226|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_134aten__view")
#loc1239 = loc("2227|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_271aten__add")
#loc1240 = loc("2232|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_91aten__view")
#loc1241 = loc("2233|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_248aten__permute")
#loc1242 = loc("2236|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_204xla__cast")
#loc1243 = loc("2249|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_206aten__einsum")
#loc1244 = loc("2249|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_206xla__cast")
#loc1245 = loc("2251|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_22aten__permute")
#loc1246 = loc("2254|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_135aten__view")
#loc1247 = loc("2253|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_251aten__permute")
#loc1248 = loc("2254|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_135aten__mm")
#loc1249 = loc("2255|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_272aten__add")
#loc1250 = loc("2256|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_273aten__add")
#loc1251 = loc("2269|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_231xla__mark_tensor")
#loc1252 = loc("2271|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_136aten__view")
#loc1253 = loc("2270|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_252aten__permute")
#loc1254 = loc("2271|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_136aten__mm")
#loc1255 = loc("2272|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_276aten__add")
#loc1256 = loc("2275|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[22].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_233xla__mark_tensor")
#loc1257 = loc("2277|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_137aten__view")
#loc1258 = loc("2276|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_253aten__permute")
#loc1259 = loc("2277|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_137aten__mm")
#loc1260 = loc("2278|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_277aten__add")
#loc1261 = loc("2279|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_278aten__add")
#loc1262 = loc("2292|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_237xla__mark_tensor")
#loc1263 = loc("2294|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_138aten__view")
#loc1264 = loc("2293|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_254aten__permute")
#loc1265 = loc("2294|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_138aten__mm")
#loc1266 = loc("2295|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_281aten__add")
#loc1267 = loc("2302|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_93aten__view")
#loc1268 = loc("2303|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_257aten__permute")
#loc1269 = loc("2308|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_211xla__cast")
#loc1270 = loc("2311|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_142aten__mul")
#loc1271 = loc("2296|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_255aten__permute")
#loc1272 = loc("2297|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_139aten__mm")
#loc1273 = loc("2297|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_139aten__view")
#loc1274 = loc("2298|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_282aten__add")
#loc1275 = loc("2304|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_94aten__view")
#loc1276 = loc("2305|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_258aten__permute")
#loc1277 = loc("2309|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_212xla__cast")
#loc1278 = loc("2312|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_260aten__permute")
#loc1279 = loc("2313|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_143aten__mul")
#loc1280 = loc("2315|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_23aten__einsum")
#loc1281 = loc("2316|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_23aten__eq")
#loc1282 = loc("2317|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_46aten__logical_not")
#loc1284 = loc("or.9907")
#loc1285 = loc("select.9908")
#loc1286 = loc("2319|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_47aten__logical_not")
#loc1287 = loc("2321|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_23aten__expand")
#loc1288 = loc("2315|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_23aten__softmax")
#loc1289 = loc("2321|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_23aten__where")
#loc1290 = loc("2299|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_256aten__permute")
#loc1291 = loc("2300|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_140aten__mm")
#loc1292 = loc("2300|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_140aten__view")
#loc1293 = loc("2301|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_283aten__add")
#loc1294 = loc("2306|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_95aten__view")
#loc1295 = loc("2307|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_259aten__permute")
#loc1296 = loc("2310|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_213xla__cast")
#loc1297 = loc("2323|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_215aten__einsum")
#loc1298 = loc("2323|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_215xla__cast")
#loc1299 = loc("2325|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_23aten__permute")
#loc1300 = loc("2328|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_141aten__view")
#loc1301 = loc("2327|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_262aten__permute")
#loc1302 = loc("2328|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_141aten__mm")
#loc1303 = loc("2329|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_284aten__add")
#loc1304 = loc("2330|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_285aten__add")
#loc1305 = loc("2343|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_241xla__mark_tensor")
#loc1306 = loc("2345|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_142aten__view")
#loc1307 = loc("2344|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_263aten__permute")
#loc1308 = loc("2345|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_142aten__mm")
#loc1309 = loc("2346|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_288aten__add")
#loc1310 = loc("2349|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[23].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_243xla__mark_tensor")
#loc1311 = loc("2351|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_143aten__view")
#loc1312 = loc("2350|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_264aten__permute")
#loc1313 = loc("2351|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_143aten__mm")
#loc1314 = loc("2352|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_289aten__add")
#loc1315 = loc("2353|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_290aten__add")
#loc1316 = loc("2366|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_247xla__mark_tensor")
#loc1317 = loc("2368|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_144aten__view")
#loc1318 = loc("2367|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_265aten__permute")
#loc1319 = loc("2368|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_144aten__mm")
#loc1320 = loc("2369|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_293aten__add")
#loc1321 = loc("2376|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_97aten__view")
#loc1322 = loc("2377|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_268aten__permute")
#loc1323 = loc("2382|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_220xla__cast")
#loc1324 = loc("2385|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_148aten__mul")
#loc1325 = loc("2370|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_266aten__permute")
#loc1326 = loc("2371|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_145aten__mm")
#loc1327 = loc("2371|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_145aten__view")
#loc1328 = loc("2372|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_294aten__add")
#loc1329 = loc("2378|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_98aten__view")
#loc1330 = loc("2379|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_269aten__permute")
#loc1331 = loc("2383|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_221xla__cast")
#loc1332 = loc("2386|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_271aten__permute")
#loc1333 = loc("2387|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_149aten__mul")
#loc1334 = loc("2389|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_24aten__einsum")
#loc1335 = loc("2390|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_24aten__eq")
#loc1336 = loc("2391|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_48aten__logical_not")
#loc1338 = loc("or.10223")
#loc1339 = loc("select.10224")
#loc1340 = loc("2393|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_49aten__logical_not")
#loc1341 = loc("2395|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_24aten__expand")
#loc1342 = loc("2389|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_24aten__softmax")
#loc1343 = loc("2395|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_24aten__where")
#loc1344 = loc("2373|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_267aten__permute")
#loc1345 = loc("2374|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_146aten__mm")
#loc1346 = loc("2374|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_146aten__view")
#loc1347 = loc("2375|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_295aten__add")
#loc1348 = loc("2380|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_99aten__view")
#loc1349 = loc("2381|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_270aten__permute")
#loc1350 = loc("2384|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_222xla__cast")
#loc1351 = loc("2397|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_224aten__einsum")
#loc1352 = loc("2397|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_224xla__cast")
#loc1353 = loc("2399|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_24aten__permute")
#loc1354 = loc("2402|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_147aten__view")
#loc1355 = loc("2401|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_273aten__permute")
#loc1356 = loc("2402|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_147aten__mm")
#loc1357 = loc("2403|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_296aten__add")
#loc1358 = loc("2404|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_297aten__add")
#loc1359 = loc("2417|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_251xla__mark_tensor")
#loc1360 = loc("2419|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_148aten__view")
#loc1361 = loc("2418|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_274aten__permute")
#loc1362 = loc("2419|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_148aten__mm")
#loc1363 = loc("2420|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_300aten__add")
#loc1364 = loc("2423|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[24].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_253xla__mark_tensor")
#loc1365 = loc("2425|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_149aten__view")
#loc1366 = loc("2424|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_275aten__permute")
#loc1367 = loc("2425|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_149aten__mm")
#loc1368 = loc("2426|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_301aten__add")
#loc1369 = loc("2427|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_302aten__add")
#loc1370 = loc("2440|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_257xla__mark_tensor")
#loc1371 = loc("2442|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_150aten__view")
#loc1372 = loc("2441|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_276aten__permute")
#loc1373 = loc("2442|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_150aten__mm")
#loc1374 = loc("2443|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_305aten__add")
#loc1375 = loc("2450|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_101aten__view")
#loc1376 = loc("2451|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_279aten__permute")
#loc1377 = loc("2456|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_229xla__cast")
#loc1378 = loc("2459|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_154aten__mul")
#loc1379 = loc("2444|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_277aten__permute")
#loc1380 = loc("2445|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_151aten__mm")
#loc1381 = loc("2445|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_151aten__view")
#loc1382 = loc("2446|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_306aten__add")
#loc1383 = loc("2452|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_102aten__view")
#loc1384 = loc("2453|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_280aten__permute")
#loc1385 = loc("2457|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_230xla__cast")
#loc1386 = loc("2460|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_282aten__permute")
#loc1387 = loc("2461|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_155aten__mul")
#loc1388 = loc("2463|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_25aten__einsum")
#loc1389 = loc("2464|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_25aten__eq")
#loc1390 = loc("2465|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_50aten__logical_not")
#loc1392 = loc("or.10539")
#loc1393 = loc("select.10540")
#loc1394 = loc("2467|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_51aten__logical_not")
#loc1395 = loc("2469|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_25aten__expand")
#loc1396 = loc("2463|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_25aten__softmax")
#loc1397 = loc("2469|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_25aten__where")
#loc1398 = loc("2447|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_278aten__permute")
#loc1399 = loc("2448|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_152aten__mm")
#loc1400 = loc("2448|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_152aten__view")
#loc1401 = loc("2449|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_307aten__add")
#loc1402 = loc("2454|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_103aten__view")
#loc1403 = loc("2455|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_281aten__permute")
#loc1404 = loc("2458|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_231xla__cast")
#loc1405 = loc("2471|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_233aten__einsum")
#loc1406 = loc("2471|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_233xla__cast")
#loc1407 = loc("2473|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_25aten__permute")
#loc1408 = loc("2476|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_153aten__view")
#loc1409 = loc("2475|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_284aten__permute")
#loc1410 = loc("2476|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_153aten__mm")
#loc1411 = loc("2477|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_308aten__add")
#loc1412 = loc("2478|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_309aten__add")
#loc1413 = loc("2491|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_261xla__mark_tensor")
#loc1414 = loc("2493|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_154aten__view")
#loc1415 = loc("2492|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_285aten__permute")
#loc1416 = loc("2493|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_154aten__mm")
#loc1417 = loc("2494|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_312aten__add")
#loc1418 = loc("2497|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[25].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_263xla__mark_tensor")
#loc1419 = loc("2499|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_155aten__view")
#loc1420 = loc("2498|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_286aten__permute")
#loc1421 = loc("2499|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_155aten__mm")
#loc1422 = loc("2500|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_313aten__add")
#loc1423 = loc("2501|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_314aten__add")
#loc1424 = loc("2514|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_267xla__mark_tensor")
#loc1425 = loc("2516|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_156aten__view")
#loc1426 = loc("2515|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_287aten__permute")
#loc1427 = loc("2516|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_156aten__mm")
#loc1428 = loc("2517|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_317aten__add")
#loc1429 = loc("2524|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_105aten__view")
#loc1430 = loc("2525|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_290aten__permute")
#loc1431 = loc("2530|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_238xla__cast")
#loc1432 = loc("2533|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_160aten__mul")
#loc1433 = loc("2518|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_288aten__permute")
#loc1434 = loc("2519|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_157aten__mm")
#loc1435 = loc("2519|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_157aten__view")
#loc1436 = loc("2520|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_318aten__add")
#loc1437 = loc("2526|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_106aten__view")
#loc1438 = loc("2527|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_291aten__permute")
#loc1439 = loc("2531|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_239xla__cast")
#loc1440 = loc("2534|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_293aten__permute")
#loc1441 = loc("2535|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_161aten__mul")
#loc1442 = loc("2537|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_26aten__einsum")
#loc1443 = loc("2538|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_26aten__eq")
#loc1444 = loc("2539|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_52aten__logical_not")
#loc1446 = loc("or.10855")
#loc1447 = loc("select.10856")
#loc1448 = loc("2541|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_53aten__logical_not")
#loc1449 = loc("2543|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_26aten__expand")
#loc1450 = loc("2537|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_26aten__softmax")
#loc1451 = loc("2543|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_26aten__where")
#loc1452 = loc("2521|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_289aten__permute")
#loc1453 = loc("2522|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_158aten__mm")
#loc1454 = loc("2522|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_158aten__view")
#loc1455 = loc("2523|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_319aten__add")
#loc1456 = loc("2528|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_107aten__view")
#loc1457 = loc("2529|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_292aten__permute")
#loc1458 = loc("2532|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_240xla__cast")
#loc1459 = loc("2545|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_242aten__einsum")
#loc1460 = loc("2545|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_242xla__cast")
#loc1461 = loc("2547|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_26aten__permute")
#loc1462 = loc("2550|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_159aten__view")
#loc1463 = loc("2549|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_295aten__permute")
#loc1464 = loc("2550|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_159aten__mm")
#loc1465 = loc("2551|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_320aten__add")
#loc1466 = loc("2552|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_321aten__add")
#loc1467 = loc("2565|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_271xla__mark_tensor")
#loc1468 = loc("2567|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_160aten__view")
#loc1469 = loc("2566|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_296aten__permute")
#loc1470 = loc("2567|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_160aten__mm")
#loc1471 = loc("2568|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_324aten__add")
#loc1472 = loc("2571|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[26].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_273xla__mark_tensor")
#loc1473 = loc("2573|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_161aten__view")
#loc1474 = loc("2572|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_297aten__permute")
#loc1475 = loc("2573|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_161aten__mm")
#loc1476 = loc("2574|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_325aten__add")
#loc1477 = loc("2575|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_326aten__add")
#loc1478 = loc("2588|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_277xla__mark_tensor")
#loc1479 = loc("2590|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_162aten__view")
#loc1480 = loc("2589|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_298aten__permute")
#loc1481 = loc("2590|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_162aten__mm")
#loc1482 = loc("2591|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_329aten__add")
#loc1483 = loc("2598|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_109aten__view")
#loc1484 = loc("2599|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_301aten__permute")
#loc1485 = loc("2604|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_247xla__cast")
#loc1486 = loc("2607|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_166aten__mul")
#loc1487 = loc("2592|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_299aten__permute")
#loc1488 = loc("2593|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_163aten__mm")
#loc1489 = loc("2593|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_163aten__view")
#loc1490 = loc("2594|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_330aten__add")
#loc1491 = loc("2600|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_110aten__view")
#loc1492 = loc("2601|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_302aten__permute")
#loc1493 = loc("2605|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_248xla__cast")
#loc1494 = loc("2608|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_304aten__permute")
#loc1495 = loc("2609|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_167aten__mul")
#loc1496 = loc("2611|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_27aten__einsum")
#loc1497 = loc("2612|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_27aten__eq")
#loc1498 = loc("2613|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_54aten__logical_not")
#loc1500 = loc("or.11171")
#loc1501 = loc("select.11172")
#loc1502 = loc("2615|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_55aten__logical_not")
#loc1503 = loc("2617|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_27aten__expand")
#loc1504 = loc("2611|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_27aten__softmax")
#loc1505 = loc("2617|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_27aten__where")
#loc1506 = loc("2595|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_300aten__permute")
#loc1507 = loc("2596|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_164aten__mm")
#loc1508 = loc("2596|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_164aten__view")
#loc1509 = loc("2597|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_331aten__add")
#loc1510 = loc("2602|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_111aten__view")
#loc1511 = loc("2603|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_303aten__permute")
#loc1512 = loc("2606|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_249xla__cast")
#loc1513 = loc("2619|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_251aten__einsum")
#loc1514 = loc("2619|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_251xla__cast")
#loc1515 = loc("2621|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_27aten__permute")
#loc1516 = loc("2624|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_165aten__view")
#loc1517 = loc("2623|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_306aten__permute")
#loc1518 = loc("2624|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_165aten__mm")
#loc1519 = loc("2625|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_332aten__add")
#loc1520 = loc("2626|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_333aten__add")
#loc1521 = loc("2639|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_281xla__mark_tensor")
#loc1522 = loc("2641|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_166aten__view")
#loc1523 = loc("2640|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_307aten__permute")
#loc1524 = loc("2641|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_166aten__mm")
#loc1525 = loc("2642|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_336aten__add")
#loc1526 = loc("2645|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[27].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_283xla__mark_tensor")
#loc1527 = loc("2647|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_167aten__view")
#loc1528 = loc("2646|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_308aten__permute")
#loc1529 = loc("2647|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_167aten__mm")
#loc1530 = loc("2648|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_337aten__add")
#loc1531 = loc("2649|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_338aten__add")
#loc1532 = loc("2662|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_287xla__mark_tensor")
#loc1533 = loc("2664|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_168aten__view")
#loc1534 = loc("2663|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_309aten__permute")
#loc1535 = loc("2664|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_168aten__mm")
#loc1536 = loc("2665|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_341aten__add")
#loc1537 = loc("2672|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_113aten__view")
#loc1538 = loc("2673|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_312aten__permute")
#loc1539 = loc("2678|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_256xla__cast")
#loc1540 = loc("2681|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_172aten__mul")
#loc1541 = loc("2666|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_310aten__permute")
#loc1542 = loc("2667|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_169aten__mm")
#loc1543 = loc("2667|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_169aten__view")
#loc1544 = loc("2668|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_342aten__add")
#loc1545 = loc("2674|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_114aten__view")
#loc1546 = loc("2675|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_313aten__permute")
#loc1547 = loc("2679|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_257xla__cast")
#loc1548 = loc("2682|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_315aten__permute")
#loc1549 = loc("2683|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_173aten__mul")
#loc1550 = loc("2685|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_28aten__einsum")
#loc1551 = loc("2686|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_28aten__eq")
#loc1552 = loc("2687|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_56aten__logical_not")
#loc1554 = loc("or.11487")
#loc1555 = loc("select.11488")
#loc1556 = loc("2689|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_57aten__logical_not")
#loc1557 = loc("2691|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_28aten__expand")
#loc1558 = loc("2685|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_28aten__softmax")
#loc1559 = loc("2691|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_28aten__where")
#loc1560 = loc("2669|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_311aten__permute")
#loc1561 = loc("2670|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_170aten__mm")
#loc1562 = loc("2670|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_170aten__view")
#loc1563 = loc("2671|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_343aten__add")
#loc1564 = loc("2676|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_115aten__view")
#loc1565 = loc("2677|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_314aten__permute")
#loc1566 = loc("2680|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_258xla__cast")
#loc1567 = loc("2693|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_260aten__einsum")
#loc1568 = loc("2693|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_260xla__cast")
#loc1569 = loc("2695|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_28aten__permute")
#loc1570 = loc("2698|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_171aten__view")
#loc1571 = loc("2697|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_317aten__permute")
#loc1572 = loc("2698|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_171aten__mm")
#loc1573 = loc("2699|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_344aten__add")
#loc1574 = loc("2700|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_345aten__add")
#loc1575 = loc("2713|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_291xla__mark_tensor")
#loc1576 = loc("2715|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_172aten__view")
#loc1577 = loc("2714|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_318aten__permute")
#loc1578 = loc("2715|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_172aten__mm")
#loc1579 = loc("2716|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_348aten__add")
#loc1580 = loc("2719|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[28].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_293xla__mark_tensor")
#loc1581 = loc("2721|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_173aten__view")
#loc1582 = loc("2720|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_319aten__permute")
#loc1583 = loc("2721|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_173aten__mm")
#loc1584 = loc("2722|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_349aten__add")
#loc1585 = loc("2723|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_350aten__add")
#loc1586 = loc("2736|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_297xla__mark_tensor")
#loc1587 = loc("2738|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_174aten__view")
#loc1588 = loc("2737|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_320aten__permute")
#loc1589 = loc("2738|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_174aten__mm")
#loc1590 = loc("2739|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_353aten__add")
#loc1591 = loc("2746|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_117aten__view")
#loc1592 = loc("2747|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_323aten__permute")
#loc1593 = loc("2752|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_265xla__cast")
#loc1594 = loc("2755|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_178aten__mul")
#loc1595 = loc("2740|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_321aten__permute")
#loc1596 = loc("2741|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_175aten__mm")
#loc1597 = loc("2741|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_175aten__view")
#loc1598 = loc("2742|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_354aten__add")
#loc1599 = loc("2748|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_118aten__view")
#loc1600 = loc("2749|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_324aten__permute")
#loc1601 = loc("2753|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_266xla__cast")
#loc1602 = loc("2756|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_326aten__permute")
#loc1603 = loc("2757|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_179aten__mul")
#loc1604 = loc("2759|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_29aten__einsum")
#loc1605 = loc("2760|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_29aten__eq")
#loc1606 = loc("2761|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_58aten__logical_not")
#loc1608 = loc("or.11803")
#loc1609 = loc("select.11804")
#loc1610 = loc("2763|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_59aten__logical_not")
#loc1611 = loc("2765|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_29aten__expand")
#loc1612 = loc("2759|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_29aten__softmax")
#loc1613 = loc("2765|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_29aten__where")
#loc1614 = loc("2743|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_322aten__permute")
#loc1615 = loc("2744|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_176aten__mm")
#loc1616 = loc("2744|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_176aten__view")
#loc1617 = loc("2745|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_355aten__add")
#loc1618 = loc("2750|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_119aten__view")
#loc1619 = loc("2751|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_325aten__permute")
#loc1620 = loc("2754|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_267xla__cast")
#loc1621 = loc("2767|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_269aten__einsum")
#loc1622 = loc("2767|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_269xla__cast")
#loc1623 = loc("2769|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_29aten__permute")
#loc1624 = loc("2772|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_177aten__view")
#loc1625 = loc("2771|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_328aten__permute")
#loc1626 = loc("2772|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_177aten__mm")
#loc1627 = loc("2773|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_356aten__add")
#loc1628 = loc("2774|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_357aten__add")
#loc1629 = loc("2787|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_301xla__mark_tensor")
#loc1630 = loc("2789|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_178aten__view")
#loc1631 = loc("2788|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_329aten__permute")
#loc1632 = loc("2789|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_178aten__mm")
#loc1633 = loc("2790|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_360aten__add")
#loc1634 = loc("2793|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[29].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_303xla__mark_tensor")
#loc1635 = loc("2795|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_179aten__view")
#loc1636 = loc("2794|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_330aten__permute")
#loc1637 = loc("2795|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_179aten__mm")
#loc1638 = loc("2796|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_361aten__add")
#loc1639 = loc("2797|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_362aten__add")
#loc1640 = loc("2810|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_307xla__mark_tensor")
#loc1641 = loc("2812|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_180aten__view")
#loc1642 = loc("2811|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_331aten__permute")
#loc1643 = loc("2812|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_180aten__mm")
#loc1644 = loc("2813|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_365aten__add")
#loc1645 = loc("2820|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_121aten__view")
#loc1646 = loc("2821|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_334aten__permute")
#loc1647 = loc("2826|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_274xla__cast")
#loc1648 = loc("2829|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_184aten__mul")
#loc1649 = loc("2814|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_332aten__permute")
#loc1650 = loc("2815|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_181aten__mm")
#loc1651 = loc("2815|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_181aten__view")
#loc1652 = loc("2816|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_366aten__add")
#loc1653 = loc("2822|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_122aten__view")
#loc1654 = loc("2823|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_335aten__permute")
#loc1655 = loc("2827|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_275xla__cast")
#loc1656 = loc("2830|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_337aten__permute")
#loc1657 = loc("2831|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_185aten__mul")
#loc1658 = loc("2833|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_30aten__einsum")
#loc1659 = loc("2834|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_30aten__eq")
#loc1660 = loc("2835|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_60aten__logical_not")
#loc1662 = loc("or.12119")
#loc1663 = loc("select.12120")
#loc1664 = loc("2837|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_61aten__logical_not")
#loc1665 = loc("2839|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_30aten__expand")
#loc1666 = loc("2833|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_30aten__softmax")
#loc1667 = loc("2839|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_30aten__where")
#loc1668 = loc("2817|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_333aten__permute")
#loc1669 = loc("2818|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_182aten__mm")
#loc1670 = loc("2818|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_182aten__view")
#loc1671 = loc("2819|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_367aten__add")
#loc1672 = loc("2824|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_123aten__view")
#loc1673 = loc("2825|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_336aten__permute")
#loc1674 = loc("2828|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_276xla__cast")
#loc1675 = loc("2841|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_278aten__einsum")
#loc1676 = loc("2841|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_278xla__cast")
#loc1677 = loc("2843|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_30aten__permute")
#loc1678 = loc("2846|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_183aten__view")
#loc1679 = loc("2845|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_339aten__permute")
#loc1680 = loc("2846|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_183aten__mm")
#loc1681 = loc("2847|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_368aten__add")
#loc1682 = loc("2848|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_369aten__add")
#loc1683 = loc("2861|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_311xla__mark_tensor")
#loc1684 = loc("2863|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_184aten__view")
#loc1685 = loc("2862|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_340aten__permute")
#loc1686 = loc("2863|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_184aten__mm")
#loc1687 = loc("2864|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_372aten__add")
#loc1688 = loc("2867|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[30].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_313xla__mark_tensor")
#loc1689 = loc("2869|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_185aten__view")
#loc1690 = loc("2868|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_341aten__permute")
#loc1691 = loc("2869|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_185aten__mm")
#loc1692 = loc("2870|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_373aten__add")
#loc1693 = loc("2871|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_374aten__add")
#loc1694 = loc("2874|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_in]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2303|matmul_193aten__view")
#loc1695 = loc("2873|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_in]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2303|permute_354aten__permute")
#loc1696 = loc("2874|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_in]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2303|matmul_193aten__mm")
#loc1697 = loc("2875|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_in]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2303|add_389aten__add")
#loc1698 = loc("2888|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_331xla__mark_tensor")
#loc1699 = loc("2902|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2248|cat_1aten__cat")
#loc1700 = loc("2906|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_195aten__view")
#loc1701 = loc("2905|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|permute_356aten__permute")
#loc1702 = loc("2906|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_195aten__mm")
#loc1703 = loc("2911|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|view_130aten__view")
#loc1704 = loc("2912|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|permute_359aten__permute")
#loc1705 = loc("2916|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_297xla__cast")
#loc1706 = loc("2919|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|permute_361aten__permute")
#loc1707 = loc("2920|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_201aten__mul")
#loc1708 = loc("2922|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_32aten__einsum")
#loc1709 = loc("2923|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|eq_32aten__eq")
#loc1710 = loc("2924|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_64aten__logical_not")
#loc1712 = loc("or.12421")
#loc1713 = loc("select.12422")
#loc1714 = loc("2926|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_65aten__logical_not")
#loc1715 = loc("2928|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_32aten__expand")
#loc1716 = loc("2922|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_32aten__softmax")
#loc1717 = loc("2928|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_32aten__where")
#loc1718 = loc("2907|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|permute_357aten__permute")
#loc1719 = loc("2908|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|matmul_196aten__mm")
#loc1720 = loc("2913|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|view_131aten__view")
#loc1721 = loc("2914|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|permute_360aten__permute")
#loc1722 = loc("2917|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_298xla__cast")
#loc1723 = loc("2930|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_300aten__einsum")
#loc1724 = loc("2930|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_300xla__cast")
#loc1725 = loc("2932|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2769|clone_32aten__permute")
#loc1726 = loc("2935|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_197aten__view")
#loc1727 = loc("2934|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|permute_363aten__permute")
#loc1728 = loc("2935|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_197aten__mm")
#loc1729 = loc("2936|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Dropout[resampler.layers[0].attn.to_out[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2775|clone_33aten__view")
#loc1730 = loc("2937|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2783|divaten__div")
#loc1731 = loc("2938|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2249|add_394aten__add")
#loc1732 = loc("2951|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_339xla__mark_tensor")
#loc1733 = loc("2953|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|GELU[getattr(resampler.layers[0].ff, '1').net[0]]|Linear[getattr(resampler.layers[0].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_198aten__view")
#loc1734 = loc("2952|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|GELU[getattr(resampler.layers[0].ff, '1').net[0]]|Linear[getattr(resampler.layers[0].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|permute_364aten__permute")
#loc1735 = loc("2953|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|GELU[getattr(resampler.layers[0].ff, '1').net[0]]|Linear[getattr(resampler.layers[0].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_198aten__mm")
#loc1736 = loc("2957|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|Dropout[getattr(resampler.layers[0].ff, '1').net[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|clone_34xla__mark_tensor")
#loc1737 = loc("2959|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|Linear[getattr(resampler.layers[0].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_199aten__view")
#loc1738 = loc("2958|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|Linear[getattr(resampler.layers[0].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|permute_365aten__permute")
#loc1739 = loc("2959|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|Linear[getattr(resampler.layers[0].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_199aten__mm")
#loc1740 = loc("2960|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_397aten__add")
#loc1741 = loc("2986|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_349xla__mark_tensor")
#loc1742 = loc("2989|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_200aten__view")
#loc1743 = loc("2988|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|permute_366aten__permute")
#loc1744 = loc("2989|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_200aten__mm")
#loc1745 = loc("2994|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|view_133aten__view")
#loc1746 = loc("2995|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|permute_369aten__permute")
#loc1747 = loc("3000|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_307xla__cast")
#loc1748 = loc("3003|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_208aten__mul")
#loc1749 = loc("2973|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_345xla__mark_tensor")
#loc1750 = loc("2987|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2248|cat_2aten__cat")
#loc1751 = loc("2991|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_201aten__view")
#loc1752 = loc("2990|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|permute_367aten__permute")
#loc1753 = loc("2991|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_201aten__mm")
#loc1754 = loc("2996|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|view_134aten__view")
#loc1755 = loc("2997|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|permute_370aten__permute")
#loc1756 = loc("3001|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_308xla__cast")
#loc1757 = loc("3004|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|permute_372aten__permute")
#loc1758 = loc("3005|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_209aten__mul")
#loc1759 = loc("3007|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_33aten__einsum")
#loc1760 = loc("3008|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|eq_33aten__eq")
#loc1761 = loc("3009|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_66aten__logical_not")
#loc1763 = loc("or.12849")
#loc1764 = loc("select.12850")
#loc1765 = loc("3011|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_67aten__logical_not")
#loc1766 = loc("3013|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_33aten__expand")
#loc1767 = loc("3007|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_33aten__softmax")
#loc1768 = loc("3013|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_33aten__where")
#loc1769 = loc("2992|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|permute_368aten__permute")
#loc1770 = loc("2993|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|matmul_202aten__mm")
#loc1771 = loc("2998|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|view_135aten__view")
#loc1772 = loc("2999|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|permute_371aten__permute")
#loc1773 = loc("3002|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_309xla__cast")
#loc1774 = loc("3015|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_311aten__einsum")
#loc1775 = loc("3015|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_311xla__cast")
#loc1776 = loc("3017|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2769|clone_35aten__permute")
#loc1777 = loc("3020|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_203aten__view")
#loc1778 = loc("3019|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|permute_374aten__permute")
#loc1779 = loc("3020|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_203aten__mm")
#loc1780 = loc("3021|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Dropout[resampler.layers[1].attn.to_out[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2775|clone_36aten__view")
#loc1781 = loc("3022|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2783|div_1aten__div")
#loc1782 = loc("3023|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2249|add_402aten__add")
#loc1783 = loc("3036|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_353xla__mark_tensor")
#loc1784 = loc("3038|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|GELU[getattr(resampler.layers[1].ff, '1').net[0]]|Linear[getattr(resampler.layers[1].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_204aten__view")
#loc1785 = loc("3037|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|GELU[getattr(resampler.layers[1].ff, '1').net[0]]|Linear[getattr(resampler.layers[1].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|permute_375aten__permute")
#loc1786 = loc("3038|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|GELU[getattr(resampler.layers[1].ff, '1').net[0]]|Linear[getattr(resampler.layers[1].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_204aten__mm")
#loc1787 = loc("3042|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|Dropout[getattr(resampler.layers[1].ff, '1').net[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|clone_37xla__mark_tensor")
#loc1788 = loc("3044|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|Linear[getattr(resampler.layers[1].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_205aten__view")
#loc1789 = loc("3043|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|Linear[getattr(resampler.layers[1].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|permute_376aten__permute")
#loc1790 = loc("3044|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|Linear[getattr(resampler.layers[1].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_205aten__mm")
#loc1791 = loc("3045|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_405aten__add")
#loc1792 = loc("3071|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_363xla__mark_tensor")
#loc1793 = loc("3074|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_206aten__view")
#loc1794 = loc("3073|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|permute_377aten__permute")
#loc1795 = loc("3074|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_206aten__mm")
#loc1796 = loc("3079|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|view_137aten__view")
#loc1797 = loc("3080|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|permute_380aten__permute")
#loc1798 = loc("3085|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_318xla__cast")
#loc1799 = loc("3088|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_216aten__mul")
#loc1800 = loc("3058|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_359xla__mark_tensor")
#loc1801 = loc("3072|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2248|cat_3aten__cat")
#loc1802 = loc("3076|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_207aten__view")
#loc1803 = loc("3075|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|permute_378aten__permute")
#loc1804 = loc("3076|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_207aten__mm")
#loc1805 = loc("3081|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|view_138aten__view")
#loc1806 = loc("3082|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|permute_381aten__permute")
#loc1807 = loc("3086|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_319xla__cast")
#loc1808 = loc("3089|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|permute_383aten__permute")
#loc1809 = loc("3090|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_217aten__mul")
#loc1810 = loc("3092|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_34aten__einsum")
#loc1811 = loc("3093|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|eq_34aten__eq")
#loc1812 = loc("3094|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_68aten__logical_not")
#loc1814 = loc("or.13277")
#loc1815 = loc("select.13278")
#loc1816 = loc("3096|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_69aten__logical_not")
#loc1817 = loc("3098|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_34aten__expand")
#loc1818 = loc("3092|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_34aten__softmax")
#loc1819 = loc("3098|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_34aten__where")
#loc1820 = loc("3077|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|permute_379aten__permute")
#loc1821 = loc("3078|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|matmul_208aten__mm")
#loc1822 = loc("3083|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|view_139aten__view")
#loc1823 = loc("3084|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|permute_382aten__permute")
#loc1824 = loc("3087|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_320xla__cast")
#loc1825 = loc("3100|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_322aten__einsum")
#loc1826 = loc("3100|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_322xla__cast")
#loc1827 = loc("3102|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2769|clone_38aten__permute")
#loc1828 = loc("3105|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_209aten__view")
#loc1829 = loc("3104|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|permute_385aten__permute")
#loc1830 = loc("3105|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_209aten__mm")
#loc1831 = loc("3106|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Dropout[resampler.layers[2].attn.to_out[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2775|clone_39aten__view")
#loc1832 = loc("3107|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2783|div_2aten__div")
#loc1833 = loc("3108|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2249|add_410aten__add")
#loc1834 = loc("3121|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_367xla__mark_tensor")
#loc1835 = loc("3123|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|GELU[getattr(resampler.layers[2].ff, '1').net[0]]|Linear[getattr(resampler.layers[2].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_210aten__view")
#loc1836 = loc("3122|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|GELU[getattr(resampler.layers[2].ff, '1').net[0]]|Linear[getattr(resampler.layers[2].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|permute_386aten__permute")
#loc1837 = loc("3123|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|GELU[getattr(resampler.layers[2].ff, '1').net[0]]|Linear[getattr(resampler.layers[2].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_210aten__mm")
#loc1838 = loc("3127|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|Dropout[getattr(resampler.layers[2].ff, '1').net[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|clone_40xla__mark_tensor")
#loc1839 = loc("3129|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|Linear[getattr(resampler.layers[2].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_211aten__view")
#loc1840 = loc("3128|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|Linear[getattr(resampler.layers[2].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|permute_387aten__permute")
#loc1841 = loc("3129|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|Linear[getattr(resampler.layers[2].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_211aten__mm")
#loc1842 = loc("3130|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_413aten__add")
#loc1843 = loc("3156|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_377xla__mark_tensor")
#loc1844 = loc("3159|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_212aten__view")
#loc1845 = loc("3158|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|permute_388aten__permute")
#loc1846 = loc("3159|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_212aten__mm")
#loc1847 = loc("3164|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|view_141aten__view")
#loc1848 = loc("3165|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|permute_391aten__permute")
#loc1849 = loc("3170|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_329xla__cast")
#loc1850 = loc("3173|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_224aten__mul")
#loc1851 = loc("3143|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_373xla__mark_tensor")
#loc1852 = loc("3157|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2248|cat_4aten__cat")
#loc1853 = loc("3161|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_213aten__view")
#loc1854 = loc("3160|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|permute_389aten__permute")
#loc1855 = loc("3161|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_213aten__mm")
#loc1856 = loc("3166|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|view_142aten__view")
#loc1857 = loc("3167|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|permute_392aten__permute")
#loc1858 = loc("3171|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_330xla__cast")
#loc1859 = loc("3174|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|permute_394aten__permute")
#loc1860 = loc("3175|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_225aten__mul")
#loc1861 = loc("3177|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_35aten__einsum")
#loc1862 = loc("3178|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|eq_35aten__eq")
#loc1863 = loc("3179|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_70aten__logical_not")
#loc1865 = loc("or.13705")
#loc1866 = loc("select.13706")
#loc1867 = loc("3181|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_71aten__logical_not")
#loc1868 = loc("3183|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_35aten__expand")
#loc1869 = loc("3177|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_35aten__softmax")
#loc1870 = loc("3183|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_35aten__where")
#loc1871 = loc("3162|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|permute_390aten__permute")
#loc1872 = loc("3163|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|matmul_214aten__mm")
#loc1873 = loc("3168|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|view_143aten__view")
#loc1874 = loc("3169|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|permute_393aten__permute")
#loc1875 = loc("3172|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_331xla__cast")
#loc1876 = loc("3185|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_333aten__einsum")
#loc1877 = loc("3185|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_333xla__cast")
#loc1878 = loc("3187|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2769|clone_41aten__permute")
#loc1879 = loc("3190|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_215aten__view")
#loc1880 = loc("3189|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|permute_396aten__permute")
#loc1881 = loc("3190|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_215aten__mm")
#loc1882 = loc("3191|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Dropout[resampler.layers[3].attn.to_out[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2775|clone_42aten__view")
#loc1883 = loc("3192|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2783|div_3aten__div")
#loc1884 = loc("3193|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2249|add_418aten__add")
#loc1885 = loc("3206|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_381xla__mark_tensor")
#loc1886 = loc("3208|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|GELU[getattr(resampler.layers[3].ff, '1').net[0]]|Linear[getattr(resampler.layers[3].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_216aten__view")
#loc1887 = loc("3207|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|GELU[getattr(resampler.layers[3].ff, '1').net[0]]|Linear[getattr(resampler.layers[3].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|permute_397aten__permute")
#loc1888 = loc("3208|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|GELU[getattr(resampler.layers[3].ff, '1').net[0]]|Linear[getattr(resampler.layers[3].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_216aten__mm")
#loc1889 = loc("3212|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|Dropout[getattr(resampler.layers[3].ff, '1').net[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|clone_43xla__mark_tensor")
#loc1890 = loc("3214|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|Linear[getattr(resampler.layers[3].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_217aten__view")
#loc1891 = loc("3213|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|Linear[getattr(resampler.layers[3].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|permute_398aten__permute")
#loc1892 = loc("3214|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|Linear[getattr(resampler.layers[3].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_217aten__mm")
#loc1893 = loc("3215|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_421aten__add")
#loc1894 = loc("3217|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2309|matmul_218aten__view")
#loc1895 = loc("3216|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2309|permute_399aten__permute")
#loc1896 = loc("3217|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2309|matmul_218aten__mm")
#loc1897 = loc("3218|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2309|add_422aten__add")
#loc1898 = loc("3231|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|mark_tensor_387xla__mark_tensor")
#loc1902 = loc("3222|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|_to_copy_336xla__cast")
#loc1903 = loc("3223|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|var_mean_78aten__var_mean")
#loc1904 = loc("3226|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|sub_78aten__sub")
#loc1905 = loc("3224|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|add_423aten__add")
#loc1906 = loc("3225|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|rsqrt_78aten__rsqrt")
#loc1907 = loc("3227|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|mul_228aten__mul")
#loc1908 = loc("3228|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|mul_229xla__cast")
#loc1909 = loc("3228|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|mul_229aten__mul")
#loc1910 = loc("3229|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|add_424aten__add")
#loc1911 = loc("3230|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|_to_copy_337xla__cast")
#loc1915 = loc("3197|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|_to_copy_334xla__cast")
#loc1916 = loc("3198|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|var_mean_77aten__var_mean")
#loc1917 = loc("3201|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|sub_77aten__sub")
#loc1918 = loc("3199|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_419aten__add")
#loc1919 = loc("3200|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|rsqrt_77aten__rsqrt")
#loc1920 = loc("3202|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_226aten__mul")
#loc1921 = loc("3203|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_227xla__cast")
#loc1922 = loc("3203|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_227aten__mul")
#loc1923 = loc("3204|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_420aten__add")
#loc1924 = loc("3205|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|_to_copy_335xla__cast")
#loc1928 = loc("3134|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|_to_copy_325xla__cast")
#loc1929 = loc("3135|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|var_mean_75aten__var_mean")
#loc1930 = loc("3138|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|sub_75aten__sub")
#loc1931 = loc("3136|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|add_414aten__add")
#loc1932 = loc("3137|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|rsqrt_75aten__rsqrt")
#loc1933 = loc("3139|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_220aten__mul")
#loc1934 = loc("3140|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_221xla__cast")
#loc1935 = loc("3140|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_221aten__mul")
#loc1936 = loc("3141|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|add_415aten__add")
#loc1937 = loc("3142|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|_to_copy_326xla__cast")
#loc1941 = loc("3049|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|_to_copy_314xla__cast")
#loc1942 = loc("3050|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|var_mean_72aten__var_mean")
#loc1943 = loc("3053|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|sub_72aten__sub")
#loc1944 = loc("3051|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|add_406aten__add")
#loc1945 = loc("3052|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|rsqrt_72aten__rsqrt")
#loc1946 = loc("3054|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_212aten__mul")
#loc1947 = loc("3055|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_213xla__cast")
#loc1948 = loc("3055|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_213aten__mul")
#loc1949 = loc("3056|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|add_407aten__add")
#loc1950 = loc("3057|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|_to_copy_315xla__cast")
#loc1952 = loc("3040|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|GELU[getattr(resampler.layers[1].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|gelu_33aten__gelu")
#loc1954 = loc("2955|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|GELU[getattr(resampler.layers[0].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|gelu_32aten__gelu")
#loc1958 = loc("2942|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|_to_copy_301xla__cast")
#loc1959 = loc("2943|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|var_mean_68aten__var_mean")
#loc1960 = loc("2946|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|sub_68aten__sub")
#loc1961 = loc("2944|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_395aten__add")
#loc1962 = loc("2945|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|rsqrt_68aten__rsqrt")
#loc1963 = loc("2947|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_202aten__mul")
#loc1964 = loc("2948|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_203xla__cast")
#loc1965 = loc("2948|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_203aten__mul")
#loc1966 = loc("2949|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_396aten__add")
#loc1967 = loc("2950|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|_to_copy_302xla__cast")
#loc1969 = loc("2792|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[29].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_29aten__gelu")
#loc1973 = loc("2727|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_263xla__cast")
#loc1974 = loc("2728|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_59aten__var_mean")
#loc1975 = loc("2731|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_59aten__sub")
#loc1976 = loc("2729|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_351aten__add")
#loc1977 = loc("2730|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_59aten__rsqrt")
#loc1978 = loc("2732|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_176aten__mul")
#loc1979 = loc("2733|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_177xla__cast")
#loc1980 = loc("2733|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_177aten__mul")
#loc1981 = loc("2734|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_352aten__add")
#loc1982 = loc("2735|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_264xla__cast")
#loc1984 = loc("2718|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[28].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_28aten__gelu")
#loc1988 = loc("2977|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|_to_copy_305xla__cast")
#loc1989 = loc("2978|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|var_mean_70aten__var_mean")
#loc1990 = loc("2981|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|sub_70aten__sub")
#loc1991 = loc("2979|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|add_400aten__add")
#loc1992 = loc("2980|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|rsqrt_70aten__rsqrt")
#loc1993 = loc("2982|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_206aten__mul")
#loc1994 = loc("2983|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_207xla__cast")
#loc1995 = loc("2983|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_207aten__mul")
#loc1996 = loc("2984|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|add_401aten__add")
#loc1997 = loc("2985|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|_to_copy_306xla__cast")
#loc2001 = loc("2704|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_261xla__cast")
#loc2002 = loc("2705|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_58aten__var_mean")
#loc2003 = loc("2708|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_58aten__sub")
#loc2004 = loc("2706|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_346aten__add")
#loc2005 = loc("2707|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_58aten__rsqrt")
#loc2006 = loc("2709|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_174aten__mul")
#loc2007 = loc("2710|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_175xla__cast")
#loc2008 = loc("2710|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_175aten__mul")
#loc2009 = loc("2711|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_347aten__add")
#loc2010 = loc("2712|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_262xla__cast")
#loc2014 = loc("2653|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_254xla__cast")
#loc2015 = loc("2654|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_57aten__var_mean")
#loc2016 = loc("2657|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_57aten__sub")
#loc2017 = loc("2655|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_339aten__add")
#loc2018 = loc("2656|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_57aten__rsqrt")
#loc2019 = loc("2658|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_170aten__mul")
#loc2020 = loc("2659|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_171xla__cast")
#loc2021 = loc("2659|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_171aten__mul")
#loc2022 = loc("2660|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_340aten__add")
#loc2023 = loc("2661|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_255xla__cast")
#loc2025 = loc("3125|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|GELU[getattr(resampler.layers[2].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|gelu_34aten__gelu")
#loc2029 = loc("2579|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_245xla__cast")
#loc2030 = loc("2580|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_55aten__var_mean")
#loc2031 = loc("2583|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_55aten__sub")
#loc2032 = loc("2581|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_327aten__add")
#loc2033 = loc("2582|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_55aten__rsqrt")
#loc2034 = loc("2584|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_164aten__mul")
#loc2035 = loc("2585|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_165xla__cast")
#loc2036 = loc("2585|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_165aten__mul")
#loc2037 = loc("2586|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_328aten__add")
#loc2038 = loc("2587|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_246xla__cast")
#loc2042 = loc("2556|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_243xla__cast")
#loc2043 = loc("2557|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_54aten__var_mean")
#loc2044 = loc("2560|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_54aten__sub")
#loc2045 = loc("2558|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_322aten__add")
#loc2046 = loc("2559|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_54aten__rsqrt")
#loc2047 = loc("2561|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_162aten__mul")
#loc2048 = loc("2562|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_163xla__cast")
#loc2049 = loc("2562|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_163aten__mul")
#loc2050 = loc("2563|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_323aten__add")
#loc2051 = loc("2564|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_244xla__cast")
#loc2055 = loc("2505|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_236xla__cast")
#loc2056 = loc("2506|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_53aten__var_mean")
#loc2057 = loc("2509|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_53aten__sub")
#loc2058 = loc("2507|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_315aten__add")
#loc2059 = loc("2508|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_53aten__rsqrt")
#loc2060 = loc("2510|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_158aten__mul")
#loc2061 = loc("2511|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_159xla__cast")
#loc2062 = loc("2511|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_159aten__mul")
#loc2063 = loc("2512|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_316aten__add")
#loc2064 = loc("2513|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_237xla__cast")
#loc2066 = loc("2496|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[25].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_25aten__gelu")
#loc2070 = loc("2482|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_234xla__cast")
#loc2071 = loc("2483|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_52aten__var_mean")
#loc2072 = loc("2486|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_52aten__sub")
#loc2073 = loc("2484|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_310aten__add")
#loc2074 = loc("2485|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_52aten__rsqrt")
#loc2075 = loc("2487|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_156aten__mul")
#loc2076 = loc("2488|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_157xla__cast")
#loc2077 = loc("2488|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_157aten__mul")
#loc2078 = loc("2489|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_311aten__add")
#loc2079 = loc("2490|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_235xla__cast")
#loc2083 = loc("2431|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_227xla__cast")
#loc2084 = loc("2432|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_51aten__var_mean")
#loc2085 = loc("2435|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_51aten__sub")
#loc2086 = loc("2433|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_303aten__add")
#loc2087 = loc("2434|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_51aten__rsqrt")
#loc2088 = loc("2436|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_152aten__mul")
#loc2089 = loc("2437|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_153xla__cast")
#loc2090 = loc("2437|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_153aten__mul")
#loc2091 = loc("2438|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_304aten__add")
#loc2092 = loc("2439|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_228xla__cast")
#loc2094 = loc("2422|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[24].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_24aten__gelu")
#loc2098 = loc("2408|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_225xla__cast")
#loc2099 = loc("2409|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_50aten__var_mean")
#loc2100 = loc("2412|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_50aten__sub")
#loc2101 = loc("2410|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_298aten__add")
#loc2102 = loc("2411|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_50aten__rsqrt")
#loc2103 = loc("2413|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_150aten__mul")
#loc2104 = loc("2414|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_151xla__cast")
#loc2105 = loc("2414|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_151aten__mul")
#loc2106 = loc("2415|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_299aten__add")
#loc2107 = loc("2416|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_226xla__cast")
#loc2111 = loc("2357|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_218xla__cast")
#loc2112 = loc("2358|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_49aten__var_mean")
#loc2113 = loc("2361|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_49aten__sub")
#loc2114 = loc("2359|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_291aten__add")
#loc2115 = loc("2360|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_49aten__rsqrt")
#loc2116 = loc("2362|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_146aten__mul")
#loc2117 = loc("2363|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_147xla__cast")
#loc2118 = loc("2363|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_147aten__mul")
#loc2119 = loc("2364|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_292aten__add")
#loc2120 = loc("2365|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_219xla__cast")
#loc2122 = loc("2348|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[23].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_23aten__gelu")
#loc2126 = loc("2334|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_216xla__cast")
#loc2127 = loc("2335|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_48aten__var_mean")
#loc2128 = loc("2338|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_48aten__sub")
#loc2129 = loc("2336|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_286aten__add")
#loc2130 = loc("2337|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_48aten__rsqrt")
#loc2131 = loc("2339|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_144aten__mul")
#loc2132 = loc("2340|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_145xla__cast")
#loc2133 = loc("2340|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_145aten__mul")
#loc2134 = loc("2341|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_287aten__add")
#loc2135 = loc("2342|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_217xla__cast")
#loc2139 = loc("2209|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_200xla__cast")
#loc2140 = loc("2210|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_45aten__var_mean")
#loc2141 = loc("2213|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_45aten__sub")
#loc2142 = loc("2211|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_267aten__add")
#loc2143 = loc("2212|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_45aten__rsqrt")
#loc2144 = loc("2214|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_134aten__mul")
#loc2145 = loc("2215|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_135xla__cast")
#loc2146 = loc("2215|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_135aten__mul")
#loc2147 = loc("2216|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_268aten__add")
#loc2148 = loc("2217|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_201xla__cast")
#loc2150 = loc("2200|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[21].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_21aten__gelu")
#loc2154 = loc("3027|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|_to_copy_312xla__cast")
#loc2155 = loc("3028|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|var_mean_71aten__var_mean")
#loc2156 = loc("3031|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|sub_71aten__sub")
#loc2157 = loc("3029|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_403aten__add")
#loc2158 = loc("3030|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|rsqrt_71aten__rsqrt")
#loc2159 = loc("3032|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_210aten__mul")
#loc2160 = loc("3033|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_211xla__cast")
#loc2161 = loc("3033|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_211aten__mul")
#loc2162 = loc("3034|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_404aten__add")
#loc2163 = loc("3035|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|_to_copy_313xla__cast")
#loc2167 = loc("2186|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_198xla__cast")
#loc2168 = loc("2187|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_44aten__var_mean")
#loc2169 = loc("2190|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_44aten__sub")
#loc2170 = loc("2188|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_262aten__add")
#loc2171 = loc("2189|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_44aten__rsqrt")
#loc2172 = loc("2191|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_132aten__mul")
#loc2173 = loc("2192|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_133xla__cast")
#loc2174 = loc("2192|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_133aten__mul")
#loc2175 = loc("2193|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_263aten__add")
#loc2176 = loc("2194|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_199xla__cast")
#loc2178 = loc("2126|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[20].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_20aten__gelu")
#loc2180 = loc("2570|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[26].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_26aten__gelu")
#loc2184 = loc("2061|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_182xla__cast")
#loc2185 = loc("2062|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_41aten__var_mean")
#loc2186 = loc("2065|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_41aten__sub")
#loc2187 = loc("2063|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_243aten__add")
#loc2188 = loc("2064|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_41aten__rsqrt")
#loc2189 = loc("2066|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_122aten__mul")
#loc2190 = loc("2067|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_123xla__cast")
#loc2191 = loc("2067|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_123aten__mul")
#loc2192 = loc("2068|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_244aten__add")
#loc2193 = loc("2069|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_183xla__cast")
#loc2195 = loc("2052|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[19].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_19aten__gelu")
#loc2199 = loc("1987|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_173xla__cast")
#loc2200 = loc("1988|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_39aten__var_mean")
#loc2201 = loc("1991|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_39aten__sub")
#loc2202 = loc("1989|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_231aten__add")
#loc2203 = loc("1990|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_39aten__rsqrt")
#loc2204 = loc("1992|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_116aten__mul")
#loc2205 = loc("1993|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_117xla__cast")
#loc2206 = loc("1993|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_117aten__mul")
#loc2207 = loc("1994|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_232aten__add")
#loc2208 = loc("1995|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_174xla__cast")
#loc2210 = loc("1238|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[8].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_8aten__gelu")
#loc2214 = loc("1321|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_92xla__cast")
#loc2215 = loc("1322|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_21aten__var_mean")
#loc2216 = loc("1325|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_21aten__sub")
#loc2217 = loc("1323|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_123aten__add")
#loc2218 = loc("1324|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_21aten__rsqrt")
#loc2219 = loc("1326|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_62aten__mul")
#loc2220 = loc("1327|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_63xla__cast")
#loc2221 = loc("1327|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_63aten__mul")
#loc2222 = loc("1328|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_124aten__add")
#loc2223 = loc("1329|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_93xla__cast")
#loc2227 = loc("1173|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_74xla__cast")
#loc2228 = loc("1174|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_17aten__var_mean")
#loc2229 = loc("1177|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_17aten__sub")
#loc2230 = loc("1175|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_99aten__add")
#loc2231 = loc("1176|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_17aten__rsqrt")
#loc2232 = loc("1178|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_50aten__mul")
#loc2233 = loc("1179|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_51xla__cast")
#loc2234 = loc("1179|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_51aten__mul")
#loc2235 = loc("1180|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_100aten__add")
#loc2236 = loc("1181|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_75xla__cast")
#loc2238 = loc("2866|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[30].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_30aten__gelu")
#loc2240 = loc("1164|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[7].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_7aten__gelu")
#loc2244 = loc("2852|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_279xla__cast")
#loc2245 = loc("2853|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_62aten__var_mean")
#loc2246 = loc("2856|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_62aten__sub")
#loc2247 = loc("2854|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_370aten__add")
#loc2248 = loc("2855|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_62aten__rsqrt")
#loc2249 = loc("2857|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_186aten__mul")
#loc2250 = loc("2858|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_187xla__cast")
#loc2251 = loc("2858|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_187aten__mul")
#loc2252 = loc("2859|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_371aten__add")
#loc2253 = loc("2860|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_280xla__cast")
#loc2255 = loc("2274|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[22].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_22aten__gelu")
#loc2259 = loc("2260|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_207xla__cast")
#loc2260 = loc("2261|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_46aten__var_mean")
#loc2261 = loc("2264|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_46aten__sub")
#loc2262 = loc("2262|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_274aten__add")
#loc2263 = loc("2263|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_46aten__rsqrt")
#loc2264 = loc("2265|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_138aten__mul")
#loc2265 = loc("2266|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_139xla__cast")
#loc2266 = loc("2266|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_139aten__mul")
#loc2267 = loc("2267|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_275aten__add")
#loc2268 = loc("2268|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_208xla__cast")
#loc2270 = loc("1090|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[6].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_6aten__gelu")
#loc2274 = loc("1076|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_63xla__cast")
#loc2275 = loc("1077|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_14aten__var_mean")
#loc2276 = loc("1080|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_14aten__sub")
#loc2277 = loc("1078|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_82aten__add")
#loc2278 = loc("1079|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_14aten__rsqrt")
#loc2279 = loc("1081|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_42aten__mul")
#loc2280 = loc("1082|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_43xla__cast")
#loc2281 = loc("1082|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_43aten__mul")
#loc2282 = loc("1083|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_83aten__add")
#loc2283 = loc("1084|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_64xla__cast")
#loc2285 = loc("646|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[0].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|geluaten__gelu")
#loc2289 = loc("3112|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|_to_copy_323xla__cast")
#loc2290 = loc("3113|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|var_mean_74aten__var_mean")
#loc2291 = loc("3116|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|sub_74aten__sub")
#loc2292 = loc("3114|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_411aten__add")
#loc2293 = loc("3115|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|rsqrt_74aten__rsqrt")
#loc2294 = loc("3117|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_218aten__mul")
#loc2295 = loc("3118|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_219xla__cast")
#loc2296 = loc("3118|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_219aten__mul")
#loc2297 = loc("3119|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_412aten__add")
#loc2298 = loc("3120|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|_to_copy_324xla__cast")
#loc2302 = loc("1224|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_81xla__cast")
#loc2303 = loc("1225|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_18aten__var_mean")
#loc2304 = loc("1228|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_18aten__sub")
#loc2305 = loc("1226|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_106aten__add")
#loc2306 = loc("1227|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_18aten__rsqrt")
#loc2307 = loc("1229|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_54aten__mul")
#loc2308 = loc("1230|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_55xla__cast")
#loc2309 = loc("1230|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_55aten__mul")
#loc2310 = loc("1231|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_107aten__add")
#loc2311 = loc("1232|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_82xla__cast")
#loc2315 = loc("729|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_20xla__cast")
#loc2316 = loc("730|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_5aten__var_mean")
#loc2317 = loc("733|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_5aten__sub")
#loc2318 = loc("731|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_27aten__add")
#loc2319 = loc("732|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_5aten__rsqrt")
#loc2320 = loc("734|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_14aten__mul")
#loc2321 = loc("735|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_15xla__cast")
#loc2322 = loc("735|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_15aten__mul")
#loc2323 = loc("736|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_28aten__add")
#loc2324 = loc("737|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_21xla__cast")
#loc2328 = loc("1150|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_72xla__cast")
#loc2329 = loc("1151|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_16aten__var_mean")
#loc2330 = loc("1154|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_16aten__sub")
#loc2331 = loc("1152|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_94aten__add")
#loc2332 = loc("1153|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_16aten__rsqrt")
#loc2333 = loc("1155|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_48aten__mul")
#loc2334 = loc("1156|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_49xla__cast")
#loc2335 = loc("1156|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_49aten__mul")
#loc2336 = loc("1157|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_95aten__add")
#loc2337 = loc("1158|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_73xla__cast")
#loc2341 = loc("1839|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_155xla__cast")
#loc2342 = loc("1840|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_35aten__var_mean")
#loc2343 = loc("1843|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_35aten__sub")
#loc2344 = loc("1841|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_207aten__add")
#loc2345 = loc("1842|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_35aten__rsqrt")
#loc2346 = loc("1844|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_104aten__mul")
#loc2347 = loc("1845|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_105xla__cast")
#loc2348 = loc("1845|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_105aten__mul")
#loc2349 = loc("1846|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_208aten__add")
#loc2350 = loc("1847|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_156xla__cast")
#loc2352 = loc("942|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[4].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_4aten__gelu")
#loc2356 = loc("2801|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_272xla__cast")
#loc2357 = loc("2802|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_61aten__var_mean")
#loc2358 = loc("2805|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_61aten__sub")
#loc2359 = loc("2803|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_363aten__add")
#loc2360 = loc("2804|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_61aten__rsqrt")
#loc2361 = loc("2806|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_182aten__mul")
#loc2362 = loc("2807|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_183xla__cast")
#loc2363 = loc("2807|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_183aten__mul")
#loc2364 = loc("2808|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_364aten__add")
#loc2365 = loc("2809|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_273xla__cast")
#loc2369 = loc("780|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_27xla__cast")
#loc2370 = loc("781|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_6aten__var_mean")
#loc2371 = loc("784|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_6aten__sub")
#loc2372 = loc("782|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_34aten__add")
#loc2373 = loc("783|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_6aten__rsqrt")
#loc2374 = loc("785|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_18aten__mul")
#loc2375 = loc("786|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_19xla__cast")
#loc2376 = loc("786|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_19aten__mul")
#loc2377 = loc("787|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_35aten__add")
#loc2378 = loc("788|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_28xla__cast")
#loc2382 = loc("1668|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_135xla__cast")
#loc2383 = loc("1669|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_30aten__var_mean")
#loc2384 = loc("1672|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_30aten__sub")
#loc2385 = loc("1670|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_178aten__add")
#loc2386 = loc("1671|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_30aten__rsqrt")
#loc2387 = loc("1673|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_90aten__mul")
#loc2388 = loc("1674|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_91xla__cast")
#loc2389 = loc("1674|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_91aten__mul")
#loc2390 = loc("1675|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_179aten__add")
#loc2391 = loc("1676|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_136xla__cast")
#loc2395 = loc("581|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_2xla__cast")
#loc2396 = loc("582|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_1aten__var_mean")
#loc2397 = loc("585|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_1aten__sub")
#loc2398 = loc("583|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_3aten__add")
#loc2399 = loc("584|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_1aten__rsqrt")
#loc2400 = loc("586|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_2aten__mul")
#loc2401 = loc("587|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_3xla__cast")
#loc2402 = loc("587|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_3aten__mul")
#loc2403 = loc("588|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_4aten__add")
#loc2404 = loc("589|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_3xla__cast")
#loc2406 = loc("1534|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[12].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_12aten__gelu")
#loc2410 = loc("632|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_9xla__cast")
#loc2411 = loc("633|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_2aten__var_mean")
#loc2412 = loc("636|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_2aten__sub")
#loc2413 = loc("634|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_10aten__add")
#loc2414 = loc("635|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_2aten__rsqrt")
#loc2415 = loc("637|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_6aten__mul")
#loc2416 = loc("638|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_7xla__cast")
#loc2417 = loc("638|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_7aten__mul")
#loc2418 = loc("639|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_11aten__add")
#loc2419 = loc("640|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_10xla__cast")
#loc2423 = loc("706|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_18xla__cast")
#loc2424 = loc("707|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_4aten__var_mean")
#loc2425 = loc("710|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_4aten__sub")
#loc2426 = loc("708|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_22aten__add")
#loc2427 = loc("709|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_4aten__rsqrt")
#loc2428 = loc("711|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_12aten__mul")
#loc2429 = loc("712|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_13xla__cast")
#loc2430 = loc("712|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_13aten__mul")
#loc2431 = loc("713|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_23aten__add")
#loc2432 = loc("714|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_19xla__cast")
#loc2434 = loc("720|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[1].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_1aten__gelu")
#loc2438 = loc("568|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|_to_copyxla__cast")
#loc2439 = loc("569|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|var_meanaten__var_mean")
#loc2440 = loc("572|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|subaten__sub")
#loc2441 = loc("570|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|add_1aten__add")
#loc2442 = loc("571|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|rsqrtaten__rsqrt")
#loc2443 = loc("573|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|mulaten__mul")
#loc2444 = loc("574|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|mul_1xla__cast")
#loc2445 = loc("574|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|mul_1aten__mul")
#loc2446 = loc("575|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|add_2aten__add")
#loc2447 = loc("576|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|_to_copy_1xla__cast")
#loc2451 = loc("928|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_45xla__cast")
#loc2452 = loc("929|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_10aten__var_mean")
#loc2453 = loc("932|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_10aten__sub")
#loc2454 = loc("930|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_58aten__add")
#loc2455 = loc("931|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_10aten__rsqrt")
#loc2456 = loc("933|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_30aten__mul")
#loc2457 = loc("934|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_31xla__cast")
#loc2458 = loc("934|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_31aten__mul")
#loc2459 = loc("935|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_59aten__add")
#loc2460 = loc("936|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_46xla__cast")
#loc2464 = loc("655|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_11xla__cast")
#loc2465 = loc("656|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_3aten__var_mean")
#loc2466 = loc("659|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_3aten__sub")
#loc2467 = loc("657|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_15aten__add")
#loc2468 = loc("658|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_3aten__rsqrt")
#loc2469 = loc("660|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_8aten__mul")
#loc2470 = loc("661|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_9xla__cast")
#loc2471 = loc("661|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_9aten__mul")
#loc2472 = loc("662|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_16aten__add")
#loc2473 = loc("663|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_12xla__cast")
#loc2477 = loc("1691|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_137xla__cast")
#loc2478 = loc("1692|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_31aten__var_mean")
#loc2479 = loc("1695|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_31aten__sub")
#loc2480 = loc("1693|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_183aten__add")
#loc2481 = loc("1694|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_31aten__rsqrt")
#loc2482 = loc("1696|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_92aten__mul")
#loc2483 = loc("1697|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_93xla__cast")
#loc2484 = loc("1697|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_93aten__mul")
#loc2485 = loc("1698|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_184aten__add")
#loc2486 = loc("1699|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_138xla__cast")
#loc2488 = loc("868|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[3].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_3aten__gelu")
#loc2492 = loc("1002|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_54xla__cast")
#loc2493 = loc("1003|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_12aten__var_mean")
#loc2494 = loc("1006|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_12aten__sub")
#loc2495 = loc("1004|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_70aten__add")
#loc2496 = loc("1005|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_12aten__rsqrt")
#loc2497 = loc("1007|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_36aten__mul")
#loc2498 = loc("1008|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_37xla__cast")
#loc2499 = loc("1008|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_37aten__mul")
#loc2500 = loc("1009|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_71aten__add")
#loc2501 = loc("1010|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_55xla__cast")
#loc2503 = loc("794|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[2].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_2aten__gelu")
#loc2507 = loc("951|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_47xla__cast")
#loc2508 = loc("952|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_11aten__var_mean")
#loc2509 = loc("955|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_11aten__sub")
#loc2510 = loc("953|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_63aten__add")
#loc2511 = loc("954|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_11aten__rsqrt")
#loc2512 = loc("956|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_32aten__mul")
#loc2513 = loc("957|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_33xla__cast")
#loc2514 = loc("957|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_33aten__mul")
#loc2515 = loc("958|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_64aten__add")
#loc2516 = loc("959|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_48xla__cast")
#loc2520 = loc("803|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_29xla__cast")
#loc2521 = loc("804|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_7aten__var_mean")
#loc2522 = loc("807|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_7aten__sub")
#loc2523 = loc("805|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_39aten__add")
#loc2524 = loc("806|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_7aten__rsqrt")
#loc2525 = loc("808|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_20aten__mul")
#loc2526 = loc("809|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_21xla__cast")
#loc2527 = loc("809|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_21aten__mul")
#loc2528 = loc("810|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_40aten__add")
#loc2529 = loc("811|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_30xla__cast")
#loc2533 = loc("1765|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_146xla__cast")
#loc2534 = loc("1766|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_33aten__var_mean")
#loc2535 = loc("1769|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_33aten__sub")
#loc2536 = loc("1767|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_195aten__add")
#loc2537 = loc("1768|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_33aten__rsqrt")
#loc2538 = loc("1770|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_98aten__mul")
#loc2539 = loc("1771|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_99xla__cast")
#loc2540 = loc("1771|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_99aten__mul")
#loc2541 = loc("1772|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_196aten__add")
#loc2542 = loc("1773|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_147xla__cast")
#loc2546 = loc("1298|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_90xla__cast")
#loc2547 = loc("1299|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_20aten__var_mean")
#loc2548 = loc("1302|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_20aten__sub")
#loc2549 = loc("1300|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_118aten__add")
#loc2550 = loc("1301|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_20aten__rsqrt")
#loc2551 = loc("1303|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_60aten__mul")
#loc2552 = loc("1304|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_61xla__cast")
#loc2553 = loc("1304|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_61aten__mul")
#loc2554 = loc("1305|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_119aten__add")
#loc2555 = loc("1306|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_91xla__cast")
#loc2559 = loc("2630|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_252xla__cast")
#loc2560 = loc("2631|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_56aten__var_mean")
#loc2561 = loc("2634|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_56aten__sub")
#loc2562 = loc("2632|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_334aten__add")
#loc2563 = loc("2633|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_56aten__rsqrt")
#loc2564 = loc("2635|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_168aten__mul")
#loc2565 = loc("2636|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_169xla__cast")
#loc2566 = loc("2636|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_169aten__mul")
#loc2567 = loc("2637|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_335aten__add")
#loc2568 = loc("2638|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_253xla__cast")
#loc2572 = loc("2112|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_189xla__cast")
#loc2573 = loc("2113|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_42aten__var_mean")
#loc2574 = loc("2116|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_42aten__sub")
#loc2575 = loc("2114|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_250aten__add")
#loc2576 = loc("2115|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_42aten__rsqrt")
#loc2577 = loc("2117|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_126aten__mul")
#loc2578 = loc("2118|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_127xla__cast")
#loc2579 = loc("2118|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_127aten__mul")
#loc2580 = loc("2119|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_251aten__add")
#loc2581 = loc("2120|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_190xla__cast")
#loc2585 = loc("1742|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_144xla__cast")
#loc2586 = loc("1743|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_32aten__var_mean")
#loc2587 = loc("1746|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_32aten__sub")
#loc2588 = loc("1744|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_190aten__add")
#loc2589 = loc("1745|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_32aten__rsqrt")
#loc2590 = loc("1747|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_96aten__mul")
#loc2591 = loc("1748|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_97xla__cast")
#loc2592 = loc("1748|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_97aten__mul")
#loc2593 = loc("1749|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_191aten__add")
#loc2594 = loc("1750|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_145xla__cast")
#loc2596 = loc("1312|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[9].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_9aten__gelu")
#loc2600 = loc("3062|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|_to_copy_316xla__cast")
#loc2601 = loc("3063|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|var_mean_73aten__var_mean")
#loc2602 = loc("3066|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|sub_73aten__sub")
#loc2603 = loc("3064|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|add_408aten__add")
#loc2604 = loc("3065|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|rsqrt_73aten__rsqrt")
#loc2605 = loc("3067|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_214aten__mul")
#loc2606 = loc("3068|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_215xla__cast")
#loc2607 = loc("3068|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_215aten__mul")
#loc2608 = loc("3069|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|add_409aten__add")
#loc2609 = loc("3070|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|_to_copy_317xla__cast")
#loc2613 = loc("1816|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_153xla__cast")
#loc2614 = loc("1817|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_34aten__var_mean")
#loc2615 = loc("1820|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_34aten__sub")
#loc2616 = loc("1818|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_202aten__add")
#loc2617 = loc("1819|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_34aten__rsqrt")
#loc2618 = loc("1821|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_102aten__mul")
#loc2619 = loc("1822|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_103xla__cast")
#loc2620 = loc("1822|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_103aten__mul")
#loc2621 = loc("1823|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_203aten__add")
#loc2622 = loc("1824|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_154xla__cast")
#loc2626 = loc("2038|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_180xla__cast")
#loc2627 = loc("2039|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_40aten__var_mean")
#loc2628 = loc("2042|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_40aten__sub")
#loc2629 = loc("2040|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_238aten__add")
#loc2630 = loc("2041|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_40aten__rsqrt")
#loc2631 = loc("2043|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_120aten__mul")
#loc2632 = loc("2044|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_121xla__cast")
#loc2633 = loc("2044|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_121aten__mul")
#loc2634 = loc("2045|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_239aten__add")
#loc2635 = loc("2046|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_181xla__cast")
#loc2639 = loc("854|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_36xla__cast")
#loc2640 = loc("855|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_8aten__var_mean")
#loc2641 = loc("858|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_8aten__sub")
#loc2642 = loc("856|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_46aten__add")
#loc2643 = loc("857|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_8aten__rsqrt")
#loc2644 = loc("859|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_24aten__mul")
#loc2645 = loc("860|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_25xla__cast")
#loc2646 = loc("860|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_25aten__mul")
#loc2647 = loc("861|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_47aten__add")
#loc2648 = loc("862|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_37xla__cast")
#loc2650 = loc("1978|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[18].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_18aten__gelu")
#loc2652 = loc("3210|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|GELU[getattr(resampler.layers[3].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|gelu_35aten__gelu")
#loc2656 = loc("1372|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_99xla__cast")
#loc2657 = loc("1373|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_22aten__var_mean")
#loc2658 = loc("1376|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_22aten__sub")
#loc2659 = loc("1374|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_130aten__add")
#loc2660 = loc("1375|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_22aten__rsqrt")
#loc2661 = loc("1377|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_66aten__mul")
#loc2662 = loc("1378|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_67xla__cast")
#loc2663 = loc("1378|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_67aten__mul")
#loc2664 = loc("1379|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_131aten__add")
#loc2665 = loc("1380|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_100xla__cast")
#loc2669 = loc("2892|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|_to_copy_294xla__cast")
#loc2670 = loc("2893|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|var_mean_67aten__var_mean")
#loc2671 = loc("2896|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|sub_67aten__sub")
#loc2672 = loc("2894|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|add_392aten__add")
#loc2673 = loc("2895|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|rsqrt_67aten__rsqrt")
#loc2674 = loc("2897|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_198aten__mul")
#loc2675 = loc("2898|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_199xla__cast")
#loc2676 = loc("2898|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_199aten__mul")
#loc2677 = loc("2899|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|add_393aten__add")
#loc2678 = loc("2900|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|_to_copy_295xla__cast")
#loc2680 = loc("1386|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[10].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_10aten__gelu")
#loc2684 = loc("2135|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_191xla__cast")
#loc2685 = loc("2136|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_43aten__var_mean")
#loc2686 = loc("2139|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_43aten__sub")
#loc2687 = loc("2137|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_255aten__add")
#loc2688 = loc("2138|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_43aten__rsqrt")
#loc2689 = loc("2140|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_128aten__mul")
#loc2690 = loc("2141|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_129xla__cast")
#loc2691 = loc("2141|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_129aten__mul")
#loc2692 = loc("2142|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_256aten__add")
#loc2693 = loc("2143|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_192xla__cast")
#loc2697 = loc("1395|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_101xla__cast")
#loc2698 = loc("1396|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_23aten__var_mean")
#loc2699 = loc("1399|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_23aten__sub")
#loc2700 = loc("1397|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_135aten__add")
#loc2701 = loc("1398|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_23aten__rsqrt")
#loc2702 = loc("1400|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_68aten__mul")
#loc2703 = loc("1401|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_69xla__cast")
#loc2704 = loc("1401|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_69aten__mul")
#loc2705 = loc("1402|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_136aten__add")
#loc2706 = loc("1403|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_102xla__cast")
#loc2710 = loc("1446|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_108xla__cast")
#loc2711 = loc("1447|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_24aten__var_mean")
#loc2712 = loc("1450|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_24aten__sub")
#loc2713 = loc("1448|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_142aten__add")
#loc2714 = loc("1449|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_24aten__rsqrt")
#loc2715 = loc("1451|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_72aten__mul")
#loc2716 = loc("1452|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_73xla__cast")
#loc2717 = loc("1452|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_73aten__mul")
#loc2718 = loc("1453|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_143aten__add")
#loc2719 = loc("1454|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_109xla__cast")
#loc2723 = loc("2283|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_209xla__cast")
#loc2724 = loc("2284|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_47aten__var_mean")
#loc2725 = loc("2287|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_47aten__sub")
#loc2726 = loc("2285|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_279aten__add")
#loc2727 = loc("2286|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_47aten__rsqrt")
#loc2728 = loc("2288|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_140aten__mul")
#loc2729 = loc("2289|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_141xla__cast")
#loc2730 = loc("2289|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_141aten__mul")
#loc2731 = loc("2290|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_280aten__add")
#loc2732 = loc("2291|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_210xla__cast")
#loc2736 = loc("1247|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_83xla__cast")
#loc2737 = loc("1248|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_19aten__var_mean")
#loc2738 = loc("1251|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_19aten__sub")
#loc2739 = loc("1249|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_111aten__add")
#loc2740 = loc("1250|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_19aten__rsqrt")
#loc2741 = loc("1252|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_56aten__mul")
#loc2742 = loc("1253|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_57xla__cast")
#loc2743 = loc("1253|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_57aten__mul")
#loc2744 = loc("1254|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_112aten__add")
#loc2745 = loc("1255|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_84xla__cast")
#loc2749 = loc("1520|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_117xla__cast")
#loc2750 = loc("1521|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_26aten__var_mean")
#loc2751 = loc("1524|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_26aten__sub")
#loc2752 = loc("1522|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_154aten__add")
#loc2753 = loc("1523|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_26aten__rsqrt")
#loc2754 = loc("1525|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_78aten__mul")
#loc2755 = loc("1526|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_79xla__cast")
#loc2756 = loc("1526|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_79aten__mul")
#loc2757 = loc("1527|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_155aten__add")
#loc2758 = loc("1528|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_118xla__cast")
#loc2760 = loc("1016|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[5].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_5aten__gelu")
#loc2764 = loc("1964|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_171xla__cast")
#loc2765 = loc("1965|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_38aten__var_mean")
#loc2766 = loc("1968|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_38aten__sub")
#loc2767 = loc("1966|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_226aten__add")
#loc2768 = loc("1967|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_38aten__rsqrt")
#loc2769 = loc("1969|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_114aten__mul")
#loc2770 = loc("1970|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_115xla__cast")
#loc2771 = loc("1970|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_115aten__mul")
#loc2772 = loc("1971|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_227aten__add")
#loc2773 = loc("1972|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_172xla__cast")
#loc2777 = loc("1594|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_126xla__cast")
#loc2778 = loc("1595|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_28aten__var_mean")
#loc2779 = loc("1598|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_28aten__sub")
#loc2780 = loc("1596|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_166aten__add")
#loc2781 = loc("1597|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_28aten__rsqrt")
#loc2782 = loc("1599|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_84aten__mul")
#loc2783 = loc("1600|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_85xla__cast")
#loc2784 = loc("1600|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_85aten__mul")
#loc2785 = loc("1601|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_167aten__add")
#loc2786 = loc("1602|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_127xla__cast")
#loc2790 = loc("2879|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|_to_copy_292xla__cast")
#loc2791 = loc("2880|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|var_mean_66aten__var_mean")
#loc2792 = loc("2883|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|sub_66aten__sub")
#loc2793 = loc("2881|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|add_390aten__add")
#loc2794 = loc("2882|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|rsqrt_66aten__rsqrt")
#loc2795 = loc("2884|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_196aten__mul")
#loc2796 = loc("2885|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_197xla__cast")
#loc2797 = loc("2885|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_197aten__mul")
#loc2798 = loc("2886|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|add_391aten__add")
#loc2799 = loc("2887|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|_to_copy_293xla__cast")
#loc2803 = loc("1025|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_56xla__cast")
#loc2804 = loc("1026|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_13aten__var_mean")
#loc2805 = loc("1029|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_13aten__sub")
#loc2806 = loc("1027|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_75aten__add")
#loc2807 = loc("1028|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_13aten__rsqrt")
#loc2808 = loc("1030|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_38aten__mul")
#loc2809 = loc("1031|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_39xla__cast")
#loc2810 = loc("1031|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_39aten__mul")
#loc2811 = loc("1032|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_76aten__add")
#loc2812 = loc("1033|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_57xla__cast")
#loc2814 = loc("1756|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[15].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_15aten__gelu")
#loc2816 = loc("1608|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[13].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_13aten__gelu")
#loc2818 = loc("2644|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[27].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_27aten__gelu")
#loc2822 = loc("1099|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_65xla__cast")
#loc2823 = loc("1100|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_15aten__var_mean")
#loc2824 = loc("1103|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_15aten__sub")
#loc2825 = loc("1101|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_87aten__add")
#loc2826 = loc("1102|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_15aten__rsqrt")
#loc2827 = loc("1104|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_44aten__mul")
#loc2828 = loc("1105|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_45xla__cast")
#loc2829 = loc("1105|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_45aten__mul")
#loc2830 = loc("1106|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_88aten__add")
#loc2831 = loc("1107|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_66xla__cast")
#loc2835 = loc("1617|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_128xla__cast")
#loc2836 = loc("1618|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_29aten__var_mean")
#loc2837 = loc("1621|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_29aten__sub")
#loc2838 = loc("1619|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_171aten__add")
#loc2839 = loc("1620|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_29aten__rsqrt")
#loc2840 = loc("1622|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_86aten__mul")
#loc2841 = loc("1623|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_87xla__cast")
#loc2842 = loc("1623|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_87aten__mul")
#loc2843 = loc("1624|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_172aten__add")
#loc2844 = loc("1625|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_129xla__cast")
#loc2848 = loc("2964|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|_to_copy_303xla__cast")
#loc2849 = loc("2965|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|var_mean_69aten__var_mean")
#loc2850 = loc("2968|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|sub_69aten__sub")
#loc2851 = loc("2966|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|add_398aten__add")
#loc2852 = loc("2967|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|rsqrt_69aten__rsqrt")
#loc2853 = loc("2969|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_204aten__mul")
#loc2854 = loc("2970|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_205xla__cast")
#loc2855 = loc("2970|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_205aten__mul")
#loc2856 = loc("2971|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|add_399aten__add")
#loc2857 = loc("2972|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|_to_copy_304xla__cast")
#loc2859 = loc("1460|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[11].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_11aten__gelu")
#loc2861 = loc("1682|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[14].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_14aten__gelu")
#loc2865 = loc("877|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_38xla__cast")
#loc2866 = loc("878|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_9aten__var_mean")
#loc2867 = loc("881|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_9aten__sub")
#loc2868 = loc("879|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_51aten__add")
#loc2869 = loc("880|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_9aten__rsqrt")
#loc2870 = loc("882|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_26aten__mul")
#loc2871 = loc("883|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_27xla__cast")
#loc2872 = loc("883|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_27aten__mul")
#loc2873 = loc("884|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_52aten__add")
#loc2874 = loc("885|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_39xla__cast")
#loc2878 = loc("1469|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_110xla__cast")
#loc2879 = loc("1470|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_25aten__var_mean")
#loc2880 = loc("1473|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_25aten__sub")
#loc2881 = loc("1471|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_147aten__add")
#loc2882 = loc("1472|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_25aten__rsqrt")
#loc2883 = loc("1474|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_74aten__mul")
#loc2884 = loc("1475|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_75xla__cast")
#loc2885 = loc("1475|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_75aten__mul")
#loc2886 = loc("1476|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_148aten__add")
#loc2887 = loc("1477|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_111xla__cast")
#loc2889 = loc("1830|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[16].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_16aten__gelu")
#loc2893 = loc("1890|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_162xla__cast")
#loc2894 = loc("1891|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_36aten__var_mean")
#loc2895 = loc("1894|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_36aten__sub")
#loc2896 = loc("1892|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_214aten__add")
#loc2897 = loc("1893|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_36aten__rsqrt")
#loc2898 = loc("1895|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_108aten__mul")
#loc2899 = loc("1896|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_109xla__cast")
#loc2900 = loc("1896|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_109aten__mul")
#loc2901 = loc("1897|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_215aten__add")
#loc2902 = loc("1898|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_163xla__cast")
#loc2906 = loc("3147|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|_to_copy_327xla__cast")
#loc2907 = loc("3148|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|var_mean_76aten__var_mean")
#loc2908 = loc("3151|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|sub_76aten__sub")
#loc2909 = loc("3149|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|add_416aten__add")
#loc2910 = loc("3150|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|rsqrt_76aten__rsqrt")
#loc2911 = loc("3152|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_222aten__mul")
#loc2912 = loc("3153|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_223xla__cast")
#loc2913 = loc("3153|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_223aten__mul")
#loc2914 = loc("3154|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|add_417aten__add")
#loc2915 = loc("3155|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|_to_copy_328xla__cast")
#loc2919 = loc("2778|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_270xla__cast")
#loc2920 = loc("2779|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_60aten__var_mean")
#loc2921 = loc("2782|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_60aten__sub")
#loc2922 = loc("2780|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_358aten__add")
#loc2923 = loc("2781|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_60aten__rsqrt")
#loc2924 = loc("2783|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_180aten__mul")
#loc2925 = loc("2784|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_181xla__cast")
#loc2926 = loc("2784|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_181aten__mul")
#loc2927 = loc("2785|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_359aten__add")
#loc2928 = loc("2786|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_271xla__cast")
#loc2932 = loc("1543|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_119xla__cast")
#loc2933 = loc("1544|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_27aten__var_mean")
#loc2934 = loc("1547|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_27aten__sub")
#loc2935 = loc("1545|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_159aten__add")
#loc2936 = loc("1546|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_27aten__rsqrt")
#loc2937 = loc("1548|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_80aten__mul")
#loc2938 = loc("1549|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_81xla__cast")
#loc2939 = loc("1549|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_81aten__mul")
#loc2940 = loc("1550|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_160aten__add")
#loc2941 = loc("1551|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_120xla__cast")
#loc2943 = loc("1904|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[17].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_17aten__gelu")
#loc2947 = loc("1913|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_164xla__cast")
#loc2948 = loc("1914|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_37aten__var_mean")
#loc2949 = loc("1917|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_37aten__sub")
#loc2950 = loc("1915|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_219aten__add")
#loc2951 = loc("1916|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_37aten__rsqrt")
#loc2952 = loc("1918|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_110aten__mul")
#loc2953 = loc("1919|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_111xla__cast")
#loc2954 = loc("1919|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_111aten__mul")
#loc2955 = loc("1920|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_220aten__add")
#loc2956 = loc("1921|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_165xla__cast")
