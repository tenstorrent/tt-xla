#loc1 = loc("-1|unknown|unknown|-1|unknownxla__device_data")
#loc41 = loc("616|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_1aten__any")
#loc46 = loc("613|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmaxaten__softmax")
#loc97 = loc("690|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_2aten__any")
#loc102 = loc("687|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_1aten__softmax")
#loc153 = loc("764|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_3aten__any")
#loc158 = loc("761|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_2aten__softmax")
#loc209 = loc("838|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_4aten__any")
#loc214 = loc("835|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_3aten__softmax")
#loc265 = loc("912|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_5aten__any")
#loc270 = loc("909|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_4aten__softmax")
#loc321 = loc("986|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_6aten__any")
#loc326 = loc("983|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_5aten__softmax")
#loc377 = loc("1060|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_7aten__any")
#loc382 = loc("1057|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_6aten__softmax")
#loc433 = loc("1134|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_8aten__any")
#loc438 = loc("1131|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_7aten__softmax")
#loc489 = loc("1208|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_9aten__any")
#loc494 = loc("1205|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_8aten__softmax")
#loc545 = loc("1282|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_10aten__any")
#loc550 = loc("1279|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_9aten__softmax")
#loc601 = loc("1356|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_11aten__any")
#loc606 = loc("1353|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_10aten__softmax")
#loc657 = loc("1430|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_12aten__any")
#loc662 = loc("1427|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_11aten__softmax")
#loc713 = loc("1504|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_13aten__any")
#loc718 = loc("1501|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_12aten__softmax")
#loc769 = loc("1578|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_14aten__any")
#loc774 = loc("1575|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_13aten__softmax")
#loc825 = loc("1652|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_15aten__any")
#loc830 = loc("1649|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_14aten__softmax")
#loc881 = loc("1726|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_16aten__any")
#loc886 = loc("1723|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_15aten__softmax")
#loc937 = loc("1800|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_17aten__any")
#loc942 = loc("1797|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_16aten__softmax")
#loc993 = loc("1874|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_18aten__any")
#loc998 = loc("1871|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_17aten__softmax")
#loc1049 = loc("1948|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_19aten__any")
#loc1054 = loc("1945|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_18aten__softmax")
#loc1105 = loc("2022|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_20aten__any")
#loc1110 = loc("2019|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_19aten__softmax")
#loc1161 = loc("2096|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_21aten__any")
#loc1166 = loc("2093|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_20aten__softmax")
#loc1217 = loc("2170|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_22aten__any")
#loc1222 = loc("2167|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_21aten__softmax")
#loc1273 = loc("2244|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_23aten__any")
#loc1278 = loc("2241|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_22aten__softmax")
#loc1329 = loc("2318|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_24aten__any")
#loc1334 = loc("2315|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_23aten__softmax")
#loc1385 = loc("2392|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_25aten__any")
#loc1390 = loc("2389|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_24aten__softmax")
#loc1441 = loc("2466|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_26aten__any")
#loc1446 = loc("2463|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_25aten__softmax")
#loc1497 = loc("2540|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_27aten__any")
#loc1502 = loc("2537|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_26aten__softmax")
#loc1553 = loc("2614|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_28aten__any")
#loc1558 = loc("2611|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_27aten__softmax")
#loc1609 = loc("2688|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_29aten__any")
#loc1614 = loc("2685|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_28aten__softmax")
#loc1665 = loc("2762|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_30aten__any")
#loc1670 = loc("2759|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_29aten__softmax")
#loc1721 = loc("2836|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_31aten__any")
#loc1726 = loc("2833|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_30aten__softmax")
#loc1773 = loc("2925|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_33aten__any")
#loc1778 = loc("2922|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_32aten__softmax")
#loc1826 = loc("3010|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_34aten__any")
#loc1831 = loc("3007|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_33aten__softmax")
#loc1879 = loc("3095|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_35aten__any")
#loc1884 = loc("3092|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_34aten__softmax")
#loc1932 = loc("3180|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_36aten__any")
#loc1937 = loc("3177|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_35aten__softmax")
#loc1969 = loc("3219|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|mark_tensor_384xla__mark_tensor")
#loc1970 = loc("3220|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|mark_tensor_385xla__mark_tensor")
#loc1971 = loc("3221|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|mark_tensor_386xla__mark_tensor")
#loc1973 = loc("3223|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|var_mean_78aten__var_mean")
#loc1984 = loc("3194|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_378xla__mark_tensor")
#loc1985 = loc("3195|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_379xla__mark_tensor")
#loc1986 = loc("3196|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_380xla__mark_tensor")
#loc1988 = loc("3198|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|var_mean_77aten__var_mean")
#loc1999 = loc("3131|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_370xla__mark_tensor")
#loc2000 = loc("3132|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_371xla__mark_tensor")
#loc2001 = loc("3133|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_372xla__mark_tensor")
#loc2003 = loc("3135|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|var_mean_75aten__var_mean")
#loc2014 = loc("3046|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_356xla__mark_tensor")
#loc2015 = loc("3047|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_357xla__mark_tensor")
#loc2016 = loc("3048|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_358xla__mark_tensor")
#loc2018 = loc("3050|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|var_mean_72aten__var_mean")
#loc2029 = loc("3039|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|GELU[getattr(resampler.layers[1].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|mark_tensor_354xla__mark_tensor")
#loc2031 = loc("2954|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|GELU[getattr(resampler.layers[0].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|mark_tensor_340xla__mark_tensor")
#loc2033 = loc("2939|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_336xla__mark_tensor")
#loc2034 = loc("2940|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_337xla__mark_tensor")
#loc2035 = loc("2941|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_338xla__mark_tensor")
#loc2037 = loc("2943|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|var_mean_68aten__var_mean")
#loc2048 = loc("2791|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[29].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_302xla__mark_tensor")
#loc2050 = loc("2724|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_294xla__mark_tensor")
#loc2051 = loc("2725|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_295xla__mark_tensor")
#loc2052 = loc("2726|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_296xla__mark_tensor")
#loc2054 = loc("2728|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_59aten__var_mean")
#loc2065 = loc("2717|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[28].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_292xla__mark_tensor")
#loc2067 = loc("2974|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_346xla__mark_tensor")
#loc2068 = loc("2975|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_347xla__mark_tensor")
#loc2069 = loc("2976|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_348xla__mark_tensor")
#loc2071 = loc("2978|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|var_mean_70aten__var_mean")
#loc2082 = loc("2701|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_288xla__mark_tensor")
#loc2083 = loc("2702|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_289xla__mark_tensor")
#loc2084 = loc("2703|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_290xla__mark_tensor")
#loc2086 = loc("2705|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_58aten__var_mean")
#loc2097 = loc("2650|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_284xla__mark_tensor")
#loc2098 = loc("2651|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_285xla__mark_tensor")
#loc2099 = loc("2652|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_286xla__mark_tensor")
#loc2101 = loc("2654|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_57aten__var_mean")
#loc2112 = loc("3124|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|GELU[getattr(resampler.layers[2].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|mark_tensor_368xla__mark_tensor")
#loc2114 = loc("2576|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_274xla__mark_tensor")
#loc2115 = loc("2577|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_275xla__mark_tensor")
#loc2116 = loc("2578|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_276xla__mark_tensor")
#loc2118 = loc("2580|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_55aten__var_mean")
#loc2129 = loc("2553|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_268xla__mark_tensor")
#loc2130 = loc("2554|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_269xla__mark_tensor")
#loc2131 = loc("2555|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_270xla__mark_tensor")
#loc2133 = loc("2557|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_54aten__var_mean")
#loc2144 = loc("2502|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_264xla__mark_tensor")
#loc2145 = loc("2503|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_265xla__mark_tensor")
#loc2146 = loc("2504|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_266xla__mark_tensor")
#loc2148 = loc("2506|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_53aten__var_mean")
#loc2159 = loc("2495|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[25].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_262xla__mark_tensor")
#loc2161 = loc("2479|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_258xla__mark_tensor")
#loc2162 = loc("2480|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_259xla__mark_tensor")
#loc2163 = loc("2481|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_260xla__mark_tensor")
#loc2165 = loc("2483|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_52aten__var_mean")
#loc2176 = loc("2428|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_254xla__mark_tensor")
#loc2177 = loc("2429|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_255xla__mark_tensor")
#loc2178 = loc("2430|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_256xla__mark_tensor")
#loc2180 = loc("2432|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_51aten__var_mean")
#loc2191 = loc("2421|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[24].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_252xla__mark_tensor")
#loc2193 = loc("2405|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_248xla__mark_tensor")
#loc2194 = loc("2406|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_249xla__mark_tensor")
#loc2195 = loc("2407|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_250xla__mark_tensor")
#loc2197 = loc("2409|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_50aten__var_mean")
#loc2208 = loc("2354|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_244xla__mark_tensor")
#loc2209 = loc("2355|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_245xla__mark_tensor")
#loc2210 = loc("2356|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_246xla__mark_tensor")
#loc2212 = loc("2358|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_49aten__var_mean")
#loc2223 = loc("2347|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[23].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_242xla__mark_tensor")
#loc2225 = loc("2331|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_238xla__mark_tensor")
#loc2226 = loc("2332|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_239xla__mark_tensor")
#loc2227 = loc("2333|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_240xla__mark_tensor")
#loc2229 = loc("2335|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_48aten__var_mean")
#loc2240 = loc("2206|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_224xla__mark_tensor")
#loc2241 = loc("2207|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_225xla__mark_tensor")
#loc2242 = loc("2208|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_226xla__mark_tensor")
#loc2244 = loc("2210|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_45aten__var_mean")
#loc2255 = loc("2199|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[21].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_222xla__mark_tensor")
#loc2257 = loc("3024|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_350xla__mark_tensor")
#loc2258 = loc("3025|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_351xla__mark_tensor")
#loc2259 = loc("3026|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_352xla__mark_tensor")
#loc2261 = loc("3028|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|var_mean_71aten__var_mean")
#loc2272 = loc("2183|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_218xla__mark_tensor")
#loc2273 = loc("2184|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_219xla__mark_tensor")
#loc2274 = loc("2185|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_220xla__mark_tensor")
#loc2276 = loc("2187|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_44aten__var_mean")
#loc2287 = loc("2125|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[20].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_212xla__mark_tensor")
#loc2289 = loc("2569|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[26].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_272xla__mark_tensor")
#loc2291 = loc("2058|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_204xla__mark_tensor")
#loc2292 = loc("2059|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_205xla__mark_tensor")
#loc2293 = loc("2060|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_206xla__mark_tensor")
#loc2295 = loc("2062|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_41aten__var_mean")
#loc2306 = loc("2051|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[19].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_202xla__mark_tensor")
#loc2308 = loc("1984|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_194xla__mark_tensor")
#loc2309 = loc("1985|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_195xla__mark_tensor")
#loc2310 = loc("1986|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_196xla__mark_tensor")
#loc2312 = loc("1988|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_39aten__var_mean")
#loc2323 = loc("1237|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[8].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_92xla__mark_tensor")
#loc2325 = loc("1318|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_104xla__mark_tensor")
#loc2326 = loc("1319|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_105xla__mark_tensor")
#loc2327 = loc("1320|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_106xla__mark_tensor")
#loc2329 = loc("1322|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_21aten__var_mean")
#loc2340 = loc("1170|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_84xla__mark_tensor")
#loc2341 = loc("1171|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_85xla__mark_tensor")
#loc2342 = loc("1172|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_86xla__mark_tensor")
#loc2344 = loc("1174|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_17aten__var_mean")
#loc2355 = loc("2865|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[30].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_312xla__mark_tensor")
#loc2357 = loc("1163|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[7].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_82xla__mark_tensor")
#loc2359 = loc("2849|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_308xla__mark_tensor")
#loc2360 = loc("2850|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_309xla__mark_tensor")
#loc2361 = loc("2851|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_310xla__mark_tensor")
#loc2363 = loc("2853|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_62aten__var_mean")
#loc2374 = loc("2273|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[22].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_232xla__mark_tensor")
#loc2376 = loc("2257|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_228xla__mark_tensor")
#loc2377 = loc("2258|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_229xla__mark_tensor")
#loc2378 = loc("2259|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_230xla__mark_tensor")
#loc2380 = loc("2261|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_46aten__var_mean")
#loc2391 = loc("1089|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[6].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_72xla__mark_tensor")
#loc2393 = loc("1073|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_68xla__mark_tensor")
#loc2394 = loc("1074|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_69xla__mark_tensor")
#loc2395 = loc("1075|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_70xla__mark_tensor")
#loc2397 = loc("1077|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_14aten__var_mean")
#loc2408 = loc("645|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[0].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_12xla__mark_tensor")
#loc2410 = loc("3109|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_364xla__mark_tensor")
#loc2411 = loc("3110|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_365xla__mark_tensor")
#loc2412 = loc("3111|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_366xla__mark_tensor")
#loc2414 = loc("3113|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|var_mean_74aten__var_mean")
#loc2425 = loc("1221|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_88xla__mark_tensor")
#loc2426 = loc("1222|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_89xla__mark_tensor")
#loc2427 = loc("1223|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_90xla__mark_tensor")
#loc2429 = loc("1225|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_18aten__var_mean")
#loc2440 = loc("726|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_24xla__mark_tensor")
#loc2441 = loc("727|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_25xla__mark_tensor")
#loc2442 = loc("728|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_26xla__mark_tensor")
#loc2444 = loc("730|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_5aten__var_mean")
#loc2455 = loc("1147|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_78xla__mark_tensor")
#loc2456 = loc("1148|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_79xla__mark_tensor")
#loc2457 = loc("1149|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_80xla__mark_tensor")
#loc2459 = loc("1151|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_16aten__var_mean")
#loc2470 = loc("1836|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_174xla__mark_tensor")
#loc2471 = loc("1837|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_175xla__mark_tensor")
#loc2472 = loc("1838|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_176xla__mark_tensor")
#loc2474 = loc("1840|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_35aten__var_mean")
#loc2485 = loc("941|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[4].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_52xla__mark_tensor")
#loc2487 = loc("2798|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_304xla__mark_tensor")
#loc2488 = loc("2799|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_305xla__mark_tensor")
#loc2489 = loc("2800|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_306xla__mark_tensor")
#loc2491 = loc("2802|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_61aten__var_mean")
#loc2502 = loc("777|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_28xla__mark_tensor")
#loc2503 = loc("778|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_29xla__mark_tensor")
#loc2504 = loc("779|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_30xla__mark_tensor")
#loc2506 = loc("781|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_6aten__var_mean")
#loc2517 = loc("1665|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_148xla__mark_tensor")
#loc2518 = loc("1666|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_149xla__mark_tensor")
#loc2519 = loc("1667|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_150xla__mark_tensor")
#loc2521 = loc("1669|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_30aten__var_mean")
#loc2532 = loc("578|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_4xla__mark_tensor")
#loc2533 = loc("579|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_5xla__mark_tensor")
#loc2534 = loc("580|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_6xla__mark_tensor")
#loc2536 = loc("582|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_1aten__var_mean")
#loc2547 = loc("1533|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[12].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_132xla__mark_tensor")
#loc2549 = loc("629|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_8xla__mark_tensor")
#loc2550 = loc("630|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_9xla__mark_tensor")
#loc2551 = loc("631|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_10xla__mark_tensor")
#loc2553 = loc("633|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_2aten__var_mean")
#loc2564 = loc("703|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_18xla__mark_tensor")
#loc2565 = loc("704|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_19xla__mark_tensor")
#loc2566 = loc("705|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_20xla__mark_tensor")
#loc2568 = loc("707|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_4aten__var_mean")
#loc2579 = loc("719|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[1].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_22xla__mark_tensor")
#loc2581 = loc("565|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|mark_tensorxla__mark_tensor")
#loc2582 = loc("566|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|mark_tensor_1xla__mark_tensor")
#loc2583 = loc("567|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|mark_tensor_2xla__mark_tensor")
#loc2585 = loc("569|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|var_meanaten__var_mean")
#loc2596 = loc("925|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_48xla__mark_tensor")
#loc2597 = loc("926|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_49xla__mark_tensor")
#loc2598 = loc("927|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_50xla__mark_tensor")
#loc2600 = loc("929|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_10aten__var_mean")
#loc2611 = loc("652|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_14xla__mark_tensor")
#loc2612 = loc("653|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_15xla__mark_tensor")
#loc2613 = loc("654|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_16xla__mark_tensor")
#loc2615 = loc("656|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_3aten__var_mean")
#loc2626 = loc("1688|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_154xla__mark_tensor")
#loc2627 = loc("1689|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_155xla__mark_tensor")
#loc2628 = loc("1690|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_156xla__mark_tensor")
#loc2630 = loc("1692|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_31aten__var_mean")
#loc2641 = loc("867|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[3].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_42xla__mark_tensor")
#loc2643 = loc("999|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_58xla__mark_tensor")
#loc2644 = loc("1000|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_59xla__mark_tensor")
#loc2645 = loc("1001|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_60xla__mark_tensor")
#loc2647 = loc("1003|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_12aten__var_mean")
#loc2658 = loc("793|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[2].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_32xla__mark_tensor")
#loc2660 = loc("948|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_54xla__mark_tensor")
#loc2661 = loc("949|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_55xla__mark_tensor")
#loc2662 = loc("950|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_56xla__mark_tensor")
#loc2664 = loc("952|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_11aten__var_mean")
#loc2675 = loc("800|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_34xla__mark_tensor")
#loc2676 = loc("801|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_35xla__mark_tensor")
#loc2677 = loc("802|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_36xla__mark_tensor")
#loc2679 = loc("804|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_7aten__var_mean")
#loc2690 = loc("1762|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_164xla__mark_tensor")
#loc2691 = loc("1763|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_165xla__mark_tensor")
#loc2692 = loc("1764|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_166xla__mark_tensor")
#loc2694 = loc("1766|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_33aten__var_mean")
#loc2705 = loc("1295|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_98xla__mark_tensor")
#loc2706 = loc("1296|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_99xla__mark_tensor")
#loc2707 = loc("1297|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_100xla__mark_tensor")
#loc2709 = loc("1299|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_20aten__var_mean")
#loc2720 = loc("2627|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_278xla__mark_tensor")
#loc2721 = loc("2628|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_279xla__mark_tensor")
#loc2722 = loc("2629|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_280xla__mark_tensor")
#loc2724 = loc("2631|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_56aten__var_mean")
#loc2735 = loc("2109|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_208xla__mark_tensor")
#loc2736 = loc("2110|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_209xla__mark_tensor")
#loc2737 = loc("2111|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_210xla__mark_tensor")
#loc2739 = loc("2113|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_42aten__var_mean")
#loc2750 = loc("1739|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_158xla__mark_tensor")
#loc2751 = loc("1740|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_159xla__mark_tensor")
#loc2752 = loc("1741|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_160xla__mark_tensor")
#loc2754 = loc("1743|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_32aten__var_mean")
#loc2765 = loc("1311|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[9].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_102xla__mark_tensor")
#loc2767 = loc("3059|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_360xla__mark_tensor")
#loc2768 = loc("3060|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_361xla__mark_tensor")
#loc2769 = loc("3061|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_362xla__mark_tensor")
#loc2771 = loc("3063|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|var_mean_73aten__var_mean")
#loc2782 = loc("1813|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_168xla__mark_tensor")
#loc2783 = loc("1814|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_169xla__mark_tensor")
#loc2784 = loc("1815|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_170xla__mark_tensor")
#loc2786 = loc("1817|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_34aten__var_mean")
#loc2797 = loc("2035|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_198xla__mark_tensor")
#loc2798 = loc("2036|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_199xla__mark_tensor")
#loc2799 = loc("2037|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_200xla__mark_tensor")
#loc2801 = loc("2039|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_40aten__var_mean")
#loc2812 = loc("851|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_38xla__mark_tensor")
#loc2813 = loc("852|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_39xla__mark_tensor")
#loc2814 = loc("853|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_40xla__mark_tensor")
#loc2816 = loc("855|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_8aten__var_mean")
#loc2827 = loc("1977|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[18].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_192xla__mark_tensor")
#loc2829 = loc("3209|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|GELU[getattr(resampler.layers[3].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|mark_tensor_382xla__mark_tensor")
#loc2831 = loc("1369|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_108xla__mark_tensor")
#loc2832 = loc("1370|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_109xla__mark_tensor")
#loc2833 = loc("1371|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_110xla__mark_tensor")
#loc2835 = loc("1373|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_22aten__var_mean")
#loc2846 = loc("2889|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_332xla__mark_tensor")
#loc2847 = loc("2890|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_333xla__mark_tensor")
#loc2848 = loc("2891|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_334xla__mark_tensor")
#loc2850 = loc("2893|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|var_mean_67aten__var_mean")
#loc2861 = loc("1385|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[10].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_112xla__mark_tensor")
#loc2863 = loc("2132|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_214xla__mark_tensor")
#loc2864 = loc("2133|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_215xla__mark_tensor")
#loc2865 = loc("2134|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_216xla__mark_tensor")
#loc2867 = loc("2136|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_43aten__var_mean")
#loc2878 = loc("1392|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_114xla__mark_tensor")
#loc2879 = loc("1393|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_115xla__mark_tensor")
#loc2880 = loc("1394|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_116xla__mark_tensor")
#loc2882 = loc("1396|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_23aten__var_mean")
#loc2893 = loc("1443|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_118xla__mark_tensor")
#loc2894 = loc("1444|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_119xla__mark_tensor")
#loc2895 = loc("1445|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_120xla__mark_tensor")
#loc2897 = loc("1447|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_24aten__var_mean")
#loc2908 = loc("2280|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_234xla__mark_tensor")
#loc2909 = loc("2281|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_235xla__mark_tensor")
#loc2910 = loc("2282|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_236xla__mark_tensor")
#loc2912 = loc("2284|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_47aten__var_mean")
#loc2923 = loc("1244|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_94xla__mark_tensor")
#loc2924 = loc("1245|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_95xla__mark_tensor")
#loc2925 = loc("1246|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_96xla__mark_tensor")
#loc2927 = loc("1248|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_19aten__var_mean")
#loc2938 = loc("1517|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_128xla__mark_tensor")
#loc2939 = loc("1518|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_129xla__mark_tensor")
#loc2940 = loc("1519|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_130xla__mark_tensor")
#loc2942 = loc("1521|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_26aten__var_mean")
#loc2953 = loc("1015|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[5].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_62xla__mark_tensor")
#loc2955 = loc("1961|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_188xla__mark_tensor")
#loc2956 = loc("1962|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_189xla__mark_tensor")
#loc2957 = loc("1963|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_190xla__mark_tensor")
#loc2959 = loc("1965|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_38aten__var_mean")
#loc2970 = loc("1591|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_138xla__mark_tensor")
#loc2971 = loc("1592|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_139xla__mark_tensor")
#loc2972 = loc("1593|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_140xla__mark_tensor")
#loc2974 = loc("1595|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_28aten__var_mean")
#loc2985 = loc("2876|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_328xla__mark_tensor")
#loc2986 = loc("2877|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_329xla__mark_tensor")
#loc2987 = loc("2878|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_330xla__mark_tensor")
#loc2989 = loc("2880|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|var_mean_66aten__var_mean")
#loc3000 = loc("1022|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_64xla__mark_tensor")
#loc3001 = loc("1023|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_65xla__mark_tensor")
#loc3002 = loc("1024|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_66xla__mark_tensor")
#loc3004 = loc("1026|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_13aten__var_mean")
#loc3015 = loc("1755|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[15].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_162xla__mark_tensor")
#loc3017 = loc("1607|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[13].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_142xla__mark_tensor")
#loc3019 = loc("2643|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[27].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_282xla__mark_tensor")
#loc3021 = loc("1096|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_74xla__mark_tensor")
#loc3022 = loc("1097|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_75xla__mark_tensor")
#loc3023 = loc("1098|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_76xla__mark_tensor")
#loc3025 = loc("1100|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_15aten__var_mean")
#loc3036 = loc("1614|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_144xla__mark_tensor")
#loc3037 = loc("1615|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_145xla__mark_tensor")
#loc3038 = loc("1616|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_146xla__mark_tensor")
#loc3040 = loc("1618|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_29aten__var_mean")
#loc3051 = loc("2961|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_342xla__mark_tensor")
#loc3052 = loc("2962|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_343xla__mark_tensor")
#loc3053 = loc("2963|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_344xla__mark_tensor")
#loc3055 = loc("2965|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|var_mean_69aten__var_mean")
#loc3066 = loc("1459|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[11].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_122xla__mark_tensor")
#loc3068 = loc("1681|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[14].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_152xla__mark_tensor")
#loc3070 = loc("874|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_44xla__mark_tensor")
#loc3071 = loc("875|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_45xla__mark_tensor")
#loc3072 = loc("876|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_46xla__mark_tensor")
#loc3074 = loc("878|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_9aten__var_mean")
#loc3085 = loc("1466|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_124xla__mark_tensor")
#loc3086 = loc("1467|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_125xla__mark_tensor")
#loc3087 = loc("1468|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_126xla__mark_tensor")
#loc3089 = loc("1470|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_25aten__var_mean")
#loc3100 = loc("1829|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[16].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_172xla__mark_tensor")
#loc3102 = loc("1887|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_178xla__mark_tensor")
#loc3103 = loc("1888|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_179xla__mark_tensor")
#loc3104 = loc("1889|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_180xla__mark_tensor")
#loc3106 = loc("1891|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_36aten__var_mean")
#loc3117 = loc("3144|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_374xla__mark_tensor")
#loc3118 = loc("3145|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_375xla__mark_tensor")
#loc3119 = loc("3146|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_376xla__mark_tensor")
#loc3121 = loc("3148|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|var_mean_76aten__var_mean")
#loc3132 = loc("2775|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_298xla__mark_tensor")
#loc3133 = loc("2776|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_299xla__mark_tensor")
#loc3134 = loc("2777|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_300xla__mark_tensor")
#loc3136 = loc("2779|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_60aten__var_mean")
#loc3147 = loc("1540|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_134xla__mark_tensor")
#loc3148 = loc("1541|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_135xla__mark_tensor")
#loc3149 = loc("1542|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_136xla__mark_tensor")
#loc3151 = loc("1544|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_27aten__var_mean")
#loc3162 = loc("1903|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[17].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_182xla__mark_tensor")
#loc3164 = loc("1910|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_184xla__mark_tensor")
#loc3165 = loc("1911|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_185xla__mark_tensor")
#loc3166 = loc("1912|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_186xla__mark_tensor")
#loc3168 = loc("1914|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_37aten__var_mean")
module @SyncTensorsGraph.13945 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<2048x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg1: !vhlo.tensor_v1<2048x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg2: !vhlo.tensor_v1<2048x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg3: !vhlo.tensor_v1<2048x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg4: !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg5: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg6: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg7: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg8: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg9: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg10: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg11: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg12: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg13: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg14: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg15: !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg16: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg17: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg18: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg19: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg20: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg21: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg22: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg23: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg24: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg25: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg26: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg27: !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg28: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg29: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg30: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg31: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg32: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg33: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg34: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg35: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg36: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg37: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg38: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg39: !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg40: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg41: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg42: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg43: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg44: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg45: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg46: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg47: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg48: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg49: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg50: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg51: !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg52: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg53: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg54: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg55: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg56: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg57: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg58: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg59: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg60: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg61: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg62: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg63: !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg64: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg65: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg66: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg67: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg68: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg69: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg70: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg71: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg72: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg73: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg74: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg75: !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg76: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg77: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg78: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg79: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg80: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg81: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg82: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg83: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg84: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg85: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg86: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg87: !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg88: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg89: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg90: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg91: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg92: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg93: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg94: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg95: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg96: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg97: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg98: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg99: !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg100: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg101: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg102: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg103: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg104: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg105: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg106: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg107: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg108: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg109: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg110: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg111: !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg112: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg113: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg114: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg115: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg116: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg117: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg118: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg119: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg120: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg121: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg122: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg123: !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg124: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg125: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg126: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg127: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg128: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg129: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg130: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg131: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg132: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg133: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg134: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg135: !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg136: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg137: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg138: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg139: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg140: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg141: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg142: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg143: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg144: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg145: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg146: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg147: !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg148: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg149: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg150: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg151: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg152: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg153: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg154: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg155: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg156: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg157: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg158: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg159: !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg160: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg161: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg162: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg163: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg164: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg165: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg166: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg167: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg168: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg169: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg170: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg171: !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg172: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg173: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg174: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg175: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg176: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg177: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg178: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg179: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg180: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg181: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg182: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg183: !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg184: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg185: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg186: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg187: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg188: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg189: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg190: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg191: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg192: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg193: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg194: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg195: !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg196: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg197: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg198: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg199: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg200: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg201: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg202: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg203: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg204: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg205: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg206: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg207: !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg208: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg209: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg210: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg211: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg212: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg213: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg214: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg215: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg216: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg217: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg218: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg219: !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg220: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg221: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg222: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg223: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg224: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg225: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg226: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg227: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg228: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg229: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg230: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg231: !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg232: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg233: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg234: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg235: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg236: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg237: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg238: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg239: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg240: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg241: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg242: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg243: !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg244: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg245: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg246: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg247: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg248: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg249: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg250: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg251: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg252: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg253: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg254: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg255: !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg256: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg257: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg258: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg259: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg260: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg261: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg262: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg263: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg264: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg265: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg266: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg267: !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg268: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg269: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg270: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg271: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg272: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg273: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg274: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg275: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg276: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg277: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg278: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg279: !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg280: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg281: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg282: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg283: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg284: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg285: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg286: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg287: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg288: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg289: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg290: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg291: !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg292: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg293: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg294: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg295: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg296: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg297: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg298: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg299: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg300: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg301: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg302: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg303: !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg304: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg305: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg306: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg307: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg308: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg309: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg310: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg311: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg312: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg313: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg314: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg315: !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg316: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg317: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg318: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg319: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg320: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg321: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg322: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg323: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg324: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg325: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg326: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg327: !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg328: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg329: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg330: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg331: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg332: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg333: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg334: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg335: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg336: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg337: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg338: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg339: !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg340: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg341: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg342: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg343: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg344: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg345: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg346: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg347: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg348: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg349: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg350: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg351: !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg352: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg353: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg354: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg355: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg356: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg357: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg358: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg359: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg360: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg361: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg362: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg363: !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg364: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg365: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg366: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg367: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg368: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg369: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg370: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg371: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg372: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg373: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg374: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg375: !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg376: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg377: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg378: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg379: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg380: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg381: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg382: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg383: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg384: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg385: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg386: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg387: !vhlo.tensor_v1<1x257x!vhlo.i64_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg388: !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg389: !vhlo.tensor_v1<1280x3x14x14x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg390: !vhlo.tensor_v1<1x3x224x224x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg391: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg392: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg393: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg394: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg395: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg396: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg397: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg398: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg399: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg400: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg401: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg402: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg403: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg404: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg405: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg406: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg407: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg408: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg409: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg410: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg411: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg412: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg413: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg414: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg415: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg416: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg417: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg418: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg419: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg420: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg421: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg422: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg423: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg424: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg425: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg426: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg427: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg428: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg429: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg430: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg431: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg432: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg433: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg434: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg435: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg436: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg437: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg438: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg439: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg440: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg441: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg442: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg443: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg444: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg445: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg446: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg447: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg448: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg449: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg450: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg451: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg452: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg453: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg454: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg455: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg456: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg457: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg458: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg459: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg460: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg461: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg462: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg463: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg464: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg465: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg466: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg467: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg468: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg469: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg470: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg471: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg472: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg473: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg474: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg475: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg476: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg477: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg478: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg479: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg480: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg481: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg482: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg483: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg484: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg485: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg486: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg487: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg488: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg489: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg490: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg491: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg492: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg493: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg494: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg495: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg496: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg497: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg498: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg499: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg500: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg501: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg502: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg503: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg504: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg505: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg506: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg507: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg508: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg509: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg510: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg511: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg512: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg513: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg514: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg515: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg516: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg517: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg518: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg519: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg520: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg521: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg522: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg523: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg524: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg525: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg526: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg527: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg528: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg529: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg530: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg531: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg532: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg533: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg534: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg535: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg536: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg537: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg538: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg539: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg540: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg541: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg542: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg543: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg544: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg545: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg546: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg547: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg548: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg549: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg550: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg551: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg552: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg553: !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg554: !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg555: !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg556: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg557: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("-1|unknown|unknown|-1|unknownxla__device_data")) -> (!vhlo.tensor_v1<1x16x2048x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x16x1280xbf16>>}> : () -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<1x20x16x273xf32>>}> : () -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFFF0000000000000> : tensor<1x20x16x273xf64>>}> : () -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f64_v1> loc(#loc)
    %3 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.353553385> : tensor<1x20x64x273xf32>>}> : () -> !vhlo.tensor_v1<1x20x64x273x!vhlo.f32_v1> loc(#loc)
    %4 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<1x16x257x257xf32>>}> : () -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc)
    %5 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFFF0000000000000> : tensor<1x16x257x257xf64>>}> : () -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc)
    %6 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.334370166> : tensor<1x16x80x257xf32>>}> : () -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc)
    %7 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.334370166> : tensor<1x16x257x80xf32>>}> : () -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc)
    %8 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.353553385> : tensor<1x20x16x64xf32>>}> : () -> !vhlo.tensor_v1<1x20x16x64x!vhlo.f32_v1> loc(#loc)
    %9 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF800000> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %10 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<true> : tensor<i1>>}> : () -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc)
    %11 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<false> : tensor<i1>>}> : () -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc)
    %12 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %13 = "vhlo.custom_call_v1"(%arg4) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_latents">}>} : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc2)
    %14 = "vhlo.reshape_v1"(%arg8) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %15 = "vhlo.custom_call_v1"(%14) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_0_ln1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %16 = "vhlo.reshape_v1"(%15) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %17 = "vhlo.reshape_v1"(%arg7) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %18 = "vhlo.custom_call_v1"(%17) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_0_ln1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %19 = "vhlo.reshape_v1"(%18) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %20 = "vhlo.composite_v1"(%13, %16, %19) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_54">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc4)
    %21 = "vhlo.reshape_v1"(%20) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x1280x!vhlo.bf16_v1> loc(#loc5)
    %22 = "vhlo.reshape_v1"(%arg517) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %23 = "vhlo.custom_call_v1"(%22) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_0_attn_to_q_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %24 = "vhlo.reshape_v1"(%23) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %25 = "vhlo.transpose_v1"(%24) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc6)
    %26 = "vhlo.dot_general_v2"(%21, %25) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x1280x!vhlo.bf16_v1> loc(#loc7)
    %27 = "vhlo.reshape_v1"(%26) : (!vhlo.tensor_v1<16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x20x64x!vhlo.bf16_v1> loc(#loc8)
    %28 = "vhlo.transpose_v1"(%27) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,20,16,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x20x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x20x16x64x!vhlo.bf16_v1> loc(#loc9)
    %29 = "vhlo.convert_v1"(%28) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,20,16,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x20x16x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x20x16x64x!vhlo.f32_v1> loc(#loc10)
    %30 = "vhlo.multiply_v1"(%29, %8) : (!vhlo.tensor_v1<1x20x16x64x!vhlo.f32_v1>, !vhlo.tensor_v1<1x20x16x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x64x!vhlo.f32_v1> loc(#loc11)
    %31 = "vhlo.reshape_v1"(%arg391) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %32 = "vhlo.custom_call_v1"(%31) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_embeddings_class_embedding">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %33 = "vhlo.custom_call_v1"(%arg390) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_0">}>} : (!vhlo.tensor_v1<1x3x224x224x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3x224x224x!vhlo.bf16_v1> loc(#loc2)
    %34 = "vhlo.custom_call_v1"(%arg389) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_embeddings_patch_embedding_weight">}>} : (!vhlo.tensor_v1<1280x3x14x14x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x3x14x14x!vhlo.bf16_v1> loc(#loc2)
    %35 = "vhlo.convolution_v1"(%33, %34) <{batch_group_count = #vhlo.integer_v1<1 : i64>, feature_group_count = #vhlo.integer_v1<1 : i64>, input_batch_dimension = #vhlo.integer_v1<0 : i64>, input_feature_dimension = #vhlo.integer_v1<1 : i64>, input_spatial_dimensions = #vhlo.tensor_v1<dense<[2, 3]> : tensor<2xi64>>, kernel_input_feature_dimension = #vhlo.integer_v1<1 : i64>, kernel_output_feature_dimension = #vhlo.integer_v1<0 : i64>, kernel_spatial_dimensions = #vhlo.tensor_v1<dense<[2, 3]> : tensor<2xi64>>, lhs_dilation = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>, output_batch_dimension = #vhlo.integer_v1<0 : i64>, output_feature_dimension = #vhlo.integer_v1<1 : i64>, output_spatial_dimensions = #vhlo.tensor_v1<dense<[2, 3]> : tensor<2xi64>>, padding = #vhlo.tensor_v1<dense<0> : tensor<2x2xi64>>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_dilation = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>, window_reversal = #vhlo.tensor_v1<dense<false> : tensor<2xi1>>, window_strides = #vhlo.tensor_v1<dense<14> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x3x224x224x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x3x14x14x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x16x16x!vhlo.bf16_v1> loc(#loc12)
    %36 = "vhlo.reshape_v1"(%35) : (!vhlo.tensor_v1<1x1280x16x16x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x256x!vhlo.bf16_v1> loc(#loc13)
    %37 = "vhlo.transpose_v1"(%36) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1]> : tensor<3xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[1, 2, 0]> : tensor<3xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,256,1280]{1,2,0}">} : (!vhlo.tensor_v1<1x1280x256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x256x1280x!vhlo.bf16_v1> loc(#loc14)
    %38 = "vhlo.concatenate_v1"(%32, %37) <{dimension = #vhlo.integer_v1<1 : i64>}> : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x256x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc15)
    %39 = "vhlo.reshape_v1"(%arg388) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc3)
    %40 = "vhlo.custom_call_v1"(%39) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_embeddings_position_embedding_weight">}>} : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2)
    %41 = "vhlo.reshape_v1"(%40) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc3)
    %42 = "vhlo.reshape_v1"(%arg387) : (!vhlo.tensor_v1<1x257x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x257x!vhlo.i64_v1> loc(#loc3)
    %43 = "vhlo.custom_call_v1"(%42) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"constant">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_embeddings_position_ids">}>} : (!vhlo.tensor_v1<1x1x257x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x257x!vhlo.i64_v1> loc(#loc2)
    %44 = "vhlo.reshape_v1"(%43) : (!vhlo.tensor_v1<1x1x257x!vhlo.i64_v1>) -> !vhlo.tensor_v1<257x!vhlo.i64_v1> loc(#loc16)
    %45 = "vhlo.convert_v1"(%44) : (!vhlo.tensor_v1<257x!vhlo.i64_v1>) -> !vhlo.tensor_v1<257x!vhlo.ui32_v1> loc(#loc17)
    %46 = "vhlo.gather_v2"(%41, %45) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 1280]> : tensor<2xi64>>, start_index_map = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<257x!vhlo.ui32_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc17)
    %47 = "vhlo.reshape_v1"(%46) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc16)
    %48 = "vhlo.add_v1"(%38, %47) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc18)
    %49 = "vhlo.reshape_v1"(%arg386) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %50 = "vhlo.custom_call_v1"(%49) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_pre_layrnorm_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %51 = "vhlo.reshape_v1"(%50) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %52 = "vhlo.reshape_v1"(%arg385) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %53 = "vhlo.custom_call_v1"(%52) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_pre_layrnorm_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %54 = "vhlo.reshape_v1"(%53) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %55 = "vhlo.composite_v1"(%48, %51, %54) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_37">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc19)
    %56 = "vhlo.reshape_v1"(%arg384) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %57 = "vhlo.custom_call_v1"(%56) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_0_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %58 = "vhlo.reshape_v1"(%57) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %59 = "vhlo.reshape_v1"(%arg383) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %60 = "vhlo.custom_call_v1"(%59) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_0_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %61 = "vhlo.reshape_v1"(%60) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %62 = "vhlo.composite_v1"(%55, %58, %61) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_34">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc20)
    %63 = "vhlo.reshape_v1"(%62) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc21)
    %64 = "vhlo.reshape_v1"(%arg395) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %65 = "vhlo.custom_call_v1"(%64) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_0_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %66 = "vhlo.reshape_v1"(%65) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %67 = "vhlo.transpose_v1"(%66) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc22)
    %68 = "vhlo.dot_general_v2"(%63, %67) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc23)
    %69 = "vhlo.reshape_v1"(%68) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc21)
    %70 = "vhlo.reshape_v1"(%arg394) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %71 = "vhlo.custom_call_v1"(%70) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_0_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %72 = "vhlo.reshape_v1"(%71) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %73 = "vhlo.broadcast_in_dim_v1"(%72) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc24)
    %74 = "vhlo.add_v1"(%69, %73) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc24)
    %75 = "vhlo.reshape_v1"(%74) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc25)
    %76 = "vhlo.transpose_v1"(%75) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc26)
    %77 = "vhlo.convert_v1"(%76) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc27)
    %78 = "vhlo.multiply_v1"(%77, %7) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc28)
    %79 = "vhlo.reshape_v1"(%arg393) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %80 = "vhlo.custom_call_v1"(%79) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_0_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %81 = "vhlo.reshape_v1"(%80) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %82 = "vhlo.transpose_v1"(%81) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc29)
    %83 = "vhlo.dot_general_v2"(%63, %82) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc30)
    %84 = "vhlo.reshape_v1"(%83) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc31)
    %85 = "vhlo.reshape_v1"(%arg392) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %86 = "vhlo.custom_call_v1"(%85) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_0_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %87 = "vhlo.reshape_v1"(%86) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %88 = "vhlo.broadcast_in_dim_v1"(%87) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc32)
    %89 = "vhlo.add_v1"(%84, %88) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc32)
    %90 = "vhlo.reshape_v1"(%89) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc33)
    %91 = "vhlo.transpose_v1"(%90) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc34)
    %92 = "vhlo.convert_v1"(%91) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc35)
    %93 = "vhlo.transpose_v1"(%92) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,80,257]{2,1,3,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc36)
    %94 = "vhlo.multiply_v1"(%93, %6) : (!vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc37)
    %95 = "vhlo.dot_general_v2"(%78, %94) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc38)
    %96 = "vhlo.convert_v1"(%95) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc39)
    %97 = "vhlo.compare_v1"(%96, %5) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc39)
    %98 = "vhlo.not_v1"(%97) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc40)
    %99 = "vhlo.reduce_v1"(%98, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("616|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_1aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("616|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_1aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc42)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc43)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc41)
    %100 = "vhlo.reshape_v1"(%99) : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc41)
    %101 = "vhlo.not_v1"(%100) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc44)
    %102 = "vhlo.reshape_v1"(%101) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc45)
    %103 = "vhlo.broadcast_in_dim_v1"(%102) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc45)
    %104 = "vhlo.reduce_v1"(%95, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("613|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmaxaten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("613|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmaxaten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc47)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc46)
    %105 = "vhlo.broadcast_in_dim_v1"(%104) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc46)
    %106 = "vhlo.subtract_v1"(%95, %105) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc46)
    %107 = "vhlo.exponential_v2"(%106) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc46)
    %108 = "vhlo.reduce_v1"(%107, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("613|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmaxaten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("613|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmaxaten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc48)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc46)
    %109 = "vhlo.broadcast_in_dim_v1"(%108) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc46)
    %110 = "vhlo.divide_v1"(%107, %109) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc46)
    %111 = "vhlo.select_v1"(%103, %4, %110) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc49)
    %112 = "vhlo.reshape_v1"(%arg382) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %113 = "vhlo.custom_call_v1"(%112) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_0_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %114 = "vhlo.reshape_v1"(%113) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %115 = "vhlo.transpose_v1"(%114) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc50)
    %116 = "vhlo.dot_general_v2"(%63, %115) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc51)
    %117 = "vhlo.reshape_v1"(%116) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc52)
    %118 = "vhlo.reshape_v1"(%arg381) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %119 = "vhlo.custom_call_v1"(%118) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_0_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %120 = "vhlo.reshape_v1"(%119) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %121 = "vhlo.broadcast_in_dim_v1"(%120) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc53)
    %122 = "vhlo.add_v1"(%117, %121) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc53)
    %123 = "vhlo.reshape_v1"(%122) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc54)
    %124 = "vhlo.transpose_v1"(%123) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc55)
    %125 = "vhlo.convert_v1"(%124) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc56)
    %126 = "vhlo.dot_general_v2"(%111, %125) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc57)
    %127 = "vhlo.convert_v1"(%126) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc58)
    %128 = "vhlo.transpose_v1"(%127) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,257,16,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc59)
    %129 = "vhlo.reshape_v1"(%128) : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc60)
    %130 = "vhlo.reshape_v1"(%arg380) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %131 = "vhlo.custom_call_v1"(%130) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_0_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %132 = "vhlo.reshape_v1"(%131) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %133 = "vhlo.transpose_v1"(%132) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc61)
    %134 = "vhlo.dot_general_v2"(%129, %133) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc62)
    %135 = "vhlo.reshape_v1"(%134) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc60)
    %136 = "vhlo.reshape_v1"(%arg379) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %137 = "vhlo.custom_call_v1"(%136) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_0_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %138 = "vhlo.reshape_v1"(%137) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %139 = "vhlo.broadcast_in_dim_v1"(%138) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc63)
    %140 = "vhlo.add_v1"(%135, %139) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc63)
    %141 = "vhlo.add_v1"(%55, %140) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc64)
    %142 = "vhlo.reshape_v1"(%arg378) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %143 = "vhlo.custom_call_v1"(%142) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_0_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %144 = "vhlo.reshape_v1"(%143) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %145 = "vhlo.reshape_v1"(%arg377) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %146 = "vhlo.custom_call_v1"(%145) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_0_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %147 = "vhlo.reshape_v1"(%146) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %148 = "vhlo.composite_v1"(%141, %144, %147) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_35">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc65)
    %149 = "vhlo.reshape_v1"(%148) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc66)
    %150 = "vhlo.reshape_v1"(%arg376) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %151 = "vhlo.custom_call_v1"(%150) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_0_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %152 = "vhlo.reshape_v1"(%151) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %153 = "vhlo.transpose_v1"(%152) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc67)
    %154 = "vhlo.dot_general_v2"(%149, %153) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc68)
    %155 = "vhlo.reshape_v1"(%154) : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc66)
    %156 = "vhlo.reshape_v1"(%arg375) : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc3)
    %157 = "vhlo.custom_call_v1"(%156) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_0_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc2)
    %158 = "vhlo.reshape_v1"(%157) : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc(#loc3)
    %159 = "vhlo.broadcast_in_dim_v1"(%158) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc69)
    %160 = "vhlo.add_v1"(%155, %159) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc69)
    %161 = "vhlo.composite_v1"(%160) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_16">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc70)
    %162 = "vhlo.reshape_v1"(%161) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc71)
    %163 = "vhlo.reshape_v1"(%arg374) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %164 = "vhlo.custom_call_v1"(%163) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_0_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %165 = "vhlo.reshape_v1"(%164) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %166 = "vhlo.transpose_v1"(%165) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc72)
    %167 = "vhlo.dot_general_v2"(%162, %166) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc73)
    %168 = "vhlo.reshape_v1"(%167) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc71)
    %169 = "vhlo.reshape_v1"(%arg373) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %170 = "vhlo.custom_call_v1"(%169) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_0_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %171 = "vhlo.reshape_v1"(%170) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %172 = "vhlo.broadcast_in_dim_v1"(%171) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc74)
    %173 = "vhlo.add_v1"(%168, %172) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc74)
    %174 = "vhlo.add_v1"(%141, %173) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc75)
    %175 = "vhlo.reshape_v1"(%arg372) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %176 = "vhlo.custom_call_v1"(%175) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_1_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %177 = "vhlo.reshape_v1"(%176) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %178 = "vhlo.reshape_v1"(%arg371) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %179 = "vhlo.custom_call_v1"(%178) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_1_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %180 = "vhlo.reshape_v1"(%179) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %181 = "vhlo.composite_v1"(%174, %177, %180) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_39">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc76)
    %182 = "vhlo.reshape_v1"(%181) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc77)
    %183 = "vhlo.reshape_v1"(%arg399) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %184 = "vhlo.custom_call_v1"(%183) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_1_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %185 = "vhlo.reshape_v1"(%184) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %186 = "vhlo.transpose_v1"(%185) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc78)
    %187 = "vhlo.dot_general_v2"(%182, %186) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc79)
    %188 = "vhlo.reshape_v1"(%187) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc77)
    %189 = "vhlo.reshape_v1"(%arg398) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %190 = "vhlo.custom_call_v1"(%189) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_1_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %191 = "vhlo.reshape_v1"(%190) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %192 = "vhlo.broadcast_in_dim_v1"(%191) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc80)
    %193 = "vhlo.add_v1"(%188, %192) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc80)
    %194 = "vhlo.reshape_v1"(%193) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc81)
    %195 = "vhlo.transpose_v1"(%194) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc82)
    %196 = "vhlo.convert_v1"(%195) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc83)
    %197 = "vhlo.multiply_v1"(%196, %7) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc84)
    %198 = "vhlo.reshape_v1"(%arg397) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %199 = "vhlo.custom_call_v1"(%198) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_1_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %200 = "vhlo.reshape_v1"(%199) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %201 = "vhlo.transpose_v1"(%200) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc85)
    %202 = "vhlo.dot_general_v2"(%182, %201) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc86)
    %203 = "vhlo.reshape_v1"(%202) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc87)
    %204 = "vhlo.reshape_v1"(%arg396) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %205 = "vhlo.custom_call_v1"(%204) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_1_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %206 = "vhlo.reshape_v1"(%205) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %207 = "vhlo.broadcast_in_dim_v1"(%206) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc88)
    %208 = "vhlo.add_v1"(%203, %207) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc88)
    %209 = "vhlo.reshape_v1"(%208) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc89)
    %210 = "vhlo.transpose_v1"(%209) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc90)
    %211 = "vhlo.convert_v1"(%210) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc91)
    %212 = "vhlo.transpose_v1"(%211) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,80,257]{2,1,3,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc92)
    %213 = "vhlo.multiply_v1"(%212, %6) : (!vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc93)
    %214 = "vhlo.dot_general_v2"(%197, %213) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc94)
    %215 = "vhlo.convert_v1"(%214) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc95)
    %216 = "vhlo.compare_v1"(%215, %5) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc95)
    %217 = "vhlo.not_v1"(%216) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc96)
    %218 = "vhlo.reduce_v1"(%217, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("690|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_2aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("690|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_2aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc98)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc99)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc97)
    %219 = "vhlo.reshape_v1"(%218) : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc97)
    %220 = "vhlo.not_v1"(%219) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc100)
    %221 = "vhlo.reshape_v1"(%220) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc101)
    %222 = "vhlo.broadcast_in_dim_v1"(%221) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc101)
    %223 = "vhlo.reduce_v1"(%214, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("687|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_1aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("687|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_1aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc103)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc102)
    %224 = "vhlo.broadcast_in_dim_v1"(%223) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc102)
    %225 = "vhlo.subtract_v1"(%214, %224) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc102)
    %226 = "vhlo.exponential_v2"(%225) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc102)
    %227 = "vhlo.reduce_v1"(%226, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("687|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_1aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("687|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_1aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc104)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc102)
    %228 = "vhlo.broadcast_in_dim_v1"(%227) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc102)
    %229 = "vhlo.divide_v1"(%226, %228) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc102)
    %230 = "vhlo.select_v1"(%222, %4, %229) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc105)
    %231 = "vhlo.reshape_v1"(%arg370) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %232 = "vhlo.custom_call_v1"(%231) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_1_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %233 = "vhlo.reshape_v1"(%232) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %234 = "vhlo.transpose_v1"(%233) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc106)
    %235 = "vhlo.dot_general_v2"(%182, %234) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc107)
    %236 = "vhlo.reshape_v1"(%235) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc108)
    %237 = "vhlo.reshape_v1"(%arg369) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %238 = "vhlo.custom_call_v1"(%237) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_1_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %239 = "vhlo.reshape_v1"(%238) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %240 = "vhlo.broadcast_in_dim_v1"(%239) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc109)
    %241 = "vhlo.add_v1"(%236, %240) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc109)
    %242 = "vhlo.reshape_v1"(%241) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc110)
    %243 = "vhlo.transpose_v1"(%242) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc111)
    %244 = "vhlo.convert_v1"(%243) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc112)
    %245 = "vhlo.dot_general_v2"(%230, %244) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc113)
    %246 = "vhlo.convert_v1"(%245) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc114)
    %247 = "vhlo.transpose_v1"(%246) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,257,16,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc115)
    %248 = "vhlo.reshape_v1"(%247) : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc116)
    %249 = "vhlo.reshape_v1"(%arg368) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %250 = "vhlo.custom_call_v1"(%249) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_1_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %251 = "vhlo.reshape_v1"(%250) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %252 = "vhlo.transpose_v1"(%251) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc117)
    %253 = "vhlo.dot_general_v2"(%248, %252) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc118)
    %254 = "vhlo.reshape_v1"(%253) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc116)
    %255 = "vhlo.reshape_v1"(%arg367) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %256 = "vhlo.custom_call_v1"(%255) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_1_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %257 = "vhlo.reshape_v1"(%256) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %258 = "vhlo.broadcast_in_dim_v1"(%257) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc119)
    %259 = "vhlo.add_v1"(%254, %258) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc119)
    %260 = "vhlo.add_v1"(%174, %259) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc120)
    %261 = "vhlo.reshape_v1"(%arg366) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %262 = "vhlo.custom_call_v1"(%261) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_1_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %263 = "vhlo.reshape_v1"(%262) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %264 = "vhlo.reshape_v1"(%arg365) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %265 = "vhlo.custom_call_v1"(%264) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_1_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %266 = "vhlo.reshape_v1"(%265) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %267 = "vhlo.composite_v1"(%260, %263, %266) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_36">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc121)
    %268 = "vhlo.reshape_v1"(%267) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc122)
    %269 = "vhlo.reshape_v1"(%arg364) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %270 = "vhlo.custom_call_v1"(%269) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_1_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %271 = "vhlo.reshape_v1"(%270) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %272 = "vhlo.transpose_v1"(%271) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc123)
    %273 = "vhlo.dot_general_v2"(%268, %272) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc124)
    %274 = "vhlo.reshape_v1"(%273) : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc122)
    %275 = "vhlo.reshape_v1"(%arg363) : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc3)
    %276 = "vhlo.custom_call_v1"(%275) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_1_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc2)
    %277 = "vhlo.reshape_v1"(%276) : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc(#loc3)
    %278 = "vhlo.broadcast_in_dim_v1"(%277) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc125)
    %279 = "vhlo.add_v1"(%274, %278) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc125)
    %280 = "vhlo.composite_v1"(%279) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_19">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc126)
    %281 = "vhlo.reshape_v1"(%280) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc127)
    %282 = "vhlo.reshape_v1"(%arg362) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %283 = "vhlo.custom_call_v1"(%282) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_1_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %284 = "vhlo.reshape_v1"(%283) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %285 = "vhlo.transpose_v1"(%284) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc128)
    %286 = "vhlo.dot_general_v2"(%281, %285) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc129)
    %287 = "vhlo.reshape_v1"(%286) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc127)
    %288 = "vhlo.reshape_v1"(%arg361) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %289 = "vhlo.custom_call_v1"(%288) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_1_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %290 = "vhlo.reshape_v1"(%289) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %291 = "vhlo.broadcast_in_dim_v1"(%290) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc130)
    %292 = "vhlo.add_v1"(%287, %291) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc130)
    %293 = "vhlo.add_v1"(%260, %292) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc131)
    %294 = "vhlo.reshape_v1"(%arg360) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %295 = "vhlo.custom_call_v1"(%294) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_2_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %296 = "vhlo.reshape_v1"(%295) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %297 = "vhlo.reshape_v1"(%arg359) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %298 = "vhlo.custom_call_v1"(%297) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_2_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %299 = "vhlo.reshape_v1"(%298) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %300 = "vhlo.composite_v1"(%293, %296, %299) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_28">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc132)
    %301 = "vhlo.reshape_v1"(%300) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc133)
    %302 = "vhlo.reshape_v1"(%arg403) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %303 = "vhlo.custom_call_v1"(%302) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_2_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %304 = "vhlo.reshape_v1"(%303) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %305 = "vhlo.transpose_v1"(%304) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc134)
    %306 = "vhlo.dot_general_v2"(%301, %305) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc135)
    %307 = "vhlo.reshape_v1"(%306) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc133)
    %308 = "vhlo.reshape_v1"(%arg402) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %309 = "vhlo.custom_call_v1"(%308) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_2_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %310 = "vhlo.reshape_v1"(%309) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %311 = "vhlo.broadcast_in_dim_v1"(%310) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc136)
    %312 = "vhlo.add_v1"(%307, %311) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc136)
    %313 = "vhlo.reshape_v1"(%312) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc137)
    %314 = "vhlo.transpose_v1"(%313) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc138)
    %315 = "vhlo.convert_v1"(%314) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc139)
    %316 = "vhlo.multiply_v1"(%315, %7) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc140)
    %317 = "vhlo.reshape_v1"(%arg401) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %318 = "vhlo.custom_call_v1"(%317) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_2_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %319 = "vhlo.reshape_v1"(%318) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %320 = "vhlo.transpose_v1"(%319) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc141)
    %321 = "vhlo.dot_general_v2"(%301, %320) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc142)
    %322 = "vhlo.reshape_v1"(%321) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc143)
    %323 = "vhlo.reshape_v1"(%arg400) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %324 = "vhlo.custom_call_v1"(%323) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_2_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %325 = "vhlo.reshape_v1"(%324) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %326 = "vhlo.broadcast_in_dim_v1"(%325) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc144)
    %327 = "vhlo.add_v1"(%322, %326) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc144)
    %328 = "vhlo.reshape_v1"(%327) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc145)
    %329 = "vhlo.transpose_v1"(%328) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc146)
    %330 = "vhlo.convert_v1"(%329) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc147)
    %331 = "vhlo.transpose_v1"(%330) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,80,257]{2,1,3,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc148)
    %332 = "vhlo.multiply_v1"(%331, %6) : (!vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc149)
    %333 = "vhlo.dot_general_v2"(%316, %332) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc150)
    %334 = "vhlo.convert_v1"(%333) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc151)
    %335 = "vhlo.compare_v1"(%334, %5) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc151)
    %336 = "vhlo.not_v1"(%335) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc152)
    %337 = "vhlo.reduce_v1"(%336, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("764|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_3aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("764|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_3aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc154)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc155)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc153)
    %338 = "vhlo.reshape_v1"(%337) : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc153)
    %339 = "vhlo.not_v1"(%338) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc156)
    %340 = "vhlo.reshape_v1"(%339) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc157)
    %341 = "vhlo.broadcast_in_dim_v1"(%340) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc157)
    %342 = "vhlo.reduce_v1"(%333, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("761|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_2aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("761|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_2aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc159)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc158)
    %343 = "vhlo.broadcast_in_dim_v1"(%342) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc158)
    %344 = "vhlo.subtract_v1"(%333, %343) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc158)
    %345 = "vhlo.exponential_v2"(%344) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc158)
    %346 = "vhlo.reduce_v1"(%345, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("761|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_2aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("761|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_2aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc160)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc158)
    %347 = "vhlo.broadcast_in_dim_v1"(%346) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc158)
    %348 = "vhlo.divide_v1"(%345, %347) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc158)
    %349 = "vhlo.select_v1"(%341, %4, %348) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc161)
    %350 = "vhlo.reshape_v1"(%arg358) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %351 = "vhlo.custom_call_v1"(%350) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_2_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %352 = "vhlo.reshape_v1"(%351) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %353 = "vhlo.transpose_v1"(%352) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc162)
    %354 = "vhlo.dot_general_v2"(%301, %353) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc163)
    %355 = "vhlo.reshape_v1"(%354) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc164)
    %356 = "vhlo.reshape_v1"(%arg357) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %357 = "vhlo.custom_call_v1"(%356) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_2_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %358 = "vhlo.reshape_v1"(%357) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %359 = "vhlo.broadcast_in_dim_v1"(%358) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc165)
    %360 = "vhlo.add_v1"(%355, %359) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc165)
    %361 = "vhlo.reshape_v1"(%360) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc166)
    %362 = "vhlo.transpose_v1"(%361) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc167)
    %363 = "vhlo.convert_v1"(%362) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc168)
    %364 = "vhlo.dot_general_v2"(%349, %363) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc169)
    %365 = "vhlo.convert_v1"(%364) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc170)
    %366 = "vhlo.transpose_v1"(%365) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,257,16,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc171)
    %367 = "vhlo.reshape_v1"(%366) : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc172)
    %368 = "vhlo.reshape_v1"(%arg356) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %369 = "vhlo.custom_call_v1"(%368) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_2_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %370 = "vhlo.reshape_v1"(%369) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %371 = "vhlo.transpose_v1"(%370) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc173)
    %372 = "vhlo.dot_general_v2"(%367, %371) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc174)
    %373 = "vhlo.reshape_v1"(%372) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc172)
    %374 = "vhlo.reshape_v1"(%arg355) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %375 = "vhlo.custom_call_v1"(%374) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_2_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %376 = "vhlo.reshape_v1"(%375) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %377 = "vhlo.broadcast_in_dim_v1"(%376) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc175)
    %378 = "vhlo.add_v1"(%373, %377) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc175)
    %379 = "vhlo.add_v1"(%293, %378) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc176)
    %380 = "vhlo.reshape_v1"(%arg354) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %381 = "vhlo.custom_call_v1"(%380) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_2_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %382 = "vhlo.reshape_v1"(%381) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %383 = "vhlo.reshape_v1"(%arg353) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %384 = "vhlo.custom_call_v1"(%383) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_2_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %385 = "vhlo.reshape_v1"(%384) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %386 = "vhlo.composite_v1"(%379, %382, %385) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_32">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc177)
    %387 = "vhlo.reshape_v1"(%386) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc178)
    %388 = "vhlo.reshape_v1"(%arg352) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %389 = "vhlo.custom_call_v1"(%388) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_2_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %390 = "vhlo.reshape_v1"(%389) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %391 = "vhlo.transpose_v1"(%390) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc179)
    %392 = "vhlo.dot_general_v2"(%387, %391) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc180)
    %393 = "vhlo.reshape_v1"(%392) : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc178)
    %394 = "vhlo.reshape_v1"(%arg351) : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc3)
    %395 = "vhlo.custom_call_v1"(%394) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_2_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc2)
    %396 = "vhlo.reshape_v1"(%395) : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc(#loc3)
    %397 = "vhlo.broadcast_in_dim_v1"(%396) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc181)
    %398 = "vhlo.add_v1"(%393, %397) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc181)
    %399 = "vhlo.composite_v1"(%398) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_21">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc182)
    %400 = "vhlo.reshape_v1"(%399) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc183)
    %401 = "vhlo.reshape_v1"(%arg350) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %402 = "vhlo.custom_call_v1"(%401) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_2_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %403 = "vhlo.reshape_v1"(%402) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %404 = "vhlo.transpose_v1"(%403) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc184)
    %405 = "vhlo.dot_general_v2"(%400, %404) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc185)
    %406 = "vhlo.reshape_v1"(%405) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc183)
    %407 = "vhlo.reshape_v1"(%arg349) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %408 = "vhlo.custom_call_v1"(%407) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_2_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %409 = "vhlo.reshape_v1"(%408) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %410 = "vhlo.broadcast_in_dim_v1"(%409) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc186)
    %411 = "vhlo.add_v1"(%406, %410) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc186)
    %412 = "vhlo.add_v1"(%379, %411) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc187)
    %413 = "vhlo.reshape_v1"(%arg348) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %414 = "vhlo.custom_call_v1"(%413) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_3_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %415 = "vhlo.reshape_v1"(%414) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %416 = "vhlo.reshape_v1"(%arg347) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %417 = "vhlo.custom_call_v1"(%416) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_3_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %418 = "vhlo.reshape_v1"(%417) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %419 = "vhlo.composite_v1"(%412, %415, %418) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_43">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc188)
    %420 = "vhlo.reshape_v1"(%419) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc189)
    %421 = "vhlo.reshape_v1"(%arg407) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %422 = "vhlo.custom_call_v1"(%421) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_3_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %423 = "vhlo.reshape_v1"(%422) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %424 = "vhlo.transpose_v1"(%423) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc190)
    %425 = "vhlo.dot_general_v2"(%420, %424) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc191)
    %426 = "vhlo.reshape_v1"(%425) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc189)
    %427 = "vhlo.reshape_v1"(%arg406) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %428 = "vhlo.custom_call_v1"(%427) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_3_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %429 = "vhlo.reshape_v1"(%428) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %430 = "vhlo.broadcast_in_dim_v1"(%429) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc192)
    %431 = "vhlo.add_v1"(%426, %430) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc192)
    %432 = "vhlo.reshape_v1"(%431) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc193)
    %433 = "vhlo.transpose_v1"(%432) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc194)
    %434 = "vhlo.convert_v1"(%433) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc195)
    %435 = "vhlo.multiply_v1"(%434, %7) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc196)
    %436 = "vhlo.reshape_v1"(%arg405) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %437 = "vhlo.custom_call_v1"(%436) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_3_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %438 = "vhlo.reshape_v1"(%437) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %439 = "vhlo.transpose_v1"(%438) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc197)
    %440 = "vhlo.dot_general_v2"(%420, %439) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc198)
    %441 = "vhlo.reshape_v1"(%440) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc199)
    %442 = "vhlo.reshape_v1"(%arg404) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %443 = "vhlo.custom_call_v1"(%442) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_3_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %444 = "vhlo.reshape_v1"(%443) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %445 = "vhlo.broadcast_in_dim_v1"(%444) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc200)
    %446 = "vhlo.add_v1"(%441, %445) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc200)
    %447 = "vhlo.reshape_v1"(%446) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc201)
    %448 = "vhlo.transpose_v1"(%447) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc202)
    %449 = "vhlo.convert_v1"(%448) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc203)
    %450 = "vhlo.transpose_v1"(%449) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,80,257]{2,1,3,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc204)
    %451 = "vhlo.multiply_v1"(%450, %6) : (!vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc205)
    %452 = "vhlo.dot_general_v2"(%435, %451) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc206)
    %453 = "vhlo.convert_v1"(%452) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc207)
    %454 = "vhlo.compare_v1"(%453, %5) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc207)
    %455 = "vhlo.not_v1"(%454) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc208)
    %456 = "vhlo.reduce_v1"(%455, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("838|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_4aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("838|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_4aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc210)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc211)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc209)
    %457 = "vhlo.reshape_v1"(%456) : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc209)
    %458 = "vhlo.not_v1"(%457) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc212)
    %459 = "vhlo.reshape_v1"(%458) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc213)
    %460 = "vhlo.broadcast_in_dim_v1"(%459) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc213)
    %461 = "vhlo.reduce_v1"(%452, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("835|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_3aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("835|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_3aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc215)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc214)
    %462 = "vhlo.broadcast_in_dim_v1"(%461) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc214)
    %463 = "vhlo.subtract_v1"(%452, %462) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc214)
    %464 = "vhlo.exponential_v2"(%463) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc214)
    %465 = "vhlo.reduce_v1"(%464, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("835|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_3aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("835|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_3aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc216)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc214)
    %466 = "vhlo.broadcast_in_dim_v1"(%465) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc214)
    %467 = "vhlo.divide_v1"(%464, %466) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc214)
    %468 = "vhlo.select_v1"(%460, %4, %467) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc217)
    %469 = "vhlo.reshape_v1"(%arg346) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %470 = "vhlo.custom_call_v1"(%469) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_3_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %471 = "vhlo.reshape_v1"(%470) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %472 = "vhlo.transpose_v1"(%471) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc218)
    %473 = "vhlo.dot_general_v2"(%420, %472) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc219)
    %474 = "vhlo.reshape_v1"(%473) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc220)
    %475 = "vhlo.reshape_v1"(%arg345) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %476 = "vhlo.custom_call_v1"(%475) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_3_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %477 = "vhlo.reshape_v1"(%476) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %478 = "vhlo.broadcast_in_dim_v1"(%477) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc221)
    %479 = "vhlo.add_v1"(%474, %478) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc221)
    %480 = "vhlo.reshape_v1"(%479) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc222)
    %481 = "vhlo.transpose_v1"(%480) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc223)
    %482 = "vhlo.convert_v1"(%481) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc224)
    %483 = "vhlo.dot_general_v2"(%468, %482) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc225)
    %484 = "vhlo.convert_v1"(%483) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc226)
    %485 = "vhlo.transpose_v1"(%484) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,257,16,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc227)
    %486 = "vhlo.reshape_v1"(%485) : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc228)
    %487 = "vhlo.reshape_v1"(%arg344) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %488 = "vhlo.custom_call_v1"(%487) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_3_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %489 = "vhlo.reshape_v1"(%488) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %490 = "vhlo.transpose_v1"(%489) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc229)
    %491 = "vhlo.dot_general_v2"(%486, %490) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc230)
    %492 = "vhlo.reshape_v1"(%491) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc228)
    %493 = "vhlo.reshape_v1"(%arg343) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %494 = "vhlo.custom_call_v1"(%493) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_3_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %495 = "vhlo.reshape_v1"(%494) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %496 = "vhlo.broadcast_in_dim_v1"(%495) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc231)
    %497 = "vhlo.add_v1"(%492, %496) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc231)
    %498 = "vhlo.add_v1"(%412, %497) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc232)
    %499 = "vhlo.reshape_v1"(%arg342) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %500 = "vhlo.custom_call_v1"(%499) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_3_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %501 = "vhlo.reshape_v1"(%500) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %502 = "vhlo.reshape_v1"(%arg341) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %503 = "vhlo.custom_call_v1"(%502) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_3_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %504 = "vhlo.reshape_v1"(%503) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %505 = "vhlo.composite_v1"(%498, %501, %504) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_52">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc233)
    %506 = "vhlo.reshape_v1"(%505) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc234)
    %507 = "vhlo.reshape_v1"(%arg340) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %508 = "vhlo.custom_call_v1"(%507) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_3_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %509 = "vhlo.reshape_v1"(%508) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %510 = "vhlo.transpose_v1"(%509) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc235)
    %511 = "vhlo.dot_general_v2"(%506, %510) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc236)
    %512 = "vhlo.reshape_v1"(%511) : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc234)
    %513 = "vhlo.reshape_v1"(%arg339) : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc3)
    %514 = "vhlo.custom_call_v1"(%513) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_3_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc2)
    %515 = "vhlo.reshape_v1"(%514) : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc(#loc3)
    %516 = "vhlo.broadcast_in_dim_v1"(%515) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc237)
    %517 = "vhlo.add_v1"(%512, %516) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc237)
    %518 = "vhlo.composite_v1"(%517) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_20">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc238)
    %519 = "vhlo.reshape_v1"(%518) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc239)
    %520 = "vhlo.reshape_v1"(%arg338) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %521 = "vhlo.custom_call_v1"(%520) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_3_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %522 = "vhlo.reshape_v1"(%521) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %523 = "vhlo.transpose_v1"(%522) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc240)
    %524 = "vhlo.dot_general_v2"(%519, %523) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc241)
    %525 = "vhlo.reshape_v1"(%524) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc239)
    %526 = "vhlo.reshape_v1"(%arg337) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %527 = "vhlo.custom_call_v1"(%526) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_3_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %528 = "vhlo.reshape_v1"(%527) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %529 = "vhlo.broadcast_in_dim_v1"(%528) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc242)
    %530 = "vhlo.add_v1"(%525, %529) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc242)
    %531 = "vhlo.add_v1"(%498, %530) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc243)
    %532 = "vhlo.reshape_v1"(%arg336) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %533 = "vhlo.custom_call_v1"(%532) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_4_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %534 = "vhlo.reshape_v1"(%533) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %535 = "vhlo.reshape_v1"(%arg335) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %536 = "vhlo.custom_call_v1"(%535) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_4_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %537 = "vhlo.reshape_v1"(%536) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %538 = "vhlo.composite_v1"(%531, %534, %537) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_68">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc244)
    %539 = "vhlo.reshape_v1"(%538) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc245)
    %540 = "vhlo.reshape_v1"(%arg411) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %541 = "vhlo.custom_call_v1"(%540) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_4_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %542 = "vhlo.reshape_v1"(%541) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %543 = "vhlo.transpose_v1"(%542) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc246)
    %544 = "vhlo.dot_general_v2"(%539, %543) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc247)
    %545 = "vhlo.reshape_v1"(%544) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc245)
    %546 = "vhlo.reshape_v1"(%arg410) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %547 = "vhlo.custom_call_v1"(%546) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_4_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %548 = "vhlo.reshape_v1"(%547) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %549 = "vhlo.broadcast_in_dim_v1"(%548) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc248)
    %550 = "vhlo.add_v1"(%545, %549) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc248)
    %551 = "vhlo.reshape_v1"(%550) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc249)
    %552 = "vhlo.transpose_v1"(%551) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc250)
    %553 = "vhlo.convert_v1"(%552) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc251)
    %554 = "vhlo.multiply_v1"(%553, %7) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc252)
    %555 = "vhlo.reshape_v1"(%arg409) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %556 = "vhlo.custom_call_v1"(%555) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_4_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %557 = "vhlo.reshape_v1"(%556) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %558 = "vhlo.transpose_v1"(%557) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc253)
    %559 = "vhlo.dot_general_v2"(%539, %558) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc254)
    %560 = "vhlo.reshape_v1"(%559) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc255)
    %561 = "vhlo.reshape_v1"(%arg408) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %562 = "vhlo.custom_call_v1"(%561) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_4_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %563 = "vhlo.reshape_v1"(%562) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %564 = "vhlo.broadcast_in_dim_v1"(%563) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc256)
    %565 = "vhlo.add_v1"(%560, %564) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc256)
    %566 = "vhlo.reshape_v1"(%565) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc257)
    %567 = "vhlo.transpose_v1"(%566) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc258)
    %568 = "vhlo.convert_v1"(%567) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc259)
    %569 = "vhlo.transpose_v1"(%568) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,80,257]{2,1,3,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc260)
    %570 = "vhlo.multiply_v1"(%569, %6) : (!vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc261)
    %571 = "vhlo.dot_general_v2"(%554, %570) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc262)
    %572 = "vhlo.convert_v1"(%571) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc263)
    %573 = "vhlo.compare_v1"(%572, %5) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc263)
    %574 = "vhlo.not_v1"(%573) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc264)
    %575 = "vhlo.reduce_v1"(%574, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("912|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_5aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("912|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_5aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc266)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc267)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc265)
    %576 = "vhlo.reshape_v1"(%575) : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc265)
    %577 = "vhlo.not_v1"(%576) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc268)
    %578 = "vhlo.reshape_v1"(%577) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc269)
    %579 = "vhlo.broadcast_in_dim_v1"(%578) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc269)
    %580 = "vhlo.reduce_v1"(%571, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("909|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_4aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("909|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_4aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc271)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc270)
    %581 = "vhlo.broadcast_in_dim_v1"(%580) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc270)
    %582 = "vhlo.subtract_v1"(%571, %581) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc270)
    %583 = "vhlo.exponential_v2"(%582) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc270)
    %584 = "vhlo.reduce_v1"(%583, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("909|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_4aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("909|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_4aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc272)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc270)
    %585 = "vhlo.broadcast_in_dim_v1"(%584) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc270)
    %586 = "vhlo.divide_v1"(%583, %585) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc270)
    %587 = "vhlo.select_v1"(%579, %4, %586) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc273)
    %588 = "vhlo.reshape_v1"(%arg334) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %589 = "vhlo.custom_call_v1"(%588) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_4_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %590 = "vhlo.reshape_v1"(%589) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %591 = "vhlo.transpose_v1"(%590) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc274)
    %592 = "vhlo.dot_general_v2"(%539, %591) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc275)
    %593 = "vhlo.reshape_v1"(%592) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc276)
    %594 = "vhlo.reshape_v1"(%arg333) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %595 = "vhlo.custom_call_v1"(%594) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_4_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %596 = "vhlo.reshape_v1"(%595) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %597 = "vhlo.broadcast_in_dim_v1"(%596) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc277)
    %598 = "vhlo.add_v1"(%593, %597) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc277)
    %599 = "vhlo.reshape_v1"(%598) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc278)
    %600 = "vhlo.transpose_v1"(%599) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc279)
    %601 = "vhlo.convert_v1"(%600) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc280)
    %602 = "vhlo.dot_general_v2"(%587, %601) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc281)
    %603 = "vhlo.convert_v1"(%602) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc282)
    %604 = "vhlo.transpose_v1"(%603) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,257,16,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc283)
    %605 = "vhlo.reshape_v1"(%604) : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc284)
    %606 = "vhlo.reshape_v1"(%arg332) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %607 = "vhlo.custom_call_v1"(%606) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_4_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %608 = "vhlo.reshape_v1"(%607) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %609 = "vhlo.transpose_v1"(%608) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc285)
    %610 = "vhlo.dot_general_v2"(%605, %609) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc286)
    %611 = "vhlo.reshape_v1"(%610) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc284)
    %612 = "vhlo.reshape_v1"(%arg331) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %613 = "vhlo.custom_call_v1"(%612) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_4_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %614 = "vhlo.reshape_v1"(%613) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %615 = "vhlo.broadcast_in_dim_v1"(%614) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc287)
    %616 = "vhlo.add_v1"(%611, %615) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc287)
    %617 = "vhlo.add_v1"(%531, %616) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc288)
    %618 = "vhlo.reshape_v1"(%arg330) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %619 = "vhlo.custom_call_v1"(%618) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_4_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %620 = "vhlo.reshape_v1"(%619) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %621 = "vhlo.reshape_v1"(%arg329) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %622 = "vhlo.custom_call_v1"(%621) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_4_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %623 = "vhlo.reshape_v1"(%622) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %624 = "vhlo.composite_v1"(%617, %620, %623) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_38">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc289)
    %625 = "vhlo.reshape_v1"(%624) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc290)
    %626 = "vhlo.reshape_v1"(%arg328) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %627 = "vhlo.custom_call_v1"(%626) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_4_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %628 = "vhlo.reshape_v1"(%627) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %629 = "vhlo.transpose_v1"(%628) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc291)
    %630 = "vhlo.dot_general_v2"(%625, %629) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc292)
    %631 = "vhlo.reshape_v1"(%630) : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc290)
    %632 = "vhlo.reshape_v1"(%arg327) : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc3)
    %633 = "vhlo.custom_call_v1"(%632) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_4_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc2)
    %634 = "vhlo.reshape_v1"(%633) : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc(#loc3)
    %635 = "vhlo.broadcast_in_dim_v1"(%634) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc293)
    %636 = "vhlo.add_v1"(%631, %635) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc293)
    %637 = "vhlo.composite_v1"(%636) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_17">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc294)
    %638 = "vhlo.reshape_v1"(%637) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc295)
    %639 = "vhlo.reshape_v1"(%arg326) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %640 = "vhlo.custom_call_v1"(%639) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_4_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %641 = "vhlo.reshape_v1"(%640) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %642 = "vhlo.transpose_v1"(%641) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc296)
    %643 = "vhlo.dot_general_v2"(%638, %642) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc297)
    %644 = "vhlo.reshape_v1"(%643) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc295)
    %645 = "vhlo.reshape_v1"(%arg325) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %646 = "vhlo.custom_call_v1"(%645) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_4_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %647 = "vhlo.reshape_v1"(%646) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %648 = "vhlo.broadcast_in_dim_v1"(%647) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc298)
    %649 = "vhlo.add_v1"(%644, %648) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc298)
    %650 = "vhlo.add_v1"(%617, %649) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc299)
    %651 = "vhlo.reshape_v1"(%arg324) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %652 = "vhlo.custom_call_v1"(%651) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_5_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %653 = "vhlo.reshape_v1"(%652) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %654 = "vhlo.reshape_v1"(%arg323) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %655 = "vhlo.custom_call_v1"(%654) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_5_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %656 = "vhlo.reshape_v1"(%655) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %657 = "vhlo.composite_v1"(%650, %653, %656) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_42">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc300)
    %658 = "vhlo.reshape_v1"(%657) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc301)
    %659 = "vhlo.reshape_v1"(%arg415) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %660 = "vhlo.custom_call_v1"(%659) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_5_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %661 = "vhlo.reshape_v1"(%660) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %662 = "vhlo.transpose_v1"(%661) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc302)
    %663 = "vhlo.dot_general_v2"(%658, %662) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc303)
    %664 = "vhlo.reshape_v1"(%663) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc301)
    %665 = "vhlo.reshape_v1"(%arg414) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %666 = "vhlo.custom_call_v1"(%665) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_5_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %667 = "vhlo.reshape_v1"(%666) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %668 = "vhlo.broadcast_in_dim_v1"(%667) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc304)
    %669 = "vhlo.add_v1"(%664, %668) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc304)
    %670 = "vhlo.reshape_v1"(%669) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc305)
    %671 = "vhlo.transpose_v1"(%670) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc306)
    %672 = "vhlo.convert_v1"(%671) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc307)
    %673 = "vhlo.multiply_v1"(%672, %7) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc308)
    %674 = "vhlo.reshape_v1"(%arg413) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %675 = "vhlo.custom_call_v1"(%674) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_5_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %676 = "vhlo.reshape_v1"(%675) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %677 = "vhlo.transpose_v1"(%676) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc309)
    %678 = "vhlo.dot_general_v2"(%658, %677) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc310)
    %679 = "vhlo.reshape_v1"(%678) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc311)
    %680 = "vhlo.reshape_v1"(%arg412) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %681 = "vhlo.custom_call_v1"(%680) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_5_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %682 = "vhlo.reshape_v1"(%681) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %683 = "vhlo.broadcast_in_dim_v1"(%682) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc312)
    %684 = "vhlo.add_v1"(%679, %683) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc312)
    %685 = "vhlo.reshape_v1"(%684) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc313)
    %686 = "vhlo.transpose_v1"(%685) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc314)
    %687 = "vhlo.convert_v1"(%686) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc315)
    %688 = "vhlo.transpose_v1"(%687) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,80,257]{2,1,3,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc316)
    %689 = "vhlo.multiply_v1"(%688, %6) : (!vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc317)
    %690 = "vhlo.dot_general_v2"(%673, %689) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc318)
    %691 = "vhlo.convert_v1"(%690) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc319)
    %692 = "vhlo.compare_v1"(%691, %5) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc319)
    %693 = "vhlo.not_v1"(%692) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc320)
    %694 = "vhlo.reduce_v1"(%693, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("986|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_6aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("986|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_6aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc322)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc323)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc321)
    %695 = "vhlo.reshape_v1"(%694) : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc321)
    %696 = "vhlo.not_v1"(%695) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc324)
    %697 = "vhlo.reshape_v1"(%696) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc325)
    %698 = "vhlo.broadcast_in_dim_v1"(%697) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc325)
    %699 = "vhlo.reduce_v1"(%690, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("983|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_5aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("983|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_5aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc327)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc326)
    %700 = "vhlo.broadcast_in_dim_v1"(%699) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc326)
    %701 = "vhlo.subtract_v1"(%690, %700) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc326)
    %702 = "vhlo.exponential_v2"(%701) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc326)
    %703 = "vhlo.reduce_v1"(%702, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("983|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_5aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("983|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_5aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc328)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc326)
    %704 = "vhlo.broadcast_in_dim_v1"(%703) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc326)
    %705 = "vhlo.divide_v1"(%702, %704) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc326)
    %706 = "vhlo.select_v1"(%698, %4, %705) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc329)
    %707 = "vhlo.reshape_v1"(%arg322) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %708 = "vhlo.custom_call_v1"(%707) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_5_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %709 = "vhlo.reshape_v1"(%708) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %710 = "vhlo.transpose_v1"(%709) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc330)
    %711 = "vhlo.dot_general_v2"(%658, %710) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc331)
    %712 = "vhlo.reshape_v1"(%711) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc332)
    %713 = "vhlo.reshape_v1"(%arg321) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %714 = "vhlo.custom_call_v1"(%713) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_5_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %715 = "vhlo.reshape_v1"(%714) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %716 = "vhlo.broadcast_in_dim_v1"(%715) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc333)
    %717 = "vhlo.add_v1"(%712, %716) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc333)
    %718 = "vhlo.reshape_v1"(%717) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc334)
    %719 = "vhlo.transpose_v1"(%718) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc335)
    %720 = "vhlo.convert_v1"(%719) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc336)
    %721 = "vhlo.dot_general_v2"(%706, %720) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc337)
    %722 = "vhlo.convert_v1"(%721) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc338)
    %723 = "vhlo.transpose_v1"(%722) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,257,16,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc339)
    %724 = "vhlo.reshape_v1"(%723) : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc340)
    %725 = "vhlo.reshape_v1"(%arg320) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %726 = "vhlo.custom_call_v1"(%725) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_5_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %727 = "vhlo.reshape_v1"(%726) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %728 = "vhlo.transpose_v1"(%727) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc341)
    %729 = "vhlo.dot_general_v2"(%724, %728) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc342)
    %730 = "vhlo.reshape_v1"(%729) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc340)
    %731 = "vhlo.reshape_v1"(%arg319) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %732 = "vhlo.custom_call_v1"(%731) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_5_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %733 = "vhlo.reshape_v1"(%732) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %734 = "vhlo.broadcast_in_dim_v1"(%733) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc343)
    %735 = "vhlo.add_v1"(%730, %734) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc343)
    %736 = "vhlo.add_v1"(%650, %735) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc344)
    %737 = "vhlo.reshape_v1"(%arg318) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %738 = "vhlo.custom_call_v1"(%737) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_5_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %739 = "vhlo.reshape_v1"(%738) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %740 = "vhlo.reshape_v1"(%arg317) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %741 = "vhlo.custom_call_v1"(%740) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_5_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %742 = "vhlo.reshape_v1"(%741) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %743 = "vhlo.composite_v1"(%736, %739, %742) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_41">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc345)
    %744 = "vhlo.reshape_v1"(%743) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc346)
    %745 = "vhlo.reshape_v1"(%arg316) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %746 = "vhlo.custom_call_v1"(%745) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_5_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %747 = "vhlo.reshape_v1"(%746) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %748 = "vhlo.transpose_v1"(%747) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc347)
    %749 = "vhlo.dot_general_v2"(%744, %748) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc348)
    %750 = "vhlo.reshape_v1"(%749) : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc346)
    %751 = "vhlo.reshape_v1"(%arg315) : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc3)
    %752 = "vhlo.custom_call_v1"(%751) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_5_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc2)
    %753 = "vhlo.reshape_v1"(%752) : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc(#loc3)
    %754 = "vhlo.broadcast_in_dim_v1"(%753) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc349)
    %755 = "vhlo.add_v1"(%750, %754) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc349)
    %756 = "vhlo.composite_v1"(%755) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_26">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc350)
    %757 = "vhlo.reshape_v1"(%756) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc351)
    %758 = "vhlo.reshape_v1"(%arg314) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %759 = "vhlo.custom_call_v1"(%758) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_5_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %760 = "vhlo.reshape_v1"(%759) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %761 = "vhlo.transpose_v1"(%760) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc352)
    %762 = "vhlo.dot_general_v2"(%757, %761) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc353)
    %763 = "vhlo.reshape_v1"(%762) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc351)
    %764 = "vhlo.reshape_v1"(%arg313) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %765 = "vhlo.custom_call_v1"(%764) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_5_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %766 = "vhlo.reshape_v1"(%765) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %767 = "vhlo.broadcast_in_dim_v1"(%766) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc354)
    %768 = "vhlo.add_v1"(%763, %767) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc354)
    %769 = "vhlo.add_v1"(%736, %768) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc355)
    %770 = "vhlo.reshape_v1"(%arg312) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %771 = "vhlo.custom_call_v1"(%770) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_6_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %772 = "vhlo.reshape_v1"(%771) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %773 = "vhlo.reshape_v1"(%arg311) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %774 = "vhlo.custom_call_v1"(%773) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_6_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %775 = "vhlo.reshape_v1"(%774) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %776 = "vhlo.composite_v1"(%769, %772, %775) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_64">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc356)
    %777 = "vhlo.reshape_v1"(%776) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc357)
    %778 = "vhlo.reshape_v1"(%arg419) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %779 = "vhlo.custom_call_v1"(%778) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_6_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %780 = "vhlo.reshape_v1"(%779) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %781 = "vhlo.transpose_v1"(%780) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc358)
    %782 = "vhlo.dot_general_v2"(%777, %781) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc359)
    %783 = "vhlo.reshape_v1"(%782) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc357)
    %784 = "vhlo.reshape_v1"(%arg418) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %785 = "vhlo.custom_call_v1"(%784) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_6_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %786 = "vhlo.reshape_v1"(%785) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %787 = "vhlo.broadcast_in_dim_v1"(%786) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc360)
    %788 = "vhlo.add_v1"(%783, %787) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc360)
    %789 = "vhlo.reshape_v1"(%788) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc361)
    %790 = "vhlo.transpose_v1"(%789) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc362)
    %791 = "vhlo.convert_v1"(%790) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc363)
    %792 = "vhlo.multiply_v1"(%791, %7) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc364)
    %793 = "vhlo.reshape_v1"(%arg417) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %794 = "vhlo.custom_call_v1"(%793) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_6_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %795 = "vhlo.reshape_v1"(%794) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %796 = "vhlo.transpose_v1"(%795) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc365)
    %797 = "vhlo.dot_general_v2"(%777, %796) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc366)
    %798 = "vhlo.reshape_v1"(%797) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc367)
    %799 = "vhlo.reshape_v1"(%arg416) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %800 = "vhlo.custom_call_v1"(%799) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_6_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %801 = "vhlo.reshape_v1"(%800) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %802 = "vhlo.broadcast_in_dim_v1"(%801) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc368)
    %803 = "vhlo.add_v1"(%798, %802) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc368)
    %804 = "vhlo.reshape_v1"(%803) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc369)
    %805 = "vhlo.transpose_v1"(%804) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc370)
    %806 = "vhlo.convert_v1"(%805) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc371)
    %807 = "vhlo.transpose_v1"(%806) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,80,257]{2,1,3,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc372)
    %808 = "vhlo.multiply_v1"(%807, %6) : (!vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc373)
    %809 = "vhlo.dot_general_v2"(%792, %808) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc374)
    %810 = "vhlo.convert_v1"(%809) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc375)
    %811 = "vhlo.compare_v1"(%810, %5) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc375)
    %812 = "vhlo.not_v1"(%811) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc376)
    %813 = "vhlo.reduce_v1"(%812, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("1060|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_7aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("1060|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_7aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc378)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc379)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc377)
    %814 = "vhlo.reshape_v1"(%813) : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc377)
    %815 = "vhlo.not_v1"(%814) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc380)
    %816 = "vhlo.reshape_v1"(%815) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc381)
    %817 = "vhlo.broadcast_in_dim_v1"(%816) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc381)
    %818 = "vhlo.reduce_v1"(%809, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1057|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_6aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1057|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_6aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc383)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc382)
    %819 = "vhlo.broadcast_in_dim_v1"(%818) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc382)
    %820 = "vhlo.subtract_v1"(%809, %819) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc382)
    %821 = "vhlo.exponential_v2"(%820) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc382)
    %822 = "vhlo.reduce_v1"(%821, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1057|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_6aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1057|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_6aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc384)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc382)
    %823 = "vhlo.broadcast_in_dim_v1"(%822) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc382)
    %824 = "vhlo.divide_v1"(%821, %823) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc382)
    %825 = "vhlo.select_v1"(%817, %4, %824) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc385)
    %826 = "vhlo.reshape_v1"(%arg310) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %827 = "vhlo.custom_call_v1"(%826) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_6_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %828 = "vhlo.reshape_v1"(%827) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %829 = "vhlo.transpose_v1"(%828) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc386)
    %830 = "vhlo.dot_general_v2"(%777, %829) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc387)
    %831 = "vhlo.reshape_v1"(%830) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc388)
    %832 = "vhlo.reshape_v1"(%arg309) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %833 = "vhlo.custom_call_v1"(%832) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_6_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %834 = "vhlo.reshape_v1"(%833) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %835 = "vhlo.broadcast_in_dim_v1"(%834) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc389)
    %836 = "vhlo.add_v1"(%831, %835) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc389)
    %837 = "vhlo.reshape_v1"(%836) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc390)
    %838 = "vhlo.transpose_v1"(%837) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc391)
    %839 = "vhlo.convert_v1"(%838) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc392)
    %840 = "vhlo.dot_general_v2"(%825, %839) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc393)
    %841 = "vhlo.convert_v1"(%840) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc394)
    %842 = "vhlo.transpose_v1"(%841) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,257,16,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc395)
    %843 = "vhlo.reshape_v1"(%842) : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc396)
    %844 = "vhlo.reshape_v1"(%arg308) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %845 = "vhlo.custom_call_v1"(%844) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_6_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %846 = "vhlo.reshape_v1"(%845) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %847 = "vhlo.transpose_v1"(%846) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc397)
    %848 = "vhlo.dot_general_v2"(%843, %847) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc398)
    %849 = "vhlo.reshape_v1"(%848) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc396)
    %850 = "vhlo.reshape_v1"(%arg307) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %851 = "vhlo.custom_call_v1"(%850) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_6_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %852 = "vhlo.reshape_v1"(%851) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %853 = "vhlo.broadcast_in_dim_v1"(%852) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc399)
    %854 = "vhlo.add_v1"(%849, %853) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc399)
    %855 = "vhlo.add_v1"(%769, %854) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc400)
    %856 = "vhlo.reshape_v1"(%arg306) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %857 = "vhlo.custom_call_v1"(%856) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_6_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %858 = "vhlo.reshape_v1"(%857) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %859 = "vhlo.reshape_v1"(%arg305) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %860 = "vhlo.custom_call_v1"(%859) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_6_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %861 = "vhlo.reshape_v1"(%860) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %862 = "vhlo.composite_v1"(%855, %858, %861) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_25">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc401)
    %863 = "vhlo.reshape_v1"(%862) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc402)
    %864 = "vhlo.reshape_v1"(%arg304) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %865 = "vhlo.custom_call_v1"(%864) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_6_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %866 = "vhlo.reshape_v1"(%865) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %867 = "vhlo.transpose_v1"(%866) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc403)
    %868 = "vhlo.dot_general_v2"(%863, %867) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc404)
    %869 = "vhlo.reshape_v1"(%868) : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc402)
    %870 = "vhlo.reshape_v1"(%arg303) : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc3)
    %871 = "vhlo.custom_call_v1"(%870) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_6_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc2)
    %872 = "vhlo.reshape_v1"(%871) : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc(#loc3)
    %873 = "vhlo.broadcast_in_dim_v1"(%872) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc405)
    %874 = "vhlo.add_v1"(%869, %873) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc405)
    %875 = "vhlo.composite_v1"(%874) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_15">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc406)
    %876 = "vhlo.reshape_v1"(%875) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc407)
    %877 = "vhlo.reshape_v1"(%arg302) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %878 = "vhlo.custom_call_v1"(%877) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_6_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %879 = "vhlo.reshape_v1"(%878) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %880 = "vhlo.transpose_v1"(%879) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc408)
    %881 = "vhlo.dot_general_v2"(%876, %880) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc409)
    %882 = "vhlo.reshape_v1"(%881) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc407)
    %883 = "vhlo.reshape_v1"(%arg301) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %884 = "vhlo.custom_call_v1"(%883) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_6_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %885 = "vhlo.reshape_v1"(%884) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %886 = "vhlo.broadcast_in_dim_v1"(%885) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc410)
    %887 = "vhlo.add_v1"(%882, %886) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc410)
    %888 = "vhlo.add_v1"(%855, %887) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc411)
    %889 = "vhlo.reshape_v1"(%arg300) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %890 = "vhlo.custom_call_v1"(%889) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_7_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %891 = "vhlo.reshape_v1"(%890) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %892 = "vhlo.reshape_v1"(%arg299) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %893 = "vhlo.custom_call_v1"(%892) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_7_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %894 = "vhlo.reshape_v1"(%893) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %895 = "vhlo.composite_v1"(%888, %891, %894) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_65">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc412)
    %896 = "vhlo.reshape_v1"(%895) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc413)
    %897 = "vhlo.reshape_v1"(%arg423) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %898 = "vhlo.custom_call_v1"(%897) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_7_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %899 = "vhlo.reshape_v1"(%898) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %900 = "vhlo.transpose_v1"(%899) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc414)
    %901 = "vhlo.dot_general_v2"(%896, %900) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc415)
    %902 = "vhlo.reshape_v1"(%901) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc413)
    %903 = "vhlo.reshape_v1"(%arg422) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %904 = "vhlo.custom_call_v1"(%903) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_7_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %905 = "vhlo.reshape_v1"(%904) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %906 = "vhlo.broadcast_in_dim_v1"(%905) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc416)
    %907 = "vhlo.add_v1"(%902, %906) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc416)
    %908 = "vhlo.reshape_v1"(%907) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc417)
    %909 = "vhlo.transpose_v1"(%908) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc418)
    %910 = "vhlo.convert_v1"(%909) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc419)
    %911 = "vhlo.multiply_v1"(%910, %7) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc420)
    %912 = "vhlo.reshape_v1"(%arg421) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %913 = "vhlo.custom_call_v1"(%912) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_7_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %914 = "vhlo.reshape_v1"(%913) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %915 = "vhlo.transpose_v1"(%914) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc421)
    %916 = "vhlo.dot_general_v2"(%896, %915) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc422)
    %917 = "vhlo.reshape_v1"(%916) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc423)
    %918 = "vhlo.reshape_v1"(%arg420) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %919 = "vhlo.custom_call_v1"(%918) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_7_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %920 = "vhlo.reshape_v1"(%919) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %921 = "vhlo.broadcast_in_dim_v1"(%920) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc424)
    %922 = "vhlo.add_v1"(%917, %921) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc424)
    %923 = "vhlo.reshape_v1"(%922) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc425)
    %924 = "vhlo.transpose_v1"(%923) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc426)
    %925 = "vhlo.convert_v1"(%924) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc427)
    %926 = "vhlo.transpose_v1"(%925) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,80,257]{2,1,3,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc428)
    %927 = "vhlo.multiply_v1"(%926, %6) : (!vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc429)
    %928 = "vhlo.dot_general_v2"(%911, %927) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc430)
    %929 = "vhlo.convert_v1"(%928) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc431)
    %930 = "vhlo.compare_v1"(%929, %5) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc431)
    %931 = "vhlo.not_v1"(%930) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc432)
    %932 = "vhlo.reduce_v1"(%931, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("1134|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_8aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("1134|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_8aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc434)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc435)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc433)
    %933 = "vhlo.reshape_v1"(%932) : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc433)
    %934 = "vhlo.not_v1"(%933) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc436)
    %935 = "vhlo.reshape_v1"(%934) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc437)
    %936 = "vhlo.broadcast_in_dim_v1"(%935) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc437)
    %937 = "vhlo.reduce_v1"(%928, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1131|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_7aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1131|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_7aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc439)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc438)
    %938 = "vhlo.broadcast_in_dim_v1"(%937) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc438)
    %939 = "vhlo.subtract_v1"(%928, %938) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc438)
    %940 = "vhlo.exponential_v2"(%939) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc438)
    %941 = "vhlo.reduce_v1"(%940, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1131|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_7aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1131|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_7aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc440)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc438)
    %942 = "vhlo.broadcast_in_dim_v1"(%941) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc438)
    %943 = "vhlo.divide_v1"(%940, %942) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc438)
    %944 = "vhlo.select_v1"(%936, %4, %943) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc441)
    %945 = "vhlo.reshape_v1"(%arg298) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %946 = "vhlo.custom_call_v1"(%945) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_7_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %947 = "vhlo.reshape_v1"(%946) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %948 = "vhlo.transpose_v1"(%947) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc442)
    %949 = "vhlo.dot_general_v2"(%896, %948) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc443)
    %950 = "vhlo.reshape_v1"(%949) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc444)
    %951 = "vhlo.reshape_v1"(%arg297) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %952 = "vhlo.custom_call_v1"(%951) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_7_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %953 = "vhlo.reshape_v1"(%952) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %954 = "vhlo.broadcast_in_dim_v1"(%953) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc445)
    %955 = "vhlo.add_v1"(%950, %954) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc445)
    %956 = "vhlo.reshape_v1"(%955) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc446)
    %957 = "vhlo.transpose_v1"(%956) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc447)
    %958 = "vhlo.convert_v1"(%957) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc448)
    %959 = "vhlo.dot_general_v2"(%944, %958) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc449)
    %960 = "vhlo.convert_v1"(%959) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc450)
    %961 = "vhlo.transpose_v1"(%960) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,257,16,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc451)
    %962 = "vhlo.reshape_v1"(%961) : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc452)
    %963 = "vhlo.reshape_v1"(%arg296) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %964 = "vhlo.custom_call_v1"(%963) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_7_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %965 = "vhlo.reshape_v1"(%964) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %966 = "vhlo.transpose_v1"(%965) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc453)
    %967 = "vhlo.dot_general_v2"(%962, %966) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc454)
    %968 = "vhlo.reshape_v1"(%967) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc452)
    %969 = "vhlo.reshape_v1"(%arg295) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %970 = "vhlo.custom_call_v1"(%969) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_7_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %971 = "vhlo.reshape_v1"(%970) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %972 = "vhlo.broadcast_in_dim_v1"(%971) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc455)
    %973 = "vhlo.add_v1"(%968, %972) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc455)
    %974 = "vhlo.add_v1"(%888, %973) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc456)
    %975 = "vhlo.reshape_v1"(%arg294) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %976 = "vhlo.custom_call_v1"(%975) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_7_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %977 = "vhlo.reshape_v1"(%976) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %978 = "vhlo.reshape_v1"(%arg293) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %979 = "vhlo.custom_call_v1"(%978) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_7_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %980 = "vhlo.reshape_v1"(%979) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %981 = "vhlo.composite_v1"(%974, %977, %980) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_29">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc457)
    %982 = "vhlo.reshape_v1"(%981) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc458)
    %983 = "vhlo.reshape_v1"(%arg292) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %984 = "vhlo.custom_call_v1"(%983) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_7_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %985 = "vhlo.reshape_v1"(%984) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %986 = "vhlo.transpose_v1"(%985) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc459)
    %987 = "vhlo.dot_general_v2"(%982, %986) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc460)
    %988 = "vhlo.reshape_v1"(%987) : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc458)
    %989 = "vhlo.reshape_v1"(%arg291) : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc3)
    %990 = "vhlo.custom_call_v1"(%989) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_7_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc2)
    %991 = "vhlo.reshape_v1"(%990) : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc(#loc3)
    %992 = "vhlo.broadcast_in_dim_v1"(%991) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc461)
    %993 = "vhlo.add_v1"(%988, %992) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc461)
    %994 = "vhlo.composite_v1"(%993) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_13">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc462)
    %995 = "vhlo.reshape_v1"(%994) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc463)
    %996 = "vhlo.reshape_v1"(%arg290) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %997 = "vhlo.custom_call_v1"(%996) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_7_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %998 = "vhlo.reshape_v1"(%997) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %999 = "vhlo.transpose_v1"(%998) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc464)
    %1000 = "vhlo.dot_general_v2"(%995, %999) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc465)
    %1001 = "vhlo.reshape_v1"(%1000) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc463)
    %1002 = "vhlo.reshape_v1"(%arg289) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1003 = "vhlo.custom_call_v1"(%1002) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_7_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1004 = "vhlo.reshape_v1"(%1003) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1005 = "vhlo.broadcast_in_dim_v1"(%1004) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc466)
    %1006 = "vhlo.add_v1"(%1001, %1005) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc466)
    %1007 = "vhlo.add_v1"(%974, %1006) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc467)
    %1008 = "vhlo.reshape_v1"(%arg288) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1009 = "vhlo.custom_call_v1"(%1008) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_8_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1010 = "vhlo.reshape_v1"(%1009) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1011 = "vhlo.reshape_v1"(%arg287) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1012 = "vhlo.custom_call_v1"(%1011) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_8_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1013 = "vhlo.reshape_v1"(%1012) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1014 = "vhlo.composite_v1"(%1007, %1010, %1013) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_22">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc468)
    %1015 = "vhlo.reshape_v1"(%1014) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc469)
    %1016 = "vhlo.reshape_v1"(%arg427) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1017 = "vhlo.custom_call_v1"(%1016) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_8_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1018 = "vhlo.reshape_v1"(%1017) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1019 = "vhlo.transpose_v1"(%1018) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc470)
    %1020 = "vhlo.dot_general_v2"(%1015, %1019) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc471)
    %1021 = "vhlo.reshape_v1"(%1020) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc469)
    %1022 = "vhlo.reshape_v1"(%arg426) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1023 = "vhlo.custom_call_v1"(%1022) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_8_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1024 = "vhlo.reshape_v1"(%1023) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1025 = "vhlo.broadcast_in_dim_v1"(%1024) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc472)
    %1026 = "vhlo.add_v1"(%1021, %1025) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc472)
    %1027 = "vhlo.reshape_v1"(%1026) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc473)
    %1028 = "vhlo.transpose_v1"(%1027) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc474)
    %1029 = "vhlo.convert_v1"(%1028) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc475)
    %1030 = "vhlo.multiply_v1"(%1029, %7) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc476)
    %1031 = "vhlo.reshape_v1"(%arg425) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1032 = "vhlo.custom_call_v1"(%1031) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_8_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1033 = "vhlo.reshape_v1"(%1032) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1034 = "vhlo.transpose_v1"(%1033) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc477)
    %1035 = "vhlo.dot_general_v2"(%1015, %1034) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc478)
    %1036 = "vhlo.reshape_v1"(%1035) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc479)
    %1037 = "vhlo.reshape_v1"(%arg424) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1038 = "vhlo.custom_call_v1"(%1037) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_8_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1039 = "vhlo.reshape_v1"(%1038) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1040 = "vhlo.broadcast_in_dim_v1"(%1039) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc480)
    %1041 = "vhlo.add_v1"(%1036, %1040) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc480)
    %1042 = "vhlo.reshape_v1"(%1041) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc481)
    %1043 = "vhlo.transpose_v1"(%1042) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc482)
    %1044 = "vhlo.convert_v1"(%1043) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc483)
    %1045 = "vhlo.transpose_v1"(%1044) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,80,257]{2,1,3,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc484)
    %1046 = "vhlo.multiply_v1"(%1045, %6) : (!vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc485)
    %1047 = "vhlo.dot_general_v2"(%1030, %1046) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc486)
    %1048 = "vhlo.convert_v1"(%1047) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc487)
    %1049 = "vhlo.compare_v1"(%1048, %5) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc487)
    %1050 = "vhlo.not_v1"(%1049) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc488)
    %1051 = "vhlo.reduce_v1"(%1050, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("1208|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_9aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("1208|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_9aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc490)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc491)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc489)
    %1052 = "vhlo.reshape_v1"(%1051) : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc489)
    %1053 = "vhlo.not_v1"(%1052) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc492)
    %1054 = "vhlo.reshape_v1"(%1053) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc493)
    %1055 = "vhlo.broadcast_in_dim_v1"(%1054) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc493)
    %1056 = "vhlo.reduce_v1"(%1047, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1205|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_8aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1205|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_8aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc495)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc494)
    %1057 = "vhlo.broadcast_in_dim_v1"(%1056) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc494)
    %1058 = "vhlo.subtract_v1"(%1047, %1057) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc494)
    %1059 = "vhlo.exponential_v2"(%1058) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc494)
    %1060 = "vhlo.reduce_v1"(%1059, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1205|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_8aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1205|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_8aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc496)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc494)
    %1061 = "vhlo.broadcast_in_dim_v1"(%1060) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc494)
    %1062 = "vhlo.divide_v1"(%1059, %1061) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc494)
    %1063 = "vhlo.select_v1"(%1055, %4, %1062) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc497)
    %1064 = "vhlo.reshape_v1"(%arg286) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1065 = "vhlo.custom_call_v1"(%1064) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_8_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1066 = "vhlo.reshape_v1"(%1065) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1067 = "vhlo.transpose_v1"(%1066) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc498)
    %1068 = "vhlo.dot_general_v2"(%1015, %1067) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc499)
    %1069 = "vhlo.reshape_v1"(%1068) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc500)
    %1070 = "vhlo.reshape_v1"(%arg285) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1071 = "vhlo.custom_call_v1"(%1070) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_8_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1072 = "vhlo.reshape_v1"(%1071) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1073 = "vhlo.broadcast_in_dim_v1"(%1072) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc501)
    %1074 = "vhlo.add_v1"(%1069, %1073) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc501)
    %1075 = "vhlo.reshape_v1"(%1074) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc502)
    %1076 = "vhlo.transpose_v1"(%1075) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc503)
    %1077 = "vhlo.convert_v1"(%1076) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc504)
    %1078 = "vhlo.dot_general_v2"(%1063, %1077) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc505)
    %1079 = "vhlo.convert_v1"(%1078) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc506)
    %1080 = "vhlo.transpose_v1"(%1079) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,257,16,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc507)
    %1081 = "vhlo.reshape_v1"(%1080) : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc508)
    %1082 = "vhlo.reshape_v1"(%arg284) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1083 = "vhlo.custom_call_v1"(%1082) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_8_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1084 = "vhlo.reshape_v1"(%1083) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1085 = "vhlo.transpose_v1"(%1084) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc509)
    %1086 = "vhlo.dot_general_v2"(%1081, %1085) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc510)
    %1087 = "vhlo.reshape_v1"(%1086) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc508)
    %1088 = "vhlo.reshape_v1"(%arg283) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1089 = "vhlo.custom_call_v1"(%1088) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_8_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1090 = "vhlo.reshape_v1"(%1089) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1091 = "vhlo.broadcast_in_dim_v1"(%1090) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc511)
    %1092 = "vhlo.add_v1"(%1087, %1091) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc511)
    %1093 = "vhlo.add_v1"(%1007, %1092) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc512)
    %1094 = "vhlo.reshape_v1"(%arg282) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1095 = "vhlo.custom_call_v1"(%1094) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_8_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1096 = "vhlo.reshape_v1"(%1095) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1097 = "vhlo.reshape_v1"(%arg281) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1098 = "vhlo.custom_call_v1"(%1097) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_8_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1099 = "vhlo.reshape_v1"(%1098) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1100 = "vhlo.composite_v1"(%1093, %1096, %1099) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_27">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc513)
    %1101 = "vhlo.reshape_v1"(%1100) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc514)
    %1102 = "vhlo.reshape_v1"(%arg280) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %1103 = "vhlo.custom_call_v1"(%1102) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_8_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %1104 = "vhlo.reshape_v1"(%1103) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %1105 = "vhlo.transpose_v1"(%1104) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc515)
    %1106 = "vhlo.dot_general_v2"(%1101, %1105) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc516)
    %1107 = "vhlo.reshape_v1"(%1106) : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc514)
    %1108 = "vhlo.reshape_v1"(%arg279) : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc3)
    %1109 = "vhlo.custom_call_v1"(%1108) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_8_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc2)
    %1110 = "vhlo.reshape_v1"(%1109) : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc(#loc3)
    %1111 = "vhlo.broadcast_in_dim_v1"(%1110) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc517)
    %1112 = "vhlo.add_v1"(%1107, %1111) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc517)
    %1113 = "vhlo.composite_v1"(%1112) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_11">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc518)
    %1114 = "vhlo.reshape_v1"(%1113) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc519)
    %1115 = "vhlo.reshape_v1"(%arg278) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %1116 = "vhlo.custom_call_v1"(%1115) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_8_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %1117 = "vhlo.reshape_v1"(%1116) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %1118 = "vhlo.transpose_v1"(%1117) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc520)
    %1119 = "vhlo.dot_general_v2"(%1114, %1118) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc521)
    %1120 = "vhlo.reshape_v1"(%1119) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc519)
    %1121 = "vhlo.reshape_v1"(%arg277) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1122 = "vhlo.custom_call_v1"(%1121) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_8_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1123 = "vhlo.reshape_v1"(%1122) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1124 = "vhlo.broadcast_in_dim_v1"(%1123) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc522)
    %1125 = "vhlo.add_v1"(%1120, %1124) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc522)
    %1126 = "vhlo.add_v1"(%1093, %1125) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc523)
    %1127 = "vhlo.reshape_v1"(%arg276) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1128 = "vhlo.custom_call_v1"(%1127) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_9_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1129 = "vhlo.reshape_v1"(%1128) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1130 = "vhlo.reshape_v1"(%arg275) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1131 = "vhlo.custom_call_v1"(%1130) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_9_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1132 = "vhlo.reshape_v1"(%1131) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1133 = "vhlo.composite_v1"(%1126, %1129, %1132) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_59">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc524)
    %1134 = "vhlo.reshape_v1"(%1133) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc525)
    %1135 = "vhlo.reshape_v1"(%arg431) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1136 = "vhlo.custom_call_v1"(%1135) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_9_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1137 = "vhlo.reshape_v1"(%1136) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1138 = "vhlo.transpose_v1"(%1137) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc526)
    %1139 = "vhlo.dot_general_v2"(%1134, %1138) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc527)
    %1140 = "vhlo.reshape_v1"(%1139) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc525)
    %1141 = "vhlo.reshape_v1"(%arg430) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1142 = "vhlo.custom_call_v1"(%1141) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_9_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1143 = "vhlo.reshape_v1"(%1142) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1144 = "vhlo.broadcast_in_dim_v1"(%1143) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc528)
    %1145 = "vhlo.add_v1"(%1140, %1144) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc528)
    %1146 = "vhlo.reshape_v1"(%1145) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc529)
    %1147 = "vhlo.transpose_v1"(%1146) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc530)
    %1148 = "vhlo.convert_v1"(%1147) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc531)
    %1149 = "vhlo.multiply_v1"(%1148, %7) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc532)
    %1150 = "vhlo.reshape_v1"(%arg429) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1151 = "vhlo.custom_call_v1"(%1150) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_9_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1152 = "vhlo.reshape_v1"(%1151) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1153 = "vhlo.transpose_v1"(%1152) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc533)
    %1154 = "vhlo.dot_general_v2"(%1134, %1153) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc534)
    %1155 = "vhlo.reshape_v1"(%1154) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc535)
    %1156 = "vhlo.reshape_v1"(%arg428) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1157 = "vhlo.custom_call_v1"(%1156) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_9_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1158 = "vhlo.reshape_v1"(%1157) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1159 = "vhlo.broadcast_in_dim_v1"(%1158) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc536)
    %1160 = "vhlo.add_v1"(%1155, %1159) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc536)
    %1161 = "vhlo.reshape_v1"(%1160) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc537)
    %1162 = "vhlo.transpose_v1"(%1161) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc538)
    %1163 = "vhlo.convert_v1"(%1162) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc539)
    %1164 = "vhlo.transpose_v1"(%1163) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,80,257]{2,1,3,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc540)
    %1165 = "vhlo.multiply_v1"(%1164, %6) : (!vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc541)
    %1166 = "vhlo.dot_general_v2"(%1149, %1165) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc542)
    %1167 = "vhlo.convert_v1"(%1166) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc543)
    %1168 = "vhlo.compare_v1"(%1167, %5) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc543)
    %1169 = "vhlo.not_v1"(%1168) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc544)
    %1170 = "vhlo.reduce_v1"(%1169, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("1282|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_10aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("1282|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_10aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc546)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc547)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc545)
    %1171 = "vhlo.reshape_v1"(%1170) : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc545)
    %1172 = "vhlo.not_v1"(%1171) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc548)
    %1173 = "vhlo.reshape_v1"(%1172) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc549)
    %1174 = "vhlo.broadcast_in_dim_v1"(%1173) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc549)
    %1175 = "vhlo.reduce_v1"(%1166, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1279|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_9aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1279|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_9aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc551)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc550)
    %1176 = "vhlo.broadcast_in_dim_v1"(%1175) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc550)
    %1177 = "vhlo.subtract_v1"(%1166, %1176) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc550)
    %1178 = "vhlo.exponential_v2"(%1177) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc550)
    %1179 = "vhlo.reduce_v1"(%1178, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1279|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_9aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1279|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_9aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc552)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc550)
    %1180 = "vhlo.broadcast_in_dim_v1"(%1179) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc550)
    %1181 = "vhlo.divide_v1"(%1178, %1180) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc550)
    %1182 = "vhlo.select_v1"(%1174, %4, %1181) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc553)
    %1183 = "vhlo.reshape_v1"(%arg274) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1184 = "vhlo.custom_call_v1"(%1183) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_9_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1185 = "vhlo.reshape_v1"(%1184) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1186 = "vhlo.transpose_v1"(%1185) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc554)
    %1187 = "vhlo.dot_general_v2"(%1134, %1186) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc555)
    %1188 = "vhlo.reshape_v1"(%1187) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc556)
    %1189 = "vhlo.reshape_v1"(%arg273) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1190 = "vhlo.custom_call_v1"(%1189) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_9_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1191 = "vhlo.reshape_v1"(%1190) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1192 = "vhlo.broadcast_in_dim_v1"(%1191) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc557)
    %1193 = "vhlo.add_v1"(%1188, %1192) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc557)
    %1194 = "vhlo.reshape_v1"(%1193) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc558)
    %1195 = "vhlo.transpose_v1"(%1194) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc559)
    %1196 = "vhlo.convert_v1"(%1195) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc560)
    %1197 = "vhlo.dot_general_v2"(%1182, %1196) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc561)
    %1198 = "vhlo.convert_v1"(%1197) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc562)
    %1199 = "vhlo.transpose_v1"(%1198) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,257,16,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc563)
    %1200 = "vhlo.reshape_v1"(%1199) : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc564)
    %1201 = "vhlo.reshape_v1"(%arg272) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1202 = "vhlo.custom_call_v1"(%1201) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_9_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1203 = "vhlo.reshape_v1"(%1202) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1204 = "vhlo.transpose_v1"(%1203) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc565)
    %1205 = "vhlo.dot_general_v2"(%1200, %1204) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc566)
    %1206 = "vhlo.reshape_v1"(%1205) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc564)
    %1207 = "vhlo.reshape_v1"(%arg271) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1208 = "vhlo.custom_call_v1"(%1207) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_9_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1209 = "vhlo.reshape_v1"(%1208) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1210 = "vhlo.broadcast_in_dim_v1"(%1209) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc567)
    %1211 = "vhlo.add_v1"(%1206, %1210) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc567)
    %1212 = "vhlo.add_v1"(%1126, %1211) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc568)
    %1213 = "vhlo.reshape_v1"(%arg270) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1214 = "vhlo.custom_call_v1"(%1213) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_9_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1215 = "vhlo.reshape_v1"(%1214) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1216 = "vhlo.reshape_v1"(%arg269) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1217 = "vhlo.custom_call_v1"(%1216) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_9_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1218 = "vhlo.reshape_v1"(%1217) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1219 = "vhlo.composite_v1"(%1212, %1215, %1218) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_45">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc569)
    %1220 = "vhlo.reshape_v1"(%1219) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc570)
    %1221 = "vhlo.reshape_v1"(%arg268) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %1222 = "vhlo.custom_call_v1"(%1221) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_9_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %1223 = "vhlo.reshape_v1"(%1222) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %1224 = "vhlo.transpose_v1"(%1223) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc571)
    %1225 = "vhlo.dot_general_v2"(%1220, %1224) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc572)
    %1226 = "vhlo.reshape_v1"(%1225) : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc570)
    %1227 = "vhlo.reshape_v1"(%arg267) : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc3)
    %1228 = "vhlo.custom_call_v1"(%1227) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_9_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc2)
    %1229 = "vhlo.reshape_v1"(%1228) : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc(#loc3)
    %1230 = "vhlo.broadcast_in_dim_v1"(%1229) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc573)
    %1231 = "vhlo.add_v1"(%1226, %1230) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc573)
    %1232 = "vhlo.composite_v1"(%1231) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_22">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc574)
    %1233 = "vhlo.reshape_v1"(%1232) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc575)
    %1234 = "vhlo.reshape_v1"(%arg266) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %1235 = "vhlo.custom_call_v1"(%1234) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_9_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %1236 = "vhlo.reshape_v1"(%1235) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %1237 = "vhlo.transpose_v1"(%1236) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc576)
    %1238 = "vhlo.dot_general_v2"(%1233, %1237) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc577)
    %1239 = "vhlo.reshape_v1"(%1238) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc575)
    %1240 = "vhlo.reshape_v1"(%arg265) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1241 = "vhlo.custom_call_v1"(%1240) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_9_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1242 = "vhlo.reshape_v1"(%1241) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1243 = "vhlo.broadcast_in_dim_v1"(%1242) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc578)
    %1244 = "vhlo.add_v1"(%1239, %1243) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc578)
    %1245 = "vhlo.add_v1"(%1212, %1244) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc579)
    %1246 = "vhlo.reshape_v1"(%arg264) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1247 = "vhlo.custom_call_v1"(%1246) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_10_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1248 = "vhlo.reshape_v1"(%1247) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1249 = "vhlo.reshape_v1"(%arg263) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1250 = "vhlo.custom_call_v1"(%1249) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_10_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1251 = "vhlo.reshape_v1"(%1250) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1252 = "vhlo.composite_v1"(%1245, %1248, %1251) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_21">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc580)
    %1253 = "vhlo.reshape_v1"(%1252) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc581)
    %1254 = "vhlo.reshape_v1"(%arg435) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1255 = "vhlo.custom_call_v1"(%1254) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_10_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1256 = "vhlo.reshape_v1"(%1255) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1257 = "vhlo.transpose_v1"(%1256) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc582)
    %1258 = "vhlo.dot_general_v2"(%1253, %1257) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc583)
    %1259 = "vhlo.reshape_v1"(%1258) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc581)
    %1260 = "vhlo.reshape_v1"(%arg434) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1261 = "vhlo.custom_call_v1"(%1260) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_10_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1262 = "vhlo.reshape_v1"(%1261) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1263 = "vhlo.broadcast_in_dim_v1"(%1262) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc584)
    %1264 = "vhlo.add_v1"(%1259, %1263) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc584)
    %1265 = "vhlo.reshape_v1"(%1264) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc585)
    %1266 = "vhlo.transpose_v1"(%1265) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc586)
    %1267 = "vhlo.convert_v1"(%1266) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc587)
    %1268 = "vhlo.multiply_v1"(%1267, %7) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc588)
    %1269 = "vhlo.reshape_v1"(%arg433) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1270 = "vhlo.custom_call_v1"(%1269) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_10_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1271 = "vhlo.reshape_v1"(%1270) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1272 = "vhlo.transpose_v1"(%1271) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc589)
    %1273 = "vhlo.dot_general_v2"(%1253, %1272) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc590)
    %1274 = "vhlo.reshape_v1"(%1273) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc591)
    %1275 = "vhlo.reshape_v1"(%arg432) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1276 = "vhlo.custom_call_v1"(%1275) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_10_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1277 = "vhlo.reshape_v1"(%1276) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1278 = "vhlo.broadcast_in_dim_v1"(%1277) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc592)
    %1279 = "vhlo.add_v1"(%1274, %1278) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc592)
    %1280 = "vhlo.reshape_v1"(%1279) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc593)
    %1281 = "vhlo.transpose_v1"(%1280) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc594)
    %1282 = "vhlo.convert_v1"(%1281) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc595)
    %1283 = "vhlo.transpose_v1"(%1282) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,80,257]{2,1,3,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc596)
    %1284 = "vhlo.multiply_v1"(%1283, %6) : (!vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc597)
    %1285 = "vhlo.dot_general_v2"(%1268, %1284) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc598)
    %1286 = "vhlo.convert_v1"(%1285) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc599)
    %1287 = "vhlo.compare_v1"(%1286, %5) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc599)
    %1288 = "vhlo.not_v1"(%1287) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc600)
    %1289 = "vhlo.reduce_v1"(%1288, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("1356|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_11aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("1356|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_11aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc602)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc603)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc601)
    %1290 = "vhlo.reshape_v1"(%1289) : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc601)
    %1291 = "vhlo.not_v1"(%1290) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc604)
    %1292 = "vhlo.reshape_v1"(%1291) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc605)
    %1293 = "vhlo.broadcast_in_dim_v1"(%1292) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc605)
    %1294 = "vhlo.reduce_v1"(%1285, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1353|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_10aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1353|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_10aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc607)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc606)
    %1295 = "vhlo.broadcast_in_dim_v1"(%1294) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc606)
    %1296 = "vhlo.subtract_v1"(%1285, %1295) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc606)
    %1297 = "vhlo.exponential_v2"(%1296) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc606)
    %1298 = "vhlo.reduce_v1"(%1297, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1353|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_10aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1353|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_10aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc608)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc606)
    %1299 = "vhlo.broadcast_in_dim_v1"(%1298) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc606)
    %1300 = "vhlo.divide_v1"(%1297, %1299) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc606)
    %1301 = "vhlo.select_v1"(%1293, %4, %1300) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc609)
    %1302 = "vhlo.reshape_v1"(%arg262) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1303 = "vhlo.custom_call_v1"(%1302) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_10_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1304 = "vhlo.reshape_v1"(%1303) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1305 = "vhlo.transpose_v1"(%1304) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc610)
    %1306 = "vhlo.dot_general_v2"(%1253, %1305) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc611)
    %1307 = "vhlo.reshape_v1"(%1306) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc612)
    %1308 = "vhlo.reshape_v1"(%arg261) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1309 = "vhlo.custom_call_v1"(%1308) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_10_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1310 = "vhlo.reshape_v1"(%1309) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1311 = "vhlo.broadcast_in_dim_v1"(%1310) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc613)
    %1312 = "vhlo.add_v1"(%1307, %1311) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc613)
    %1313 = "vhlo.reshape_v1"(%1312) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc614)
    %1314 = "vhlo.transpose_v1"(%1313) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc615)
    %1315 = "vhlo.convert_v1"(%1314) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc616)
    %1316 = "vhlo.dot_general_v2"(%1301, %1315) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc617)
    %1317 = "vhlo.convert_v1"(%1316) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc618)
    %1318 = "vhlo.transpose_v1"(%1317) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,257,16,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc619)
    %1319 = "vhlo.reshape_v1"(%1318) : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc620)
    %1320 = "vhlo.reshape_v1"(%arg260) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1321 = "vhlo.custom_call_v1"(%1320) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_10_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1322 = "vhlo.reshape_v1"(%1321) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1323 = "vhlo.transpose_v1"(%1322) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc621)
    %1324 = "vhlo.dot_general_v2"(%1319, %1323) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc622)
    %1325 = "vhlo.reshape_v1"(%1324) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc620)
    %1326 = "vhlo.reshape_v1"(%arg259) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1327 = "vhlo.custom_call_v1"(%1326) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_10_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1328 = "vhlo.reshape_v1"(%1327) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1329 = "vhlo.broadcast_in_dim_v1"(%1328) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc623)
    %1330 = "vhlo.add_v1"(%1325, %1329) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc623)
    %1331 = "vhlo.add_v1"(%1245, %1330) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc624)
    %1332 = "vhlo.reshape_v1"(%arg258) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1333 = "vhlo.custom_call_v1"(%1332) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_10_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1334 = "vhlo.reshape_v1"(%1333) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1335 = "vhlo.reshape_v1"(%arg257) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1336 = "vhlo.custom_call_v1"(%1335) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_10_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1337 = "vhlo.reshape_v1"(%1336) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1338 = "vhlo.composite_v1"(%1331, %1334, %1337) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_53">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc625)
    %1339 = "vhlo.reshape_v1"(%1338) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc626)
    %1340 = "vhlo.reshape_v1"(%arg256) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %1341 = "vhlo.custom_call_v1"(%1340) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_10_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %1342 = "vhlo.reshape_v1"(%1341) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %1343 = "vhlo.transpose_v1"(%1342) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc627)
    %1344 = "vhlo.dot_general_v2"(%1339, %1343) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc628)
    %1345 = "vhlo.reshape_v1"(%1344) : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc626)
    %1346 = "vhlo.reshape_v1"(%arg255) : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc3)
    %1347 = "vhlo.custom_call_v1"(%1346) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_10_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc2)
    %1348 = "vhlo.reshape_v1"(%1347) : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc(#loc3)
    %1349 = "vhlo.broadcast_in_dim_v1"(%1348) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc629)
    %1350 = "vhlo.add_v1"(%1345, %1349) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc629)
    %1351 = "vhlo.composite_v1"(%1350) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_25">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc630)
    %1352 = "vhlo.reshape_v1"(%1351) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc631)
    %1353 = "vhlo.reshape_v1"(%arg254) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %1354 = "vhlo.custom_call_v1"(%1353) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_10_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %1355 = "vhlo.reshape_v1"(%1354) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %1356 = "vhlo.transpose_v1"(%1355) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc632)
    %1357 = "vhlo.dot_general_v2"(%1352, %1356) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc633)
    %1358 = "vhlo.reshape_v1"(%1357) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc631)
    %1359 = "vhlo.reshape_v1"(%arg253) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1360 = "vhlo.custom_call_v1"(%1359) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_10_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1361 = "vhlo.reshape_v1"(%1360) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1362 = "vhlo.broadcast_in_dim_v1"(%1361) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc634)
    %1363 = "vhlo.add_v1"(%1358, %1362) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc634)
    %1364 = "vhlo.add_v1"(%1331, %1363) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc635)
    %1365 = "vhlo.reshape_v1"(%arg252) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1366 = "vhlo.custom_call_v1"(%1365) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_11_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1367 = "vhlo.reshape_v1"(%1366) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1368 = "vhlo.reshape_v1"(%arg251) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1369 = "vhlo.custom_call_v1"(%1368) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_11_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1370 = "vhlo.reshape_v1"(%1369) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1371 = "vhlo.composite_v1"(%1364, %1367, %1370) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_56">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc636)
    %1372 = "vhlo.reshape_v1"(%1371) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc637)
    %1373 = "vhlo.reshape_v1"(%arg439) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1374 = "vhlo.custom_call_v1"(%1373) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_11_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1375 = "vhlo.reshape_v1"(%1374) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1376 = "vhlo.transpose_v1"(%1375) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc638)
    %1377 = "vhlo.dot_general_v2"(%1372, %1376) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc639)
    %1378 = "vhlo.reshape_v1"(%1377) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc637)
    %1379 = "vhlo.reshape_v1"(%arg438) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1380 = "vhlo.custom_call_v1"(%1379) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_11_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1381 = "vhlo.reshape_v1"(%1380) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1382 = "vhlo.broadcast_in_dim_v1"(%1381) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc640)
    %1383 = "vhlo.add_v1"(%1378, %1382) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc640)
    %1384 = "vhlo.reshape_v1"(%1383) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc641)
    %1385 = "vhlo.transpose_v1"(%1384) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc642)
    %1386 = "vhlo.convert_v1"(%1385) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc643)
    %1387 = "vhlo.multiply_v1"(%1386, %7) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc644)
    %1388 = "vhlo.reshape_v1"(%arg437) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1389 = "vhlo.custom_call_v1"(%1388) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_11_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1390 = "vhlo.reshape_v1"(%1389) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1391 = "vhlo.transpose_v1"(%1390) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc645)
    %1392 = "vhlo.dot_general_v2"(%1372, %1391) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc646)
    %1393 = "vhlo.reshape_v1"(%1392) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc647)
    %1394 = "vhlo.reshape_v1"(%arg436) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1395 = "vhlo.custom_call_v1"(%1394) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_11_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1396 = "vhlo.reshape_v1"(%1395) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1397 = "vhlo.broadcast_in_dim_v1"(%1396) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc648)
    %1398 = "vhlo.add_v1"(%1393, %1397) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc648)
    %1399 = "vhlo.reshape_v1"(%1398) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc649)
    %1400 = "vhlo.transpose_v1"(%1399) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc650)
    %1401 = "vhlo.convert_v1"(%1400) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc651)
    %1402 = "vhlo.transpose_v1"(%1401) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,80,257]{2,1,3,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc652)
    %1403 = "vhlo.multiply_v1"(%1402, %6) : (!vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc653)
    %1404 = "vhlo.dot_general_v2"(%1387, %1403) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc654)
    %1405 = "vhlo.convert_v1"(%1404) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc655)
    %1406 = "vhlo.compare_v1"(%1405, %5) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc655)
    %1407 = "vhlo.not_v1"(%1406) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc656)
    %1408 = "vhlo.reduce_v1"(%1407, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("1430|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_12aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("1430|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_12aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc658)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc659)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc657)
    %1409 = "vhlo.reshape_v1"(%1408) : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc657)
    %1410 = "vhlo.not_v1"(%1409) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc660)
    %1411 = "vhlo.reshape_v1"(%1410) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc661)
    %1412 = "vhlo.broadcast_in_dim_v1"(%1411) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc661)
    %1413 = "vhlo.reduce_v1"(%1404, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1427|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_11aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1427|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_11aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc663)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc662)
    %1414 = "vhlo.broadcast_in_dim_v1"(%1413) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc662)
    %1415 = "vhlo.subtract_v1"(%1404, %1414) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc662)
    %1416 = "vhlo.exponential_v2"(%1415) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc662)
    %1417 = "vhlo.reduce_v1"(%1416, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1427|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_11aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1427|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_11aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc664)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc662)
    %1418 = "vhlo.broadcast_in_dim_v1"(%1417) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc662)
    %1419 = "vhlo.divide_v1"(%1416, %1418) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc662)
    %1420 = "vhlo.select_v1"(%1412, %4, %1419) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc665)
    %1421 = "vhlo.reshape_v1"(%arg250) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1422 = "vhlo.custom_call_v1"(%1421) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_11_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1423 = "vhlo.reshape_v1"(%1422) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1424 = "vhlo.transpose_v1"(%1423) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc666)
    %1425 = "vhlo.dot_general_v2"(%1372, %1424) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc667)
    %1426 = "vhlo.reshape_v1"(%1425) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc668)
    %1427 = "vhlo.reshape_v1"(%arg249) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1428 = "vhlo.custom_call_v1"(%1427) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_11_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1429 = "vhlo.reshape_v1"(%1428) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1430 = "vhlo.broadcast_in_dim_v1"(%1429) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc669)
    %1431 = "vhlo.add_v1"(%1426, %1430) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc669)
    %1432 = "vhlo.reshape_v1"(%1431) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc670)
    %1433 = "vhlo.transpose_v1"(%1432) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc671)
    %1434 = "vhlo.convert_v1"(%1433) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc672)
    %1435 = "vhlo.dot_general_v2"(%1420, %1434) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc673)
    %1436 = "vhlo.convert_v1"(%1435) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc674)
    %1437 = "vhlo.transpose_v1"(%1436) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,257,16,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc675)
    %1438 = "vhlo.reshape_v1"(%1437) : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc676)
    %1439 = "vhlo.reshape_v1"(%arg248) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1440 = "vhlo.custom_call_v1"(%1439) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_11_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1441 = "vhlo.reshape_v1"(%1440) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1442 = "vhlo.transpose_v1"(%1441) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc677)
    %1443 = "vhlo.dot_general_v2"(%1438, %1442) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc678)
    %1444 = "vhlo.reshape_v1"(%1443) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc676)
    %1445 = "vhlo.reshape_v1"(%arg247) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1446 = "vhlo.custom_call_v1"(%1445) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_11_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1447 = "vhlo.reshape_v1"(%1446) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1448 = "vhlo.broadcast_in_dim_v1"(%1447) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc679)
    %1449 = "vhlo.add_v1"(%1444, %1448) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc679)
    %1450 = "vhlo.add_v1"(%1364, %1449) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc680)
    %1451 = "vhlo.reshape_v1"(%arg246) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1452 = "vhlo.custom_call_v1"(%1451) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_11_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1453 = "vhlo.reshape_v1"(%1452) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1454 = "vhlo.reshape_v1"(%arg245) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1455 = "vhlo.custom_call_v1"(%1454) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_11_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1456 = "vhlo.reshape_v1"(%1455) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1457 = "vhlo.composite_v1"(%1450, %1453, %1456) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_57">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc681)
    %1458 = "vhlo.reshape_v1"(%1457) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc682)
    %1459 = "vhlo.reshape_v1"(%arg244) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %1460 = "vhlo.custom_call_v1"(%1459) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_11_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %1461 = "vhlo.reshape_v1"(%1460) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %1462 = "vhlo.transpose_v1"(%1461) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc683)
    %1463 = "vhlo.dot_general_v2"(%1458, %1462) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc684)
    %1464 = "vhlo.reshape_v1"(%1463) : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc682)
    %1465 = "vhlo.reshape_v1"(%arg243) : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc3)
    %1466 = "vhlo.custom_call_v1"(%1465) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_11_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc2)
    %1467 = "vhlo.reshape_v1"(%1466) : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc(#loc3)
    %1468 = "vhlo.broadcast_in_dim_v1"(%1467) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc685)
    %1469 = "vhlo.add_v1"(%1464, %1468) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc685)
    %1470 = "vhlo.composite_v1"(%1469) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_30">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc686)
    %1471 = "vhlo.reshape_v1"(%1470) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc687)
    %1472 = "vhlo.reshape_v1"(%arg242) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %1473 = "vhlo.custom_call_v1"(%1472) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_11_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %1474 = "vhlo.reshape_v1"(%1473) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %1475 = "vhlo.transpose_v1"(%1474) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc688)
    %1476 = "vhlo.dot_general_v2"(%1471, %1475) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc689)
    %1477 = "vhlo.reshape_v1"(%1476) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc687)
    %1478 = "vhlo.reshape_v1"(%arg241) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1479 = "vhlo.custom_call_v1"(%1478) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_11_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1480 = "vhlo.reshape_v1"(%1479) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1481 = "vhlo.broadcast_in_dim_v1"(%1480) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc690)
    %1482 = "vhlo.add_v1"(%1477, %1481) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc690)
    %1483 = "vhlo.add_v1"(%1450, %1482) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc691)
    %1484 = "vhlo.reshape_v1"(%arg240) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1485 = "vhlo.custom_call_v1"(%1484) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_12_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1486 = "vhlo.reshape_v1"(%1485) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1487 = "vhlo.reshape_v1"(%arg239) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1488 = "vhlo.custom_call_v1"(%1487) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_12_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1489 = "vhlo.reshape_v1"(%1488) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1490 = "vhlo.composite_v1"(%1483, %1486, %1489) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_69">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc692)
    %1491 = "vhlo.reshape_v1"(%1490) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc693)
    %1492 = "vhlo.reshape_v1"(%arg443) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1493 = "vhlo.custom_call_v1"(%1492) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_12_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1494 = "vhlo.reshape_v1"(%1493) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1495 = "vhlo.transpose_v1"(%1494) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc694)
    %1496 = "vhlo.dot_general_v2"(%1491, %1495) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc695)
    %1497 = "vhlo.reshape_v1"(%1496) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc693)
    %1498 = "vhlo.reshape_v1"(%arg442) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1499 = "vhlo.custom_call_v1"(%1498) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_12_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1500 = "vhlo.reshape_v1"(%1499) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1501 = "vhlo.broadcast_in_dim_v1"(%1500) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc696)
    %1502 = "vhlo.add_v1"(%1497, %1501) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc696)
    %1503 = "vhlo.reshape_v1"(%1502) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc697)
    %1504 = "vhlo.transpose_v1"(%1503) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc698)
    %1505 = "vhlo.convert_v1"(%1504) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc699)
    %1506 = "vhlo.multiply_v1"(%1505, %7) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc700)
    %1507 = "vhlo.reshape_v1"(%arg441) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1508 = "vhlo.custom_call_v1"(%1507) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_12_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1509 = "vhlo.reshape_v1"(%1508) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1510 = "vhlo.transpose_v1"(%1509) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc701)
    %1511 = "vhlo.dot_general_v2"(%1491, %1510) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc702)
    %1512 = "vhlo.reshape_v1"(%1511) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc703)
    %1513 = "vhlo.reshape_v1"(%arg440) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1514 = "vhlo.custom_call_v1"(%1513) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_12_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1515 = "vhlo.reshape_v1"(%1514) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1516 = "vhlo.broadcast_in_dim_v1"(%1515) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc704)
    %1517 = "vhlo.add_v1"(%1512, %1516) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc704)
    %1518 = "vhlo.reshape_v1"(%1517) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc705)
    %1519 = "vhlo.transpose_v1"(%1518) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc706)
    %1520 = "vhlo.convert_v1"(%1519) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc707)
    %1521 = "vhlo.transpose_v1"(%1520) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,80,257]{2,1,3,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc708)
    %1522 = "vhlo.multiply_v1"(%1521, %6) : (!vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc709)
    %1523 = "vhlo.dot_general_v2"(%1506, %1522) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc710)
    %1524 = "vhlo.convert_v1"(%1523) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc711)
    %1525 = "vhlo.compare_v1"(%1524, %5) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc711)
    %1526 = "vhlo.not_v1"(%1525) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc712)
    %1527 = "vhlo.reduce_v1"(%1526, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("1504|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_13aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("1504|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_13aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc714)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc715)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc713)
    %1528 = "vhlo.reshape_v1"(%1527) : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc713)
    %1529 = "vhlo.not_v1"(%1528) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc716)
    %1530 = "vhlo.reshape_v1"(%1529) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc717)
    %1531 = "vhlo.broadcast_in_dim_v1"(%1530) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc717)
    %1532 = "vhlo.reduce_v1"(%1523, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1501|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_12aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1501|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_12aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc719)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc718)
    %1533 = "vhlo.broadcast_in_dim_v1"(%1532) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc718)
    %1534 = "vhlo.subtract_v1"(%1523, %1533) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc718)
    %1535 = "vhlo.exponential_v2"(%1534) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc718)
    %1536 = "vhlo.reduce_v1"(%1535, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1501|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_12aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1501|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_12aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc720)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc718)
    %1537 = "vhlo.broadcast_in_dim_v1"(%1536) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc718)
    %1538 = "vhlo.divide_v1"(%1535, %1537) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc718)
    %1539 = "vhlo.select_v1"(%1531, %4, %1538) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc721)
    %1540 = "vhlo.reshape_v1"(%arg238) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1541 = "vhlo.custom_call_v1"(%1540) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_12_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1542 = "vhlo.reshape_v1"(%1541) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1543 = "vhlo.transpose_v1"(%1542) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc722)
    %1544 = "vhlo.dot_general_v2"(%1491, %1543) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc723)
    %1545 = "vhlo.reshape_v1"(%1544) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc724)
    %1546 = "vhlo.reshape_v1"(%arg237) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1547 = "vhlo.custom_call_v1"(%1546) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_12_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1548 = "vhlo.reshape_v1"(%1547) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1549 = "vhlo.broadcast_in_dim_v1"(%1548) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc725)
    %1550 = "vhlo.add_v1"(%1545, %1549) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc725)
    %1551 = "vhlo.reshape_v1"(%1550) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc726)
    %1552 = "vhlo.transpose_v1"(%1551) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc727)
    %1553 = "vhlo.convert_v1"(%1552) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc728)
    %1554 = "vhlo.dot_general_v2"(%1539, %1553) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc729)
    %1555 = "vhlo.convert_v1"(%1554) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc730)
    %1556 = "vhlo.transpose_v1"(%1555) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,257,16,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc731)
    %1557 = "vhlo.reshape_v1"(%1556) : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc732)
    %1558 = "vhlo.reshape_v1"(%arg236) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1559 = "vhlo.custom_call_v1"(%1558) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_12_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1560 = "vhlo.reshape_v1"(%1559) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1561 = "vhlo.transpose_v1"(%1560) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc733)
    %1562 = "vhlo.dot_general_v2"(%1557, %1561) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc734)
    %1563 = "vhlo.reshape_v1"(%1562) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc732)
    %1564 = "vhlo.reshape_v1"(%arg235) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1565 = "vhlo.custom_call_v1"(%1564) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_12_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1566 = "vhlo.reshape_v1"(%1565) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1567 = "vhlo.broadcast_in_dim_v1"(%1566) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc735)
    %1568 = "vhlo.add_v1"(%1563, %1567) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc735)
    %1569 = "vhlo.add_v1"(%1483, %1568) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc736)
    %1570 = "vhlo.reshape_v1"(%arg234) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1571 = "vhlo.custom_call_v1"(%1570) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_12_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1572 = "vhlo.reshape_v1"(%1571) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1573 = "vhlo.reshape_v1"(%arg233) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1574 = "vhlo.custom_call_v1"(%1573) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_12_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1575 = "vhlo.reshape_v1"(%1574) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1576 = "vhlo.composite_v1"(%1569, %1572, %1575) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_60">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc737)
    %1577 = "vhlo.reshape_v1"(%1576) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc738)
    %1578 = "vhlo.reshape_v1"(%arg232) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %1579 = "vhlo.custom_call_v1"(%1578) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_12_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %1580 = "vhlo.reshape_v1"(%1579) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %1581 = "vhlo.transpose_v1"(%1580) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc739)
    %1582 = "vhlo.dot_general_v2"(%1577, %1581) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc740)
    %1583 = "vhlo.reshape_v1"(%1582) : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc738)
    %1584 = "vhlo.reshape_v1"(%arg231) : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc3)
    %1585 = "vhlo.custom_call_v1"(%1584) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_12_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc2)
    %1586 = "vhlo.reshape_v1"(%1585) : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc(#loc3)
    %1587 = "vhlo.broadcast_in_dim_v1"(%1586) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc741)
    %1588 = "vhlo.add_v1"(%1583, %1587) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc741)
    %1589 = "vhlo.composite_v1"(%1588) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_18">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc742)
    %1590 = "vhlo.reshape_v1"(%1589) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc743)
    %1591 = "vhlo.reshape_v1"(%arg230) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %1592 = "vhlo.custom_call_v1"(%1591) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_12_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %1593 = "vhlo.reshape_v1"(%1592) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %1594 = "vhlo.transpose_v1"(%1593) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc744)
    %1595 = "vhlo.dot_general_v2"(%1590, %1594) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc745)
    %1596 = "vhlo.reshape_v1"(%1595) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc743)
    %1597 = "vhlo.reshape_v1"(%arg229) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1598 = "vhlo.custom_call_v1"(%1597) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_12_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1599 = "vhlo.reshape_v1"(%1598) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1600 = "vhlo.broadcast_in_dim_v1"(%1599) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc746)
    %1601 = "vhlo.add_v1"(%1596, %1600) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc746)
    %1602 = "vhlo.add_v1"(%1569, %1601) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc747)
    %1603 = "vhlo.reshape_v1"(%arg228) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1604 = "vhlo.custom_call_v1"(%1603) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_13_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1605 = "vhlo.reshape_v1"(%1604) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1606 = "vhlo.reshape_v1"(%arg227) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1607 = "vhlo.custom_call_v1"(%1606) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_13_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1608 = "vhlo.reshape_v1"(%1607) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1609 = "vhlo.composite_v1"(%1602, %1605, %1608) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_73">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc748)
    %1610 = "vhlo.reshape_v1"(%1609) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc749)
    %1611 = "vhlo.reshape_v1"(%arg447) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1612 = "vhlo.custom_call_v1"(%1611) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_13_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1613 = "vhlo.reshape_v1"(%1612) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1614 = "vhlo.transpose_v1"(%1613) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc750)
    %1615 = "vhlo.dot_general_v2"(%1610, %1614) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc751)
    %1616 = "vhlo.reshape_v1"(%1615) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc749)
    %1617 = "vhlo.reshape_v1"(%arg446) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1618 = "vhlo.custom_call_v1"(%1617) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_13_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1619 = "vhlo.reshape_v1"(%1618) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1620 = "vhlo.broadcast_in_dim_v1"(%1619) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc752)
    %1621 = "vhlo.add_v1"(%1616, %1620) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc752)
    %1622 = "vhlo.reshape_v1"(%1621) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc753)
    %1623 = "vhlo.transpose_v1"(%1622) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc754)
    %1624 = "vhlo.convert_v1"(%1623) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc755)
    %1625 = "vhlo.multiply_v1"(%1624, %7) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc756)
    %1626 = "vhlo.reshape_v1"(%arg445) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1627 = "vhlo.custom_call_v1"(%1626) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_13_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1628 = "vhlo.reshape_v1"(%1627) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1629 = "vhlo.transpose_v1"(%1628) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc757)
    %1630 = "vhlo.dot_general_v2"(%1610, %1629) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc758)
    %1631 = "vhlo.reshape_v1"(%1630) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc759)
    %1632 = "vhlo.reshape_v1"(%arg444) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1633 = "vhlo.custom_call_v1"(%1632) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_13_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1634 = "vhlo.reshape_v1"(%1633) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1635 = "vhlo.broadcast_in_dim_v1"(%1634) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc760)
    %1636 = "vhlo.add_v1"(%1631, %1635) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc760)
    %1637 = "vhlo.reshape_v1"(%1636) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc761)
    %1638 = "vhlo.transpose_v1"(%1637) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc762)
    %1639 = "vhlo.convert_v1"(%1638) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc763)
    %1640 = "vhlo.transpose_v1"(%1639) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,80,257]{2,1,3,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc764)
    %1641 = "vhlo.multiply_v1"(%1640, %6) : (!vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc765)
    %1642 = "vhlo.dot_general_v2"(%1625, %1641) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc766)
    %1643 = "vhlo.convert_v1"(%1642) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc767)
    %1644 = "vhlo.compare_v1"(%1643, %5) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc767)
    %1645 = "vhlo.not_v1"(%1644) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc768)
    %1646 = "vhlo.reduce_v1"(%1645, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("1578|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_14aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("1578|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_14aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc770)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc771)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc769)
    %1647 = "vhlo.reshape_v1"(%1646) : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc769)
    %1648 = "vhlo.not_v1"(%1647) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc772)
    %1649 = "vhlo.reshape_v1"(%1648) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc773)
    %1650 = "vhlo.broadcast_in_dim_v1"(%1649) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc773)
    %1651 = "vhlo.reduce_v1"(%1642, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1575|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_13aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1575|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_13aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc775)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc774)
    %1652 = "vhlo.broadcast_in_dim_v1"(%1651) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc774)
    %1653 = "vhlo.subtract_v1"(%1642, %1652) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc774)
    %1654 = "vhlo.exponential_v2"(%1653) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc774)
    %1655 = "vhlo.reduce_v1"(%1654, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1575|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_13aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1575|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_13aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc776)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc774)
    %1656 = "vhlo.broadcast_in_dim_v1"(%1655) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc774)
    %1657 = "vhlo.divide_v1"(%1654, %1656) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc774)
    %1658 = "vhlo.select_v1"(%1650, %4, %1657) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc777)
    %1659 = "vhlo.reshape_v1"(%arg226) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1660 = "vhlo.custom_call_v1"(%1659) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_13_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1661 = "vhlo.reshape_v1"(%1660) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1662 = "vhlo.transpose_v1"(%1661) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc778)
    %1663 = "vhlo.dot_general_v2"(%1610, %1662) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc779)
    %1664 = "vhlo.reshape_v1"(%1663) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc780)
    %1665 = "vhlo.reshape_v1"(%arg225) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1666 = "vhlo.custom_call_v1"(%1665) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_13_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1667 = "vhlo.reshape_v1"(%1666) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1668 = "vhlo.broadcast_in_dim_v1"(%1667) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc781)
    %1669 = "vhlo.add_v1"(%1664, %1668) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc781)
    %1670 = "vhlo.reshape_v1"(%1669) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc782)
    %1671 = "vhlo.transpose_v1"(%1670) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc783)
    %1672 = "vhlo.convert_v1"(%1671) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc784)
    %1673 = "vhlo.dot_general_v2"(%1658, %1672) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc785)
    %1674 = "vhlo.convert_v1"(%1673) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc786)
    %1675 = "vhlo.transpose_v1"(%1674) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,257,16,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc787)
    %1676 = "vhlo.reshape_v1"(%1675) : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc788)
    %1677 = "vhlo.reshape_v1"(%arg224) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1678 = "vhlo.custom_call_v1"(%1677) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_13_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1679 = "vhlo.reshape_v1"(%1678) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1680 = "vhlo.transpose_v1"(%1679) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc789)
    %1681 = "vhlo.dot_general_v2"(%1676, %1680) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc790)
    %1682 = "vhlo.reshape_v1"(%1681) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc788)
    %1683 = "vhlo.reshape_v1"(%arg223) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1684 = "vhlo.custom_call_v1"(%1683) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_13_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1685 = "vhlo.reshape_v1"(%1684) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1686 = "vhlo.broadcast_in_dim_v1"(%1685) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc791)
    %1687 = "vhlo.add_v1"(%1682, %1686) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc791)
    %1688 = "vhlo.add_v1"(%1602, %1687) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc792)
    %1689 = "vhlo.reshape_v1"(%arg222) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1690 = "vhlo.custom_call_v1"(%1689) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_13_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1691 = "vhlo.reshape_v1"(%1690) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1692 = "vhlo.reshape_v1"(%arg221) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1693 = "vhlo.custom_call_v1"(%1692) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_13_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1694 = "vhlo.reshape_v1"(%1693) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1695 = "vhlo.composite_v1"(%1688, %1691, %1694) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_62">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc793)
    %1696 = "vhlo.reshape_v1"(%1695) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc794)
    %1697 = "vhlo.reshape_v1"(%arg220) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %1698 = "vhlo.custom_call_v1"(%1697) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_13_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %1699 = "vhlo.reshape_v1"(%1698) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %1700 = "vhlo.transpose_v1"(%1699) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc795)
    %1701 = "vhlo.dot_general_v2"(%1696, %1700) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc796)
    %1702 = "vhlo.reshape_v1"(%1701) : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc794)
    %1703 = "vhlo.reshape_v1"(%arg219) : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc3)
    %1704 = "vhlo.custom_call_v1"(%1703) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_13_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc2)
    %1705 = "vhlo.reshape_v1"(%1704) : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc(#loc3)
    %1706 = "vhlo.broadcast_in_dim_v1"(%1705) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc797)
    %1707 = "vhlo.add_v1"(%1702, %1706) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc797)
    %1708 = "vhlo.composite_v1"(%1707) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_28">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc798)
    %1709 = "vhlo.reshape_v1"(%1708) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc799)
    %1710 = "vhlo.reshape_v1"(%arg218) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %1711 = "vhlo.custom_call_v1"(%1710) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_13_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %1712 = "vhlo.reshape_v1"(%1711) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %1713 = "vhlo.transpose_v1"(%1712) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc800)
    %1714 = "vhlo.dot_general_v2"(%1709, %1713) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc801)
    %1715 = "vhlo.reshape_v1"(%1714) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc799)
    %1716 = "vhlo.reshape_v1"(%arg217) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1717 = "vhlo.custom_call_v1"(%1716) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_13_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1718 = "vhlo.reshape_v1"(%1717) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1719 = "vhlo.broadcast_in_dim_v1"(%1718) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc802)
    %1720 = "vhlo.add_v1"(%1715, %1719) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc802)
    %1721 = "vhlo.add_v1"(%1688, %1720) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc803)
    %1722 = "vhlo.reshape_v1"(%arg216) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1723 = "vhlo.custom_call_v1"(%1722) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_14_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1724 = "vhlo.reshape_v1"(%1723) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1725 = "vhlo.reshape_v1"(%arg215) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1726 = "vhlo.custom_call_v1"(%1725) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_14_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1727 = "vhlo.reshape_v1"(%1726) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1728 = "vhlo.composite_v1"(%1721, %1724, %1727) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_66">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc804)
    %1729 = "vhlo.reshape_v1"(%1728) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc805)
    %1730 = "vhlo.reshape_v1"(%arg451) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1731 = "vhlo.custom_call_v1"(%1730) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_14_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1732 = "vhlo.reshape_v1"(%1731) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1733 = "vhlo.transpose_v1"(%1732) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc806)
    %1734 = "vhlo.dot_general_v2"(%1729, %1733) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc807)
    %1735 = "vhlo.reshape_v1"(%1734) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc805)
    %1736 = "vhlo.reshape_v1"(%arg450) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1737 = "vhlo.custom_call_v1"(%1736) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_14_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1738 = "vhlo.reshape_v1"(%1737) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1739 = "vhlo.broadcast_in_dim_v1"(%1738) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc808)
    %1740 = "vhlo.add_v1"(%1735, %1739) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc808)
    %1741 = "vhlo.reshape_v1"(%1740) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc809)
    %1742 = "vhlo.transpose_v1"(%1741) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc810)
    %1743 = "vhlo.convert_v1"(%1742) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc811)
    %1744 = "vhlo.multiply_v1"(%1743, %7) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc812)
    %1745 = "vhlo.reshape_v1"(%arg449) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1746 = "vhlo.custom_call_v1"(%1745) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_14_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1747 = "vhlo.reshape_v1"(%1746) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1748 = "vhlo.transpose_v1"(%1747) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc813)
    %1749 = "vhlo.dot_general_v2"(%1729, %1748) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc814)
    %1750 = "vhlo.reshape_v1"(%1749) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc815)
    %1751 = "vhlo.reshape_v1"(%arg448) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1752 = "vhlo.custom_call_v1"(%1751) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_14_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1753 = "vhlo.reshape_v1"(%1752) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1754 = "vhlo.broadcast_in_dim_v1"(%1753) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc816)
    %1755 = "vhlo.add_v1"(%1750, %1754) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc816)
    %1756 = "vhlo.reshape_v1"(%1755) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc817)
    %1757 = "vhlo.transpose_v1"(%1756) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc818)
    %1758 = "vhlo.convert_v1"(%1757) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc819)
    %1759 = "vhlo.transpose_v1"(%1758) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,80,257]{2,1,3,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc820)
    %1760 = "vhlo.multiply_v1"(%1759, %6) : (!vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc821)
    %1761 = "vhlo.dot_general_v2"(%1744, %1760) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc822)
    %1762 = "vhlo.convert_v1"(%1761) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc823)
    %1763 = "vhlo.compare_v1"(%1762, %5) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc823)
    %1764 = "vhlo.not_v1"(%1763) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc824)
    %1765 = "vhlo.reduce_v1"(%1764, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("1652|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_15aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("1652|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_15aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc826)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc827)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc825)
    %1766 = "vhlo.reshape_v1"(%1765) : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc825)
    %1767 = "vhlo.not_v1"(%1766) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc828)
    %1768 = "vhlo.reshape_v1"(%1767) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc829)
    %1769 = "vhlo.broadcast_in_dim_v1"(%1768) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc829)
    %1770 = "vhlo.reduce_v1"(%1761, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1649|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_14aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1649|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_14aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc831)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc830)
    %1771 = "vhlo.broadcast_in_dim_v1"(%1770) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc830)
    %1772 = "vhlo.subtract_v1"(%1761, %1771) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc830)
    %1773 = "vhlo.exponential_v2"(%1772) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc830)
    %1774 = "vhlo.reduce_v1"(%1773, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1649|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_14aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1649|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_14aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc832)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc830)
    %1775 = "vhlo.broadcast_in_dim_v1"(%1774) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc830)
    %1776 = "vhlo.divide_v1"(%1773, %1775) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc830)
    %1777 = "vhlo.select_v1"(%1769, %4, %1776) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc833)
    %1778 = "vhlo.reshape_v1"(%arg214) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1779 = "vhlo.custom_call_v1"(%1778) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_14_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1780 = "vhlo.reshape_v1"(%1779) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1781 = "vhlo.transpose_v1"(%1780) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc834)
    %1782 = "vhlo.dot_general_v2"(%1729, %1781) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc835)
    %1783 = "vhlo.reshape_v1"(%1782) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc836)
    %1784 = "vhlo.reshape_v1"(%arg213) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1785 = "vhlo.custom_call_v1"(%1784) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_14_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1786 = "vhlo.reshape_v1"(%1785) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1787 = "vhlo.broadcast_in_dim_v1"(%1786) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc837)
    %1788 = "vhlo.add_v1"(%1783, %1787) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc837)
    %1789 = "vhlo.reshape_v1"(%1788) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc838)
    %1790 = "vhlo.transpose_v1"(%1789) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc839)
    %1791 = "vhlo.convert_v1"(%1790) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc840)
    %1792 = "vhlo.dot_general_v2"(%1777, %1791) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc841)
    %1793 = "vhlo.convert_v1"(%1792) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc842)
    %1794 = "vhlo.transpose_v1"(%1793) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,257,16,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc843)
    %1795 = "vhlo.reshape_v1"(%1794) : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc844)
    %1796 = "vhlo.reshape_v1"(%arg212) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1797 = "vhlo.custom_call_v1"(%1796) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_14_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1798 = "vhlo.reshape_v1"(%1797) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1799 = "vhlo.transpose_v1"(%1798) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc845)
    %1800 = "vhlo.dot_general_v2"(%1795, %1799) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc846)
    %1801 = "vhlo.reshape_v1"(%1800) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc844)
    %1802 = "vhlo.reshape_v1"(%arg211) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1803 = "vhlo.custom_call_v1"(%1802) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_14_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1804 = "vhlo.reshape_v1"(%1803) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1805 = "vhlo.broadcast_in_dim_v1"(%1804) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc847)
    %1806 = "vhlo.add_v1"(%1801, %1805) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc847)
    %1807 = "vhlo.add_v1"(%1721, %1806) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc848)
    %1808 = "vhlo.reshape_v1"(%arg210) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1809 = "vhlo.custom_call_v1"(%1808) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_14_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1810 = "vhlo.reshape_v1"(%1809) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1811 = "vhlo.reshape_v1"(%arg209) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1812 = "vhlo.custom_call_v1"(%1811) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_14_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1813 = "vhlo.reshape_v1"(%1812) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1814 = "vhlo.composite_v1"(%1807, %1810, %1813) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_33">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc849)
    %1815 = "vhlo.reshape_v1"(%1814) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc850)
    %1816 = "vhlo.reshape_v1"(%arg208) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %1817 = "vhlo.custom_call_v1"(%1816) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_14_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %1818 = "vhlo.reshape_v1"(%1817) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %1819 = "vhlo.transpose_v1"(%1818) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc851)
    %1820 = "vhlo.dot_general_v2"(%1815, %1819) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc852)
    %1821 = "vhlo.reshape_v1"(%1820) : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc850)
    %1822 = "vhlo.reshape_v1"(%arg207) : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc3)
    %1823 = "vhlo.custom_call_v1"(%1822) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_14_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc2)
    %1824 = "vhlo.reshape_v1"(%1823) : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc(#loc3)
    %1825 = "vhlo.broadcast_in_dim_v1"(%1824) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc853)
    %1826 = "vhlo.add_v1"(%1821, %1825) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc853)
    %1827 = "vhlo.composite_v1"(%1826) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_31">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc854)
    %1828 = "vhlo.reshape_v1"(%1827) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc855)
    %1829 = "vhlo.reshape_v1"(%arg206) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %1830 = "vhlo.custom_call_v1"(%1829) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_14_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %1831 = "vhlo.reshape_v1"(%1830) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %1832 = "vhlo.transpose_v1"(%1831) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc856)
    %1833 = "vhlo.dot_general_v2"(%1828, %1832) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc857)
    %1834 = "vhlo.reshape_v1"(%1833) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc855)
    %1835 = "vhlo.reshape_v1"(%arg205) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1836 = "vhlo.custom_call_v1"(%1835) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_14_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1837 = "vhlo.reshape_v1"(%1836) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1838 = "vhlo.broadcast_in_dim_v1"(%1837) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc858)
    %1839 = "vhlo.add_v1"(%1834, %1838) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc858)
    %1840 = "vhlo.add_v1"(%1807, %1839) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc859)
    %1841 = "vhlo.reshape_v1"(%arg204) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1842 = "vhlo.custom_call_v1"(%1841) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_15_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1843 = "vhlo.reshape_v1"(%1842) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1844 = "vhlo.reshape_v1"(%arg203) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1845 = "vhlo.custom_call_v1"(%1844) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_15_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1846 = "vhlo.reshape_v1"(%1845) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1847 = "vhlo.composite_v1"(%1840, %1843, %1846) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_40">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc860)
    %1848 = "vhlo.reshape_v1"(%1847) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc861)
    %1849 = "vhlo.reshape_v1"(%arg455) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1850 = "vhlo.custom_call_v1"(%1849) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_15_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1851 = "vhlo.reshape_v1"(%1850) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1852 = "vhlo.transpose_v1"(%1851) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc862)
    %1853 = "vhlo.dot_general_v2"(%1848, %1852) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc863)
    %1854 = "vhlo.reshape_v1"(%1853) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc861)
    %1855 = "vhlo.reshape_v1"(%arg454) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1856 = "vhlo.custom_call_v1"(%1855) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_15_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1857 = "vhlo.reshape_v1"(%1856) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1858 = "vhlo.broadcast_in_dim_v1"(%1857) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc864)
    %1859 = "vhlo.add_v1"(%1854, %1858) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc864)
    %1860 = "vhlo.reshape_v1"(%1859) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc865)
    %1861 = "vhlo.transpose_v1"(%1860) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc866)
    %1862 = "vhlo.convert_v1"(%1861) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc867)
    %1863 = "vhlo.multiply_v1"(%1862, %7) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc868)
    %1864 = "vhlo.reshape_v1"(%arg453) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1865 = "vhlo.custom_call_v1"(%1864) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_15_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1866 = "vhlo.reshape_v1"(%1865) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1867 = "vhlo.transpose_v1"(%1866) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc869)
    %1868 = "vhlo.dot_general_v2"(%1848, %1867) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc870)
    %1869 = "vhlo.reshape_v1"(%1868) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc871)
    %1870 = "vhlo.reshape_v1"(%arg452) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1871 = "vhlo.custom_call_v1"(%1870) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_15_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1872 = "vhlo.reshape_v1"(%1871) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1873 = "vhlo.broadcast_in_dim_v1"(%1872) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc872)
    %1874 = "vhlo.add_v1"(%1869, %1873) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc872)
    %1875 = "vhlo.reshape_v1"(%1874) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc873)
    %1876 = "vhlo.transpose_v1"(%1875) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc874)
    %1877 = "vhlo.convert_v1"(%1876) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc875)
    %1878 = "vhlo.transpose_v1"(%1877) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,80,257]{2,1,3,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc876)
    %1879 = "vhlo.multiply_v1"(%1878, %6) : (!vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc877)
    %1880 = "vhlo.dot_general_v2"(%1863, %1879) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc878)
    %1881 = "vhlo.convert_v1"(%1880) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc879)
    %1882 = "vhlo.compare_v1"(%1881, %5) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc879)
    %1883 = "vhlo.not_v1"(%1882) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc880)
    %1884 = "vhlo.reduce_v1"(%1883, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("1726|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_16aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("1726|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_16aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc882)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc883)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc881)
    %1885 = "vhlo.reshape_v1"(%1884) : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc881)
    %1886 = "vhlo.not_v1"(%1885) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc884)
    %1887 = "vhlo.reshape_v1"(%1886) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc885)
    %1888 = "vhlo.broadcast_in_dim_v1"(%1887) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc885)
    %1889 = "vhlo.reduce_v1"(%1880, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1723|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_15aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1723|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_15aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc887)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc886)
    %1890 = "vhlo.broadcast_in_dim_v1"(%1889) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc886)
    %1891 = "vhlo.subtract_v1"(%1880, %1890) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc886)
    %1892 = "vhlo.exponential_v2"(%1891) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc886)
    %1893 = "vhlo.reduce_v1"(%1892, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1723|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_15aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1723|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_15aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc888)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc886)
    %1894 = "vhlo.broadcast_in_dim_v1"(%1893) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc886)
    %1895 = "vhlo.divide_v1"(%1892, %1894) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc886)
    %1896 = "vhlo.select_v1"(%1888, %4, %1895) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc889)
    %1897 = "vhlo.reshape_v1"(%arg202) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1898 = "vhlo.custom_call_v1"(%1897) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_15_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1899 = "vhlo.reshape_v1"(%1898) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1900 = "vhlo.transpose_v1"(%1899) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc890)
    %1901 = "vhlo.dot_general_v2"(%1848, %1900) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc891)
    %1902 = "vhlo.reshape_v1"(%1901) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc892)
    %1903 = "vhlo.reshape_v1"(%arg201) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1904 = "vhlo.custom_call_v1"(%1903) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_15_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1905 = "vhlo.reshape_v1"(%1904) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1906 = "vhlo.broadcast_in_dim_v1"(%1905) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc893)
    %1907 = "vhlo.add_v1"(%1902, %1906) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc893)
    %1908 = "vhlo.reshape_v1"(%1907) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc894)
    %1909 = "vhlo.transpose_v1"(%1908) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc895)
    %1910 = "vhlo.convert_v1"(%1909) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc896)
    %1911 = "vhlo.dot_general_v2"(%1896, %1910) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc897)
    %1912 = "vhlo.convert_v1"(%1911) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc898)
    %1913 = "vhlo.transpose_v1"(%1912) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,257,16,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc899)
    %1914 = "vhlo.reshape_v1"(%1913) : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc900)
    %1915 = "vhlo.reshape_v1"(%arg200) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1916 = "vhlo.custom_call_v1"(%1915) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_15_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1917 = "vhlo.reshape_v1"(%1916) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1918 = "vhlo.transpose_v1"(%1917) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc901)
    %1919 = "vhlo.dot_general_v2"(%1914, %1918) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc902)
    %1920 = "vhlo.reshape_v1"(%1919) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc900)
    %1921 = "vhlo.reshape_v1"(%arg199) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1922 = "vhlo.custom_call_v1"(%1921) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_15_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1923 = "vhlo.reshape_v1"(%1922) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1924 = "vhlo.broadcast_in_dim_v1"(%1923) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc903)
    %1925 = "vhlo.add_v1"(%1920, %1924) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc903)
    %1926 = "vhlo.add_v1"(%1840, %1925) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc904)
    %1927 = "vhlo.reshape_v1"(%arg198) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1928 = "vhlo.custom_call_v1"(%1927) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_15_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1929 = "vhlo.reshape_v1"(%1928) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1930 = "vhlo.reshape_v1"(%arg197) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1931 = "vhlo.custom_call_v1"(%1930) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_15_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1932 = "vhlo.reshape_v1"(%1931) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1933 = "vhlo.composite_v1"(%1926, %1929, %1932) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_48">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc905)
    %1934 = "vhlo.reshape_v1"(%1933) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc906)
    %1935 = "vhlo.reshape_v1"(%arg196) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %1936 = "vhlo.custom_call_v1"(%1935) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_15_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %1937 = "vhlo.reshape_v1"(%1936) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %1938 = "vhlo.transpose_v1"(%1937) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc907)
    %1939 = "vhlo.dot_general_v2"(%1934, %1938) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc908)
    %1940 = "vhlo.reshape_v1"(%1939) : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc906)
    %1941 = "vhlo.reshape_v1"(%arg195) : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc3)
    %1942 = "vhlo.custom_call_v1"(%1941) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_15_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc2)
    %1943 = "vhlo.reshape_v1"(%1942) : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc(#loc3)
    %1944 = "vhlo.broadcast_in_dim_v1"(%1943) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc909)
    %1945 = "vhlo.add_v1"(%1940, %1944) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc909)
    %1946 = "vhlo.composite_v1"(%1945) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_27">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc910)
    %1947 = "vhlo.reshape_v1"(%1946) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc911)
    %1948 = "vhlo.reshape_v1"(%arg194) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %1949 = "vhlo.custom_call_v1"(%1948) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_15_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %1950 = "vhlo.reshape_v1"(%1949) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %1951 = "vhlo.transpose_v1"(%1950) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc912)
    %1952 = "vhlo.dot_general_v2"(%1947, %1951) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc913)
    %1953 = "vhlo.reshape_v1"(%1952) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc911)
    %1954 = "vhlo.reshape_v1"(%arg193) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1955 = "vhlo.custom_call_v1"(%1954) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_15_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1956 = "vhlo.reshape_v1"(%1955) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1957 = "vhlo.broadcast_in_dim_v1"(%1956) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc914)
    %1958 = "vhlo.add_v1"(%1953, %1957) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc914)
    %1959 = "vhlo.add_v1"(%1926, %1958) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc915)
    %1960 = "vhlo.reshape_v1"(%arg192) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1961 = "vhlo.custom_call_v1"(%1960) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_16_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1962 = "vhlo.reshape_v1"(%1961) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1963 = "vhlo.reshape_v1"(%arg191) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1964 = "vhlo.custom_call_v1"(%1963) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_16_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1965 = "vhlo.reshape_v1"(%1964) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1966 = "vhlo.composite_v1"(%1959, %1962, %1965) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_44">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc916)
    %1967 = "vhlo.reshape_v1"(%1966) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc917)
    %1968 = "vhlo.reshape_v1"(%arg459) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1969 = "vhlo.custom_call_v1"(%1968) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_16_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1970 = "vhlo.reshape_v1"(%1969) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1971 = "vhlo.transpose_v1"(%1970) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc918)
    %1972 = "vhlo.dot_general_v2"(%1967, %1971) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc919)
    %1973 = "vhlo.reshape_v1"(%1972) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc917)
    %1974 = "vhlo.reshape_v1"(%arg458) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1975 = "vhlo.custom_call_v1"(%1974) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_16_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1976 = "vhlo.reshape_v1"(%1975) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1977 = "vhlo.broadcast_in_dim_v1"(%1976) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc920)
    %1978 = "vhlo.add_v1"(%1973, %1977) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc920)
    %1979 = "vhlo.reshape_v1"(%1978) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc921)
    %1980 = "vhlo.transpose_v1"(%1979) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc922)
    %1981 = "vhlo.convert_v1"(%1980) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc923)
    %1982 = "vhlo.multiply_v1"(%1981, %7) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc924)
    %1983 = "vhlo.reshape_v1"(%arg457) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1984 = "vhlo.custom_call_v1"(%1983) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_16_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %1985 = "vhlo.reshape_v1"(%1984) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %1986 = "vhlo.transpose_v1"(%1985) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc925)
    %1987 = "vhlo.dot_general_v2"(%1967, %1986) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc926)
    %1988 = "vhlo.reshape_v1"(%1987) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc927)
    %1989 = "vhlo.reshape_v1"(%arg456) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %1990 = "vhlo.custom_call_v1"(%1989) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_16_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %1991 = "vhlo.reshape_v1"(%1990) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %1992 = "vhlo.broadcast_in_dim_v1"(%1991) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc928)
    %1993 = "vhlo.add_v1"(%1988, %1992) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc928)
    %1994 = "vhlo.reshape_v1"(%1993) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc929)
    %1995 = "vhlo.transpose_v1"(%1994) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc930)
    %1996 = "vhlo.convert_v1"(%1995) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc931)
    %1997 = "vhlo.transpose_v1"(%1996) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,80,257]{2,1,3,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc932)
    %1998 = "vhlo.multiply_v1"(%1997, %6) : (!vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc933)
    %1999 = "vhlo.dot_general_v2"(%1982, %1998) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc934)
    %2000 = "vhlo.convert_v1"(%1999) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc935)
    %2001 = "vhlo.compare_v1"(%2000, %5) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc935)
    %2002 = "vhlo.not_v1"(%2001) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc936)
    %2003 = "vhlo.reduce_v1"(%2002, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("1800|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_17aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("1800|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_17aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc938)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc939)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc937)
    %2004 = "vhlo.reshape_v1"(%2003) : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc937)
    %2005 = "vhlo.not_v1"(%2004) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc940)
    %2006 = "vhlo.reshape_v1"(%2005) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc941)
    %2007 = "vhlo.broadcast_in_dim_v1"(%2006) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc941)
    %2008 = "vhlo.reduce_v1"(%1999, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1797|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_16aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1797|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_16aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc943)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc942)
    %2009 = "vhlo.broadcast_in_dim_v1"(%2008) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc942)
    %2010 = "vhlo.subtract_v1"(%1999, %2009) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc942)
    %2011 = "vhlo.exponential_v2"(%2010) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc942)
    %2012 = "vhlo.reduce_v1"(%2011, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1797|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_16aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1797|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_16aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc944)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc942)
    %2013 = "vhlo.broadcast_in_dim_v1"(%2012) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc942)
    %2014 = "vhlo.divide_v1"(%2011, %2013) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc942)
    %2015 = "vhlo.select_v1"(%2007, %4, %2014) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc945)
    %2016 = "vhlo.reshape_v1"(%arg190) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2017 = "vhlo.custom_call_v1"(%2016) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_16_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2018 = "vhlo.reshape_v1"(%2017) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2019 = "vhlo.transpose_v1"(%2018) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc946)
    %2020 = "vhlo.dot_general_v2"(%1967, %2019) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc947)
    %2021 = "vhlo.reshape_v1"(%2020) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc948)
    %2022 = "vhlo.reshape_v1"(%arg189) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2023 = "vhlo.custom_call_v1"(%2022) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_16_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2024 = "vhlo.reshape_v1"(%2023) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2025 = "vhlo.broadcast_in_dim_v1"(%2024) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc949)
    %2026 = "vhlo.add_v1"(%2021, %2025) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc949)
    %2027 = "vhlo.reshape_v1"(%2026) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc950)
    %2028 = "vhlo.transpose_v1"(%2027) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc951)
    %2029 = "vhlo.convert_v1"(%2028) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc952)
    %2030 = "vhlo.dot_general_v2"(%2015, %2029) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc953)
    %2031 = "vhlo.convert_v1"(%2030) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc954)
    %2032 = "vhlo.transpose_v1"(%2031) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,257,16,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc955)
    %2033 = "vhlo.reshape_v1"(%2032) : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc956)
    %2034 = "vhlo.reshape_v1"(%arg188) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2035 = "vhlo.custom_call_v1"(%2034) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_16_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2036 = "vhlo.reshape_v1"(%2035) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2037 = "vhlo.transpose_v1"(%2036) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc957)
    %2038 = "vhlo.dot_general_v2"(%2033, %2037) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc958)
    %2039 = "vhlo.reshape_v1"(%2038) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc956)
    %2040 = "vhlo.reshape_v1"(%arg187) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2041 = "vhlo.custom_call_v1"(%2040) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_16_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2042 = "vhlo.reshape_v1"(%2041) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2043 = "vhlo.broadcast_in_dim_v1"(%2042) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc959)
    %2044 = "vhlo.add_v1"(%2039, %2043) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc959)
    %2045 = "vhlo.add_v1"(%1959, %2044) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc960)
    %2046 = "vhlo.reshape_v1"(%arg186) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2047 = "vhlo.custom_call_v1"(%2046) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_16_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2048 = "vhlo.reshape_v1"(%2047) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2049 = "vhlo.reshape_v1"(%arg185) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2050 = "vhlo.custom_call_v1"(%2049) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_16_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2051 = "vhlo.reshape_v1"(%2050) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2052 = "vhlo.composite_v1"(%2045, %2048, %2051) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_50">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc961)
    %2053 = "vhlo.reshape_v1"(%2052) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc962)
    %2054 = "vhlo.reshape_v1"(%arg184) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %2055 = "vhlo.custom_call_v1"(%2054) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_16_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %2056 = "vhlo.reshape_v1"(%2055) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %2057 = "vhlo.transpose_v1"(%2056) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc963)
    %2058 = "vhlo.dot_general_v2"(%2053, %2057) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc964)
    %2059 = "vhlo.reshape_v1"(%2058) : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc962)
    %2060 = "vhlo.reshape_v1"(%arg183) : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc3)
    %2061 = "vhlo.custom_call_v1"(%2060) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_16_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc2)
    %2062 = "vhlo.reshape_v1"(%2061) : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc(#loc3)
    %2063 = "vhlo.broadcast_in_dim_v1"(%2062) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc965)
    %2064 = "vhlo.add_v1"(%2059, %2063) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc965)
    %2065 = "vhlo.composite_v1"(%2064) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_32">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc966)
    %2066 = "vhlo.reshape_v1"(%2065) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc967)
    %2067 = "vhlo.reshape_v1"(%arg182) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %2068 = "vhlo.custom_call_v1"(%2067) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_16_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %2069 = "vhlo.reshape_v1"(%2068) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %2070 = "vhlo.transpose_v1"(%2069) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc968)
    %2071 = "vhlo.dot_general_v2"(%2066, %2070) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc969)
    %2072 = "vhlo.reshape_v1"(%2071) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc967)
    %2073 = "vhlo.reshape_v1"(%arg181) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2074 = "vhlo.custom_call_v1"(%2073) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_16_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2075 = "vhlo.reshape_v1"(%2074) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2076 = "vhlo.broadcast_in_dim_v1"(%2075) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc970)
    %2077 = "vhlo.add_v1"(%2072, %2076) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc970)
    %2078 = "vhlo.add_v1"(%2045, %2077) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc971)
    %2079 = "vhlo.reshape_v1"(%arg180) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2080 = "vhlo.custom_call_v1"(%2079) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_17_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2081 = "vhlo.reshape_v1"(%2080) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2082 = "vhlo.reshape_v1"(%arg179) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2083 = "vhlo.custom_call_v1"(%2082) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_17_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2084 = "vhlo.reshape_v1"(%2083) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2085 = "vhlo.composite_v1"(%2078, %2081, %2084) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_30">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc972)
    %2086 = "vhlo.reshape_v1"(%2085) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc973)
    %2087 = "vhlo.reshape_v1"(%arg463) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2088 = "vhlo.custom_call_v1"(%2087) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_17_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2089 = "vhlo.reshape_v1"(%2088) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2090 = "vhlo.transpose_v1"(%2089) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc974)
    %2091 = "vhlo.dot_general_v2"(%2086, %2090) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc975)
    %2092 = "vhlo.reshape_v1"(%2091) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc973)
    %2093 = "vhlo.reshape_v1"(%arg462) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2094 = "vhlo.custom_call_v1"(%2093) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_17_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2095 = "vhlo.reshape_v1"(%2094) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2096 = "vhlo.broadcast_in_dim_v1"(%2095) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc976)
    %2097 = "vhlo.add_v1"(%2092, %2096) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc976)
    %2098 = "vhlo.reshape_v1"(%2097) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc977)
    %2099 = "vhlo.transpose_v1"(%2098) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc978)
    %2100 = "vhlo.convert_v1"(%2099) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc979)
    %2101 = "vhlo.multiply_v1"(%2100, %7) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc980)
    %2102 = "vhlo.reshape_v1"(%arg461) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2103 = "vhlo.custom_call_v1"(%2102) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_17_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2104 = "vhlo.reshape_v1"(%2103) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2105 = "vhlo.transpose_v1"(%2104) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc981)
    %2106 = "vhlo.dot_general_v2"(%2086, %2105) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc982)
    %2107 = "vhlo.reshape_v1"(%2106) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc983)
    %2108 = "vhlo.reshape_v1"(%arg460) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2109 = "vhlo.custom_call_v1"(%2108) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_17_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2110 = "vhlo.reshape_v1"(%2109) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2111 = "vhlo.broadcast_in_dim_v1"(%2110) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc984)
    %2112 = "vhlo.add_v1"(%2107, %2111) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc984)
    %2113 = "vhlo.reshape_v1"(%2112) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc985)
    %2114 = "vhlo.transpose_v1"(%2113) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc986)
    %2115 = "vhlo.convert_v1"(%2114) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc987)
    %2116 = "vhlo.transpose_v1"(%2115) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,80,257]{2,1,3,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc988)
    %2117 = "vhlo.multiply_v1"(%2116, %6) : (!vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc989)
    %2118 = "vhlo.dot_general_v2"(%2101, %2117) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc990)
    %2119 = "vhlo.convert_v1"(%2118) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc991)
    %2120 = "vhlo.compare_v1"(%2119, %5) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc991)
    %2121 = "vhlo.not_v1"(%2120) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc992)
    %2122 = "vhlo.reduce_v1"(%2121, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("1874|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_18aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("1874|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_18aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc994)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc995)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc993)
    %2123 = "vhlo.reshape_v1"(%2122) : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc993)
    %2124 = "vhlo.not_v1"(%2123) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc996)
    %2125 = "vhlo.reshape_v1"(%2124) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc997)
    %2126 = "vhlo.broadcast_in_dim_v1"(%2125) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc997)
    %2127 = "vhlo.reduce_v1"(%2118, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1871|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_17aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1871|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_17aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc999)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc998)
    %2128 = "vhlo.broadcast_in_dim_v1"(%2127) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc998)
    %2129 = "vhlo.subtract_v1"(%2118, %2128) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc998)
    %2130 = "vhlo.exponential_v2"(%2129) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc998)
    %2131 = "vhlo.reduce_v1"(%2130, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1871|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_17aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1871|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_17aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1000)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc998)
    %2132 = "vhlo.broadcast_in_dim_v1"(%2131) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc998)
    %2133 = "vhlo.divide_v1"(%2130, %2132) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc998)
    %2134 = "vhlo.select_v1"(%2126, %4, %2133) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1001)
    %2135 = "vhlo.reshape_v1"(%arg178) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2136 = "vhlo.custom_call_v1"(%2135) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_17_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2137 = "vhlo.reshape_v1"(%2136) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2138 = "vhlo.transpose_v1"(%2137) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1002)
    %2139 = "vhlo.dot_general_v2"(%2086, %2138) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1003)
    %2140 = "vhlo.reshape_v1"(%2139) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1004)
    %2141 = "vhlo.reshape_v1"(%arg177) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2142 = "vhlo.custom_call_v1"(%2141) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_17_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2143 = "vhlo.reshape_v1"(%2142) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2144 = "vhlo.broadcast_in_dim_v1"(%2143) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1005)
    %2145 = "vhlo.add_v1"(%2140, %2144) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1005)
    %2146 = "vhlo.reshape_v1"(%2145) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1006)
    %2147 = "vhlo.transpose_v1"(%2146) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1007)
    %2148 = "vhlo.convert_v1"(%2147) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1008)
    %2149 = "vhlo.dot_general_v2"(%2134, %2148) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1009)
    %2150 = "vhlo.convert_v1"(%2149) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1010)
    %2151 = "vhlo.transpose_v1"(%2150) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,257,16,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1011)
    %2152 = "vhlo.reshape_v1"(%2151) : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1012)
    %2153 = "vhlo.reshape_v1"(%arg176) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2154 = "vhlo.custom_call_v1"(%2153) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_17_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2155 = "vhlo.reshape_v1"(%2154) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2156 = "vhlo.transpose_v1"(%2155) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1013)
    %2157 = "vhlo.dot_general_v2"(%2152, %2156) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1014)
    %2158 = "vhlo.reshape_v1"(%2157) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1012)
    %2159 = "vhlo.reshape_v1"(%arg175) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2160 = "vhlo.custom_call_v1"(%2159) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_17_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2161 = "vhlo.reshape_v1"(%2160) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2162 = "vhlo.broadcast_in_dim_v1"(%2161) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1015)
    %2163 = "vhlo.add_v1"(%2158, %2162) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1015)
    %2164 = "vhlo.add_v1"(%2078, %2163) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1016)
    %2165 = "vhlo.reshape_v1"(%arg174) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2166 = "vhlo.custom_call_v1"(%2165) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_17_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2167 = "vhlo.reshape_v1"(%2166) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2168 = "vhlo.reshape_v1"(%arg173) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2169 = "vhlo.custom_call_v1"(%2168) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_17_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2170 = "vhlo.reshape_v1"(%2169) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2171 = "vhlo.composite_v1"(%2164, %2167, %2170) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_70">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1017)
    %2172 = "vhlo.reshape_v1"(%2171) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1018)
    %2173 = "vhlo.reshape_v1"(%arg172) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %2174 = "vhlo.custom_call_v1"(%2173) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_17_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %2175 = "vhlo.reshape_v1"(%2174) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %2176 = "vhlo.transpose_v1"(%2175) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc1019)
    %2177 = "vhlo.dot_general_v2"(%2172, %2176) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc1020)
    %2178 = "vhlo.reshape_v1"(%2177) : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1018)
    %2179 = "vhlo.reshape_v1"(%arg171) : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc3)
    %2180 = "vhlo.custom_call_v1"(%2179) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_17_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc2)
    %2181 = "vhlo.reshape_v1"(%2180) : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc(#loc3)
    %2182 = "vhlo.broadcast_in_dim_v1"(%2181) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1021)
    %2183 = "vhlo.add_v1"(%2178, %2182) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1021)
    %2184 = "vhlo.composite_v1"(%2183) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_33">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1022)
    %2185 = "vhlo.reshape_v1"(%2184) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc1023)
    %2186 = "vhlo.reshape_v1"(%arg170) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %2187 = "vhlo.custom_call_v1"(%2186) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_17_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %2188 = "vhlo.reshape_v1"(%2187) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %2189 = "vhlo.transpose_v1"(%2188) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc1024)
    %2190 = "vhlo.dot_general_v2"(%2185, %2189) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1025)
    %2191 = "vhlo.reshape_v1"(%2190) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1023)
    %2192 = "vhlo.reshape_v1"(%arg169) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2193 = "vhlo.custom_call_v1"(%2192) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_17_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2194 = "vhlo.reshape_v1"(%2193) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2195 = "vhlo.broadcast_in_dim_v1"(%2194) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1026)
    %2196 = "vhlo.add_v1"(%2191, %2195) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1026)
    %2197 = "vhlo.add_v1"(%2164, %2196) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1027)
    %2198 = "vhlo.reshape_v1"(%arg168) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2199 = "vhlo.custom_call_v1"(%2198) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_18_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2200 = "vhlo.reshape_v1"(%2199) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2201 = "vhlo.reshape_v1"(%arg167) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2202 = "vhlo.custom_call_v1"(%2201) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_18_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2203 = "vhlo.reshape_v1"(%2202) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2204 = "vhlo.composite_v1"(%2197, %2200, %2203) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_74">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1028)
    %2205 = "vhlo.reshape_v1"(%2204) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1029)
    %2206 = "vhlo.reshape_v1"(%arg467) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2207 = "vhlo.custom_call_v1"(%2206) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_18_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2208 = "vhlo.reshape_v1"(%2207) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2209 = "vhlo.transpose_v1"(%2208) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1030)
    %2210 = "vhlo.dot_general_v2"(%2205, %2209) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1031)
    %2211 = "vhlo.reshape_v1"(%2210) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1029)
    %2212 = "vhlo.reshape_v1"(%arg466) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2213 = "vhlo.custom_call_v1"(%2212) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_18_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2214 = "vhlo.reshape_v1"(%2213) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2215 = "vhlo.broadcast_in_dim_v1"(%2214) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1032)
    %2216 = "vhlo.add_v1"(%2211, %2215) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1032)
    %2217 = "vhlo.reshape_v1"(%2216) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1033)
    %2218 = "vhlo.transpose_v1"(%2217) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1034)
    %2219 = "vhlo.convert_v1"(%2218) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1035)
    %2220 = "vhlo.multiply_v1"(%2219, %7) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1036)
    %2221 = "vhlo.reshape_v1"(%arg465) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2222 = "vhlo.custom_call_v1"(%2221) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_18_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2223 = "vhlo.reshape_v1"(%2222) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2224 = "vhlo.transpose_v1"(%2223) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1037)
    %2225 = "vhlo.dot_general_v2"(%2205, %2224) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1038)
    %2226 = "vhlo.reshape_v1"(%2225) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1039)
    %2227 = "vhlo.reshape_v1"(%arg464) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2228 = "vhlo.custom_call_v1"(%2227) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_18_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2229 = "vhlo.reshape_v1"(%2228) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2230 = "vhlo.broadcast_in_dim_v1"(%2229) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1040)
    %2231 = "vhlo.add_v1"(%2226, %2230) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1040)
    %2232 = "vhlo.reshape_v1"(%2231) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1041)
    %2233 = "vhlo.transpose_v1"(%2232) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1042)
    %2234 = "vhlo.convert_v1"(%2233) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1043)
    %2235 = "vhlo.transpose_v1"(%2234) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,80,257]{2,1,3,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc1044)
    %2236 = "vhlo.multiply_v1"(%2235, %6) : (!vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc1045)
    %2237 = "vhlo.dot_general_v2"(%2220, %2236) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1046)
    %2238 = "vhlo.convert_v1"(%2237) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc1047)
    %2239 = "vhlo.compare_v1"(%2238, %5) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1047)
    %2240 = "vhlo.not_v1"(%2239) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1048)
    %2241 = "vhlo.reduce_v1"(%2240, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("1948|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_19aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("1948|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_19aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1050)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1051)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc1049)
    %2242 = "vhlo.reshape_v1"(%2241) : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc1049)
    %2243 = "vhlo.not_v1"(%2242) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc1052)
    %2244 = "vhlo.reshape_v1"(%2243) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc1053)
    %2245 = "vhlo.broadcast_in_dim_v1"(%2244) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1053)
    %2246 = "vhlo.reduce_v1"(%2237, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1945|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_18aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1945|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_18aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1055)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc1054)
    %2247 = "vhlo.broadcast_in_dim_v1"(%2246) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1054)
    %2248 = "vhlo.subtract_v1"(%2237, %2247) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1054)
    %2249 = "vhlo.exponential_v2"(%2248) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1054)
    %2250 = "vhlo.reduce_v1"(%2249, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1945|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_18aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1945|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_18aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1056)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc1054)
    %2251 = "vhlo.broadcast_in_dim_v1"(%2250) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1054)
    %2252 = "vhlo.divide_v1"(%2249, %2251) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1054)
    %2253 = "vhlo.select_v1"(%2245, %4, %2252) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1057)
    %2254 = "vhlo.reshape_v1"(%arg166) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2255 = "vhlo.custom_call_v1"(%2254) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_18_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2256 = "vhlo.reshape_v1"(%2255) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2257 = "vhlo.transpose_v1"(%2256) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1058)
    %2258 = "vhlo.dot_general_v2"(%2205, %2257) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1059)
    %2259 = "vhlo.reshape_v1"(%2258) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1060)
    %2260 = "vhlo.reshape_v1"(%arg165) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2261 = "vhlo.custom_call_v1"(%2260) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_18_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2262 = "vhlo.reshape_v1"(%2261) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2263 = "vhlo.broadcast_in_dim_v1"(%2262) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1061)
    %2264 = "vhlo.add_v1"(%2259, %2263) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1061)
    %2265 = "vhlo.reshape_v1"(%2264) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1062)
    %2266 = "vhlo.transpose_v1"(%2265) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1063)
    %2267 = "vhlo.convert_v1"(%2266) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1064)
    %2268 = "vhlo.dot_general_v2"(%2253, %2267) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1065)
    %2269 = "vhlo.convert_v1"(%2268) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1066)
    %2270 = "vhlo.transpose_v1"(%2269) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,257,16,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1067)
    %2271 = "vhlo.reshape_v1"(%2270) : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1068)
    %2272 = "vhlo.reshape_v1"(%arg164) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2273 = "vhlo.custom_call_v1"(%2272) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_18_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2274 = "vhlo.reshape_v1"(%2273) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2275 = "vhlo.transpose_v1"(%2274) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1069)
    %2276 = "vhlo.dot_general_v2"(%2271, %2275) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1070)
    %2277 = "vhlo.reshape_v1"(%2276) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1068)
    %2278 = "vhlo.reshape_v1"(%arg163) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2279 = "vhlo.custom_call_v1"(%2278) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_18_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2280 = "vhlo.reshape_v1"(%2279) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2281 = "vhlo.broadcast_in_dim_v1"(%2280) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1071)
    %2282 = "vhlo.add_v1"(%2277, %2281) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1071)
    %2283 = "vhlo.add_v1"(%2197, %2282) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1072)
    %2284 = "vhlo.reshape_v1"(%arg162) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2285 = "vhlo.custom_call_v1"(%2284) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_18_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2286 = "vhlo.reshape_v1"(%2285) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2287 = "vhlo.reshape_v1"(%arg161) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2288 = "vhlo.custom_call_v1"(%2287) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_18_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2289 = "vhlo.reshape_v1"(%2288) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2290 = "vhlo.composite_v1"(%2283, %2286, %2289) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_61">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1073)
    %2291 = "vhlo.reshape_v1"(%2290) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1074)
    %2292 = "vhlo.reshape_v1"(%arg160) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %2293 = "vhlo.custom_call_v1"(%2292) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_18_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %2294 = "vhlo.reshape_v1"(%2293) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %2295 = "vhlo.transpose_v1"(%2294) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc1075)
    %2296 = "vhlo.dot_general_v2"(%2291, %2295) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc1076)
    %2297 = "vhlo.reshape_v1"(%2296) : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1074)
    %2298 = "vhlo.reshape_v1"(%arg159) : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc3)
    %2299 = "vhlo.custom_call_v1"(%2298) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_18_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc2)
    %2300 = "vhlo.reshape_v1"(%2299) : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc(#loc3)
    %2301 = "vhlo.broadcast_in_dim_v1"(%2300) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1077)
    %2302 = "vhlo.add_v1"(%2297, %2301) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1077)
    %2303 = "vhlo.composite_v1"(%2302) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_23">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1078)
    %2304 = "vhlo.reshape_v1"(%2303) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc1079)
    %2305 = "vhlo.reshape_v1"(%arg158) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %2306 = "vhlo.custom_call_v1"(%2305) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_18_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %2307 = "vhlo.reshape_v1"(%2306) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %2308 = "vhlo.transpose_v1"(%2307) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc1080)
    %2309 = "vhlo.dot_general_v2"(%2304, %2308) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1081)
    %2310 = "vhlo.reshape_v1"(%2309) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1079)
    %2311 = "vhlo.reshape_v1"(%arg157) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2312 = "vhlo.custom_call_v1"(%2311) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_18_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2313 = "vhlo.reshape_v1"(%2312) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2314 = "vhlo.broadcast_in_dim_v1"(%2313) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1082)
    %2315 = "vhlo.add_v1"(%2310, %2314) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1082)
    %2316 = "vhlo.add_v1"(%2283, %2315) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1083)
    %2317 = "vhlo.reshape_v1"(%arg156) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2318 = "vhlo.custom_call_v1"(%2317) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_19_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2319 = "vhlo.reshape_v1"(%2318) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2320 = "vhlo.reshape_v1"(%arg155) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2321 = "vhlo.custom_call_v1"(%2320) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_19_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2322 = "vhlo.reshape_v1"(%2321) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2323 = "vhlo.composite_v1"(%2316, %2319, %2322) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_20">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1084)
    %2324 = "vhlo.reshape_v1"(%2323) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1085)
    %2325 = "vhlo.reshape_v1"(%arg471) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2326 = "vhlo.custom_call_v1"(%2325) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_19_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2327 = "vhlo.reshape_v1"(%2326) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2328 = "vhlo.transpose_v1"(%2327) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1086)
    %2329 = "vhlo.dot_general_v2"(%2324, %2328) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1087)
    %2330 = "vhlo.reshape_v1"(%2329) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1085)
    %2331 = "vhlo.reshape_v1"(%arg470) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2332 = "vhlo.custom_call_v1"(%2331) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_19_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2333 = "vhlo.reshape_v1"(%2332) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2334 = "vhlo.broadcast_in_dim_v1"(%2333) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1088)
    %2335 = "vhlo.add_v1"(%2330, %2334) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1088)
    %2336 = "vhlo.reshape_v1"(%2335) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1089)
    %2337 = "vhlo.transpose_v1"(%2336) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1090)
    %2338 = "vhlo.convert_v1"(%2337) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1091)
    %2339 = "vhlo.multiply_v1"(%2338, %7) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1092)
    %2340 = "vhlo.reshape_v1"(%arg469) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2341 = "vhlo.custom_call_v1"(%2340) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_19_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2342 = "vhlo.reshape_v1"(%2341) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2343 = "vhlo.transpose_v1"(%2342) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1093)
    %2344 = "vhlo.dot_general_v2"(%2324, %2343) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1094)
    %2345 = "vhlo.reshape_v1"(%2344) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1095)
    %2346 = "vhlo.reshape_v1"(%arg468) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2347 = "vhlo.custom_call_v1"(%2346) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_19_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2348 = "vhlo.reshape_v1"(%2347) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2349 = "vhlo.broadcast_in_dim_v1"(%2348) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1096)
    %2350 = "vhlo.add_v1"(%2345, %2349) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1096)
    %2351 = "vhlo.reshape_v1"(%2350) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1097)
    %2352 = "vhlo.transpose_v1"(%2351) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1098)
    %2353 = "vhlo.convert_v1"(%2352) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1099)
    %2354 = "vhlo.transpose_v1"(%2353) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,80,257]{2,1,3,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc1100)
    %2355 = "vhlo.multiply_v1"(%2354, %6) : (!vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc1101)
    %2356 = "vhlo.dot_general_v2"(%2339, %2355) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1102)
    %2357 = "vhlo.convert_v1"(%2356) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc1103)
    %2358 = "vhlo.compare_v1"(%2357, %5) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1103)
    %2359 = "vhlo.not_v1"(%2358) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1104)
    %2360 = "vhlo.reduce_v1"(%2359, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("2022|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_20aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("2022|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_20aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1106)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1107)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc1105)
    %2361 = "vhlo.reshape_v1"(%2360) : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc1105)
    %2362 = "vhlo.not_v1"(%2361) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc1108)
    %2363 = "vhlo.reshape_v1"(%2362) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc1109)
    %2364 = "vhlo.broadcast_in_dim_v1"(%2363) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1109)
    %2365 = "vhlo.reduce_v1"(%2356, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2019|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_19aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2019|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_19aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1111)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc1110)
    %2366 = "vhlo.broadcast_in_dim_v1"(%2365) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1110)
    %2367 = "vhlo.subtract_v1"(%2356, %2366) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1110)
    %2368 = "vhlo.exponential_v2"(%2367) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1110)
    %2369 = "vhlo.reduce_v1"(%2368, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2019|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_19aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2019|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_19aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1112)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc1110)
    %2370 = "vhlo.broadcast_in_dim_v1"(%2369) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1110)
    %2371 = "vhlo.divide_v1"(%2368, %2370) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1110)
    %2372 = "vhlo.select_v1"(%2364, %4, %2371) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1113)
    %2373 = "vhlo.reshape_v1"(%arg154) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2374 = "vhlo.custom_call_v1"(%2373) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_19_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2375 = "vhlo.reshape_v1"(%2374) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2376 = "vhlo.transpose_v1"(%2375) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1114)
    %2377 = "vhlo.dot_general_v2"(%2324, %2376) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1115)
    %2378 = "vhlo.reshape_v1"(%2377) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1116)
    %2379 = "vhlo.reshape_v1"(%arg153) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2380 = "vhlo.custom_call_v1"(%2379) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_19_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2381 = "vhlo.reshape_v1"(%2380) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2382 = "vhlo.broadcast_in_dim_v1"(%2381) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1117)
    %2383 = "vhlo.add_v1"(%2378, %2382) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1117)
    %2384 = "vhlo.reshape_v1"(%2383) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1118)
    %2385 = "vhlo.transpose_v1"(%2384) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1119)
    %2386 = "vhlo.convert_v1"(%2385) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1120)
    %2387 = "vhlo.dot_general_v2"(%2372, %2386) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1121)
    %2388 = "vhlo.convert_v1"(%2387) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1122)
    %2389 = "vhlo.transpose_v1"(%2388) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,257,16,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1123)
    %2390 = "vhlo.reshape_v1"(%2389) : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1124)
    %2391 = "vhlo.reshape_v1"(%arg152) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2392 = "vhlo.custom_call_v1"(%2391) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_19_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2393 = "vhlo.reshape_v1"(%2392) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2394 = "vhlo.transpose_v1"(%2393) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1125)
    %2395 = "vhlo.dot_general_v2"(%2390, %2394) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1126)
    %2396 = "vhlo.reshape_v1"(%2395) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1124)
    %2397 = "vhlo.reshape_v1"(%arg151) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2398 = "vhlo.custom_call_v1"(%2397) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_19_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2399 = "vhlo.reshape_v1"(%2398) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2400 = "vhlo.broadcast_in_dim_v1"(%2399) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1127)
    %2401 = "vhlo.add_v1"(%2396, %2400) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1127)
    %2402 = "vhlo.add_v1"(%2316, %2401) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1128)
    %2403 = "vhlo.reshape_v1"(%arg150) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2404 = "vhlo.custom_call_v1"(%2403) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_19_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2405 = "vhlo.reshape_v1"(%2404) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2406 = "vhlo.reshape_v1"(%arg149) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2407 = "vhlo.custom_call_v1"(%2406) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_19_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2408 = "vhlo.reshape_v1"(%2407) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2409 = "vhlo.composite_v1"(%2402, %2405, %2408) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_51">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1129)
    %2410 = "vhlo.reshape_v1"(%2409) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1130)
    %2411 = "vhlo.reshape_v1"(%arg148) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %2412 = "vhlo.custom_call_v1"(%2411) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_19_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %2413 = "vhlo.reshape_v1"(%2412) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %2414 = "vhlo.transpose_v1"(%2413) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc1131)
    %2415 = "vhlo.dot_general_v2"(%2410, %2414) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc1132)
    %2416 = "vhlo.reshape_v1"(%2415) : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1130)
    %2417 = "vhlo.reshape_v1"(%arg147) : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc3)
    %2418 = "vhlo.custom_call_v1"(%2417) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_19_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc2)
    %2419 = "vhlo.reshape_v1"(%2418) : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc(#loc3)
    %2420 = "vhlo.broadcast_in_dim_v1"(%2419) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1133)
    %2421 = "vhlo.add_v1"(%2416, %2420) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1133)
    %2422 = "vhlo.composite_v1"(%2421) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_10">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1134)
    %2423 = "vhlo.reshape_v1"(%2422) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc1135)
    %2424 = "vhlo.reshape_v1"(%arg146) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %2425 = "vhlo.custom_call_v1"(%2424) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_19_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %2426 = "vhlo.reshape_v1"(%2425) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %2427 = "vhlo.transpose_v1"(%2426) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc1136)
    %2428 = "vhlo.dot_general_v2"(%2423, %2427) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1137)
    %2429 = "vhlo.reshape_v1"(%2428) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1135)
    %2430 = "vhlo.reshape_v1"(%arg145) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2431 = "vhlo.custom_call_v1"(%2430) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_19_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2432 = "vhlo.reshape_v1"(%2431) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2433 = "vhlo.broadcast_in_dim_v1"(%2432) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1138)
    %2434 = "vhlo.add_v1"(%2429, %2433) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1138)
    %2435 = "vhlo.add_v1"(%2402, %2434) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1139)
    %2436 = "vhlo.reshape_v1"(%arg144) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2437 = "vhlo.custom_call_v1"(%2436) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_20_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2438 = "vhlo.reshape_v1"(%2437) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2439 = "vhlo.reshape_v1"(%arg143) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2440 = "vhlo.custom_call_v1"(%2439) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_20_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2441 = "vhlo.reshape_v1"(%2440) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2442 = "vhlo.composite_v1"(%2435, %2438, %2441) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_19">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1140)
    %2443 = "vhlo.reshape_v1"(%2442) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1141)
    %2444 = "vhlo.reshape_v1"(%arg475) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2445 = "vhlo.custom_call_v1"(%2444) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_20_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2446 = "vhlo.reshape_v1"(%2445) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2447 = "vhlo.transpose_v1"(%2446) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1142)
    %2448 = "vhlo.dot_general_v2"(%2443, %2447) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1143)
    %2449 = "vhlo.reshape_v1"(%2448) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1141)
    %2450 = "vhlo.reshape_v1"(%arg474) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2451 = "vhlo.custom_call_v1"(%2450) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_20_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2452 = "vhlo.reshape_v1"(%2451) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2453 = "vhlo.broadcast_in_dim_v1"(%2452) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1144)
    %2454 = "vhlo.add_v1"(%2449, %2453) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1144)
    %2455 = "vhlo.reshape_v1"(%2454) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1145)
    %2456 = "vhlo.transpose_v1"(%2455) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1146)
    %2457 = "vhlo.convert_v1"(%2456) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1147)
    %2458 = "vhlo.multiply_v1"(%2457, %7) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1148)
    %2459 = "vhlo.reshape_v1"(%arg473) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2460 = "vhlo.custom_call_v1"(%2459) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_20_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2461 = "vhlo.reshape_v1"(%2460) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2462 = "vhlo.transpose_v1"(%2461) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1149)
    %2463 = "vhlo.dot_general_v2"(%2443, %2462) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1150)
    %2464 = "vhlo.reshape_v1"(%2463) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1151)
    %2465 = "vhlo.reshape_v1"(%arg472) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2466 = "vhlo.custom_call_v1"(%2465) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_20_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2467 = "vhlo.reshape_v1"(%2466) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2468 = "vhlo.broadcast_in_dim_v1"(%2467) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1152)
    %2469 = "vhlo.add_v1"(%2464, %2468) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1152)
    %2470 = "vhlo.reshape_v1"(%2469) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1153)
    %2471 = "vhlo.transpose_v1"(%2470) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1154)
    %2472 = "vhlo.convert_v1"(%2471) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1155)
    %2473 = "vhlo.transpose_v1"(%2472) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,80,257]{2,1,3,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc1156)
    %2474 = "vhlo.multiply_v1"(%2473, %6) : (!vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc1157)
    %2475 = "vhlo.dot_general_v2"(%2458, %2474) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1158)
    %2476 = "vhlo.convert_v1"(%2475) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc1159)
    %2477 = "vhlo.compare_v1"(%2476, %5) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1159)
    %2478 = "vhlo.not_v1"(%2477) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1160)
    %2479 = "vhlo.reduce_v1"(%2478, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("2096|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_21aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("2096|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_21aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1162)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1163)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc1161)
    %2480 = "vhlo.reshape_v1"(%2479) : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc1161)
    %2481 = "vhlo.not_v1"(%2480) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc1164)
    %2482 = "vhlo.reshape_v1"(%2481) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc1165)
    %2483 = "vhlo.broadcast_in_dim_v1"(%2482) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1165)
    %2484 = "vhlo.reduce_v1"(%2475, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2093|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_20aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2093|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_20aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1167)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc1166)
    %2485 = "vhlo.broadcast_in_dim_v1"(%2484) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1166)
    %2486 = "vhlo.subtract_v1"(%2475, %2485) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1166)
    %2487 = "vhlo.exponential_v2"(%2486) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1166)
    %2488 = "vhlo.reduce_v1"(%2487, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2093|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_20aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2093|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_20aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1168)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc1166)
    %2489 = "vhlo.broadcast_in_dim_v1"(%2488) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1166)
    %2490 = "vhlo.divide_v1"(%2487, %2489) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1166)
    %2491 = "vhlo.select_v1"(%2483, %4, %2490) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1169)
    %2492 = "vhlo.reshape_v1"(%arg142) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2493 = "vhlo.custom_call_v1"(%2492) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_20_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2494 = "vhlo.reshape_v1"(%2493) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2495 = "vhlo.transpose_v1"(%2494) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1170)
    %2496 = "vhlo.dot_general_v2"(%2443, %2495) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1171)
    %2497 = "vhlo.reshape_v1"(%2496) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1172)
    %2498 = "vhlo.reshape_v1"(%arg141) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2499 = "vhlo.custom_call_v1"(%2498) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_20_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2500 = "vhlo.reshape_v1"(%2499) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2501 = "vhlo.broadcast_in_dim_v1"(%2500) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1173)
    %2502 = "vhlo.add_v1"(%2497, %2501) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1173)
    %2503 = "vhlo.reshape_v1"(%2502) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1174)
    %2504 = "vhlo.transpose_v1"(%2503) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1175)
    %2505 = "vhlo.convert_v1"(%2504) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1176)
    %2506 = "vhlo.dot_general_v2"(%2491, %2505) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1177)
    %2507 = "vhlo.convert_v1"(%2506) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1178)
    %2508 = "vhlo.transpose_v1"(%2507) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,257,16,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1179)
    %2509 = "vhlo.reshape_v1"(%2508) : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1180)
    %2510 = "vhlo.reshape_v1"(%arg140) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2511 = "vhlo.custom_call_v1"(%2510) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_20_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2512 = "vhlo.reshape_v1"(%2511) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2513 = "vhlo.transpose_v1"(%2512) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1181)
    %2514 = "vhlo.dot_general_v2"(%2509, %2513) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1182)
    %2515 = "vhlo.reshape_v1"(%2514) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1180)
    %2516 = "vhlo.reshape_v1"(%arg139) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2517 = "vhlo.custom_call_v1"(%2516) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_20_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2518 = "vhlo.reshape_v1"(%2517) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2519 = "vhlo.broadcast_in_dim_v1"(%2518) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1183)
    %2520 = "vhlo.add_v1"(%2515, %2519) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1183)
    %2521 = "vhlo.add_v1"(%2435, %2520) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1184)
    %2522 = "vhlo.reshape_v1"(%arg138) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2523 = "vhlo.custom_call_v1"(%2522) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_20_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2524 = "vhlo.reshape_v1"(%2523) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2525 = "vhlo.reshape_v1"(%arg137) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2526 = "vhlo.custom_call_v1"(%2525) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_20_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2527 = "vhlo.reshape_v1"(%2526) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2528 = "vhlo.composite_v1"(%2521, %2524, %2527) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_47">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1185)
    %2529 = "vhlo.reshape_v1"(%2528) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1186)
    %2530 = "vhlo.reshape_v1"(%arg136) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %2531 = "vhlo.custom_call_v1"(%2530) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_20_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %2532 = "vhlo.reshape_v1"(%2531) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %2533 = "vhlo.transpose_v1"(%2532) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc1187)
    %2534 = "vhlo.dot_general_v2"(%2529, %2533) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc1188)
    %2535 = "vhlo.reshape_v1"(%2534) : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1186)
    %2536 = "vhlo.reshape_v1"(%arg135) : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc3)
    %2537 = "vhlo.custom_call_v1"(%2536) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_20_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc2)
    %2538 = "vhlo.reshape_v1"(%2537) : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc(#loc3)
    %2539 = "vhlo.broadcast_in_dim_v1"(%2538) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1189)
    %2540 = "vhlo.add_v1"(%2535, %2539) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1189)
    %2541 = "vhlo.composite_v1"(%2540) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_8">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1190)
    %2542 = "vhlo.reshape_v1"(%2541) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc1191)
    %2543 = "vhlo.reshape_v1"(%arg134) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %2544 = "vhlo.custom_call_v1"(%2543) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_20_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %2545 = "vhlo.reshape_v1"(%2544) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %2546 = "vhlo.transpose_v1"(%2545) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc1192)
    %2547 = "vhlo.dot_general_v2"(%2542, %2546) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1193)
    %2548 = "vhlo.reshape_v1"(%2547) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1191)
    %2549 = "vhlo.reshape_v1"(%arg133) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2550 = "vhlo.custom_call_v1"(%2549) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_20_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2551 = "vhlo.reshape_v1"(%2550) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2552 = "vhlo.broadcast_in_dim_v1"(%2551) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1194)
    %2553 = "vhlo.add_v1"(%2548, %2552) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1194)
    %2554 = "vhlo.add_v1"(%2521, %2553) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1195)
    %2555 = "vhlo.reshape_v1"(%arg132) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2556 = "vhlo.custom_call_v1"(%2555) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_21_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2557 = "vhlo.reshape_v1"(%2556) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2558 = "vhlo.reshape_v1"(%arg131) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2559 = "vhlo.custom_call_v1"(%2558) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_21_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2560 = "vhlo.reshape_v1"(%2559) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2561 = "vhlo.composite_v1"(%2554, %2557, %2560) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_55">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1196)
    %2562 = "vhlo.reshape_v1"(%2561) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1197)
    %2563 = "vhlo.reshape_v1"(%arg479) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2564 = "vhlo.custom_call_v1"(%2563) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_21_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2565 = "vhlo.reshape_v1"(%2564) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2566 = "vhlo.transpose_v1"(%2565) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1198)
    %2567 = "vhlo.dot_general_v2"(%2562, %2566) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1199)
    %2568 = "vhlo.reshape_v1"(%2567) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1197)
    %2569 = "vhlo.reshape_v1"(%arg478) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2570 = "vhlo.custom_call_v1"(%2569) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_21_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2571 = "vhlo.reshape_v1"(%2570) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2572 = "vhlo.broadcast_in_dim_v1"(%2571) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1200)
    %2573 = "vhlo.add_v1"(%2568, %2572) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1200)
    %2574 = "vhlo.reshape_v1"(%2573) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1201)
    %2575 = "vhlo.transpose_v1"(%2574) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1202)
    %2576 = "vhlo.convert_v1"(%2575) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1203)
    %2577 = "vhlo.multiply_v1"(%2576, %7) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1204)
    %2578 = "vhlo.reshape_v1"(%arg477) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2579 = "vhlo.custom_call_v1"(%2578) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_21_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2580 = "vhlo.reshape_v1"(%2579) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2581 = "vhlo.transpose_v1"(%2580) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1205)
    %2582 = "vhlo.dot_general_v2"(%2562, %2581) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1206)
    %2583 = "vhlo.reshape_v1"(%2582) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1207)
    %2584 = "vhlo.reshape_v1"(%arg476) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2585 = "vhlo.custom_call_v1"(%2584) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_21_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2586 = "vhlo.reshape_v1"(%2585) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2587 = "vhlo.broadcast_in_dim_v1"(%2586) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1208)
    %2588 = "vhlo.add_v1"(%2583, %2587) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1208)
    %2589 = "vhlo.reshape_v1"(%2588) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1209)
    %2590 = "vhlo.transpose_v1"(%2589) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1210)
    %2591 = "vhlo.convert_v1"(%2590) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1211)
    %2592 = "vhlo.transpose_v1"(%2591) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,80,257]{2,1,3,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc1212)
    %2593 = "vhlo.multiply_v1"(%2592, %6) : (!vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc1213)
    %2594 = "vhlo.dot_general_v2"(%2577, %2593) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1214)
    %2595 = "vhlo.convert_v1"(%2594) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc1215)
    %2596 = "vhlo.compare_v1"(%2595, %5) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1215)
    %2597 = "vhlo.not_v1"(%2596) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1216)
    %2598 = "vhlo.reduce_v1"(%2597, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("2170|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_22aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("2170|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_22aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1218)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1219)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc1217)
    %2599 = "vhlo.reshape_v1"(%2598) : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc1217)
    %2600 = "vhlo.not_v1"(%2599) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc1220)
    %2601 = "vhlo.reshape_v1"(%2600) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc1221)
    %2602 = "vhlo.broadcast_in_dim_v1"(%2601) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1221)
    %2603 = "vhlo.reduce_v1"(%2594, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2167|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_21aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2167|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_21aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1223)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc1222)
    %2604 = "vhlo.broadcast_in_dim_v1"(%2603) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1222)
    %2605 = "vhlo.subtract_v1"(%2594, %2604) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1222)
    %2606 = "vhlo.exponential_v2"(%2605) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1222)
    %2607 = "vhlo.reduce_v1"(%2606, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2167|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_21aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2167|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_21aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1224)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc1222)
    %2608 = "vhlo.broadcast_in_dim_v1"(%2607) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1222)
    %2609 = "vhlo.divide_v1"(%2606, %2608) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1222)
    %2610 = "vhlo.select_v1"(%2602, %4, %2609) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1225)
    %2611 = "vhlo.reshape_v1"(%arg130) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2612 = "vhlo.custom_call_v1"(%2611) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_21_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2613 = "vhlo.reshape_v1"(%2612) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2614 = "vhlo.transpose_v1"(%2613) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1226)
    %2615 = "vhlo.dot_general_v2"(%2562, %2614) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1227)
    %2616 = "vhlo.reshape_v1"(%2615) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1228)
    %2617 = "vhlo.reshape_v1"(%arg129) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2618 = "vhlo.custom_call_v1"(%2617) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_21_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2619 = "vhlo.reshape_v1"(%2618) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2620 = "vhlo.broadcast_in_dim_v1"(%2619) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1229)
    %2621 = "vhlo.add_v1"(%2616, %2620) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1229)
    %2622 = "vhlo.reshape_v1"(%2621) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1230)
    %2623 = "vhlo.transpose_v1"(%2622) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1231)
    %2624 = "vhlo.convert_v1"(%2623) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1232)
    %2625 = "vhlo.dot_general_v2"(%2610, %2624) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1233)
    %2626 = "vhlo.convert_v1"(%2625) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1234)
    %2627 = "vhlo.transpose_v1"(%2626) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,257,16,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1235)
    %2628 = "vhlo.reshape_v1"(%2627) : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1236)
    %2629 = "vhlo.reshape_v1"(%arg128) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2630 = "vhlo.custom_call_v1"(%2629) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_21_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2631 = "vhlo.reshape_v1"(%2630) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2632 = "vhlo.transpose_v1"(%2631) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1237)
    %2633 = "vhlo.dot_general_v2"(%2628, %2632) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1238)
    %2634 = "vhlo.reshape_v1"(%2633) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1236)
    %2635 = "vhlo.reshape_v1"(%arg127) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2636 = "vhlo.custom_call_v1"(%2635) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_21_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2637 = "vhlo.reshape_v1"(%2636) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2638 = "vhlo.broadcast_in_dim_v1"(%2637) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1239)
    %2639 = "vhlo.add_v1"(%2634, %2638) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1239)
    %2640 = "vhlo.add_v1"(%2554, %2639) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1240)
    %2641 = "vhlo.reshape_v1"(%arg126) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2642 = "vhlo.custom_call_v1"(%2641) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_21_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2643 = "vhlo.reshape_v1"(%2642) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2644 = "vhlo.reshape_v1"(%arg125) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2645 = "vhlo.custom_call_v1"(%2644) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_21_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2646 = "vhlo.reshape_v1"(%2645) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2647 = "vhlo.composite_v1"(%2640, %2643, %2646) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_18">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1241)
    %2648 = "vhlo.reshape_v1"(%2647) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1242)
    %2649 = "vhlo.reshape_v1"(%arg124) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %2650 = "vhlo.custom_call_v1"(%2649) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_21_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %2651 = "vhlo.reshape_v1"(%2650) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %2652 = "vhlo.transpose_v1"(%2651) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc1243)
    %2653 = "vhlo.dot_general_v2"(%2648, %2652) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc1244)
    %2654 = "vhlo.reshape_v1"(%2653) : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1242)
    %2655 = "vhlo.reshape_v1"(%arg123) : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc3)
    %2656 = "vhlo.custom_call_v1"(%2655) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_21_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc2)
    %2657 = "vhlo.reshape_v1"(%2656) : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc(#loc3)
    %2658 = "vhlo.broadcast_in_dim_v1"(%2657) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1245)
    %2659 = "vhlo.add_v1"(%2654, %2658) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1245)
    %2660 = "vhlo.composite_v1"(%2659) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_7">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1246)
    %2661 = "vhlo.reshape_v1"(%2660) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc1247)
    %2662 = "vhlo.reshape_v1"(%arg122) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %2663 = "vhlo.custom_call_v1"(%2662) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_21_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %2664 = "vhlo.reshape_v1"(%2663) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %2665 = "vhlo.transpose_v1"(%2664) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc1248)
    %2666 = "vhlo.dot_general_v2"(%2661, %2665) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1249)
    %2667 = "vhlo.reshape_v1"(%2666) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1247)
    %2668 = "vhlo.reshape_v1"(%arg121) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2669 = "vhlo.custom_call_v1"(%2668) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_21_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2670 = "vhlo.reshape_v1"(%2669) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2671 = "vhlo.broadcast_in_dim_v1"(%2670) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1250)
    %2672 = "vhlo.add_v1"(%2667, %2671) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1250)
    %2673 = "vhlo.add_v1"(%2640, %2672) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1251)
    %2674 = "vhlo.reshape_v1"(%arg120) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2675 = "vhlo.custom_call_v1"(%2674) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_22_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2676 = "vhlo.reshape_v1"(%2675) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2677 = "vhlo.reshape_v1"(%arg119) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2678 = "vhlo.custom_call_v1"(%2677) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_22_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2679 = "vhlo.reshape_v1"(%2678) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2680 = "vhlo.composite_v1"(%2673, %2676, %2679) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_16">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1252)
    %2681 = "vhlo.reshape_v1"(%2680) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1253)
    %2682 = "vhlo.reshape_v1"(%arg483) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2683 = "vhlo.custom_call_v1"(%2682) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_22_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2684 = "vhlo.reshape_v1"(%2683) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2685 = "vhlo.transpose_v1"(%2684) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1254)
    %2686 = "vhlo.dot_general_v2"(%2681, %2685) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1255)
    %2687 = "vhlo.reshape_v1"(%2686) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1253)
    %2688 = "vhlo.reshape_v1"(%arg482) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2689 = "vhlo.custom_call_v1"(%2688) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_22_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2690 = "vhlo.reshape_v1"(%2689) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2691 = "vhlo.broadcast_in_dim_v1"(%2690) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1256)
    %2692 = "vhlo.add_v1"(%2687, %2691) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1256)
    %2693 = "vhlo.reshape_v1"(%2692) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1257)
    %2694 = "vhlo.transpose_v1"(%2693) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1258)
    %2695 = "vhlo.convert_v1"(%2694) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1259)
    %2696 = "vhlo.multiply_v1"(%2695, %7) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1260)
    %2697 = "vhlo.reshape_v1"(%arg481) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2698 = "vhlo.custom_call_v1"(%2697) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_22_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2699 = "vhlo.reshape_v1"(%2698) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2700 = "vhlo.transpose_v1"(%2699) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1261)
    %2701 = "vhlo.dot_general_v2"(%2681, %2700) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1262)
    %2702 = "vhlo.reshape_v1"(%2701) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1263)
    %2703 = "vhlo.reshape_v1"(%arg480) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2704 = "vhlo.custom_call_v1"(%2703) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_22_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2705 = "vhlo.reshape_v1"(%2704) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2706 = "vhlo.broadcast_in_dim_v1"(%2705) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1264)
    %2707 = "vhlo.add_v1"(%2702, %2706) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1264)
    %2708 = "vhlo.reshape_v1"(%2707) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1265)
    %2709 = "vhlo.transpose_v1"(%2708) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1266)
    %2710 = "vhlo.convert_v1"(%2709) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1267)
    %2711 = "vhlo.transpose_v1"(%2710) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,80,257]{2,1,3,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc1268)
    %2712 = "vhlo.multiply_v1"(%2711, %6) : (!vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc1269)
    %2713 = "vhlo.dot_general_v2"(%2696, %2712) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1270)
    %2714 = "vhlo.convert_v1"(%2713) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc1271)
    %2715 = "vhlo.compare_v1"(%2714, %5) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1271)
    %2716 = "vhlo.not_v1"(%2715) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1272)
    %2717 = "vhlo.reduce_v1"(%2716, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("2244|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_23aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("2244|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_23aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1274)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1275)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc1273)
    %2718 = "vhlo.reshape_v1"(%2717) : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc1273)
    %2719 = "vhlo.not_v1"(%2718) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc1276)
    %2720 = "vhlo.reshape_v1"(%2719) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc1277)
    %2721 = "vhlo.broadcast_in_dim_v1"(%2720) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1277)
    %2722 = "vhlo.reduce_v1"(%2713, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2241|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_22aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2241|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_22aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1279)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc1278)
    %2723 = "vhlo.broadcast_in_dim_v1"(%2722) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1278)
    %2724 = "vhlo.subtract_v1"(%2713, %2723) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1278)
    %2725 = "vhlo.exponential_v2"(%2724) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1278)
    %2726 = "vhlo.reduce_v1"(%2725, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2241|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_22aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2241|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_22aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1280)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc1278)
    %2727 = "vhlo.broadcast_in_dim_v1"(%2726) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1278)
    %2728 = "vhlo.divide_v1"(%2725, %2727) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1278)
    %2729 = "vhlo.select_v1"(%2721, %4, %2728) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1281)
    %2730 = "vhlo.reshape_v1"(%arg118) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2731 = "vhlo.custom_call_v1"(%2730) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_22_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2732 = "vhlo.reshape_v1"(%2731) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2733 = "vhlo.transpose_v1"(%2732) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1282)
    %2734 = "vhlo.dot_general_v2"(%2681, %2733) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1283)
    %2735 = "vhlo.reshape_v1"(%2734) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1284)
    %2736 = "vhlo.reshape_v1"(%arg117) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2737 = "vhlo.custom_call_v1"(%2736) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_22_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2738 = "vhlo.reshape_v1"(%2737) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2739 = "vhlo.broadcast_in_dim_v1"(%2738) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1285)
    %2740 = "vhlo.add_v1"(%2735, %2739) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1285)
    %2741 = "vhlo.reshape_v1"(%2740) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1286)
    %2742 = "vhlo.transpose_v1"(%2741) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1287)
    %2743 = "vhlo.convert_v1"(%2742) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1288)
    %2744 = "vhlo.dot_general_v2"(%2729, %2743) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1289)
    %2745 = "vhlo.convert_v1"(%2744) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1290)
    %2746 = "vhlo.transpose_v1"(%2745) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,257,16,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1291)
    %2747 = "vhlo.reshape_v1"(%2746) : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1292)
    %2748 = "vhlo.reshape_v1"(%arg116) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2749 = "vhlo.custom_call_v1"(%2748) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_22_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2750 = "vhlo.reshape_v1"(%2749) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2751 = "vhlo.transpose_v1"(%2750) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1293)
    %2752 = "vhlo.dot_general_v2"(%2747, %2751) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1294)
    %2753 = "vhlo.reshape_v1"(%2752) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1292)
    %2754 = "vhlo.reshape_v1"(%arg115) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2755 = "vhlo.custom_call_v1"(%2754) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_22_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2756 = "vhlo.reshape_v1"(%2755) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2757 = "vhlo.broadcast_in_dim_v1"(%2756) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1295)
    %2758 = "vhlo.add_v1"(%2753, %2757) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1295)
    %2759 = "vhlo.add_v1"(%2673, %2758) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1296)
    %2760 = "vhlo.reshape_v1"(%arg114) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2761 = "vhlo.custom_call_v1"(%2760) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_22_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2762 = "vhlo.reshape_v1"(%2761) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2763 = "vhlo.reshape_v1"(%arg113) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2764 = "vhlo.custom_call_v1"(%2763) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_22_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2765 = "vhlo.reshape_v1"(%2764) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2766 = "vhlo.composite_v1"(%2759, %2762, %2765) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_24">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1297)
    %2767 = "vhlo.reshape_v1"(%2766) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1298)
    %2768 = "vhlo.reshape_v1"(%arg112) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %2769 = "vhlo.custom_call_v1"(%2768) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_22_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %2770 = "vhlo.reshape_v1"(%2769) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %2771 = "vhlo.transpose_v1"(%2770) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc1299)
    %2772 = "vhlo.dot_general_v2"(%2767, %2771) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc1300)
    %2773 = "vhlo.reshape_v1"(%2772) : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1298)
    %2774 = "vhlo.reshape_v1"(%arg111) : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc3)
    %2775 = "vhlo.custom_call_v1"(%2774) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_22_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc2)
    %2776 = "vhlo.reshape_v1"(%2775) : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc(#loc3)
    %2777 = "vhlo.broadcast_in_dim_v1"(%2776) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1301)
    %2778 = "vhlo.add_v1"(%2773, %2777) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1301)
    %2779 = "vhlo.composite_v1"(%2778) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_14">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1302)
    %2780 = "vhlo.reshape_v1"(%2779) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc1303)
    %2781 = "vhlo.reshape_v1"(%arg110) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %2782 = "vhlo.custom_call_v1"(%2781) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_22_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %2783 = "vhlo.reshape_v1"(%2782) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %2784 = "vhlo.transpose_v1"(%2783) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc1304)
    %2785 = "vhlo.dot_general_v2"(%2780, %2784) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1305)
    %2786 = "vhlo.reshape_v1"(%2785) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1303)
    %2787 = "vhlo.reshape_v1"(%arg109) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2788 = "vhlo.custom_call_v1"(%2787) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_22_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2789 = "vhlo.reshape_v1"(%2788) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2790 = "vhlo.broadcast_in_dim_v1"(%2789) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1306)
    %2791 = "vhlo.add_v1"(%2786, %2790) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1306)
    %2792 = "vhlo.add_v1"(%2759, %2791) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1307)
    %2793 = "vhlo.reshape_v1"(%arg108) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2794 = "vhlo.custom_call_v1"(%2793) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_23_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2795 = "vhlo.reshape_v1"(%2794) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2796 = "vhlo.reshape_v1"(%arg107) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2797 = "vhlo.custom_call_v1"(%2796) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_23_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2798 = "vhlo.reshape_v1"(%2797) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2799 = "vhlo.composite_v1"(%2792, %2795, %2798) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_58">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1308)
    %2800 = "vhlo.reshape_v1"(%2799) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1309)
    %2801 = "vhlo.reshape_v1"(%arg487) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2802 = "vhlo.custom_call_v1"(%2801) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_23_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2803 = "vhlo.reshape_v1"(%2802) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2804 = "vhlo.transpose_v1"(%2803) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1310)
    %2805 = "vhlo.dot_general_v2"(%2800, %2804) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1311)
    %2806 = "vhlo.reshape_v1"(%2805) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1309)
    %2807 = "vhlo.reshape_v1"(%arg486) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2808 = "vhlo.custom_call_v1"(%2807) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_23_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2809 = "vhlo.reshape_v1"(%2808) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2810 = "vhlo.broadcast_in_dim_v1"(%2809) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1312)
    %2811 = "vhlo.add_v1"(%2806, %2810) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1312)
    %2812 = "vhlo.reshape_v1"(%2811) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1313)
    %2813 = "vhlo.transpose_v1"(%2812) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1314)
    %2814 = "vhlo.convert_v1"(%2813) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1315)
    %2815 = "vhlo.multiply_v1"(%2814, %7) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1316)
    %2816 = "vhlo.reshape_v1"(%arg485) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2817 = "vhlo.custom_call_v1"(%2816) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_23_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2818 = "vhlo.reshape_v1"(%2817) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2819 = "vhlo.transpose_v1"(%2818) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1317)
    %2820 = "vhlo.dot_general_v2"(%2800, %2819) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1318)
    %2821 = "vhlo.reshape_v1"(%2820) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1319)
    %2822 = "vhlo.reshape_v1"(%arg484) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2823 = "vhlo.custom_call_v1"(%2822) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_23_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2824 = "vhlo.reshape_v1"(%2823) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2825 = "vhlo.broadcast_in_dim_v1"(%2824) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1320)
    %2826 = "vhlo.add_v1"(%2821, %2825) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1320)
    %2827 = "vhlo.reshape_v1"(%2826) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1321)
    %2828 = "vhlo.transpose_v1"(%2827) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1322)
    %2829 = "vhlo.convert_v1"(%2828) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1323)
    %2830 = "vhlo.transpose_v1"(%2829) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,80,257]{2,1,3,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc1324)
    %2831 = "vhlo.multiply_v1"(%2830, %6) : (!vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc1325)
    %2832 = "vhlo.dot_general_v2"(%2815, %2831) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1326)
    %2833 = "vhlo.convert_v1"(%2832) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc1327)
    %2834 = "vhlo.compare_v1"(%2833, %5) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1327)
    %2835 = "vhlo.not_v1"(%2834) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1328)
    %2836 = "vhlo.reduce_v1"(%2835, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("2318|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_24aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("2318|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_24aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1330)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1331)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc1329)
    %2837 = "vhlo.reshape_v1"(%2836) : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc1329)
    %2838 = "vhlo.not_v1"(%2837) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc1332)
    %2839 = "vhlo.reshape_v1"(%2838) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc1333)
    %2840 = "vhlo.broadcast_in_dim_v1"(%2839) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1333)
    %2841 = "vhlo.reduce_v1"(%2832, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2315|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_23aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2315|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_23aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1335)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc1334)
    %2842 = "vhlo.broadcast_in_dim_v1"(%2841) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1334)
    %2843 = "vhlo.subtract_v1"(%2832, %2842) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1334)
    %2844 = "vhlo.exponential_v2"(%2843) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1334)
    %2845 = "vhlo.reduce_v1"(%2844, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2315|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_23aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2315|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_23aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1336)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc1334)
    %2846 = "vhlo.broadcast_in_dim_v1"(%2845) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1334)
    %2847 = "vhlo.divide_v1"(%2844, %2846) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1334)
    %2848 = "vhlo.select_v1"(%2840, %4, %2847) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1337)
    %2849 = "vhlo.reshape_v1"(%arg106) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2850 = "vhlo.custom_call_v1"(%2849) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_23_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2851 = "vhlo.reshape_v1"(%2850) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2852 = "vhlo.transpose_v1"(%2851) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1338)
    %2853 = "vhlo.dot_general_v2"(%2800, %2852) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1339)
    %2854 = "vhlo.reshape_v1"(%2853) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1340)
    %2855 = "vhlo.reshape_v1"(%arg105) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2856 = "vhlo.custom_call_v1"(%2855) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_23_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2857 = "vhlo.reshape_v1"(%2856) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2858 = "vhlo.broadcast_in_dim_v1"(%2857) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1341)
    %2859 = "vhlo.add_v1"(%2854, %2858) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1341)
    %2860 = "vhlo.reshape_v1"(%2859) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1342)
    %2861 = "vhlo.transpose_v1"(%2860) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1343)
    %2862 = "vhlo.convert_v1"(%2861) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1344)
    %2863 = "vhlo.dot_general_v2"(%2848, %2862) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1345)
    %2864 = "vhlo.convert_v1"(%2863) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1346)
    %2865 = "vhlo.transpose_v1"(%2864) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,257,16,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1347)
    %2866 = "vhlo.reshape_v1"(%2865) : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1348)
    %2867 = "vhlo.reshape_v1"(%arg104) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2868 = "vhlo.custom_call_v1"(%2867) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_23_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2869 = "vhlo.reshape_v1"(%2868) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2870 = "vhlo.transpose_v1"(%2869) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1349)
    %2871 = "vhlo.dot_general_v2"(%2866, %2870) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1350)
    %2872 = "vhlo.reshape_v1"(%2871) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1348)
    %2873 = "vhlo.reshape_v1"(%arg103) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2874 = "vhlo.custom_call_v1"(%2873) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_23_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2875 = "vhlo.reshape_v1"(%2874) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2876 = "vhlo.broadcast_in_dim_v1"(%2875) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1351)
    %2877 = "vhlo.add_v1"(%2872, %2876) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1351)
    %2878 = "vhlo.add_v1"(%2792, %2877) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1352)
    %2879 = "vhlo.reshape_v1"(%arg102) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2880 = "vhlo.custom_call_v1"(%2879) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_23_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2881 = "vhlo.reshape_v1"(%2880) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2882 = "vhlo.reshape_v1"(%arg101) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2883 = "vhlo.custom_call_v1"(%2882) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_23_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2884 = "vhlo.reshape_v1"(%2883) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2885 = "vhlo.composite_v1"(%2878, %2881, %2884) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_15">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1353)
    %2886 = "vhlo.reshape_v1"(%2885) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1354)
    %2887 = "vhlo.reshape_v1"(%arg100) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %2888 = "vhlo.custom_call_v1"(%2887) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_23_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %2889 = "vhlo.reshape_v1"(%2888) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %2890 = "vhlo.transpose_v1"(%2889) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc1355)
    %2891 = "vhlo.dot_general_v2"(%2886, %2890) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc1356)
    %2892 = "vhlo.reshape_v1"(%2891) : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1354)
    %2893 = "vhlo.reshape_v1"(%arg99) : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc3)
    %2894 = "vhlo.custom_call_v1"(%2893) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_23_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc2)
    %2895 = "vhlo.reshape_v1"(%2894) : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc(#loc3)
    %2896 = "vhlo.broadcast_in_dim_v1"(%2895) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1357)
    %2897 = "vhlo.add_v1"(%2892, %2896) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1357)
    %2898 = "vhlo.composite_v1"(%2897) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_6">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1358)
    %2899 = "vhlo.reshape_v1"(%2898) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc1359)
    %2900 = "vhlo.reshape_v1"(%arg98) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %2901 = "vhlo.custom_call_v1"(%2900) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_23_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %2902 = "vhlo.reshape_v1"(%2901) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %2903 = "vhlo.transpose_v1"(%2902) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc1360)
    %2904 = "vhlo.dot_general_v2"(%2899, %2903) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1361)
    %2905 = "vhlo.reshape_v1"(%2904) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1359)
    %2906 = "vhlo.reshape_v1"(%arg97) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2907 = "vhlo.custom_call_v1"(%2906) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_23_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2908 = "vhlo.reshape_v1"(%2907) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2909 = "vhlo.broadcast_in_dim_v1"(%2908) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1362)
    %2910 = "vhlo.add_v1"(%2905, %2909) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1362)
    %2911 = "vhlo.add_v1"(%2878, %2910) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1363)
    %2912 = "vhlo.reshape_v1"(%arg96) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2913 = "vhlo.custom_call_v1"(%2912) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_24_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2914 = "vhlo.reshape_v1"(%2913) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2915 = "vhlo.reshape_v1"(%arg95) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2916 = "vhlo.custom_call_v1"(%2915) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_24_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2917 = "vhlo.reshape_v1"(%2916) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2918 = "vhlo.composite_v1"(%2911, %2914, %2917) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_14">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1364)
    %2919 = "vhlo.reshape_v1"(%2918) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1365)
    %2920 = "vhlo.reshape_v1"(%arg491) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2921 = "vhlo.custom_call_v1"(%2920) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_24_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2922 = "vhlo.reshape_v1"(%2921) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2923 = "vhlo.transpose_v1"(%2922) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1366)
    %2924 = "vhlo.dot_general_v2"(%2919, %2923) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1367)
    %2925 = "vhlo.reshape_v1"(%2924) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1365)
    %2926 = "vhlo.reshape_v1"(%arg490) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2927 = "vhlo.custom_call_v1"(%2926) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_24_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2928 = "vhlo.reshape_v1"(%2927) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2929 = "vhlo.broadcast_in_dim_v1"(%2928) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1368)
    %2930 = "vhlo.add_v1"(%2925, %2929) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1368)
    %2931 = "vhlo.reshape_v1"(%2930) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1369)
    %2932 = "vhlo.transpose_v1"(%2931) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1370)
    %2933 = "vhlo.convert_v1"(%2932) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1371)
    %2934 = "vhlo.multiply_v1"(%2933, %7) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1372)
    %2935 = "vhlo.reshape_v1"(%arg489) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2936 = "vhlo.custom_call_v1"(%2935) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_24_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2937 = "vhlo.reshape_v1"(%2936) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2938 = "vhlo.transpose_v1"(%2937) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1373)
    %2939 = "vhlo.dot_general_v2"(%2919, %2938) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1374)
    %2940 = "vhlo.reshape_v1"(%2939) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1375)
    %2941 = "vhlo.reshape_v1"(%arg488) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2942 = "vhlo.custom_call_v1"(%2941) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_24_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2943 = "vhlo.reshape_v1"(%2942) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2944 = "vhlo.broadcast_in_dim_v1"(%2943) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1376)
    %2945 = "vhlo.add_v1"(%2940, %2944) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1376)
    %2946 = "vhlo.reshape_v1"(%2945) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1377)
    %2947 = "vhlo.transpose_v1"(%2946) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1378)
    %2948 = "vhlo.convert_v1"(%2947) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1379)
    %2949 = "vhlo.transpose_v1"(%2948) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,80,257]{2,1,3,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc1380)
    %2950 = "vhlo.multiply_v1"(%2949, %6) : (!vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc1381)
    %2951 = "vhlo.dot_general_v2"(%2934, %2950) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1382)
    %2952 = "vhlo.convert_v1"(%2951) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc1383)
    %2953 = "vhlo.compare_v1"(%2952, %5) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1383)
    %2954 = "vhlo.not_v1"(%2953) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1384)
    %2955 = "vhlo.reduce_v1"(%2954, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("2392|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_25aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("2392|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_25aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1386)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1387)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc1385)
    %2956 = "vhlo.reshape_v1"(%2955) : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc1385)
    %2957 = "vhlo.not_v1"(%2956) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc1388)
    %2958 = "vhlo.reshape_v1"(%2957) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc1389)
    %2959 = "vhlo.broadcast_in_dim_v1"(%2958) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1389)
    %2960 = "vhlo.reduce_v1"(%2951, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2389|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_24aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2389|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_24aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1391)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc1390)
    %2961 = "vhlo.broadcast_in_dim_v1"(%2960) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1390)
    %2962 = "vhlo.subtract_v1"(%2951, %2961) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1390)
    %2963 = "vhlo.exponential_v2"(%2962) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1390)
    %2964 = "vhlo.reduce_v1"(%2963, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2389|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_24aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2389|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_24aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1392)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc1390)
    %2965 = "vhlo.broadcast_in_dim_v1"(%2964) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1390)
    %2966 = "vhlo.divide_v1"(%2963, %2965) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1390)
    %2967 = "vhlo.select_v1"(%2959, %4, %2966) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1393)
    %2968 = "vhlo.reshape_v1"(%arg94) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2969 = "vhlo.custom_call_v1"(%2968) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_24_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2970 = "vhlo.reshape_v1"(%2969) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2971 = "vhlo.transpose_v1"(%2970) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1394)
    %2972 = "vhlo.dot_general_v2"(%2919, %2971) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1395)
    %2973 = "vhlo.reshape_v1"(%2972) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1396)
    %2974 = "vhlo.reshape_v1"(%arg93) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2975 = "vhlo.custom_call_v1"(%2974) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_24_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2976 = "vhlo.reshape_v1"(%2975) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2977 = "vhlo.broadcast_in_dim_v1"(%2976) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1397)
    %2978 = "vhlo.add_v1"(%2973, %2977) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1397)
    %2979 = "vhlo.reshape_v1"(%2978) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1398)
    %2980 = "vhlo.transpose_v1"(%2979) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1399)
    %2981 = "vhlo.convert_v1"(%2980) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1400)
    %2982 = "vhlo.dot_general_v2"(%2967, %2981) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1401)
    %2983 = "vhlo.convert_v1"(%2982) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1402)
    %2984 = "vhlo.transpose_v1"(%2983) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,257,16,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1403)
    %2985 = "vhlo.reshape_v1"(%2984) : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1404)
    %2986 = "vhlo.reshape_v1"(%arg92) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2987 = "vhlo.custom_call_v1"(%2986) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_24_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %2988 = "vhlo.reshape_v1"(%2987) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %2989 = "vhlo.transpose_v1"(%2988) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1405)
    %2990 = "vhlo.dot_general_v2"(%2985, %2989) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1406)
    %2991 = "vhlo.reshape_v1"(%2990) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1404)
    %2992 = "vhlo.reshape_v1"(%arg91) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2993 = "vhlo.custom_call_v1"(%2992) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_24_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %2994 = "vhlo.reshape_v1"(%2993) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %2995 = "vhlo.broadcast_in_dim_v1"(%2994) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1407)
    %2996 = "vhlo.add_v1"(%2991, %2995) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1407)
    %2997 = "vhlo.add_v1"(%2911, %2996) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1408)
    %2998 = "vhlo.reshape_v1"(%arg90) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %2999 = "vhlo.custom_call_v1"(%2998) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_24_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3000 = "vhlo.reshape_v1"(%2999) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3001 = "vhlo.reshape_v1"(%arg89) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3002 = "vhlo.custom_call_v1"(%3001) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_24_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3003 = "vhlo.reshape_v1"(%3002) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3004 = "vhlo.composite_v1"(%2997, %3000, %3003) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_13">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1409)
    %3005 = "vhlo.reshape_v1"(%3004) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1410)
    %3006 = "vhlo.reshape_v1"(%arg88) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %3007 = "vhlo.custom_call_v1"(%3006) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_24_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %3008 = "vhlo.reshape_v1"(%3007) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %3009 = "vhlo.transpose_v1"(%3008) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc1411)
    %3010 = "vhlo.dot_general_v2"(%3005, %3009) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc1412)
    %3011 = "vhlo.reshape_v1"(%3010) : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1410)
    %3012 = "vhlo.reshape_v1"(%arg87) : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc3)
    %3013 = "vhlo.custom_call_v1"(%3012) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_24_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc2)
    %3014 = "vhlo.reshape_v1"(%3013) : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc(#loc3)
    %3015 = "vhlo.broadcast_in_dim_v1"(%3014) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1413)
    %3016 = "vhlo.add_v1"(%3011, %3015) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1413)
    %3017 = "vhlo.composite_v1"(%3016) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_5">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1414)
    %3018 = "vhlo.reshape_v1"(%3017) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc1415)
    %3019 = "vhlo.reshape_v1"(%arg86) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %3020 = "vhlo.custom_call_v1"(%3019) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_24_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %3021 = "vhlo.reshape_v1"(%3020) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %3022 = "vhlo.transpose_v1"(%3021) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc1416)
    %3023 = "vhlo.dot_general_v2"(%3018, %3022) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1417)
    %3024 = "vhlo.reshape_v1"(%3023) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1415)
    %3025 = "vhlo.reshape_v1"(%arg85) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3026 = "vhlo.custom_call_v1"(%3025) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_24_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3027 = "vhlo.reshape_v1"(%3026) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3028 = "vhlo.broadcast_in_dim_v1"(%3027) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1418)
    %3029 = "vhlo.add_v1"(%3024, %3028) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1418)
    %3030 = "vhlo.add_v1"(%2997, %3029) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1419)
    %3031 = "vhlo.reshape_v1"(%arg84) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3032 = "vhlo.custom_call_v1"(%3031) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_25_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3033 = "vhlo.reshape_v1"(%3032) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3034 = "vhlo.reshape_v1"(%arg83) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3035 = "vhlo.custom_call_v1"(%3034) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_25_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3036 = "vhlo.reshape_v1"(%3035) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3037 = "vhlo.composite_v1"(%3030, %3033, %3036) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_12">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1420)
    %3038 = "vhlo.reshape_v1"(%3037) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1421)
    %3039 = "vhlo.reshape_v1"(%arg495) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3040 = "vhlo.custom_call_v1"(%3039) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_25_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3041 = "vhlo.reshape_v1"(%3040) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3042 = "vhlo.transpose_v1"(%3041) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1422)
    %3043 = "vhlo.dot_general_v2"(%3038, %3042) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1423)
    %3044 = "vhlo.reshape_v1"(%3043) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1421)
    %3045 = "vhlo.reshape_v1"(%arg494) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3046 = "vhlo.custom_call_v1"(%3045) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_25_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3047 = "vhlo.reshape_v1"(%3046) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3048 = "vhlo.broadcast_in_dim_v1"(%3047) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1424)
    %3049 = "vhlo.add_v1"(%3044, %3048) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1424)
    %3050 = "vhlo.reshape_v1"(%3049) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1425)
    %3051 = "vhlo.transpose_v1"(%3050) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1426)
    %3052 = "vhlo.convert_v1"(%3051) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1427)
    %3053 = "vhlo.multiply_v1"(%3052, %7) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1428)
    %3054 = "vhlo.reshape_v1"(%arg493) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3055 = "vhlo.custom_call_v1"(%3054) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_25_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3056 = "vhlo.reshape_v1"(%3055) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3057 = "vhlo.transpose_v1"(%3056) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1429)
    %3058 = "vhlo.dot_general_v2"(%3038, %3057) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1430)
    %3059 = "vhlo.reshape_v1"(%3058) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1431)
    %3060 = "vhlo.reshape_v1"(%arg492) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3061 = "vhlo.custom_call_v1"(%3060) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_25_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3062 = "vhlo.reshape_v1"(%3061) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3063 = "vhlo.broadcast_in_dim_v1"(%3062) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1432)
    %3064 = "vhlo.add_v1"(%3059, %3063) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1432)
    %3065 = "vhlo.reshape_v1"(%3064) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1433)
    %3066 = "vhlo.transpose_v1"(%3065) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1434)
    %3067 = "vhlo.convert_v1"(%3066) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1435)
    %3068 = "vhlo.transpose_v1"(%3067) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,80,257]{2,1,3,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc1436)
    %3069 = "vhlo.multiply_v1"(%3068, %6) : (!vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc1437)
    %3070 = "vhlo.dot_general_v2"(%3053, %3069) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1438)
    %3071 = "vhlo.convert_v1"(%3070) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc1439)
    %3072 = "vhlo.compare_v1"(%3071, %5) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1439)
    %3073 = "vhlo.not_v1"(%3072) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1440)
    %3074 = "vhlo.reduce_v1"(%3073, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("2466|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_26aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("2466|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_26aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1442)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1443)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc1441)
    %3075 = "vhlo.reshape_v1"(%3074) : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc1441)
    %3076 = "vhlo.not_v1"(%3075) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc1444)
    %3077 = "vhlo.reshape_v1"(%3076) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc1445)
    %3078 = "vhlo.broadcast_in_dim_v1"(%3077) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1445)
    %3079 = "vhlo.reduce_v1"(%3070, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2463|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_25aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2463|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_25aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1447)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc1446)
    %3080 = "vhlo.broadcast_in_dim_v1"(%3079) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1446)
    %3081 = "vhlo.subtract_v1"(%3070, %3080) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1446)
    %3082 = "vhlo.exponential_v2"(%3081) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1446)
    %3083 = "vhlo.reduce_v1"(%3082, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2463|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_25aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2463|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_25aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1448)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc1446)
    %3084 = "vhlo.broadcast_in_dim_v1"(%3083) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1446)
    %3085 = "vhlo.divide_v1"(%3082, %3084) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1446)
    %3086 = "vhlo.select_v1"(%3078, %4, %3085) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1449)
    %3087 = "vhlo.reshape_v1"(%arg82) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3088 = "vhlo.custom_call_v1"(%3087) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_25_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3089 = "vhlo.reshape_v1"(%3088) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3090 = "vhlo.transpose_v1"(%3089) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1450)
    %3091 = "vhlo.dot_general_v2"(%3038, %3090) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1451)
    %3092 = "vhlo.reshape_v1"(%3091) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1452)
    %3093 = "vhlo.reshape_v1"(%arg81) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3094 = "vhlo.custom_call_v1"(%3093) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_25_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3095 = "vhlo.reshape_v1"(%3094) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3096 = "vhlo.broadcast_in_dim_v1"(%3095) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1453)
    %3097 = "vhlo.add_v1"(%3092, %3096) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1453)
    %3098 = "vhlo.reshape_v1"(%3097) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1454)
    %3099 = "vhlo.transpose_v1"(%3098) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1455)
    %3100 = "vhlo.convert_v1"(%3099) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1456)
    %3101 = "vhlo.dot_general_v2"(%3086, %3100) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1457)
    %3102 = "vhlo.convert_v1"(%3101) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1458)
    %3103 = "vhlo.transpose_v1"(%3102) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,257,16,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1459)
    %3104 = "vhlo.reshape_v1"(%3103) : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1460)
    %3105 = "vhlo.reshape_v1"(%arg80) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3106 = "vhlo.custom_call_v1"(%3105) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_25_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3107 = "vhlo.reshape_v1"(%3106) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3108 = "vhlo.transpose_v1"(%3107) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1461)
    %3109 = "vhlo.dot_general_v2"(%3104, %3108) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1462)
    %3110 = "vhlo.reshape_v1"(%3109) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1460)
    %3111 = "vhlo.reshape_v1"(%arg79) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3112 = "vhlo.custom_call_v1"(%3111) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_25_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3113 = "vhlo.reshape_v1"(%3112) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3114 = "vhlo.broadcast_in_dim_v1"(%3113) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1463)
    %3115 = "vhlo.add_v1"(%3110, %3114) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1463)
    %3116 = "vhlo.add_v1"(%3030, %3115) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1464)
    %3117 = "vhlo.reshape_v1"(%arg78) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3118 = "vhlo.custom_call_v1"(%3117) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_25_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3119 = "vhlo.reshape_v1"(%3118) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3120 = "vhlo.reshape_v1"(%arg77) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3121 = "vhlo.custom_call_v1"(%3120) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_25_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3122 = "vhlo.reshape_v1"(%3121) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3123 = "vhlo.composite_v1"(%3116, %3119, %3122) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_11">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1465)
    %3124 = "vhlo.reshape_v1"(%3123) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1466)
    %3125 = "vhlo.reshape_v1"(%arg76) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %3126 = "vhlo.custom_call_v1"(%3125) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_25_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %3127 = "vhlo.reshape_v1"(%3126) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %3128 = "vhlo.transpose_v1"(%3127) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc1467)
    %3129 = "vhlo.dot_general_v2"(%3124, %3128) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc1468)
    %3130 = "vhlo.reshape_v1"(%3129) : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1466)
    %3131 = "vhlo.reshape_v1"(%arg75) : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc3)
    %3132 = "vhlo.custom_call_v1"(%3131) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_25_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc2)
    %3133 = "vhlo.reshape_v1"(%3132) : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc(#loc3)
    %3134 = "vhlo.broadcast_in_dim_v1"(%3133) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1469)
    %3135 = "vhlo.add_v1"(%3130, %3134) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1469)
    %3136 = "vhlo.composite_v1"(%3135) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_4">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1470)
    %3137 = "vhlo.reshape_v1"(%3136) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc1471)
    %3138 = "vhlo.reshape_v1"(%arg74) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %3139 = "vhlo.custom_call_v1"(%3138) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_25_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %3140 = "vhlo.reshape_v1"(%3139) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %3141 = "vhlo.transpose_v1"(%3140) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc1472)
    %3142 = "vhlo.dot_general_v2"(%3137, %3141) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1473)
    %3143 = "vhlo.reshape_v1"(%3142) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1471)
    %3144 = "vhlo.reshape_v1"(%arg73) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3145 = "vhlo.custom_call_v1"(%3144) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_25_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3146 = "vhlo.reshape_v1"(%3145) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3147 = "vhlo.broadcast_in_dim_v1"(%3146) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1474)
    %3148 = "vhlo.add_v1"(%3143, %3147) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1474)
    %3149 = "vhlo.add_v1"(%3116, %3148) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1475)
    %3150 = "vhlo.reshape_v1"(%arg72) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3151 = "vhlo.custom_call_v1"(%3150) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_26_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3152 = "vhlo.reshape_v1"(%3151) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3153 = "vhlo.reshape_v1"(%arg71) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3154 = "vhlo.custom_call_v1"(%3153) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_26_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3155 = "vhlo.reshape_v1"(%3154) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3156 = "vhlo.composite_v1"(%3149, %3152, %3155) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_10">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1476)
    %3157 = "vhlo.reshape_v1"(%3156) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1477)
    %3158 = "vhlo.reshape_v1"(%arg499) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3159 = "vhlo.custom_call_v1"(%3158) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_26_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3160 = "vhlo.reshape_v1"(%3159) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3161 = "vhlo.transpose_v1"(%3160) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1478)
    %3162 = "vhlo.dot_general_v2"(%3157, %3161) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1479)
    %3163 = "vhlo.reshape_v1"(%3162) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1477)
    %3164 = "vhlo.reshape_v1"(%arg498) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3165 = "vhlo.custom_call_v1"(%3164) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_26_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3166 = "vhlo.reshape_v1"(%3165) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3167 = "vhlo.broadcast_in_dim_v1"(%3166) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1480)
    %3168 = "vhlo.add_v1"(%3163, %3167) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1480)
    %3169 = "vhlo.reshape_v1"(%3168) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1481)
    %3170 = "vhlo.transpose_v1"(%3169) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1482)
    %3171 = "vhlo.convert_v1"(%3170) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1483)
    %3172 = "vhlo.multiply_v1"(%3171, %7) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1484)
    %3173 = "vhlo.reshape_v1"(%arg497) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3174 = "vhlo.custom_call_v1"(%3173) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_26_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3175 = "vhlo.reshape_v1"(%3174) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3176 = "vhlo.transpose_v1"(%3175) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1485)
    %3177 = "vhlo.dot_general_v2"(%3157, %3176) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1486)
    %3178 = "vhlo.reshape_v1"(%3177) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1487)
    %3179 = "vhlo.reshape_v1"(%arg496) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3180 = "vhlo.custom_call_v1"(%3179) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_26_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3181 = "vhlo.reshape_v1"(%3180) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3182 = "vhlo.broadcast_in_dim_v1"(%3181) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1488)
    %3183 = "vhlo.add_v1"(%3178, %3182) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1488)
    %3184 = "vhlo.reshape_v1"(%3183) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1489)
    %3185 = "vhlo.transpose_v1"(%3184) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1490)
    %3186 = "vhlo.convert_v1"(%3185) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1491)
    %3187 = "vhlo.transpose_v1"(%3186) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,80,257]{2,1,3,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc1492)
    %3188 = "vhlo.multiply_v1"(%3187, %6) : (!vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc1493)
    %3189 = "vhlo.dot_general_v2"(%3172, %3188) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1494)
    %3190 = "vhlo.convert_v1"(%3189) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc1495)
    %3191 = "vhlo.compare_v1"(%3190, %5) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1495)
    %3192 = "vhlo.not_v1"(%3191) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1496)
    %3193 = "vhlo.reduce_v1"(%3192, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("2540|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_27aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("2540|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_27aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1498)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1499)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc1497)
    %3194 = "vhlo.reshape_v1"(%3193) : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc1497)
    %3195 = "vhlo.not_v1"(%3194) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc1500)
    %3196 = "vhlo.reshape_v1"(%3195) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc1501)
    %3197 = "vhlo.broadcast_in_dim_v1"(%3196) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1501)
    %3198 = "vhlo.reduce_v1"(%3189, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2537|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_26aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2537|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_26aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1503)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc1502)
    %3199 = "vhlo.broadcast_in_dim_v1"(%3198) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1502)
    %3200 = "vhlo.subtract_v1"(%3189, %3199) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1502)
    %3201 = "vhlo.exponential_v2"(%3200) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1502)
    %3202 = "vhlo.reduce_v1"(%3201, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2537|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_26aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2537|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_26aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1504)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc1502)
    %3203 = "vhlo.broadcast_in_dim_v1"(%3202) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1502)
    %3204 = "vhlo.divide_v1"(%3201, %3203) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1502)
    %3205 = "vhlo.select_v1"(%3197, %4, %3204) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1505)
    %3206 = "vhlo.reshape_v1"(%arg70) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3207 = "vhlo.custom_call_v1"(%3206) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_26_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3208 = "vhlo.reshape_v1"(%3207) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3209 = "vhlo.transpose_v1"(%3208) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1506)
    %3210 = "vhlo.dot_general_v2"(%3157, %3209) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1507)
    %3211 = "vhlo.reshape_v1"(%3210) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1508)
    %3212 = "vhlo.reshape_v1"(%arg69) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3213 = "vhlo.custom_call_v1"(%3212) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_26_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3214 = "vhlo.reshape_v1"(%3213) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3215 = "vhlo.broadcast_in_dim_v1"(%3214) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1509)
    %3216 = "vhlo.add_v1"(%3211, %3215) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1509)
    %3217 = "vhlo.reshape_v1"(%3216) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1510)
    %3218 = "vhlo.transpose_v1"(%3217) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1511)
    %3219 = "vhlo.convert_v1"(%3218) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1512)
    %3220 = "vhlo.dot_general_v2"(%3205, %3219) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1513)
    %3221 = "vhlo.convert_v1"(%3220) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1514)
    %3222 = "vhlo.transpose_v1"(%3221) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,257,16,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1515)
    %3223 = "vhlo.reshape_v1"(%3222) : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1516)
    %3224 = "vhlo.reshape_v1"(%arg68) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3225 = "vhlo.custom_call_v1"(%3224) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_26_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3226 = "vhlo.reshape_v1"(%3225) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3227 = "vhlo.transpose_v1"(%3226) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1517)
    %3228 = "vhlo.dot_general_v2"(%3223, %3227) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1518)
    %3229 = "vhlo.reshape_v1"(%3228) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1516)
    %3230 = "vhlo.reshape_v1"(%arg67) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3231 = "vhlo.custom_call_v1"(%3230) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_26_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3232 = "vhlo.reshape_v1"(%3231) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3233 = "vhlo.broadcast_in_dim_v1"(%3232) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1519)
    %3234 = "vhlo.add_v1"(%3229, %3233) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1519)
    %3235 = "vhlo.add_v1"(%3149, %3234) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1520)
    %3236 = "vhlo.reshape_v1"(%arg66) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3237 = "vhlo.custom_call_v1"(%3236) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_26_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3238 = "vhlo.reshape_v1"(%3237) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3239 = "vhlo.reshape_v1"(%arg65) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3240 = "vhlo.custom_call_v1"(%3239) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_26_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3241 = "vhlo.reshape_v1"(%3240) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3242 = "vhlo.composite_v1"(%3235, %3238, %3241) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_9">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1521)
    %3243 = "vhlo.reshape_v1"(%3242) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1522)
    %3244 = "vhlo.reshape_v1"(%arg64) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %3245 = "vhlo.custom_call_v1"(%3244) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_26_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %3246 = "vhlo.reshape_v1"(%3245) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %3247 = "vhlo.transpose_v1"(%3246) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc1523)
    %3248 = "vhlo.dot_general_v2"(%3243, %3247) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc1524)
    %3249 = "vhlo.reshape_v1"(%3248) : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1522)
    %3250 = "vhlo.reshape_v1"(%arg63) : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc3)
    %3251 = "vhlo.custom_call_v1"(%3250) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_26_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc2)
    %3252 = "vhlo.reshape_v1"(%3251) : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc(#loc3)
    %3253 = "vhlo.broadcast_in_dim_v1"(%3252) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1525)
    %3254 = "vhlo.add_v1"(%3249, %3253) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1525)
    %3255 = "vhlo.composite_v1"(%3254) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_9">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1526)
    %3256 = "vhlo.reshape_v1"(%3255) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc1527)
    %3257 = "vhlo.reshape_v1"(%arg62) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %3258 = "vhlo.custom_call_v1"(%3257) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_26_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %3259 = "vhlo.reshape_v1"(%3258) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %3260 = "vhlo.transpose_v1"(%3259) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc1528)
    %3261 = "vhlo.dot_general_v2"(%3256, %3260) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1529)
    %3262 = "vhlo.reshape_v1"(%3261) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1527)
    %3263 = "vhlo.reshape_v1"(%arg61) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3264 = "vhlo.custom_call_v1"(%3263) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_26_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3265 = "vhlo.reshape_v1"(%3264) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3266 = "vhlo.broadcast_in_dim_v1"(%3265) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1530)
    %3267 = "vhlo.add_v1"(%3262, %3266) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1530)
    %3268 = "vhlo.add_v1"(%3235, %3267) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1531)
    %3269 = "vhlo.reshape_v1"(%arg60) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3270 = "vhlo.custom_call_v1"(%3269) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_27_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3271 = "vhlo.reshape_v1"(%3270) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3272 = "vhlo.reshape_v1"(%arg59) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3273 = "vhlo.custom_call_v1"(%3272) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_27_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3274 = "vhlo.reshape_v1"(%3273) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3275 = "vhlo.composite_v1"(%3268, %3271, %3274) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_8">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1532)
    %3276 = "vhlo.reshape_v1"(%3275) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1533)
    %3277 = "vhlo.reshape_v1"(%arg503) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3278 = "vhlo.custom_call_v1"(%3277) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_27_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3279 = "vhlo.reshape_v1"(%3278) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3280 = "vhlo.transpose_v1"(%3279) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1534)
    %3281 = "vhlo.dot_general_v2"(%3276, %3280) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1535)
    %3282 = "vhlo.reshape_v1"(%3281) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1533)
    %3283 = "vhlo.reshape_v1"(%arg502) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3284 = "vhlo.custom_call_v1"(%3283) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_27_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3285 = "vhlo.reshape_v1"(%3284) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3286 = "vhlo.broadcast_in_dim_v1"(%3285) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1536)
    %3287 = "vhlo.add_v1"(%3282, %3286) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1536)
    %3288 = "vhlo.reshape_v1"(%3287) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1537)
    %3289 = "vhlo.transpose_v1"(%3288) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1538)
    %3290 = "vhlo.convert_v1"(%3289) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1539)
    %3291 = "vhlo.multiply_v1"(%3290, %7) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1540)
    %3292 = "vhlo.reshape_v1"(%arg501) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3293 = "vhlo.custom_call_v1"(%3292) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_27_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3294 = "vhlo.reshape_v1"(%3293) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3295 = "vhlo.transpose_v1"(%3294) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1541)
    %3296 = "vhlo.dot_general_v2"(%3276, %3295) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1542)
    %3297 = "vhlo.reshape_v1"(%3296) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1543)
    %3298 = "vhlo.reshape_v1"(%arg500) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3299 = "vhlo.custom_call_v1"(%3298) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_27_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3300 = "vhlo.reshape_v1"(%3299) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3301 = "vhlo.broadcast_in_dim_v1"(%3300) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1544)
    %3302 = "vhlo.add_v1"(%3297, %3301) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1544)
    %3303 = "vhlo.reshape_v1"(%3302) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1545)
    %3304 = "vhlo.transpose_v1"(%3303) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1546)
    %3305 = "vhlo.convert_v1"(%3304) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1547)
    %3306 = "vhlo.transpose_v1"(%3305) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,80,257]{2,1,3,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc1548)
    %3307 = "vhlo.multiply_v1"(%3306, %6) : (!vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc1549)
    %3308 = "vhlo.dot_general_v2"(%3291, %3307) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1550)
    %3309 = "vhlo.convert_v1"(%3308) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc1551)
    %3310 = "vhlo.compare_v1"(%3309, %5) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1551)
    %3311 = "vhlo.not_v1"(%3310) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1552)
    %3312 = "vhlo.reduce_v1"(%3311, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("2614|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_28aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("2614|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_28aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1554)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1555)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc1553)
    %3313 = "vhlo.reshape_v1"(%3312) : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc1553)
    %3314 = "vhlo.not_v1"(%3313) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc1556)
    %3315 = "vhlo.reshape_v1"(%3314) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc1557)
    %3316 = "vhlo.broadcast_in_dim_v1"(%3315) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1557)
    %3317 = "vhlo.reduce_v1"(%3308, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2611|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_27aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2611|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_27aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1559)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc1558)
    %3318 = "vhlo.broadcast_in_dim_v1"(%3317) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1558)
    %3319 = "vhlo.subtract_v1"(%3308, %3318) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1558)
    %3320 = "vhlo.exponential_v2"(%3319) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1558)
    %3321 = "vhlo.reduce_v1"(%3320, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2611|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_27aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2611|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_27aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1560)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc1558)
    %3322 = "vhlo.broadcast_in_dim_v1"(%3321) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1558)
    %3323 = "vhlo.divide_v1"(%3320, %3322) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1558)
    %3324 = "vhlo.select_v1"(%3316, %4, %3323) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1561)
    %3325 = "vhlo.reshape_v1"(%arg58) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3326 = "vhlo.custom_call_v1"(%3325) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_27_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3327 = "vhlo.reshape_v1"(%3326) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3328 = "vhlo.transpose_v1"(%3327) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1562)
    %3329 = "vhlo.dot_general_v2"(%3276, %3328) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1563)
    %3330 = "vhlo.reshape_v1"(%3329) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1564)
    %3331 = "vhlo.reshape_v1"(%arg57) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3332 = "vhlo.custom_call_v1"(%3331) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_27_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3333 = "vhlo.reshape_v1"(%3332) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3334 = "vhlo.broadcast_in_dim_v1"(%3333) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1565)
    %3335 = "vhlo.add_v1"(%3330, %3334) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1565)
    %3336 = "vhlo.reshape_v1"(%3335) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1566)
    %3337 = "vhlo.transpose_v1"(%3336) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1567)
    %3338 = "vhlo.convert_v1"(%3337) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1568)
    %3339 = "vhlo.dot_general_v2"(%3324, %3338) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1569)
    %3340 = "vhlo.convert_v1"(%3339) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1570)
    %3341 = "vhlo.transpose_v1"(%3340) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,257,16,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1571)
    %3342 = "vhlo.reshape_v1"(%3341) : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1572)
    %3343 = "vhlo.reshape_v1"(%arg56) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3344 = "vhlo.custom_call_v1"(%3343) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_27_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3345 = "vhlo.reshape_v1"(%3344) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3346 = "vhlo.transpose_v1"(%3345) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1573)
    %3347 = "vhlo.dot_general_v2"(%3342, %3346) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1574)
    %3348 = "vhlo.reshape_v1"(%3347) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1572)
    %3349 = "vhlo.reshape_v1"(%arg55) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3350 = "vhlo.custom_call_v1"(%3349) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_27_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3351 = "vhlo.reshape_v1"(%3350) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3352 = "vhlo.broadcast_in_dim_v1"(%3351) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1575)
    %3353 = "vhlo.add_v1"(%3348, %3352) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1575)
    %3354 = "vhlo.add_v1"(%3268, %3353) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1576)
    %3355 = "vhlo.reshape_v1"(%arg54) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3356 = "vhlo.custom_call_v1"(%3355) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_27_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3357 = "vhlo.reshape_v1"(%3356) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3358 = "vhlo.reshape_v1"(%arg53) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3359 = "vhlo.custom_call_v1"(%3358) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_27_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3360 = "vhlo.reshape_v1"(%3359) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3361 = "vhlo.composite_v1"(%3354, %3357, %3360) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_46">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1577)
    %3362 = "vhlo.reshape_v1"(%3361) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1578)
    %3363 = "vhlo.reshape_v1"(%arg52) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %3364 = "vhlo.custom_call_v1"(%3363) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_27_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %3365 = "vhlo.reshape_v1"(%3364) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %3366 = "vhlo.transpose_v1"(%3365) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc1579)
    %3367 = "vhlo.dot_general_v2"(%3362, %3366) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc1580)
    %3368 = "vhlo.reshape_v1"(%3367) : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1578)
    %3369 = "vhlo.reshape_v1"(%arg51) : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc3)
    %3370 = "vhlo.custom_call_v1"(%3369) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_27_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc2)
    %3371 = "vhlo.reshape_v1"(%3370) : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc(#loc3)
    %3372 = "vhlo.broadcast_in_dim_v1"(%3371) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1581)
    %3373 = "vhlo.add_v1"(%3368, %3372) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1581)
    %3374 = "vhlo.composite_v1"(%3373) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_29">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1582)
    %3375 = "vhlo.reshape_v1"(%3374) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc1583)
    %3376 = "vhlo.reshape_v1"(%arg50) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %3377 = "vhlo.custom_call_v1"(%3376) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_27_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %3378 = "vhlo.reshape_v1"(%3377) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %3379 = "vhlo.transpose_v1"(%3378) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc1584)
    %3380 = "vhlo.dot_general_v2"(%3375, %3379) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1585)
    %3381 = "vhlo.reshape_v1"(%3380) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1583)
    %3382 = "vhlo.reshape_v1"(%arg49) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3383 = "vhlo.custom_call_v1"(%3382) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_27_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3384 = "vhlo.reshape_v1"(%3383) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3385 = "vhlo.broadcast_in_dim_v1"(%3384) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1586)
    %3386 = "vhlo.add_v1"(%3381, %3385) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1586)
    %3387 = "vhlo.add_v1"(%3354, %3386) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1587)
    %3388 = "vhlo.reshape_v1"(%arg48) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3389 = "vhlo.custom_call_v1"(%3388) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_28_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3390 = "vhlo.reshape_v1"(%3389) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3391 = "vhlo.reshape_v1"(%arg47) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3392 = "vhlo.custom_call_v1"(%3391) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_28_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3393 = "vhlo.reshape_v1"(%3392) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3394 = "vhlo.composite_v1"(%3387, %3390, %3393) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_7">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1588)
    %3395 = "vhlo.reshape_v1"(%3394) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1589)
    %3396 = "vhlo.reshape_v1"(%arg507) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3397 = "vhlo.custom_call_v1"(%3396) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_28_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3398 = "vhlo.reshape_v1"(%3397) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3399 = "vhlo.transpose_v1"(%3398) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1590)
    %3400 = "vhlo.dot_general_v2"(%3395, %3399) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1591)
    %3401 = "vhlo.reshape_v1"(%3400) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1589)
    %3402 = "vhlo.reshape_v1"(%arg506) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3403 = "vhlo.custom_call_v1"(%3402) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_28_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3404 = "vhlo.reshape_v1"(%3403) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3405 = "vhlo.broadcast_in_dim_v1"(%3404) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1592)
    %3406 = "vhlo.add_v1"(%3401, %3405) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1592)
    %3407 = "vhlo.reshape_v1"(%3406) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1593)
    %3408 = "vhlo.transpose_v1"(%3407) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1594)
    %3409 = "vhlo.convert_v1"(%3408) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1595)
    %3410 = "vhlo.multiply_v1"(%3409, %7) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1596)
    %3411 = "vhlo.reshape_v1"(%arg505) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3412 = "vhlo.custom_call_v1"(%3411) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_28_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3413 = "vhlo.reshape_v1"(%3412) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3414 = "vhlo.transpose_v1"(%3413) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1597)
    %3415 = "vhlo.dot_general_v2"(%3395, %3414) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1598)
    %3416 = "vhlo.reshape_v1"(%3415) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1599)
    %3417 = "vhlo.reshape_v1"(%arg504) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3418 = "vhlo.custom_call_v1"(%3417) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_28_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3419 = "vhlo.reshape_v1"(%3418) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3420 = "vhlo.broadcast_in_dim_v1"(%3419) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1600)
    %3421 = "vhlo.add_v1"(%3416, %3420) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1600)
    %3422 = "vhlo.reshape_v1"(%3421) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1601)
    %3423 = "vhlo.transpose_v1"(%3422) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1602)
    %3424 = "vhlo.convert_v1"(%3423) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1603)
    %3425 = "vhlo.transpose_v1"(%3424) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,80,257]{2,1,3,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc1604)
    %3426 = "vhlo.multiply_v1"(%3425, %6) : (!vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc1605)
    %3427 = "vhlo.dot_general_v2"(%3410, %3426) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1606)
    %3428 = "vhlo.convert_v1"(%3427) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc1607)
    %3429 = "vhlo.compare_v1"(%3428, %5) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1607)
    %3430 = "vhlo.not_v1"(%3429) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1608)
    %3431 = "vhlo.reduce_v1"(%3430, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("2688|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_29aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("2688|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_29aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1610)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1611)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc1609)
    %3432 = "vhlo.reshape_v1"(%3431) : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc1609)
    %3433 = "vhlo.not_v1"(%3432) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc1612)
    %3434 = "vhlo.reshape_v1"(%3433) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc1613)
    %3435 = "vhlo.broadcast_in_dim_v1"(%3434) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1613)
    %3436 = "vhlo.reduce_v1"(%3427, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2685|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_28aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2685|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_28aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1615)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc1614)
    %3437 = "vhlo.broadcast_in_dim_v1"(%3436) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1614)
    %3438 = "vhlo.subtract_v1"(%3427, %3437) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1614)
    %3439 = "vhlo.exponential_v2"(%3438) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1614)
    %3440 = "vhlo.reduce_v1"(%3439, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2685|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_28aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2685|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_28aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1616)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc1614)
    %3441 = "vhlo.broadcast_in_dim_v1"(%3440) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1614)
    %3442 = "vhlo.divide_v1"(%3439, %3441) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1614)
    %3443 = "vhlo.select_v1"(%3435, %4, %3442) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1617)
    %3444 = "vhlo.reshape_v1"(%arg46) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3445 = "vhlo.custom_call_v1"(%3444) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_28_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3446 = "vhlo.reshape_v1"(%3445) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3447 = "vhlo.transpose_v1"(%3446) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1618)
    %3448 = "vhlo.dot_general_v2"(%3395, %3447) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1619)
    %3449 = "vhlo.reshape_v1"(%3448) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1620)
    %3450 = "vhlo.reshape_v1"(%arg45) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3451 = "vhlo.custom_call_v1"(%3450) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_28_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3452 = "vhlo.reshape_v1"(%3451) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3453 = "vhlo.broadcast_in_dim_v1"(%3452) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1621)
    %3454 = "vhlo.add_v1"(%3449, %3453) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1621)
    %3455 = "vhlo.reshape_v1"(%3454) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1622)
    %3456 = "vhlo.transpose_v1"(%3455) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1623)
    %3457 = "vhlo.convert_v1"(%3456) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1624)
    %3458 = "vhlo.dot_general_v2"(%3443, %3457) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1625)
    %3459 = "vhlo.convert_v1"(%3458) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1626)
    %3460 = "vhlo.transpose_v1"(%3459) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,257,16,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1627)
    %3461 = "vhlo.reshape_v1"(%3460) : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1628)
    %3462 = "vhlo.reshape_v1"(%arg44) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3463 = "vhlo.custom_call_v1"(%3462) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_28_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3464 = "vhlo.reshape_v1"(%3463) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3465 = "vhlo.transpose_v1"(%3464) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1629)
    %3466 = "vhlo.dot_general_v2"(%3461, %3465) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1630)
    %3467 = "vhlo.reshape_v1"(%3466) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1628)
    %3468 = "vhlo.reshape_v1"(%arg43) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3469 = "vhlo.custom_call_v1"(%3468) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_28_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3470 = "vhlo.reshape_v1"(%3469) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3471 = "vhlo.broadcast_in_dim_v1"(%3470) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1631)
    %3472 = "vhlo.add_v1"(%3467, %3471) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1631)
    %3473 = "vhlo.add_v1"(%3387, %3472) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1632)
    %3474 = "vhlo.reshape_v1"(%arg42) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3475 = "vhlo.custom_call_v1"(%3474) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_28_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3476 = "vhlo.reshape_v1"(%3475) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3477 = "vhlo.reshape_v1"(%arg41) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3478 = "vhlo.custom_call_v1"(%3477) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_28_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3479 = "vhlo.reshape_v1"(%3478) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3480 = "vhlo.composite_v1"(%3473, %3476, %3479) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_6">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1633)
    %3481 = "vhlo.reshape_v1"(%3480) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1634)
    %3482 = "vhlo.reshape_v1"(%arg40) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %3483 = "vhlo.custom_call_v1"(%3482) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_28_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %3484 = "vhlo.reshape_v1"(%3483) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %3485 = "vhlo.transpose_v1"(%3484) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc1635)
    %3486 = "vhlo.dot_general_v2"(%3481, %3485) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc1636)
    %3487 = "vhlo.reshape_v1"(%3486) : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1634)
    %3488 = "vhlo.reshape_v1"(%arg39) : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc3)
    %3489 = "vhlo.custom_call_v1"(%3488) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_28_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc2)
    %3490 = "vhlo.reshape_v1"(%3489) : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc(#loc3)
    %3491 = "vhlo.broadcast_in_dim_v1"(%3490) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1637)
    %3492 = "vhlo.add_v1"(%3487, %3491) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1637)
    %3493 = "vhlo.composite_v1"(%3492) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_2">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1638)
    %3494 = "vhlo.reshape_v1"(%3493) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc1639)
    %3495 = "vhlo.reshape_v1"(%arg38) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %3496 = "vhlo.custom_call_v1"(%3495) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_28_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %3497 = "vhlo.reshape_v1"(%3496) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %3498 = "vhlo.transpose_v1"(%3497) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc1640)
    %3499 = "vhlo.dot_general_v2"(%3494, %3498) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1641)
    %3500 = "vhlo.reshape_v1"(%3499) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1639)
    %3501 = "vhlo.reshape_v1"(%arg37) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3502 = "vhlo.custom_call_v1"(%3501) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_28_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3503 = "vhlo.reshape_v1"(%3502) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3504 = "vhlo.broadcast_in_dim_v1"(%3503) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1642)
    %3505 = "vhlo.add_v1"(%3500, %3504) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1642)
    %3506 = "vhlo.add_v1"(%3473, %3505) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1643)
    %3507 = "vhlo.reshape_v1"(%arg36) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3508 = "vhlo.custom_call_v1"(%3507) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_29_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3509 = "vhlo.reshape_v1"(%3508) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3510 = "vhlo.reshape_v1"(%arg35) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3511 = "vhlo.custom_call_v1"(%3510) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_29_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3512 = "vhlo.reshape_v1"(%3511) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3513 = "vhlo.composite_v1"(%3506, %3509, %3512) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_4">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1644)
    %3514 = "vhlo.reshape_v1"(%3513) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1645)
    %3515 = "vhlo.reshape_v1"(%arg511) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3516 = "vhlo.custom_call_v1"(%3515) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_29_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3517 = "vhlo.reshape_v1"(%3516) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3518 = "vhlo.transpose_v1"(%3517) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1646)
    %3519 = "vhlo.dot_general_v2"(%3514, %3518) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1647)
    %3520 = "vhlo.reshape_v1"(%3519) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1645)
    %3521 = "vhlo.reshape_v1"(%arg510) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3522 = "vhlo.custom_call_v1"(%3521) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_29_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3523 = "vhlo.reshape_v1"(%3522) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3524 = "vhlo.broadcast_in_dim_v1"(%3523) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1648)
    %3525 = "vhlo.add_v1"(%3520, %3524) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1648)
    %3526 = "vhlo.reshape_v1"(%3525) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1649)
    %3527 = "vhlo.transpose_v1"(%3526) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1650)
    %3528 = "vhlo.convert_v1"(%3527) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1651)
    %3529 = "vhlo.multiply_v1"(%3528, %7) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1652)
    %3530 = "vhlo.reshape_v1"(%arg509) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3531 = "vhlo.custom_call_v1"(%3530) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_29_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3532 = "vhlo.reshape_v1"(%3531) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3533 = "vhlo.transpose_v1"(%3532) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1653)
    %3534 = "vhlo.dot_general_v2"(%3514, %3533) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1654)
    %3535 = "vhlo.reshape_v1"(%3534) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1655)
    %3536 = "vhlo.reshape_v1"(%arg508) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3537 = "vhlo.custom_call_v1"(%3536) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_29_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3538 = "vhlo.reshape_v1"(%3537) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3539 = "vhlo.broadcast_in_dim_v1"(%3538) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1656)
    %3540 = "vhlo.add_v1"(%3535, %3539) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1656)
    %3541 = "vhlo.reshape_v1"(%3540) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1657)
    %3542 = "vhlo.transpose_v1"(%3541) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1658)
    %3543 = "vhlo.convert_v1"(%3542) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1659)
    %3544 = "vhlo.transpose_v1"(%3543) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,80,257]{2,1,3,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc1660)
    %3545 = "vhlo.multiply_v1"(%3544, %6) : (!vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc1661)
    %3546 = "vhlo.dot_general_v2"(%3529, %3545) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1662)
    %3547 = "vhlo.convert_v1"(%3546) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc1663)
    %3548 = "vhlo.compare_v1"(%3547, %5) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1663)
    %3549 = "vhlo.not_v1"(%3548) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1664)
    %3550 = "vhlo.reduce_v1"(%3549, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("2762|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_30aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("2762|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_30aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1666)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1667)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc1665)
    %3551 = "vhlo.reshape_v1"(%3550) : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc1665)
    %3552 = "vhlo.not_v1"(%3551) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc1668)
    %3553 = "vhlo.reshape_v1"(%3552) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc1669)
    %3554 = "vhlo.broadcast_in_dim_v1"(%3553) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1669)
    %3555 = "vhlo.reduce_v1"(%3546, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2759|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_29aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2759|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_29aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1671)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc1670)
    %3556 = "vhlo.broadcast_in_dim_v1"(%3555) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1670)
    %3557 = "vhlo.subtract_v1"(%3546, %3556) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1670)
    %3558 = "vhlo.exponential_v2"(%3557) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1670)
    %3559 = "vhlo.reduce_v1"(%3558, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2759|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_29aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2759|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_29aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1672)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc1670)
    %3560 = "vhlo.broadcast_in_dim_v1"(%3559) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1670)
    %3561 = "vhlo.divide_v1"(%3558, %3560) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1670)
    %3562 = "vhlo.select_v1"(%3554, %4, %3561) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1673)
    %3563 = "vhlo.reshape_v1"(%arg34) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3564 = "vhlo.custom_call_v1"(%3563) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_29_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3565 = "vhlo.reshape_v1"(%3564) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3566 = "vhlo.transpose_v1"(%3565) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1674)
    %3567 = "vhlo.dot_general_v2"(%3514, %3566) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1675)
    %3568 = "vhlo.reshape_v1"(%3567) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1676)
    %3569 = "vhlo.reshape_v1"(%arg33) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3570 = "vhlo.custom_call_v1"(%3569) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_29_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3571 = "vhlo.reshape_v1"(%3570) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3572 = "vhlo.broadcast_in_dim_v1"(%3571) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1677)
    %3573 = "vhlo.add_v1"(%3568, %3572) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1677)
    %3574 = "vhlo.reshape_v1"(%3573) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1678)
    %3575 = "vhlo.transpose_v1"(%3574) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1679)
    %3576 = "vhlo.convert_v1"(%3575) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1680)
    %3577 = "vhlo.dot_general_v2"(%3562, %3576) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1681)
    %3578 = "vhlo.convert_v1"(%3577) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1682)
    %3579 = "vhlo.transpose_v1"(%3578) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,257,16,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1683)
    %3580 = "vhlo.reshape_v1"(%3579) : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1684)
    %3581 = "vhlo.reshape_v1"(%arg32) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3582 = "vhlo.custom_call_v1"(%3581) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_29_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3583 = "vhlo.reshape_v1"(%3582) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3584 = "vhlo.transpose_v1"(%3583) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1685)
    %3585 = "vhlo.dot_general_v2"(%3580, %3584) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1686)
    %3586 = "vhlo.reshape_v1"(%3585) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1684)
    %3587 = "vhlo.reshape_v1"(%arg31) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3588 = "vhlo.custom_call_v1"(%3587) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_29_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3589 = "vhlo.reshape_v1"(%3588) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3590 = "vhlo.broadcast_in_dim_v1"(%3589) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1687)
    %3591 = "vhlo.add_v1"(%3586, %3590) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1687)
    %3592 = "vhlo.add_v1"(%3506, %3591) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1688)
    %3593 = "vhlo.reshape_v1"(%arg30) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3594 = "vhlo.custom_call_v1"(%3593) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_29_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3595 = "vhlo.reshape_v1"(%3594) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3596 = "vhlo.reshape_v1"(%arg29) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3597 = "vhlo.custom_call_v1"(%3596) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_29_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3598 = "vhlo.reshape_v1"(%3597) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3599 = "vhlo.composite_v1"(%3592, %3595, %3598) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_72">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1689)
    %3600 = "vhlo.reshape_v1"(%3599) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1690)
    %3601 = "vhlo.reshape_v1"(%arg28) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %3602 = "vhlo.custom_call_v1"(%3601) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_29_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %3603 = "vhlo.reshape_v1"(%3602) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %3604 = "vhlo.transpose_v1"(%3603) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc1691)
    %3605 = "vhlo.dot_general_v2"(%3600, %3604) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc1692)
    %3606 = "vhlo.reshape_v1"(%3605) : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1690)
    %3607 = "vhlo.reshape_v1"(%arg27) : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc3)
    %3608 = "vhlo.custom_call_v1"(%3607) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_29_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc2)
    %3609 = "vhlo.reshape_v1"(%3608) : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc(#loc3)
    %3610 = "vhlo.broadcast_in_dim_v1"(%3609) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1693)
    %3611 = "vhlo.add_v1"(%3606, %3610) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1693)
    %3612 = "vhlo.composite_v1"(%3611) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_1">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1694)
    %3613 = "vhlo.reshape_v1"(%3612) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc1695)
    %3614 = "vhlo.reshape_v1"(%arg26) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %3615 = "vhlo.custom_call_v1"(%3614) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_29_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %3616 = "vhlo.reshape_v1"(%3615) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %3617 = "vhlo.transpose_v1"(%3616) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc1696)
    %3618 = "vhlo.dot_general_v2"(%3613, %3617) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1697)
    %3619 = "vhlo.reshape_v1"(%3618) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1695)
    %3620 = "vhlo.reshape_v1"(%arg25) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3621 = "vhlo.custom_call_v1"(%3620) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_29_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3622 = "vhlo.reshape_v1"(%3621) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3623 = "vhlo.broadcast_in_dim_v1"(%3622) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1698)
    %3624 = "vhlo.add_v1"(%3619, %3623) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1698)
    %3625 = "vhlo.add_v1"(%3592, %3624) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1699)
    %3626 = "vhlo.reshape_v1"(%arg24) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3627 = "vhlo.custom_call_v1"(%3626) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_30_layer_norm1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3628 = "vhlo.reshape_v1"(%3627) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3629 = "vhlo.reshape_v1"(%arg23) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3630 = "vhlo.custom_call_v1"(%3629) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_30_layer_norm1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3631 = "vhlo.reshape_v1"(%3630) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3632 = "vhlo.composite_v1"(%3625, %3628, %3631) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_31">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1700)
    %3633 = "vhlo.reshape_v1"(%3632) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1701)
    %3634 = "vhlo.reshape_v1"(%arg515) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3635 = "vhlo.custom_call_v1"(%3634) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_30_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3636 = "vhlo.reshape_v1"(%3635) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3637 = "vhlo.transpose_v1"(%3636) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1702)
    %3638 = "vhlo.dot_general_v2"(%3633, %3637) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1703)
    %3639 = "vhlo.reshape_v1"(%3638) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1701)
    %3640 = "vhlo.reshape_v1"(%arg514) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3641 = "vhlo.custom_call_v1"(%3640) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_30_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3642 = "vhlo.reshape_v1"(%3641) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3643 = "vhlo.broadcast_in_dim_v1"(%3642) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1704)
    %3644 = "vhlo.add_v1"(%3639, %3643) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1704)
    %3645 = "vhlo.reshape_v1"(%3644) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1705)
    %3646 = "vhlo.transpose_v1"(%3645) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1706)
    %3647 = "vhlo.convert_v1"(%3646) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1707)
    %3648 = "vhlo.multiply_v1"(%3647, %7) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1708)
    %3649 = "vhlo.reshape_v1"(%arg513) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3650 = "vhlo.custom_call_v1"(%3649) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_30_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3651 = "vhlo.reshape_v1"(%3650) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3652 = "vhlo.transpose_v1"(%3651) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1709)
    %3653 = "vhlo.dot_general_v2"(%3633, %3652) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1710)
    %3654 = "vhlo.reshape_v1"(%3653) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1711)
    %3655 = "vhlo.reshape_v1"(%arg512) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3656 = "vhlo.custom_call_v1"(%3655) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_30_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3657 = "vhlo.reshape_v1"(%3656) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3658 = "vhlo.broadcast_in_dim_v1"(%3657) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1712)
    %3659 = "vhlo.add_v1"(%3654, %3658) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1712)
    %3660 = "vhlo.reshape_v1"(%3659) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1713)
    %3661 = "vhlo.transpose_v1"(%3660) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1714)
    %3662 = "vhlo.convert_v1"(%3661) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1715)
    %3663 = "vhlo.transpose_v1"(%3662) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,80,257]{2,1,3,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc1716)
    %3664 = "vhlo.multiply_v1"(%3663, %6) : (!vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1> loc(#loc1717)
    %3665 = "vhlo.dot_general_v2"(%3648, %3664) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x80x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1718)
    %3666 = "vhlo.convert_v1"(%3665) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1> loc(#loc1719)
    %3667 = "vhlo.compare_v1"(%3666, %5) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1719)
    %3668 = "vhlo.not_v1"(%3667) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1720)
    %3669 = "vhlo.reduce_v1"(%3668, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("2836|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_31aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("2836|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_31aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1722)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1723)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc1721)
    %3670 = "vhlo.reshape_v1"(%3669) : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc1721)
    %3671 = "vhlo.not_v1"(%3670) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1> loc(#loc1724)
    %3672 = "vhlo.reshape_v1"(%3671) : (!vhlo.tensor_v1<1x16x257x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.bool_v1> loc(#loc1725)
    %3673 = "vhlo.broadcast_in_dim_v1"(%3672) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1> loc(#loc1725)
    %3674 = "vhlo.reduce_v1"(%3665, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2833|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_30aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2833|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_30aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1727)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc1726)
    %3675 = "vhlo.broadcast_in_dim_v1"(%3674) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1726)
    %3676 = "vhlo.subtract_v1"(%3665, %3675) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1726)
    %3677 = "vhlo.exponential_v2"(%3676) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1726)
    %3678 = "vhlo.reduce_v1"(%3677, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2833|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_30aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2833|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_30aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1728)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x!vhlo.f32_v1> loc(#loc1726)
    %3679 = "vhlo.broadcast_in_dim_v1"(%3678) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x16x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1726)
    %3680 = "vhlo.divide_v1"(%3677, %3679) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1726)
    %3681 = "vhlo.select_v1"(%3673, %4, %3680) : (!vhlo.tensor_v1<1x16x257x257x!vhlo.bool_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1> loc(#loc1729)
    %3682 = "vhlo.reshape_v1"(%arg22) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3683 = "vhlo.custom_call_v1"(%3682) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_30_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3684 = "vhlo.reshape_v1"(%3683) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3685 = "vhlo.transpose_v1"(%3684) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1730)
    %3686 = "vhlo.dot_general_v2"(%3633, %3685) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1731)
    %3687 = "vhlo.reshape_v1"(%3686) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1732)
    %3688 = "vhlo.reshape_v1"(%arg21) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3689 = "vhlo.custom_call_v1"(%3688) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_30_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3690 = "vhlo.reshape_v1"(%3689) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3691 = "vhlo.broadcast_in_dim_v1"(%3690) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1733)
    %3692 = "vhlo.add_v1"(%3687, %3691) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1733)
    %3693 = "vhlo.reshape_v1"(%3692) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1734)
    %3694 = "vhlo.transpose_v1"(%3693) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1735)
    %3695 = "vhlo.convert_v1"(%3694) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,16,257,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1736)
    %3696 = "vhlo.dot_general_v2"(%3681, %3695) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x16x257x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1> loc(#loc1737)
    %3697 = "vhlo.convert_v1"(%3696) : (!vhlo.tensor_v1<1x16x257x80x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1> loc(#loc1738)
    %3698 = "vhlo.transpose_v1"(%3697) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,257,16,80]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x257x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1> loc(#loc1739)
    %3699 = "vhlo.reshape_v1"(%3698) : (!vhlo.tensor_v1<1x257x16x80x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1740)
    %3700 = "vhlo.reshape_v1"(%arg20) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3701 = "vhlo.custom_call_v1"(%3700) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_30_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3702 = "vhlo.reshape_v1"(%3701) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3703 = "vhlo.transpose_v1"(%3702) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1741)
    %3704 = "vhlo.dot_general_v2"(%3699, %3703) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1742)
    %3705 = "vhlo.reshape_v1"(%3704) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1740)
    %3706 = "vhlo.reshape_v1"(%arg19) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3707 = "vhlo.custom_call_v1"(%3706) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_30_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3708 = "vhlo.reshape_v1"(%3707) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3709 = "vhlo.broadcast_in_dim_v1"(%3708) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1743)
    %3710 = "vhlo.add_v1"(%3705, %3709) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1743)
    %3711 = "vhlo.add_v1"(%3625, %3710) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1744)
    %3712 = "vhlo.reshape_v1"(%arg18) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3713 = "vhlo.custom_call_v1"(%3712) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_30_layer_norm2_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3714 = "vhlo.reshape_v1"(%3713) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3715 = "vhlo.reshape_v1"(%arg17) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3716 = "vhlo.custom_call_v1"(%3715) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_30_layer_norm2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3717 = "vhlo.reshape_v1"(%3716) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3718 = "vhlo.composite_v1"(%3711, %3714, %3717) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_23">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1745)
    %3719 = "vhlo.reshape_v1"(%3718) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1746)
    %3720 = "vhlo.reshape_v1"(%arg16) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %3721 = "vhlo.custom_call_v1"(%3720) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_30_mlp_fc1_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %3722 = "vhlo.reshape_v1"(%3721) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %3723 = "vhlo.transpose_v1"(%3722) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc1747)
    %3724 = "vhlo.dot_general_v2"(%3719, %3723) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc1748)
    %3725 = "vhlo.reshape_v1"(%3724) : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1746)
    %3726 = "vhlo.reshape_v1"(%arg15) : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc3)
    %3727 = "vhlo.custom_call_v1"(%3726) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_30_mlp_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1> loc(#loc2)
    %3728 = "vhlo.reshape_v1"(%3727) : (!vhlo.tensor_v1<1x1x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x!vhlo.bf16_v1> loc(#loc3)
    %3729 = "vhlo.broadcast_in_dim_v1"(%3728) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1749)
    %3730 = "vhlo.add_v1"(%3725, %3729) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1749)
    %3731 = "vhlo.composite_v1"(%3730) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_12">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc1750)
    %3732 = "vhlo.reshape_v1"(%3731) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x5120x!vhlo.bf16_v1> loc(#loc1751)
    %3733 = "vhlo.reshape_v1"(%arg14) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %3734 = "vhlo.custom_call_v1"(%3733) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_30_mlp_fc2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %3735 = "vhlo.reshape_v1"(%3734) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %3736 = "vhlo.transpose_v1"(%3735) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc1752)
    %3737 = "vhlo.dot_general_v2"(%3732, %3736) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1753)
    %3738 = "vhlo.reshape_v1"(%3737) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1751)
    %3739 = "vhlo.reshape_v1"(%arg13) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3740 = "vhlo.custom_call_v1"(%3739) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___image_encoder_vision_model_encoder_layers_30_mlp_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3741 = "vhlo.reshape_v1"(%3740) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3742 = "vhlo.broadcast_in_dim_v1"(%3741) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1754)
    %3743 = "vhlo.add_v1"(%3738, %3742) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1754)
    %3744 = "vhlo.add_v1"(%3711, %3743) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1755)
    %3745 = "vhlo.reshape_v1"(%3744) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1756)
    %3746 = "vhlo.reshape_v1"(%arg12) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3747 = "vhlo.custom_call_v1"(%3746) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_proj_in_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3748 = "vhlo.reshape_v1"(%3747) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3749 = "vhlo.transpose_v1"(%3748) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1757)
    %3750 = "vhlo.dot_general_v2"(%3745, %3749) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<257x1280x!vhlo.bf16_v1> loc(#loc1758)
    %3751 = "vhlo.reshape_v1"(%3750) : (!vhlo.tensor_v1<257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1756)
    %3752 = "vhlo.reshape_v1"(%arg11) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3753 = "vhlo.custom_call_v1"(%3752) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_proj_in_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3754 = "vhlo.reshape_v1"(%3753) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3755 = "vhlo.broadcast_in_dim_v1"(%3754) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1759)
    %3756 = "vhlo.add_v1"(%3751, %3755) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1759)
    %3757 = "vhlo.reshape_v1"(%arg10) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3758 = "vhlo.custom_call_v1"(%3757) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_0_ln0_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3759 = "vhlo.reshape_v1"(%3758) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3760 = "vhlo.reshape_v1"(%arg9) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3761 = "vhlo.custom_call_v1"(%3760) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_0_ln0_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3762 = "vhlo.reshape_v1"(%3761) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3763 = "vhlo.composite_v1"(%3756, %3759, %3762) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_63">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1760)
    %3764 = "vhlo.concatenate_v1"(%3763, %20) <{dimension = #vhlo.integer_v1<1 : i64>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x273x1280x!vhlo.bf16_v1> loc(#loc1761)
    %3765 = "vhlo.reshape_v1"(%3764) : (!vhlo.tensor_v1<1x273x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<273x1280x!vhlo.bf16_v1> loc(#loc1762)
    %3766 = "vhlo.reshape_v1"(%arg516) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3767 = "vhlo.custom_call_v1"(%3766) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_0_attn_to_k_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3768 = "vhlo.reshape_v1"(%3767) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3769 = "vhlo.transpose_v1"(%3768) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1763)
    %3770 = "vhlo.dot_general_v2"(%3765, %3769) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<273x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<273x1280x!vhlo.bf16_v1> loc(#loc1764)
    %3771 = "vhlo.reshape_v1"(%3770) : (!vhlo.tensor_v1<273x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x273x20x64x!vhlo.bf16_v1> loc(#loc1765)
    %3772 = "vhlo.transpose_v1"(%3771) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,20,273,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x273x20x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x20x273x64x!vhlo.bf16_v1> loc(#loc1766)
    %3773 = "vhlo.convert_v1"(%3772) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,20,273,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x20x273x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x20x273x64x!vhlo.f32_v1> loc(#loc1767)
    %3774 = "vhlo.transpose_v1"(%3773) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,20,64,273]{2,1,3,0}">} : (!vhlo.tensor_v1<1x20x273x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x64x273x!vhlo.f32_v1> loc(#loc1768)
    %3775 = "vhlo.multiply_v1"(%3774, %3) : (!vhlo.tensor_v1<1x20x64x273x!vhlo.f32_v1>, !vhlo.tensor_v1<1x20x64x273x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x64x273x!vhlo.f32_v1> loc(#loc1769)
    %3776 = "vhlo.dot_general_v2"(%30, %3775) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x20x16x64x!vhlo.f32_v1>, !vhlo.tensor_v1<1x20x64x273x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1> loc(#loc1770)
    %3777 = "vhlo.convert_v1"(%3776) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f64_v1> loc(#loc1771)
    %3778 = "vhlo.compare_v1"(%3777, %2) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f64_v1>, !vhlo.tensor_v1<1x20x16x273x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.bool_v1> loc(#loc1771)
    %3779 = "vhlo.not_v1"(%3778) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.bool_v1> loc(#loc1772)
    %3780 = "vhlo.reduce_v1"(%3779, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("2925|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_33aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("2925|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_33aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1774)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1775)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x20x16x!vhlo.bool_v1> loc(#loc1773)
    %3781 = "vhlo.reshape_v1"(%3780) : (!vhlo.tensor_v1<1x20x16x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x20x16x1x!vhlo.bool_v1> loc(#loc1773)
    %3782 = "vhlo.not_v1"(%3781) : (!vhlo.tensor_v1<1x20x16x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x20x16x1x!vhlo.bool_v1> loc(#loc1776)
    %3783 = "vhlo.reshape_v1"(%3782) : (!vhlo.tensor_v1<1x20x16x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x20x16x!vhlo.bool_v1> loc(#loc1777)
    %3784 = "vhlo.broadcast_in_dim_v1"(%3783) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x20x16x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.bool_v1> loc(#loc1777)
    %3785 = "vhlo.reduce_v1"(%3776, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2922|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_32aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2922|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_32aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1779)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x!vhlo.f32_v1> loc(#loc1778)
    %3786 = "vhlo.broadcast_in_dim_v1"(%3785) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x20x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1> loc(#loc1778)
    %3787 = "vhlo.subtract_v1"(%3776, %3786) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>, !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1> loc(#loc1778)
    %3788 = "vhlo.exponential_v2"(%3787) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1> loc(#loc1778)
    %3789 = "vhlo.reduce_v1"(%3788, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2922|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_32aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2922|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_32aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1780)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x!vhlo.f32_v1> loc(#loc1778)
    %3790 = "vhlo.broadcast_in_dim_v1"(%3789) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x20x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1> loc(#loc1778)
    %3791 = "vhlo.divide_v1"(%3788, %3790) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>, !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1> loc(#loc1778)
    %3792 = "vhlo.select_v1"(%3784, %1, %3791) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.bool_v1>, !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>, !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1> loc(#loc1781)
    %3793 = "vhlo.reshape_v1"(%arg6) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3794 = "vhlo.custom_call_v1"(%3793) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_0_attn_to_v_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3795 = "vhlo.reshape_v1"(%3794) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3796 = "vhlo.transpose_v1"(%3795) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1782)
    %3797 = "vhlo.dot_general_v2"(%3765, %3796) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<273x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<273x1280x!vhlo.bf16_v1> loc(#loc1783)
    %3798 = "vhlo.reshape_v1"(%3797) : (!vhlo.tensor_v1<273x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x273x20x64x!vhlo.bf16_v1> loc(#loc1784)
    %3799 = "vhlo.transpose_v1"(%3798) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,20,273,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x273x20x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x20x273x64x!vhlo.bf16_v1> loc(#loc1785)
    %3800 = "vhlo.convert_v1"(%3799) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,20,273,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x20x273x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x20x273x64x!vhlo.f32_v1> loc(#loc1786)
    %3801 = "vhlo.dot_general_v2"(%3792, %3800) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>, !vhlo.tensor_v1<1x20x273x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x64x!vhlo.f32_v1> loc(#loc1787)
    %3802 = "vhlo.convert_v1"(%3801) : (!vhlo.tensor_v1<1x20x16x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x64x!vhlo.bf16_v1> loc(#loc1788)
    %3803 = "vhlo.transpose_v1"(%3802) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,20,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x20x16x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x20x64x!vhlo.bf16_v1> loc(#loc1789)
    %3804 = "vhlo.reshape_v1"(%3803) : (!vhlo.tensor_v1<1x16x20x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x1280x!vhlo.bf16_v1> loc(#loc1790)
    %3805 = "vhlo.reshape_v1"(%arg5) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3806 = "vhlo.custom_call_v1"(%3805) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_0_attn_to_out_0_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3807 = "vhlo.reshape_v1"(%3806) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3808 = "vhlo.transpose_v1"(%3807) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1791)
    %3809 = "vhlo.dot_general_v2"(%3804, %3808) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x1280x!vhlo.bf16_v1> loc(#loc1792)
    %3810 = "vhlo.reshape_v1"(%3809) : (!vhlo.tensor_v1<16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc1793)
    %3811 = "vhlo.divide_v1"(%3810, %0) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc1794)
    %3812 = "vhlo.add_v1"(%3811, %13) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc1795)
    %3813 = "vhlo.reshape_v1"(%arg521) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3814 = "vhlo.custom_call_v1"(%3813) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_0_ff_0_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3815 = "vhlo.reshape_v1"(%3814) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3816 = "vhlo.reshape_v1"(%arg520) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3817 = "vhlo.custom_call_v1"(%3816) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_0_ff_0_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3818 = "vhlo.reshape_v1"(%3817) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3819 = "vhlo.composite_v1"(%3812, %3815, %3818) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_3">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc1796)
    %3820 = "vhlo.reshape_v1"(%3819) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x1280x!vhlo.bf16_v1> loc(#loc1797)
    %3821 = "vhlo.reshape_v1"(%arg519) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %3822 = "vhlo.custom_call_v1"(%3821) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"getattr_l__self___resampler_layers_0_ff___1___net_0_proj_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %3823 = "vhlo.reshape_v1"(%3822) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %3824 = "vhlo.transpose_v1"(%3823) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc1798)
    %3825 = "vhlo.dot_general_v2"(%3820, %3824) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x5120x!vhlo.bf16_v1> loc(#loc1799)
    %3826 = "vhlo.reshape_v1"(%3825) : (!vhlo.tensor_v1<16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc1797)
    %3827 = "vhlo.composite_v1"(%3826) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_0">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc1800)
    %3828 = "vhlo.reshape_v1"(%3827) : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x5120x!vhlo.bf16_v1> loc(#loc1801)
    %3829 = "vhlo.reshape_v1"(%arg518) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %3830 = "vhlo.custom_call_v1"(%3829) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"getattr_l__self___resampler_layers_0_ff___1___net_2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %3831 = "vhlo.reshape_v1"(%3830) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %3832 = "vhlo.transpose_v1"(%3831) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc1802)
    %3833 = "vhlo.dot_general_v2"(%3828, %3832) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<16x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x1280x!vhlo.bf16_v1> loc(#loc1803)
    %3834 = "vhlo.reshape_v1"(%3833) : (!vhlo.tensor_v1<16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc1801)
    %3835 = "vhlo.add_v1"(%3834, %3812) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc1804)
    %3836 = "vhlo.reshape_v1"(%arg525) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3837 = "vhlo.custom_call_v1"(%3836) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_1_ln1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3838 = "vhlo.reshape_v1"(%3837) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3839 = "vhlo.reshape_v1"(%arg524) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3840 = "vhlo.custom_call_v1"(%3839) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_1_ln1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3841 = "vhlo.reshape_v1"(%3840) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3842 = "vhlo.composite_v1"(%3835, %3838, %3841) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_5">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc1805)
    %3843 = "vhlo.reshape_v1"(%3842) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x1280x!vhlo.bf16_v1> loc(#loc1806)
    %3844 = "vhlo.reshape_v1"(%arg529) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3845 = "vhlo.custom_call_v1"(%3844) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_1_attn_to_q_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3846 = "vhlo.reshape_v1"(%3845) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3847 = "vhlo.transpose_v1"(%3846) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1807)
    %3848 = "vhlo.dot_general_v2"(%3843, %3847) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x1280x!vhlo.bf16_v1> loc(#loc1808)
    %3849 = "vhlo.reshape_v1"(%3848) : (!vhlo.tensor_v1<16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x20x64x!vhlo.bf16_v1> loc(#loc1809)
    %3850 = "vhlo.transpose_v1"(%3849) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,20,16,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x20x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x20x16x64x!vhlo.bf16_v1> loc(#loc1810)
    %3851 = "vhlo.convert_v1"(%3850) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,20,16,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x20x16x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x20x16x64x!vhlo.f32_v1> loc(#loc1811)
    %3852 = "vhlo.multiply_v1"(%3851, %8) : (!vhlo.tensor_v1<1x20x16x64x!vhlo.f32_v1>, !vhlo.tensor_v1<1x20x16x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x64x!vhlo.f32_v1> loc(#loc1812)
    %3853 = "vhlo.reshape_v1"(%arg527) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3854 = "vhlo.custom_call_v1"(%3853) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_1_ln0_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3855 = "vhlo.reshape_v1"(%3854) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3856 = "vhlo.reshape_v1"(%arg526) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3857 = "vhlo.custom_call_v1"(%3856) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_1_ln0_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3858 = "vhlo.reshape_v1"(%3857) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3859 = "vhlo.composite_v1"(%3756, %3855, %3858) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_67">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1813)
    %3860 = "vhlo.concatenate_v1"(%3859, %3842) <{dimension = #vhlo.integer_v1<1 : i64>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x273x1280x!vhlo.bf16_v1> loc(#loc1814)
    %3861 = "vhlo.reshape_v1"(%3860) : (!vhlo.tensor_v1<1x273x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<273x1280x!vhlo.bf16_v1> loc(#loc1815)
    %3862 = "vhlo.reshape_v1"(%arg528) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3863 = "vhlo.custom_call_v1"(%3862) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_1_attn_to_k_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3864 = "vhlo.reshape_v1"(%3863) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3865 = "vhlo.transpose_v1"(%3864) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1816)
    %3866 = "vhlo.dot_general_v2"(%3861, %3865) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<273x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<273x1280x!vhlo.bf16_v1> loc(#loc1817)
    %3867 = "vhlo.reshape_v1"(%3866) : (!vhlo.tensor_v1<273x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x273x20x64x!vhlo.bf16_v1> loc(#loc1818)
    %3868 = "vhlo.transpose_v1"(%3867) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,20,273,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x273x20x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x20x273x64x!vhlo.bf16_v1> loc(#loc1819)
    %3869 = "vhlo.convert_v1"(%3868) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,20,273,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x20x273x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x20x273x64x!vhlo.f32_v1> loc(#loc1820)
    %3870 = "vhlo.transpose_v1"(%3869) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,20,64,273]{2,1,3,0}">} : (!vhlo.tensor_v1<1x20x273x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x64x273x!vhlo.f32_v1> loc(#loc1821)
    %3871 = "vhlo.multiply_v1"(%3870, %3) : (!vhlo.tensor_v1<1x20x64x273x!vhlo.f32_v1>, !vhlo.tensor_v1<1x20x64x273x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x64x273x!vhlo.f32_v1> loc(#loc1822)
    %3872 = "vhlo.dot_general_v2"(%3852, %3871) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x20x16x64x!vhlo.f32_v1>, !vhlo.tensor_v1<1x20x64x273x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1> loc(#loc1823)
    %3873 = "vhlo.convert_v1"(%3872) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f64_v1> loc(#loc1824)
    %3874 = "vhlo.compare_v1"(%3873, %2) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f64_v1>, !vhlo.tensor_v1<1x20x16x273x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.bool_v1> loc(#loc1824)
    %3875 = "vhlo.not_v1"(%3874) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.bool_v1> loc(#loc1825)
    %3876 = "vhlo.reduce_v1"(%3875, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("3010|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_34aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("3010|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_34aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1827)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1828)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x20x16x!vhlo.bool_v1> loc(#loc1826)
    %3877 = "vhlo.reshape_v1"(%3876) : (!vhlo.tensor_v1<1x20x16x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x20x16x1x!vhlo.bool_v1> loc(#loc1826)
    %3878 = "vhlo.not_v1"(%3877) : (!vhlo.tensor_v1<1x20x16x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x20x16x1x!vhlo.bool_v1> loc(#loc1829)
    %3879 = "vhlo.reshape_v1"(%3878) : (!vhlo.tensor_v1<1x20x16x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x20x16x!vhlo.bool_v1> loc(#loc1830)
    %3880 = "vhlo.broadcast_in_dim_v1"(%3879) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x20x16x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.bool_v1> loc(#loc1830)
    %3881 = "vhlo.reduce_v1"(%3872, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3007|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_33aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3007|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_33aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1832)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x!vhlo.f32_v1> loc(#loc1831)
    %3882 = "vhlo.broadcast_in_dim_v1"(%3881) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x20x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1> loc(#loc1831)
    %3883 = "vhlo.subtract_v1"(%3872, %3882) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>, !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1> loc(#loc1831)
    %3884 = "vhlo.exponential_v2"(%3883) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1> loc(#loc1831)
    %3885 = "vhlo.reduce_v1"(%3884, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3007|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_33aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3007|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_33aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1833)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x!vhlo.f32_v1> loc(#loc1831)
    %3886 = "vhlo.broadcast_in_dim_v1"(%3885) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x20x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1> loc(#loc1831)
    %3887 = "vhlo.divide_v1"(%3884, %3886) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>, !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1> loc(#loc1831)
    %3888 = "vhlo.select_v1"(%3880, %1, %3887) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.bool_v1>, !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>, !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1> loc(#loc1834)
    %3889 = "vhlo.reshape_v1"(%arg523) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3890 = "vhlo.custom_call_v1"(%3889) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_1_attn_to_v_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3891 = "vhlo.reshape_v1"(%3890) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3892 = "vhlo.transpose_v1"(%3891) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1835)
    %3893 = "vhlo.dot_general_v2"(%3861, %3892) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<273x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<273x1280x!vhlo.bf16_v1> loc(#loc1836)
    %3894 = "vhlo.reshape_v1"(%3893) : (!vhlo.tensor_v1<273x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x273x20x64x!vhlo.bf16_v1> loc(#loc1837)
    %3895 = "vhlo.transpose_v1"(%3894) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,20,273,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x273x20x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x20x273x64x!vhlo.bf16_v1> loc(#loc1838)
    %3896 = "vhlo.convert_v1"(%3895) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,20,273,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x20x273x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x20x273x64x!vhlo.f32_v1> loc(#loc1839)
    %3897 = "vhlo.dot_general_v2"(%3888, %3896) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>, !vhlo.tensor_v1<1x20x273x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x64x!vhlo.f32_v1> loc(#loc1840)
    %3898 = "vhlo.convert_v1"(%3897) : (!vhlo.tensor_v1<1x20x16x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x64x!vhlo.bf16_v1> loc(#loc1841)
    %3899 = "vhlo.transpose_v1"(%3898) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,20,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x20x16x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x20x64x!vhlo.bf16_v1> loc(#loc1842)
    %3900 = "vhlo.reshape_v1"(%3899) : (!vhlo.tensor_v1<1x16x20x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x1280x!vhlo.bf16_v1> loc(#loc1843)
    %3901 = "vhlo.reshape_v1"(%arg522) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3902 = "vhlo.custom_call_v1"(%3901) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_1_attn_to_out_0_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3903 = "vhlo.reshape_v1"(%3902) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3904 = "vhlo.transpose_v1"(%3903) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1844)
    %3905 = "vhlo.dot_general_v2"(%3900, %3904) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x1280x!vhlo.bf16_v1> loc(#loc1845)
    %3906 = "vhlo.reshape_v1"(%3905) : (!vhlo.tensor_v1<16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc1846)
    %3907 = "vhlo.divide_v1"(%3906, %0) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc1847)
    %3908 = "vhlo.add_v1"(%3907, %3835) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc1848)
    %3909 = "vhlo.reshape_v1"(%arg533) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3910 = "vhlo.custom_call_v1"(%3909) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_1_ff_0_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3911 = "vhlo.reshape_v1"(%3910) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3912 = "vhlo.reshape_v1"(%arg532) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3913 = "vhlo.custom_call_v1"(%3912) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_1_ff_0_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3914 = "vhlo.reshape_v1"(%3913) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3915 = "vhlo.composite_v1"(%3908, %3911, %3914) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_17">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc1849)
    %3916 = "vhlo.reshape_v1"(%3915) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x1280x!vhlo.bf16_v1> loc(#loc1850)
    %3917 = "vhlo.reshape_v1"(%arg531) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %3918 = "vhlo.custom_call_v1"(%3917) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"getattr_l__self___resampler_layers_1_ff___1___net_0_proj_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %3919 = "vhlo.reshape_v1"(%3918) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %3920 = "vhlo.transpose_v1"(%3919) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc1851)
    %3921 = "vhlo.dot_general_v2"(%3916, %3920) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x5120x!vhlo.bf16_v1> loc(#loc1852)
    %3922 = "vhlo.reshape_v1"(%3921) : (!vhlo.tensor_v1<16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc1850)
    %3923 = "vhlo.composite_v1"(%3922) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc1853)
    %3924 = "vhlo.reshape_v1"(%3923) : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x5120x!vhlo.bf16_v1> loc(#loc1854)
    %3925 = "vhlo.reshape_v1"(%arg530) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %3926 = "vhlo.custom_call_v1"(%3925) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"getattr_l__self___resampler_layers_1_ff___1___net_2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %3927 = "vhlo.reshape_v1"(%3926) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %3928 = "vhlo.transpose_v1"(%3927) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc1855)
    %3929 = "vhlo.dot_general_v2"(%3924, %3928) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<16x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x1280x!vhlo.bf16_v1> loc(#loc1856)
    %3930 = "vhlo.reshape_v1"(%3929) : (!vhlo.tensor_v1<16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc1854)
    %3931 = "vhlo.add_v1"(%3930, %3908) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc1857)
    %3932 = "vhlo.reshape_v1"(%arg537) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3933 = "vhlo.custom_call_v1"(%3932) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_2_ln1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3934 = "vhlo.reshape_v1"(%3933) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3935 = "vhlo.reshape_v1"(%arg536) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3936 = "vhlo.custom_call_v1"(%3935) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_2_ln1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3937 = "vhlo.reshape_v1"(%3936) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3938 = "vhlo.composite_v1"(%3931, %3934, %3937) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_49">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc1858)
    %3939 = "vhlo.reshape_v1"(%3938) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x1280x!vhlo.bf16_v1> loc(#loc1859)
    %3940 = "vhlo.reshape_v1"(%arg541) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3941 = "vhlo.custom_call_v1"(%3940) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_2_attn_to_q_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3942 = "vhlo.reshape_v1"(%3941) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3943 = "vhlo.transpose_v1"(%3942) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1860)
    %3944 = "vhlo.dot_general_v2"(%3939, %3943) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x1280x!vhlo.bf16_v1> loc(#loc1861)
    %3945 = "vhlo.reshape_v1"(%3944) : (!vhlo.tensor_v1<16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x20x64x!vhlo.bf16_v1> loc(#loc1862)
    %3946 = "vhlo.transpose_v1"(%3945) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,20,16,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x20x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x20x16x64x!vhlo.bf16_v1> loc(#loc1863)
    %3947 = "vhlo.convert_v1"(%3946) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,20,16,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x20x16x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x20x16x64x!vhlo.f32_v1> loc(#loc1864)
    %3948 = "vhlo.multiply_v1"(%3947, %8) : (!vhlo.tensor_v1<1x20x16x64x!vhlo.f32_v1>, !vhlo.tensor_v1<1x20x16x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x64x!vhlo.f32_v1> loc(#loc1865)
    %3949 = "vhlo.reshape_v1"(%arg539) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3950 = "vhlo.custom_call_v1"(%3949) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_2_ln0_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3951 = "vhlo.reshape_v1"(%3950) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3952 = "vhlo.reshape_v1"(%arg538) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %3953 = "vhlo.custom_call_v1"(%3952) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_2_ln0_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %3954 = "vhlo.reshape_v1"(%3953) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %3955 = "vhlo.composite_v1"(%3756, %3951, %3954) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_2">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1866)
    %3956 = "vhlo.concatenate_v1"(%3955, %3938) <{dimension = #vhlo.integer_v1<1 : i64>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x273x1280x!vhlo.bf16_v1> loc(#loc1867)
    %3957 = "vhlo.reshape_v1"(%3956) : (!vhlo.tensor_v1<1x273x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<273x1280x!vhlo.bf16_v1> loc(#loc1868)
    %3958 = "vhlo.reshape_v1"(%arg540) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3959 = "vhlo.custom_call_v1"(%3958) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_2_attn_to_k_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3960 = "vhlo.reshape_v1"(%3959) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3961 = "vhlo.transpose_v1"(%3960) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1869)
    %3962 = "vhlo.dot_general_v2"(%3957, %3961) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<273x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<273x1280x!vhlo.bf16_v1> loc(#loc1870)
    %3963 = "vhlo.reshape_v1"(%3962) : (!vhlo.tensor_v1<273x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x273x20x64x!vhlo.bf16_v1> loc(#loc1871)
    %3964 = "vhlo.transpose_v1"(%3963) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,20,273,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x273x20x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x20x273x64x!vhlo.bf16_v1> loc(#loc1872)
    %3965 = "vhlo.convert_v1"(%3964) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,20,273,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x20x273x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x20x273x64x!vhlo.f32_v1> loc(#loc1873)
    %3966 = "vhlo.transpose_v1"(%3965) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,20,64,273]{2,1,3,0}">} : (!vhlo.tensor_v1<1x20x273x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x64x273x!vhlo.f32_v1> loc(#loc1874)
    %3967 = "vhlo.multiply_v1"(%3966, %3) : (!vhlo.tensor_v1<1x20x64x273x!vhlo.f32_v1>, !vhlo.tensor_v1<1x20x64x273x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x64x273x!vhlo.f32_v1> loc(#loc1875)
    %3968 = "vhlo.dot_general_v2"(%3948, %3967) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x20x16x64x!vhlo.f32_v1>, !vhlo.tensor_v1<1x20x64x273x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1> loc(#loc1876)
    %3969 = "vhlo.convert_v1"(%3968) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f64_v1> loc(#loc1877)
    %3970 = "vhlo.compare_v1"(%3969, %2) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f64_v1>, !vhlo.tensor_v1<1x20x16x273x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.bool_v1> loc(#loc1877)
    %3971 = "vhlo.not_v1"(%3970) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.bool_v1> loc(#loc1878)
    %3972 = "vhlo.reduce_v1"(%3971, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("3095|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_35aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("3095|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_35aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1880)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1881)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x20x16x!vhlo.bool_v1> loc(#loc1879)
    %3973 = "vhlo.reshape_v1"(%3972) : (!vhlo.tensor_v1<1x20x16x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x20x16x1x!vhlo.bool_v1> loc(#loc1879)
    %3974 = "vhlo.not_v1"(%3973) : (!vhlo.tensor_v1<1x20x16x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x20x16x1x!vhlo.bool_v1> loc(#loc1882)
    %3975 = "vhlo.reshape_v1"(%3974) : (!vhlo.tensor_v1<1x20x16x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x20x16x!vhlo.bool_v1> loc(#loc1883)
    %3976 = "vhlo.broadcast_in_dim_v1"(%3975) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x20x16x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.bool_v1> loc(#loc1883)
    %3977 = "vhlo.reduce_v1"(%3968, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3092|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_34aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3092|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_34aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1885)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x!vhlo.f32_v1> loc(#loc1884)
    %3978 = "vhlo.broadcast_in_dim_v1"(%3977) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x20x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1> loc(#loc1884)
    %3979 = "vhlo.subtract_v1"(%3968, %3978) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>, !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1> loc(#loc1884)
    %3980 = "vhlo.exponential_v2"(%3979) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1> loc(#loc1884)
    %3981 = "vhlo.reduce_v1"(%3980, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3092|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_34aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3092|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_34aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1886)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x!vhlo.f32_v1> loc(#loc1884)
    %3982 = "vhlo.broadcast_in_dim_v1"(%3981) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x20x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1> loc(#loc1884)
    %3983 = "vhlo.divide_v1"(%3980, %3982) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>, !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1> loc(#loc1884)
    %3984 = "vhlo.select_v1"(%3976, %1, %3983) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.bool_v1>, !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>, !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1> loc(#loc1887)
    %3985 = "vhlo.reshape_v1"(%arg535) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3986 = "vhlo.custom_call_v1"(%3985) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_2_attn_to_v_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3987 = "vhlo.reshape_v1"(%3986) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3988 = "vhlo.transpose_v1"(%3987) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1888)
    %3989 = "vhlo.dot_general_v2"(%3957, %3988) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<273x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<273x1280x!vhlo.bf16_v1> loc(#loc1889)
    %3990 = "vhlo.reshape_v1"(%3989) : (!vhlo.tensor_v1<273x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x273x20x64x!vhlo.bf16_v1> loc(#loc1890)
    %3991 = "vhlo.transpose_v1"(%3990) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,20,273,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x273x20x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x20x273x64x!vhlo.bf16_v1> loc(#loc1891)
    %3992 = "vhlo.convert_v1"(%3991) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,20,273,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x20x273x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x20x273x64x!vhlo.f32_v1> loc(#loc1892)
    %3993 = "vhlo.dot_general_v2"(%3984, %3992) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>, !vhlo.tensor_v1<1x20x273x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x64x!vhlo.f32_v1> loc(#loc1893)
    %3994 = "vhlo.convert_v1"(%3993) : (!vhlo.tensor_v1<1x20x16x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x64x!vhlo.bf16_v1> loc(#loc1894)
    %3995 = "vhlo.transpose_v1"(%3994) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,20,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x20x16x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x20x64x!vhlo.bf16_v1> loc(#loc1895)
    %3996 = "vhlo.reshape_v1"(%3995) : (!vhlo.tensor_v1<1x16x20x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x1280x!vhlo.bf16_v1> loc(#loc1896)
    %3997 = "vhlo.reshape_v1"(%arg534) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %3998 = "vhlo.custom_call_v1"(%3997) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_2_attn_to_out_0_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %3999 = "vhlo.reshape_v1"(%3998) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %4000 = "vhlo.transpose_v1"(%3999) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1897)
    %4001 = "vhlo.dot_general_v2"(%3996, %4000) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x1280x!vhlo.bf16_v1> loc(#loc1898)
    %4002 = "vhlo.reshape_v1"(%4001) : (!vhlo.tensor_v1<16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc1899)
    %4003 = "vhlo.divide_v1"(%4002, %0) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc1900)
    %4004 = "vhlo.add_v1"(%4003, %3931) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc1901)
    %4005 = "vhlo.reshape_v1"(%arg545) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %4006 = "vhlo.custom_call_v1"(%4005) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_2_ff_0_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %4007 = "vhlo.reshape_v1"(%4006) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %4008 = "vhlo.reshape_v1"(%arg544) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %4009 = "vhlo.custom_call_v1"(%4008) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_2_ff_0_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %4010 = "vhlo.reshape_v1"(%4009) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %4011 = "vhlo.composite_v1"(%4004, %4007, %4010) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_26">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc1902)
    %4012 = "vhlo.reshape_v1"(%4011) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x1280x!vhlo.bf16_v1> loc(#loc1903)
    %4013 = "vhlo.reshape_v1"(%arg543) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %4014 = "vhlo.custom_call_v1"(%4013) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"getattr_l__self___resampler_layers_2_ff___1___net_0_proj_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %4015 = "vhlo.reshape_v1"(%4014) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %4016 = "vhlo.transpose_v1"(%4015) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc1904)
    %4017 = "vhlo.dot_general_v2"(%4012, %4016) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x5120x!vhlo.bf16_v1> loc(#loc1905)
    %4018 = "vhlo.reshape_v1"(%4017) : (!vhlo.tensor_v1<16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc1903)
    %4019 = "vhlo.composite_v1"(%4018) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_3">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc1906)
    %4020 = "vhlo.reshape_v1"(%4019) : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x5120x!vhlo.bf16_v1> loc(#loc1907)
    %4021 = "vhlo.reshape_v1"(%arg542) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %4022 = "vhlo.custom_call_v1"(%4021) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"getattr_l__self___resampler_layers_2_ff___1___net_2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %4023 = "vhlo.reshape_v1"(%4022) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %4024 = "vhlo.transpose_v1"(%4023) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc1908)
    %4025 = "vhlo.dot_general_v2"(%4020, %4024) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<16x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x1280x!vhlo.bf16_v1> loc(#loc1909)
    %4026 = "vhlo.reshape_v1"(%4025) : (!vhlo.tensor_v1<16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc1907)
    %4027 = "vhlo.add_v1"(%4026, %4004) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc1910)
    %4028 = "vhlo.reshape_v1"(%arg549) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %4029 = "vhlo.custom_call_v1"(%4028) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_3_ln1_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %4030 = "vhlo.reshape_v1"(%4029) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %4031 = "vhlo.reshape_v1"(%arg548) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %4032 = "vhlo.custom_call_v1"(%4031) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_3_ln1_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %4033 = "vhlo.reshape_v1"(%4032) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %4034 = "vhlo.composite_v1"(%4027, %4030, %4033) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_71">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc1911)
    %4035 = "vhlo.reshape_v1"(%4034) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x1280x!vhlo.bf16_v1> loc(#loc1912)
    %4036 = "vhlo.reshape_v1"(%arg553) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %4037 = "vhlo.custom_call_v1"(%4036) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_3_attn_to_q_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %4038 = "vhlo.reshape_v1"(%4037) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %4039 = "vhlo.transpose_v1"(%4038) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1913)
    %4040 = "vhlo.dot_general_v2"(%4035, %4039) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x1280x!vhlo.bf16_v1> loc(#loc1914)
    %4041 = "vhlo.reshape_v1"(%4040) : (!vhlo.tensor_v1<16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x20x64x!vhlo.bf16_v1> loc(#loc1915)
    %4042 = "vhlo.transpose_v1"(%4041) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,20,16,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x16x20x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x20x16x64x!vhlo.bf16_v1> loc(#loc1916)
    %4043 = "vhlo.convert_v1"(%4042) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,20,16,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x20x16x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x20x16x64x!vhlo.f32_v1> loc(#loc1917)
    %4044 = "vhlo.multiply_v1"(%4043, %8) : (!vhlo.tensor_v1<1x20x16x64x!vhlo.f32_v1>, !vhlo.tensor_v1<1x20x16x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x64x!vhlo.f32_v1> loc(#loc1918)
    %4045 = "vhlo.reshape_v1"(%arg551) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %4046 = "vhlo.custom_call_v1"(%4045) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_3_ln0_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %4047 = "vhlo.reshape_v1"(%4046) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %4048 = "vhlo.reshape_v1"(%arg550) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %4049 = "vhlo.custom_call_v1"(%4048) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_3_ln0_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %4050 = "vhlo.reshape_v1"(%4049) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %4051 = "vhlo.composite_v1"(%3756, %4047, %4050) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_1">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc1919)
    %4052 = "vhlo.concatenate_v1"(%4051, %4034) <{dimension = #vhlo.integer_v1<1 : i64>}> : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x273x1280x!vhlo.bf16_v1> loc(#loc1920)
    %4053 = "vhlo.reshape_v1"(%4052) : (!vhlo.tensor_v1<1x273x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<273x1280x!vhlo.bf16_v1> loc(#loc1921)
    %4054 = "vhlo.reshape_v1"(%arg552) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %4055 = "vhlo.custom_call_v1"(%4054) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_3_attn_to_k_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %4056 = "vhlo.reshape_v1"(%4055) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %4057 = "vhlo.transpose_v1"(%4056) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1922)
    %4058 = "vhlo.dot_general_v2"(%4053, %4057) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<273x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<273x1280x!vhlo.bf16_v1> loc(#loc1923)
    %4059 = "vhlo.reshape_v1"(%4058) : (!vhlo.tensor_v1<273x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x273x20x64x!vhlo.bf16_v1> loc(#loc1924)
    %4060 = "vhlo.transpose_v1"(%4059) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,20,273,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x273x20x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x20x273x64x!vhlo.bf16_v1> loc(#loc1925)
    %4061 = "vhlo.convert_v1"(%4060) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,20,273,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x20x273x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x20x273x64x!vhlo.f32_v1> loc(#loc1926)
    %4062 = "vhlo.transpose_v1"(%4061) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 1, 3, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,20,64,273]{2,1,3,0}">} : (!vhlo.tensor_v1<1x20x273x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x64x273x!vhlo.f32_v1> loc(#loc1927)
    %4063 = "vhlo.multiply_v1"(%4062, %3) : (!vhlo.tensor_v1<1x20x64x273x!vhlo.f32_v1>, !vhlo.tensor_v1<1x20x64x273x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x64x273x!vhlo.f32_v1> loc(#loc1928)
    %4064 = "vhlo.dot_general_v2"(%4044, %4063) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x20x16x64x!vhlo.f32_v1>, !vhlo.tensor_v1<1x20x64x273x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1> loc(#loc1929)
    %4065 = "vhlo.convert_v1"(%4064) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f64_v1> loc(#loc1930)
    %4066 = "vhlo.compare_v1"(%4065, %2) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f64_v1>, !vhlo.tensor_v1<1x20x16x273x!vhlo.f64_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.bool_v1> loc(#loc1930)
    %4067 = "vhlo.not_v1"(%4066) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.bool_v1> loc(#loc1931)
    %4068 = "vhlo.reduce_v1"(%4067, %11) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.bool_v1> loc("3180|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_36aten__any"), %arg559: !vhlo.tensor_v1<!vhlo.bool_v1> loc("3180|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_36aten__any")):
      %4143 = "vhlo.or_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1933)
      %4144 = "vhlo.select_v1"(%4143, %10, %11) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc1934)
      "vhlo.return_v1"(%4144) : (!vhlo.tensor_v1<!vhlo.bool_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x20x16x!vhlo.bool_v1> loc(#loc1932)
    %4069 = "vhlo.reshape_v1"(%4068) : (!vhlo.tensor_v1<1x20x16x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x20x16x1x!vhlo.bool_v1> loc(#loc1932)
    %4070 = "vhlo.not_v1"(%4069) : (!vhlo.tensor_v1<1x20x16x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x20x16x1x!vhlo.bool_v1> loc(#loc1935)
    %4071 = "vhlo.reshape_v1"(%4070) : (!vhlo.tensor_v1<1x20x16x1x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x20x16x!vhlo.bool_v1> loc(#loc1936)
    %4072 = "vhlo.broadcast_in_dim_v1"(%4071) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x20x16x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.bool_v1> loc(#loc1936)
    %4073 = "vhlo.reduce_v1"(%4064, %9) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3177|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_35aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3177|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_35aten__softmax")):
      %4143 = "vhlo.maximum_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1938)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x!vhlo.f32_v1> loc(#loc1937)
    %4074 = "vhlo.broadcast_in_dim_v1"(%4073) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x20x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1> loc(#loc1937)
    %4075 = "vhlo.subtract_v1"(%4064, %4074) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>, !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1> loc(#loc1937)
    %4076 = "vhlo.exponential_v2"(%4075) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1> loc(#loc1937)
    %4077 = "vhlo.reduce_v1"(%4076, %12) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg558: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3177|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_35aten__softmax"), %arg559: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3177|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_35aten__softmax")):
      %4143 = "vhlo.add_v1"(%arg558, %arg559) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1939)
      "vhlo.return_v1"(%4143) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x!vhlo.f32_v1> loc(#loc1937)
    %4078 = "vhlo.broadcast_in_dim_v1"(%4077) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x20x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1> loc(#loc1937)
    %4079 = "vhlo.divide_v1"(%4076, %4078) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>, !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1> loc(#loc1937)
    %4080 = "vhlo.select_v1"(%4072, %1, %4079) : (!vhlo.tensor_v1<1x20x16x273x!vhlo.bool_v1>, !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>, !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1> loc(#loc1940)
    %4081 = "vhlo.reshape_v1"(%arg547) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %4082 = "vhlo.custom_call_v1"(%4081) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_3_attn_to_v_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %4083 = "vhlo.reshape_v1"(%4082) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %4084 = "vhlo.transpose_v1"(%4083) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1941)
    %4085 = "vhlo.dot_general_v2"(%4053, %4084) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<273x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<273x1280x!vhlo.bf16_v1> loc(#loc1942)
    %4086 = "vhlo.reshape_v1"(%4085) : (!vhlo.tensor_v1<273x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x273x20x64x!vhlo.bf16_v1> loc(#loc1943)
    %4087 = "vhlo.transpose_v1"(%4086) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,20,273,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x273x20x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x20x273x64x!vhlo.bf16_v1> loc(#loc1944)
    %4088 = "vhlo.convert_v1"(%4087) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,20,273,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x20x273x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x20x273x64x!vhlo.f32_v1> loc(#loc1945)
    %4089 = "vhlo.dot_general_v2"(%4080, %4088) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x20x16x273x!vhlo.f32_v1>, !vhlo.tensor_v1<1x20x273x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x64x!vhlo.f32_v1> loc(#loc1946)
    %4090 = "vhlo.convert_v1"(%4089) : (!vhlo.tensor_v1<1x20x16x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x20x16x64x!vhlo.bf16_v1> loc(#loc1947)
    %4091 = "vhlo.transpose_v1"(%4090) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,16,20,64]{3,1,2,0}">} : (!vhlo.tensor_v1<1x20x16x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x20x64x!vhlo.bf16_v1> loc(#loc1948)
    %4092 = "vhlo.reshape_v1"(%4091) : (!vhlo.tensor_v1<1x16x20x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x1280x!vhlo.bf16_v1> loc(#loc1949)
    %4093 = "vhlo.reshape_v1"(%arg546) : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %4094 = "vhlo.custom_call_v1"(%4093) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_3_attn_to_out_0_weight">}>} : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1> loc(#loc2)
    %4095 = "vhlo.reshape_v1"(%4094) : (!vhlo.tensor_v1<1x1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc3)
    %4096 = "vhlo.transpose_v1"(%4095) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,1280]{0,1}">} : (!vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1> loc(#loc1950)
    %4097 = "vhlo.dot_general_v2"(%4092, %4096) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x1280x!vhlo.bf16_v1> loc(#loc1951)
    %4098 = "vhlo.reshape_v1"(%4097) : (!vhlo.tensor_v1<16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc1952)
    %4099 = "vhlo.divide_v1"(%4098, %0) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc1953)
    %4100 = "vhlo.add_v1"(%4099, %4027) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc1954)
    %4101 = "vhlo.reshape_v1"(%arg557) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %4102 = "vhlo.custom_call_v1"(%4101) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_3_ff_0_weight">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %4103 = "vhlo.reshape_v1"(%4102) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %4104 = "vhlo.reshape_v1"(%arg556) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc3)
    %4105 = "vhlo.custom_call_v1"(%4104) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_layers_3_ff_0_bias">}>} : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1> loc(#loc2)
    %4106 = "vhlo.reshape_v1"(%4105) : (!vhlo.tensor_v1<1x1x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc(#loc3)
    %4107 = "vhlo.composite_v1"(%4100, %4103, %4106) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<1280> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl_0">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc1955)
    %4108 = "vhlo.reshape_v1"(%4107) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x1280x!vhlo.bf16_v1> loc(#loc1956)
    %4109 = "vhlo.reshape_v1"(%arg555) : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %4110 = "vhlo.custom_call_v1"(%4109) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"getattr_l__self___resampler_layers_3_ff___1___net_0_proj_weight">}>} : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1> loc(#loc2)
    %4111 = "vhlo.reshape_v1"(%4110) : (!vhlo.tensor_v1<1x5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc3)
    %4112 = "vhlo.transpose_v1"(%4111) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,5120]{0,1}">} : (!vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc1957)
    %4113 = "vhlo.dot_general_v2"(%4108, %4112) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x5120x!vhlo.bf16_v1> loc(#loc1958)
    %4114 = "vhlo.reshape_v1"(%4113) : (!vhlo.tensor_v1<16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc1956)
    %4115 = "vhlo.composite_v1"(%4114) <{composite_attributes = #vhlo.dict_v1<{}>, decomposition = #vhlo.string_v1<"tenstorrent.gelu.impl_24">, name = #vhlo.string_v1<"tenstorrent.gelu">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc1959)
    %4116 = "vhlo.reshape_v1"(%4115) : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x5120x!vhlo.bf16_v1> loc(#loc1960)
    %4117 = "vhlo.reshape_v1"(%arg554) : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %4118 = "vhlo.custom_call_v1"(%4117) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"getattr_l__self___resampler_layers_3_ff___1___net_2_weight">}>} : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1> loc(#loc2)
    %4119 = "vhlo.reshape_v1"(%4118) : (!vhlo.tensor_v1<1x1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1> loc(#loc3)
    %4120 = "vhlo.transpose_v1"(%4119) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[5120,1280]{0,1}">} : (!vhlo.tensor_v1<1280x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1> loc(#loc1961)
    %4121 = "vhlo.dot_general_v2"(%4116, %4120) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<16x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<5120x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x1280x!vhlo.bf16_v1> loc(#loc1962)
    %4122 = "vhlo.reshape_v1"(%4121) : (!vhlo.tensor_v1<16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc1960)
    %4123 = "vhlo.add_v1"(%4122, %4100) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc1963)
    %4124 = "vhlo.reshape_v1"(%4123) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x1280x!vhlo.bf16_v1> loc(#loc1964)
    %4125 = "vhlo.reshape_v1"(%arg3) : (!vhlo.tensor_v1<2048x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x2048x1280x!vhlo.bf16_v1> loc(#loc3)
    %4126 = "vhlo.custom_call_v1"(%4125) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_proj_out_weight">}>} : (!vhlo.tensor_v1<1x2048x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x2048x1280x!vhlo.bf16_v1> loc(#loc2)
    %4127 = "vhlo.reshape_v1"(%4126) : (!vhlo.tensor_v1<1x2048x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x1280x!vhlo.bf16_v1> loc(#loc3)
    %4128 = "vhlo.transpose_v1"(%4127) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[1280,2048]{0,1}">} : (!vhlo.tensor_v1<2048x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x2048x!vhlo.bf16_v1> loc(#loc1965)
    %4129 = "vhlo.dot_general_v2"(%4124, %4128) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<16x1280x!vhlo.bf16_v1>, !vhlo.tensor_v1<1280x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<16x2048x!vhlo.bf16_v1> loc(#loc1966)
    %4130 = "vhlo.reshape_v1"(%4129) : (!vhlo.tensor_v1<16x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x2048x!vhlo.bf16_v1> loc(#loc1964)
    %4131 = "vhlo.reshape_v1"(%arg2) : (!vhlo.tensor_v1<2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x2048x!vhlo.bf16_v1> loc(#loc3)
    %4132 = "vhlo.custom_call_v1"(%4131) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_proj_out_bias">}>} : (!vhlo.tensor_v1<1x1x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x2048x!vhlo.bf16_v1> loc(#loc2)
    %4133 = "vhlo.reshape_v1"(%4132) : (!vhlo.tensor_v1<1x1x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x!vhlo.bf16_v1> loc(#loc3)
    %4134 = "vhlo.broadcast_in_dim_v1"(%4133) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x2048x!vhlo.bf16_v1> loc(#loc1967)
    %4135 = "vhlo.add_v1"(%4130, %4134) : (!vhlo.tensor_v1<1x16x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x2048x!vhlo.bf16_v1> loc(#loc1967)
    %4136 = "vhlo.reshape_v1"(%arg1) : (!vhlo.tensor_v1<2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x2048x!vhlo.bf16_v1> loc(#loc3)
    %4137 = "vhlo.custom_call_v1"(%4136) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_norm_out_weight">}>} : (!vhlo.tensor_v1<1x1x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x2048x!vhlo.bf16_v1> loc(#loc2)
    %4138 = "vhlo.reshape_v1"(%4137) : (!vhlo.tensor_v1<1x1x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x!vhlo.bf16_v1> loc(#loc3)
    %4139 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x2048x!vhlo.bf16_v1> loc(#loc3)
    %4140 = "vhlo.custom_call_v1"(%4139) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___resampler_norm_out_bias">}>} : (!vhlo.tensor_v1<1x1x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x2048x!vhlo.bf16_v1> loc(#loc2)
    %4141 = "vhlo.reshape_v1"(%4140) : (!vhlo.tensor_v1<1x1x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x!vhlo.bf16_v1> loc(#loc3)
    %4142 = "vhlo.composite_v1"(%4135, %4138, %4141) <{composite_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"epsilon"> = #vhlo.float_v1<9.99999974E-6 : !vhlo.f32_v1>, #vhlo.string_v1<"normalized_shape"> = #vhlo.tensor_v1<dense<2048> : tensor<1xi64>>}>, decomposition = #vhlo.string_v1<"tenstorrent.layer_norm.impl">, name = #vhlo.string_v1<"tenstorrent.layer_norm">, version = #vhlo.integer_v1<0 : i32>}> : (!vhlo.tensor_v1<1x16x2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x!vhlo.bf16_v1>, !vhlo.tensor_v1<2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x2048x!vhlo.bf16_v1> loc(#loc1968)
    "vhlo.return_v1"(%4142) : (!vhlo.tensor_v1<1x16x2048x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl(%arg0: !vhlo.tensor_v1<1x16x2048x!vhlo.bf16_v1> loc("3219|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|mark_tensor_384xla__mark_tensor"), %arg1: !vhlo.tensor_v1<2048x!vhlo.bf16_v1> loc("3220|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|mark_tensor_385xla__mark_tensor"), %arg2: !vhlo.tensor_v1<2048x!vhlo.bf16_v1> loc("3221|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|mark_tensor_386xla__mark_tensor")) -> (!vhlo.tensor_v1<1x16x2048x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<4.8828125E-4> : tensor<1x16xf32>>}> : () -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x16x1xf32>>}> : () -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x16x2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x2048x!vhlo.f32_v1> loc(#loc1972)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3223|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|var_mean_78aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3223|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|var_mean_78aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1974)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x2048x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc1973)
    %5 = "vhlo.multiply_v1"(%4, %0) : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc1973)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x2048x!vhlo.f32_v1> loc(#loc1975)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x16x2048x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x2048x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x2048x!vhlo.f32_v1> loc(#loc1975)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x16x2048x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x2048x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x2048x!vhlo.f32_v1> loc(#loc1973)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3223|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|var_mean_78aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3223|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|var_mean_78aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1976)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x2048x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc1973)
    %10 = "vhlo.multiply_v1"(%9, %0) : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc1973)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc1973)
    %12 = "vhlo.add_v1"(%11, %1) : (!vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc1977)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc1978)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc1979)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x2048x!vhlo.f32_v1> loc(#loc1979)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x16x2048x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x2048x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x2048x!vhlo.f32_v1> loc(#loc1979)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x!vhlo.f32_v1> loc(#loc1980)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2048x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x2048x!vhlo.f32_v1> loc(#loc1981)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x16x2048x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x2048x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x2048x!vhlo.f32_v1> loc(#loc1981)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<2048x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2048x!vhlo.f32_v1> loc(#loc1982)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<2048x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x2048x!vhlo.f32_v1> loc(#loc1982)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x16x2048x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x2048x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x2048x!vhlo.f32_v1> loc(#loc1982)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x16x2048x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x2048x!vhlo.bf16_v1> loc(#loc1983)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x16x2048x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_0(%arg0: !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc("3194|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_378xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("3195|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_379xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("3196|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_380xla__mark_tensor")) -> (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x16x1xf32>>}> : () -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x16xf32>>}> : () -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc1987)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3198|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|var_mean_77aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3198|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|var_mean_77aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1989)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc1988)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc1988)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc1990)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc1990)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc1988)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3198|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|var_mean_77aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3198|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|var_mean_77aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc1991)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc1988)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc1988)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc1988)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc1992)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc1993)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc1994)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc1994)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc1994)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc1995)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc1996)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc1996)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc1997)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc1997)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc1997)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc1998)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_1(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("3131|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_370xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("3132|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_371xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("3133|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_372xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2002)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3135|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|var_mean_75aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3135|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|var_mean_75aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2004)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2003)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2003)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2005)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2005)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2003)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3135|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|var_mean_75aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3135|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|var_mean_75aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2006)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2003)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2003)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2003)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2007)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2008)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2009)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2009)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2009)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2010)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2011)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2011)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2012)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2012)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2012)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2013)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_2(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("3046|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_356xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("3047|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_357xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("3048|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_358xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2017)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3050|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|var_mean_72aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3050|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|var_mean_72aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2019)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2018)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2018)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2020)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2020)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2018)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3050|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|var_mean_72aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3050|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|var_mean_72aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2021)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2018)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2018)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2018)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2022)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2023)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2024)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2024)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2024)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2025)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2026)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2026)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2027)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2027)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2027)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2028)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl(%arg0: !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc("3039|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|GELU[getattr(resampler.layers[1].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|mark_tensor_354xla__mark_tensor")) -> (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x16x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x16x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x16x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc2030)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc2030)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc2030)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc2030)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc2030)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_0(%arg0: !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc("2954|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|GELU[getattr(resampler.layers[0].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|mark_tensor_340xla__mark_tensor")) -> (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x16x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x16x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x16x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc2032)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc2032)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc2032)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc2032)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc2032)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_3(%arg0: !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc("2939|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_336xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2940|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_337xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2941|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_338xla__mark_tensor")) -> (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x16x1xf32>>}> : () -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x16xf32>>}> : () -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2036)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2943|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|var_mean_68aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2943|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|var_mean_68aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2038)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc2037)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc2037)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2039)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2039)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2037)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2943|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|var_mean_68aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2943|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|var_mean_68aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2040)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc2037)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc2037)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc2037)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc2041)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc2042)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc2043)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2043)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2043)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2044)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2045)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2045)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2046)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2046)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2046)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc2047)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_1(%arg0: !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc("2791|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[29].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_302xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2049)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2049)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2049)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2049)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2049)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_4(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("2724|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_294xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2725|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_295xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2726|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_296xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2053)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2728|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_59aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2728|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_59aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2055)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2054)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2054)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2056)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2056)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2054)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2728|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_59aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2728|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_59aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2057)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2054)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2054)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2054)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2058)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2059)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2060)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2060)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2060)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2061)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2062)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2062)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2063)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2063)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2063)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2064)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_2(%arg0: !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc("2717|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[28].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_292xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2066)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2066)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2066)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2066)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2066)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_5(%arg0: !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc("2974|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_346xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2975|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_347xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2976|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_348xla__mark_tensor")) -> (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x16x1xf32>>}> : () -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x16xf32>>}> : () -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2070)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2978|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|var_mean_70aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2978|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|var_mean_70aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2072)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc2071)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc2071)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2073)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2073)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2071)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2978|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|var_mean_70aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2978|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|var_mean_70aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2074)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc2071)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc2071)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc2071)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc2075)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc2076)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc2077)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2077)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2077)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2078)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2079)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2079)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2080)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2080)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2080)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc2081)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_6(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("2701|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_288xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2702|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_289xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2703|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_290xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2085)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2705|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_58aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2705|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_58aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2087)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2086)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2086)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2088)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2088)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2086)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2705|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_58aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2705|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_58aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2089)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2086)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2086)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2086)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2090)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2091)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2092)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2092)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2092)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2093)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2094)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2094)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2095)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2095)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2095)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2096)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_7(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("2650|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_284xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2651|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_285xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2652|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_286xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2100)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2654|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_57aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2654|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_57aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2102)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2101)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2101)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2103)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2103)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2101)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2654|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_57aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2654|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_57aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2104)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2101)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2101)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2101)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2105)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2106)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2107)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2107)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2107)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2108)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2109)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2109)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2110)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2110)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2110)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2111)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_3(%arg0: !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc("3124|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|GELU[getattr(resampler.layers[2].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|mark_tensor_368xla__mark_tensor")) -> (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x16x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x16x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x16x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc2113)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc2113)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc2113)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc2113)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc2113)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_8(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("2576|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_274xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2577|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_275xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2578|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_276xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2117)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2580|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_55aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2580|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_55aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2119)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2118)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2118)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2120)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2120)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2118)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2580|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_55aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2580|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_55aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2121)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2118)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2118)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2118)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2122)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2123)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2124)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2124)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2124)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2125)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2126)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2126)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2127)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2127)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2127)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2128)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_9(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("2553|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_268xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2554|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_269xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2555|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_270xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2132)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2557|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_54aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2557|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_54aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2134)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2133)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2133)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2135)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2135)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2133)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2557|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_54aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2557|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_54aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2136)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2133)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2133)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2133)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2137)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2138)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2139)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2139)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2139)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2140)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2141)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2141)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2142)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2142)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2142)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2143)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_10(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("2502|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_264xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2503|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_265xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2504|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_266xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2147)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2506|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_53aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2506|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_53aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2149)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2148)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2148)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2150)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2150)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2148)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2506|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_53aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2506|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_53aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2151)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2148)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2148)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2148)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2152)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2153)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2154)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2154)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2154)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2155)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2156)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2156)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2157)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2157)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2157)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2158)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_4(%arg0: !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc("2495|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[25].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_262xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2160)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2160)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2160)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2160)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2160)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_11(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("2479|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_258xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2480|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_259xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2481|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_260xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2164)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2483|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_52aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2483|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_52aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2166)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2165)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2165)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2167)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2167)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2165)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2483|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_52aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2483|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_52aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2168)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2165)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2165)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2165)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2169)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2170)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2171)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2171)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2171)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2172)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2173)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2173)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2174)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2174)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2174)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2175)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_12(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("2428|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_254xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2429|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_255xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2430|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_256xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2179)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2432|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_51aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2432|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_51aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2181)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2180)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2180)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2182)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2182)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2180)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2432|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_51aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2432|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_51aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2183)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2180)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2180)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2180)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2184)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2185)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2186)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2186)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2186)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2187)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2188)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2188)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2189)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2189)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2189)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2190)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_5(%arg0: !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc("2421|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[24].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_252xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2192)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2192)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2192)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2192)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2192)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_13(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("2405|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_248xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2406|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_249xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2407|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_250xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2196)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2409|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_50aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2409|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_50aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2198)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2197)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2197)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2199)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2199)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2197)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2409|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_50aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2409|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_50aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2200)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2197)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2197)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2197)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2201)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2202)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2203)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2203)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2203)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2204)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2205)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2205)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2206)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2206)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2206)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2207)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_14(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("2354|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_244xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2355|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_245xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2356|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_246xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2211)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2358|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_49aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2358|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_49aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2213)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2212)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2212)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2214)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2214)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2212)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2358|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_49aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2358|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_49aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2215)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2212)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2212)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2212)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2216)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2217)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2218)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2218)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2218)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2219)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2220)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2220)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2221)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2221)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2221)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2222)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_6(%arg0: !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc("2347|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[23].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_242xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2224)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2224)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2224)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2224)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2224)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_15(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("2331|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_238xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2332|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_239xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2333|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_240xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2228)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2335|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_48aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2335|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_48aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2230)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2229)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2229)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2231)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2231)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2229)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2335|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_48aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2335|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_48aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2232)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2229)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2229)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2229)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2233)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2234)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2235)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2235)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2235)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2236)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2237)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2237)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2238)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2238)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2238)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2239)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_16(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("2206|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_224xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2207|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_225xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2208|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_226xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2243)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2210|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_45aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2210|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_45aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2245)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2244)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2244)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2246)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2246)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2244)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2210|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_45aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2210|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_45aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2247)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2244)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2244)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2244)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2248)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2249)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2250)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2250)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2250)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2251)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2252)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2252)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2253)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2253)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2253)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2254)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_7(%arg0: !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc("2199|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[21].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_222xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2256)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2256)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2256)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2256)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2256)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_17(%arg0: !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc("3024|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_350xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("3025|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_351xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("3026|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_352xla__mark_tensor")) -> (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x16x1xf32>>}> : () -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x16xf32>>}> : () -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2260)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3028|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|var_mean_71aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3028|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|var_mean_71aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2262)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc2261)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc2261)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2263)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2263)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2261)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3028|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|var_mean_71aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3028|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|var_mean_71aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2264)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc2261)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc2261)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc2261)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc2265)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc2266)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc2267)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2267)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2267)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2268)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2269)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2269)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2270)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2270)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2270)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc2271)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_18(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("2183|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_218xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2184|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_219xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2185|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_220xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2275)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2187|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_44aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2187|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_44aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2277)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2276)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2276)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2278)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2278)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2276)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2187|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_44aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2187|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_44aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2279)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2276)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2276)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2276)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2280)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2281)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2282)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2282)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2282)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2283)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2284)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2284)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2285)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2285)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2285)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2286)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_8(%arg0: !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc("2125|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[20].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_212xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2288)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2288)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2288)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2288)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2288)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_9(%arg0: !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc("2569|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[26].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_272xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2290)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2290)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2290)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2290)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2290)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_19(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("2058|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_204xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2059|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_205xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2060|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_206xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2294)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2062|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_41aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2062|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_41aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2296)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2295)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2295)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2297)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2297)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2295)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2062|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_41aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2062|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_41aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2298)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2295)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2295)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2295)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2299)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2300)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2301)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2301)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2301)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2302)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2303)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2303)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2304)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2304)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2304)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2305)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_10(%arg0: !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc("2051|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[19].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_202xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2307)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2307)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2307)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2307)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2307)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_20(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("1984|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_194xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1985|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_195xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1986|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_196xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2311)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1988|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_39aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1988|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_39aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2313)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2312)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2312)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2314)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2314)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2312)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1988|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_39aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1988|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_39aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2315)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2312)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2312)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2312)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2316)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2317)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2318)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2318)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2318)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2319)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2320)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2320)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2321)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2321)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2321)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2322)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_11(%arg0: !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc("1237|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[8].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_92xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2324)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2324)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2324)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2324)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2324)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_21(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("1318|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_104xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1319|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_105xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1320|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_106xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2328)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1322|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_21aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1322|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_21aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2330)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2329)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2329)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2331)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2331)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2329)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1322|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_21aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1322|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_21aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2332)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2329)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2329)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2329)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2333)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2334)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2335)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2335)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2335)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2336)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2337)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2337)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2338)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2338)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2338)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2339)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_22(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("1170|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_84xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1171|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_85xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1172|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_86xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2343)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1174|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_17aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1174|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_17aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2345)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2344)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2344)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2346)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2346)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2344)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1174|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_17aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1174|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_17aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2347)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2344)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2344)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2344)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2348)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2349)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2350)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2350)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2350)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2351)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2352)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2352)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2353)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2353)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2353)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2354)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_12(%arg0: !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc("2865|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[30].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_312xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2356)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2356)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2356)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2356)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2356)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_13(%arg0: !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc("1163|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[7].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_82xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2358)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2358)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2358)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2358)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2358)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_23(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("2849|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_308xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2850|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_309xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2851|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_310xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2362)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2853|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_62aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2853|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_62aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2364)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2363)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2363)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2365)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2365)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2363)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2853|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_62aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2853|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_62aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2366)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2363)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2363)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2363)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2367)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2368)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2369)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2369)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2369)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2370)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2371)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2371)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2372)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2372)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2372)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2373)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_14(%arg0: !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc("2273|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[22].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_232xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2375)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2375)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2375)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2375)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2375)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_24(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("2257|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_228xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2258|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_229xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2259|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_230xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2379)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2261|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_46aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2261|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_46aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2381)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2380)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2380)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2382)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2382)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2380)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2261|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_46aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2261|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_46aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2383)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2380)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2380)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2380)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2384)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2385)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2386)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2386)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2386)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2387)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2388)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2388)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2389)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2389)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2389)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2390)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_15(%arg0: !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc("1089|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[6].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_72xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2392)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2392)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2392)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2392)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2392)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_25(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("1073|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_68xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1074|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_69xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1075|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_70xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2396)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1077|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_14aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1077|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_14aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2398)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2397)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2397)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2399)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2399)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2397)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1077|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_14aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1077|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_14aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2400)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2397)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2397)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2397)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2401)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2402)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2403)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2403)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2403)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2404)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2405)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2405)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2406)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2406)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2406)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2407)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_16(%arg0: !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc("645|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[0].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_12xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2409)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2409)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2409)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2409)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2409)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_26(%arg0: !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc("3109|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_364xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("3110|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_365xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("3111|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_366xla__mark_tensor")) -> (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x16x1xf32>>}> : () -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x16xf32>>}> : () -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2413)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3113|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|var_mean_74aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3113|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|var_mean_74aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2415)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc2414)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc2414)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2416)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2416)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2414)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3113|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|var_mean_74aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3113|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|var_mean_74aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2417)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc2414)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc2414)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc2414)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc2418)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc2419)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc2420)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2420)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2420)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2421)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2422)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2422)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2423)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2423)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2423)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc2424)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_27(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("1221|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_88xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1222|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_89xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1223|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_90xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2428)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1225|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_18aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1225|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_18aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2430)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2429)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2429)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2431)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2431)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2429)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1225|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_18aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1225|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_18aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2432)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2429)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2429)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2429)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2433)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2434)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2435)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2435)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2435)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2436)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2437)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2437)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2438)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2438)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2438)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2439)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_28(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("726|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_24xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("727|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_25xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("728|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_26xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2443)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("730|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_5aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("730|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_5aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2445)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2444)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2444)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2446)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2446)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2444)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("730|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_5aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("730|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_5aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2447)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2444)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2444)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2444)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2448)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2449)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2450)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2450)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2450)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2451)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2452)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2452)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2453)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2453)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2453)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2454)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_29(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("1147|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_78xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1148|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_79xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1149|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_80xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2458)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1151|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_16aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1151|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_16aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2460)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2459)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2459)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2461)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2461)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2459)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1151|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_16aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1151|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_16aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2462)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2459)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2459)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2459)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2463)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2464)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2465)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2465)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2465)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2466)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2467)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2467)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2468)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2468)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2468)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2469)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_30(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("1836|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_174xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1837|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_175xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1838|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_176xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2473)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1840|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_35aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1840|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_35aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2475)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2474)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2474)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2476)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2476)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2474)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1840|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_35aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1840|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_35aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2477)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2474)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2474)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2474)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2478)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2479)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2480)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2480)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2480)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2481)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2482)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2482)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2483)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2483)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2483)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2484)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_17(%arg0: !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc("941|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[4].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_52xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2486)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2486)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2486)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2486)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2486)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_31(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("2798|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_304xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2799|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_305xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2800|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_306xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2490)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2802|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_61aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2802|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_61aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2492)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2491)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2491)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2493)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2493)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2491)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2802|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_61aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2802|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_61aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2494)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2491)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2491)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2491)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2495)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2496)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2497)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2497)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2497)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2498)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2499)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2499)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2500)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2500)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2500)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2501)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_32(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("777|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_28xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("778|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_29xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("779|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_30xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2505)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("781|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_6aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("781|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_6aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2507)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2506)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2506)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2508)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2508)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2506)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("781|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_6aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("781|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_6aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2509)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2506)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2506)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2506)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2510)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2511)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2512)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2512)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2512)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2513)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2514)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2514)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2515)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2515)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2515)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2516)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_33(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("1665|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_148xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1666|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_149xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1667|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_150xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2520)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1669|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_30aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1669|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_30aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2522)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2521)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2521)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2523)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2523)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2521)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1669|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_30aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1669|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_30aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2524)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2521)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2521)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2521)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2525)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2526)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2527)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2527)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2527)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2528)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2529)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2529)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2530)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2530)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2530)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2531)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_34(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("578|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_4xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("579|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_5xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("580|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_6xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2535)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("582|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_1aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("582|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_1aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2537)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2536)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2536)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2538)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2538)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2536)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("582|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_1aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("582|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_1aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2539)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2536)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2536)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2536)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2540)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2541)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2542)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2542)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2542)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2543)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2544)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2544)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2545)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2545)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2545)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2546)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_18(%arg0: !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc("1533|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[12].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_132xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2548)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2548)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2548)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2548)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2548)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_35(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("629|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_8xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("630|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_9xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("631|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_10xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2552)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("633|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_2aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("633|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_2aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2554)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2553)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2553)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2555)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2555)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2553)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("633|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_2aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("633|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_2aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2556)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2553)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2553)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2553)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2557)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2558)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2559)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2559)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2559)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2560)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2561)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2561)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2562)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2562)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2562)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2563)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_36(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("703|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_18xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("704|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_19xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("705|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_20xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2567)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("707|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_4aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("707|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_4aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2569)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2568)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2568)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2570)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2570)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2568)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("707|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_4aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("707|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_4aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2571)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2568)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2568)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2568)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2572)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2573)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2574)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2574)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2574)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2575)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2576)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2576)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2577)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2577)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2577)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2578)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_19(%arg0: !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc("719|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[1].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_22xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2580)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2580)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2580)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2580)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2580)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_37(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("565|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|mark_tensorxla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("566|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|mark_tensor_1xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("567|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|mark_tensor_2xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2584)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("569|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|var_meanaten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("569|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|var_meanaten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2586)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2585)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2585)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2587)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2587)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2585)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("569|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|var_meanaten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("569|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|var_meanaten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2588)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2585)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2585)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2585)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2589)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2590)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2591)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2591)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2591)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2592)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2593)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2593)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2594)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2594)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2594)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2595)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_38(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("925|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_48xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("926|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_49xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("927|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_50xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2599)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("929|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_10aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("929|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_10aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2601)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2600)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2600)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2602)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2602)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2600)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("929|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_10aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("929|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_10aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2603)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2600)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2600)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2600)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2604)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2605)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2606)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2606)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2606)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2607)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2608)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2608)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2609)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2609)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2609)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2610)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_39(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("652|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_14xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("653|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_15xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("654|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_16xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2614)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("656|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_3aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("656|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_3aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2616)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2615)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2615)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2617)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2617)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2615)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("656|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_3aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("656|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_3aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2618)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2615)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2615)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2615)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2619)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2620)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2621)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2621)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2621)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2622)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2623)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2623)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2624)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2624)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2624)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2625)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_40(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("1688|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_154xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1689|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_155xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1690|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_156xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2629)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1692|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_31aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1692|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_31aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2631)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2630)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2630)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2632)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2632)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2630)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1692|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_31aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1692|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_31aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2633)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2630)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2630)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2630)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2634)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2635)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2636)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2636)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2636)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2637)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2638)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2638)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2639)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2639)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2639)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2640)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_20(%arg0: !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc("867|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[3].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_42xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2642)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2642)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2642)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2642)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2642)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_41(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("999|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_58xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1000|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_59xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1001|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_60xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2646)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1003|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_12aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1003|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_12aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2648)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2647)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2647)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2649)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2649)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2647)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1003|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_12aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1003|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_12aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2650)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2647)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2647)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2647)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2651)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2652)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2653)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2653)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2653)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2654)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2655)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2655)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2656)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2656)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2656)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2657)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_21(%arg0: !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc("793|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[2].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_32xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2659)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2659)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2659)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2659)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2659)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_42(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("948|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_54xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("949|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_55xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("950|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_56xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2663)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("952|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_11aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("952|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_11aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2665)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2664)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2664)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2666)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2666)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2664)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("952|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_11aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("952|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_11aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2667)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2664)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2664)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2664)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2668)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2669)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2670)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2670)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2670)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2671)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2672)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2672)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2673)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2673)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2673)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2674)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_43(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("800|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_34xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("801|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_35xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("802|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_36xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2678)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("804|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_7aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("804|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_7aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2680)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2679)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2679)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2681)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2681)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2679)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("804|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_7aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("804|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_7aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2682)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2679)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2679)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2679)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2683)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2684)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2685)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2685)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2685)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2686)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2687)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2687)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2688)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2688)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2688)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2689)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_44(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("1762|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_164xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1763|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_165xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1764|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_166xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2693)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1766|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_33aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1766|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_33aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2695)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2694)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2694)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2696)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2696)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2694)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1766|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_33aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1766|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_33aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2697)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2694)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2694)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2694)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2698)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2699)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2700)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2700)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2700)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2701)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2702)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2702)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2703)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2703)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2703)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2704)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_45(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("1295|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_98xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1296|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_99xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1297|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_100xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2708)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1299|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_20aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1299|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_20aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2710)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2709)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2709)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2711)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2711)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2709)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1299|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_20aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1299|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_20aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2712)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2709)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2709)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2709)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2713)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2714)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2715)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2715)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2715)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2716)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2717)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2717)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2718)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2718)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2718)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2719)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_46(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("2627|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_278xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2628|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_279xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2629|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_280xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2723)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2631|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_56aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2631|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_56aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2725)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2724)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2724)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2726)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2726)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2724)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2631|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_56aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2631|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_56aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2727)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2724)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2724)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2724)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2728)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2729)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2730)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2730)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2730)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2731)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2732)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2732)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2733)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2733)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2733)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2734)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_47(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("2109|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_208xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2110|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_209xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2111|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_210xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2738)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2113|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_42aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2113|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_42aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2740)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2739)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2739)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2741)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2741)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2739)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2113|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_42aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2113|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_42aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2742)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2739)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2739)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2739)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2743)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2744)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2745)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2745)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2745)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2746)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2747)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2747)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2748)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2748)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2748)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2749)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_48(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("1739|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_158xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1740|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_159xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1741|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_160xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2753)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1743|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_32aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1743|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_32aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2755)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2754)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2754)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2756)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2756)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2754)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1743|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_32aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1743|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_32aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2757)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2754)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2754)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2754)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2758)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2759)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2760)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2760)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2760)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2761)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2762)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2762)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2763)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2763)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2763)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2764)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_22(%arg0: !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc("1311|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[9].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_102xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2766)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2766)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2766)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2766)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2766)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_49(%arg0: !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc("3059|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_360xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("3060|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_361xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("3061|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_362xla__mark_tensor")) -> (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x16x1xf32>>}> : () -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x16xf32>>}> : () -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2770)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3063|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|var_mean_73aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3063|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|var_mean_73aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2772)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc2771)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc2771)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2773)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2773)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2771)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3063|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|var_mean_73aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3063|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|var_mean_73aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2774)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc2771)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc2771)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc2771)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc2775)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc2776)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc2777)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2777)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2777)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2778)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2779)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2779)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2780)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2780)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2780)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc2781)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_50(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("1813|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_168xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1814|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_169xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1815|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_170xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2785)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1817|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_34aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1817|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_34aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2787)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2786)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2786)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2788)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2788)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2786)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1817|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_34aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1817|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_34aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2789)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2786)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2786)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2786)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2790)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2791)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2792)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2792)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2792)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2793)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2794)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2794)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2795)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2795)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2795)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2796)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_51(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("2035|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_198xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2036|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_199xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2037|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_200xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2800)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2039|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_40aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2039|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_40aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2802)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2801)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2801)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2803)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2803)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2801)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2039|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_40aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2039|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_40aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2804)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2801)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2801)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2801)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2805)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2806)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2807)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2807)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2807)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2808)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2809)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2809)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2810)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2810)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2810)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2811)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_52(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("851|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_38xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("852|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_39xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("853|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_40xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2815)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("855|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_8aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("855|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_8aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2817)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2816)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2816)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2818)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2818)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2816)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("855|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_8aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("855|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_8aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2819)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2816)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2816)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2816)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2820)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2821)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2822)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2822)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2822)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2823)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2824)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2824)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2825)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2825)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2825)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2826)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_23(%arg0: !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc("1977|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[18].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_192xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2828)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2828)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2828)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2828)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2828)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_24(%arg0: !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc("3209|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|GELU[getattr(resampler.layers[3].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|mark_tensor_382xla__mark_tensor")) -> (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x16x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x16x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x16x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc2830)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc2830)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc2830)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc2830)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1> loc(#loc2830)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x16x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_53(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("1369|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_108xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1370|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_109xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1371|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_110xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2834)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1373|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_22aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1373|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_22aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2836)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2835)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2835)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2837)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2837)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2835)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1373|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_22aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1373|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_22aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2838)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2835)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2835)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2835)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2839)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2840)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2841)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2841)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2841)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2842)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2843)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2843)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2844)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2844)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2844)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2845)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_54(%arg0: !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc("2889|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_332xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2890|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_333xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2891|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_334xla__mark_tensor")) -> (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x16x1xf32>>}> : () -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x16xf32>>}> : () -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2849)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2893|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|var_mean_67aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2893|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|var_mean_67aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2851)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc2850)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc2850)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2852)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2852)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2850)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2893|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|var_mean_67aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2893|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|var_mean_67aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2853)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc2850)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc2850)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc2850)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc2854)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc2855)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc2856)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2856)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2856)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2857)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2858)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2858)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2859)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2859)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc2859)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc2860)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_25(%arg0: !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc("1385|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[10].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_112xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2862)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2862)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2862)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2862)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2862)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_55(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("2132|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_214xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2133|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_215xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2134|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_216xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2866)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2136|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_43aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2136|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_43aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2868)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2867)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2867)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2869)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2869)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2867)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2136|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_43aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2136|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_43aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2870)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2867)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2867)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2867)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2871)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2872)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2873)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2873)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2873)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2874)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2875)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2875)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2876)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2876)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2876)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2877)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_56(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("1392|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_114xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1393|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_115xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1394|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_116xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2881)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1396|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_23aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1396|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_23aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2883)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2882)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2882)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2884)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2884)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2882)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1396|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_23aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1396|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_23aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2885)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2882)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2882)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2882)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2886)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2887)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2888)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2888)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2888)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2889)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2890)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2890)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2891)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2891)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2891)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2892)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_57(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("1443|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_118xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1444|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_119xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1445|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_120xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2896)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1447|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_24aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1447|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_24aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2898)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2897)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2897)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2899)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2899)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2897)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1447|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_24aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1447|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_24aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2900)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2897)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2897)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2897)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2901)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2902)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2903)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2903)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2903)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2904)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2905)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2905)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2906)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2906)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2906)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2907)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_58(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("2280|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_234xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2281|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_235xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2282|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_236xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2911)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2284|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_47aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2284|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_47aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2913)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2912)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2912)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2914)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2914)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2912)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2284|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_47aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2284|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_47aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2915)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2912)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2912)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2912)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2916)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2917)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2918)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2918)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2918)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2919)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2920)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2920)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2921)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2921)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2921)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2922)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_59(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("1244|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_94xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1245|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_95xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1246|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_96xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2926)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1248|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_19aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1248|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_19aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2928)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2927)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2927)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2929)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2929)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2927)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1248|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_19aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1248|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_19aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2930)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2927)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2927)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2927)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2931)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2932)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2933)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2933)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2933)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2934)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2935)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2935)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2936)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2936)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2936)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2937)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_60(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("1517|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_128xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1518|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_129xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1519|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_130xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2941)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1521|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_26aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1521|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_26aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2943)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2942)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2942)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2944)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2944)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2942)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1521|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_26aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1521|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_26aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2945)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2942)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2942)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2942)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2946)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2947)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2948)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2948)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2948)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2949)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2950)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2950)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2951)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2951)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2951)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2952)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_26(%arg0: !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc("1015|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[5].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_62xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2954)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2954)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2954)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2954)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc2954)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_61(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("1961|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_188xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1962|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_189xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1963|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_190xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2958)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1965|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_38aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1965|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_38aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2960)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2959)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2959)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2961)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2961)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2959)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1965|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_38aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1965|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_38aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2962)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2959)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2959)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2959)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2963)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2964)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2965)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2965)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2965)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2966)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2967)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2967)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2968)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2968)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2968)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2969)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_62(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("1591|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_138xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1592|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_139xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1593|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_140xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2973)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1595|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_28aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1595|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_28aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2975)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2974)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2974)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2976)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2976)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2974)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1595|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_28aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1595|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_28aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2977)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2974)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2974)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2974)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2978)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2979)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2980)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2980)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2980)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2981)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2982)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2982)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2983)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2983)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2983)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2984)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_63(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("2876|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_328xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2877|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_329xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2878|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_330xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2988)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2880|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|var_mean_66aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2880|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|var_mean_66aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2990)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2989)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2989)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2991)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2991)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2989)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2880|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|var_mean_66aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2880|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|var_mean_66aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc2992)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2989)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2989)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2989)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2993)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc2994)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc2995)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2995)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2995)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2996)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2997)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2997)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc2998)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2998)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc2998)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc2999)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_64(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("1022|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_64xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1023|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_65xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1024|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_66xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3003)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1026|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_13aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1026|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_13aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc3005)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3004)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3004)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3006)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3006)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3004)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1026|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_13aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1026|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_13aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc3007)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3004)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3004)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc3004)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc3008)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc3009)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3010)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3010)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3010)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc3011)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3012)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3012)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc3013)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3013)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3013)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc3014)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_27(%arg0: !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc("1755|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[15].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_162xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3016)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3016)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3016)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3016)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3016)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_28(%arg0: !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc("1607|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[13].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_142xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3018)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3018)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3018)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3018)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3018)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_29(%arg0: !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc("2643|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[27].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_282xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3020)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3020)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3020)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3020)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3020)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_65(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("1096|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_74xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1097|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_75xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1098|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_76xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3024)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1100|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_15aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1100|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_15aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc3026)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3025)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3025)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3027)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3027)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3025)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1100|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_15aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1100|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_15aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc3028)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3025)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3025)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc3025)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc3029)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc3030)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3031)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3031)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3031)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc3032)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3033)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3033)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc3034)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3034)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3034)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc3035)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_66(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("1614|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_144xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1615|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_145xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1616|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_146xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3039)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1618|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_29aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1618|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_29aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc3041)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3040)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3040)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3042)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3042)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3040)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1618|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_29aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1618|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_29aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc3043)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3040)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3040)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc3040)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc3044)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc3045)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3046)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3046)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3046)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc3047)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3048)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3048)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc3049)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3049)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3049)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc3050)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_67(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("2961|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_342xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2962|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_343xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2963|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_344xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3054)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2965|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|var_mean_69aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2965|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|var_mean_69aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc3056)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3055)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3055)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3057)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3057)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3055)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2965|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|var_mean_69aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2965|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|var_mean_69aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc3058)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3055)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3055)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc3055)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc3059)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc3060)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3061)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3061)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3061)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc3062)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3063)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3063)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc3064)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3064)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3064)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc3065)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_30(%arg0: !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc("1459|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[11].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_122xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3067)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3067)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3067)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3067)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3067)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_31(%arg0: !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc("1681|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[14].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_152xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3069)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3069)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3069)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3069)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3069)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_68(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("874|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_44xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("875|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_45xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("876|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_46xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3073)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("878|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_9aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("878|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_9aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc3075)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3074)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3074)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3076)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3076)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3074)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("878|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_9aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("878|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_9aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc3077)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3074)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3074)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc3074)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc3078)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc3079)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3080)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3080)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3080)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc3081)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3082)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3082)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc3083)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3083)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3083)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc3084)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_69(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("1466|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_124xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1467|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_125xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1468|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_126xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3088)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1470|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_25aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1470|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_25aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc3090)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3089)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3089)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3091)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3091)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3089)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1470|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_25aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1470|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_25aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc3092)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3089)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3089)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc3089)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc3093)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc3094)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3095)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3095)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3095)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc3096)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3097)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3097)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc3098)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3098)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3098)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc3099)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_32(%arg0: !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc("1829|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[16].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_172xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3101)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3101)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3101)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3101)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3101)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_70(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("1887|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_178xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1888|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_179xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1889|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_180xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3105)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1891|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_36aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1891|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_36aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc3107)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3106)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3106)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3108)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3108)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3106)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1891|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_36aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1891|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_36aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc3109)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3106)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3106)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc3106)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc3110)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc3111)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3112)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3112)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3112)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc3113)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3114)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3114)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc3115)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3115)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3115)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc3116)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_71(%arg0: !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc("3144|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_374xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("3145|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_375xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("3146|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_376xla__mark_tensor")) -> (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x16x1xf32>>}> : () -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x16xf32>>}> : () -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc3120)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3148|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|var_mean_76aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3148|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|var_mean_76aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc3122)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc3121)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc3121)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc3123)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc3123)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc3121)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3148|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|var_mean_76aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("3148|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|var_mean_76aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc3124)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc3121)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc3121)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc3121)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc3125)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1x!vhlo.f32_v1> loc(#loc3126)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x16x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x!vhlo.f32_v1> loc(#loc3127)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc3127)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc3127)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc3128)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc3129)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc3129)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc3130)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc3130)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1> loc(#loc3130)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x16x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1> loc(#loc3131)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x16x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_72(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("2775|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_298xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2776|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_299xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("2777|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_300xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3135)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2779|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_60aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2779|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_60aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc3137)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3136)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3136)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3138)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3138)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3136)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2779|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_60aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("2779|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_60aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc3139)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3136)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3136)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc3136)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc3140)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc3141)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3142)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3142)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3142)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc3143)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3144)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3144)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc3145)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3145)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3145)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc3146)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_73(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("1540|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_134xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1541|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_135xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1542|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_136xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3150)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1544|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_27aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1544|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_27aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc3152)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3151)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3151)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3153)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3153)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3151)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1544|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_27aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1544|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_27aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc3154)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3151)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3151)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc3151)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc3155)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc3156)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3157)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3157)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3157)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc3158)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3159)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3159)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc3160)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3160)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3160)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc3161)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.gelu.impl_33(%arg0: !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc("1903|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[17].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_182xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.000000e+00> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.070310e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<5.000000e-01> : tensor<1x257x5120xbf16>>}> : () -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc)
    %3 = "vhlo.multiply_v1"(%arg0, %2) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3163)
    %4 = "vhlo.multiply_v1"(%arg0, %1) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3163)
    %5 = "vhlo.custom_call_v1"(%4) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"mhlo.erf">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.attributes = #vhlo.dict_v1<{}>, mhlo.version = #vhlo.integer_v1<1 : i64>} : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3163)
    %6 = "vhlo.add_v1"(%5, %0) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3163)
    %7 = "vhlo.multiply_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1> loc(#loc3163)
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<1x257x5120x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
  vhlo.func_v1 @tenstorrent.layer_norm.impl_74(%arg0: !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc("1910|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_184xla__mark_tensor"), %arg1: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1911|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_185xla__mark_tensor"), %arg2: !vhlo.tensor_v1<1280x!vhlo.bf16_v1> loc("1912|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_186xla__mark_tensor")) -> (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x257x1xf32>>}> : () -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-04> : tensor<1x257xf32>>}> : () -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.convert_v1"(%arg0) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3167)
    %4 = "vhlo.reduce_v1"(%3, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1914|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_37aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1914|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_37aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc3169)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3168)
    %5 = "vhlo.multiply_v1"(%4, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3168)
    %6 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3170)
    %7 = "vhlo.subtract_v1"(%3, %6) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3170)
    %8 = "vhlo.multiply_v1"(%7, %7) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3168)
    %9 = "vhlo.reduce_v1"(%8, %2) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg3: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1914|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_37aten__var_mean"), %arg4: !vhlo.tensor_v1<!vhlo.f32_v1> loc("1914|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_37aten__var_mean")):
      %24 = "vhlo.add_v1"(%arg3, %arg4) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc3171)
      "vhlo.return_v1"(%24) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3168)
    %10 = "vhlo.multiply_v1"(%9, %1) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3168)
    %11 = "vhlo.reshape_v1"(%10) : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc3168)
    %12 = "vhlo.add_v1"(%11, %0) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc3172)
    %13 = "vhlo.rsqrt_v2"(%12) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1x!vhlo.f32_v1> loc(#loc3173)
    %14 = "vhlo.reshape_v1"(%13) : (!vhlo.tensor_v1<1x257x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x!vhlo.f32_v1> loc(#loc3174)
    %15 = "vhlo.broadcast_in_dim_v1"(%14) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x257x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3174)
    %16 = "vhlo.multiply_v1"(%7, %15) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3174)
    %17 = "vhlo.convert_v1"(%arg1) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc3175)
    %18 = "vhlo.broadcast_in_dim_v1"(%17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3176)
    %19 = "vhlo.multiply_v1"(%16, %18) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3176)
    %20 = "vhlo.convert_v1"(%arg2) : (!vhlo.tensor_v1<1280x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1280x!vhlo.f32_v1> loc(#loc3177)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3177)
    %22 = "vhlo.add_v1"(%19, %21) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>, !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1> loc(#loc3177)
    %23 = "vhlo.convert_v1"(%22) : (!vhlo.tensor_v1<1x257x1280x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1> loc(#loc3178)
    "vhlo.return_v1"(%23) : (!vhlo.tensor_v1<1x257x1280x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"private">} loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("-1|unknown|unknown|-1|unknownxla__custom_call")
#loc3 = loc("-1|unknown|unknown|-1|unknownaten__view")
#loc4 = loc("2901|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_335xla__mark_tensor")
#loc5 = loc("2904|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_194aten__view")
#loc6 = loc("2903|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|permute_355aten__permute")
#loc7 = loc("2904|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_194aten__mm")
#loc8 = loc("2909|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|view_129aten__view")
#loc9 = loc("2910|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|permute_358aten__permute")
#loc10 = loc("2915|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_296xla__cast")
#loc11 = loc("2918|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_200aten__mul")
#loc12 = loc("558|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPVisionEmbeddings[image_encoder.vision_model.embeddings]|Conv2d[image_encoder.vision_model.embeddings.patch_embedding]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:195|forward|202|convolutionaten__convolution_overrideable")
#loc13 = loc("559|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPVisionEmbeddings[image_encoder.vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:195|forward|203|viewaten__view")
#loc14 = loc("560|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPVisionEmbeddings[image_encoder.vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:195|forward|203|permuteaten__permute")
#loc15 = loc("562|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPVisionEmbeddings[image_encoder.vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:195|forward|206|cataten__cat")
#loc16 = loc("563|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPVisionEmbeddings[image_encoder.vision_model.embeddings]|Embedding[image_encoder.vision_model.embeddings.position_embedding]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:195|forward|210|embeddingaten__view")
#loc17 = loc("563|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPVisionEmbeddings[image_encoder.vision_model.embeddings]|Embedding[image_encoder.vision_model.embeddings.position_embedding]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:195|forward|210|embeddingaten__index_select")
#loc18 = loc("564|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPVisionEmbeddings[image_encoder.vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:195|forward|210|addaten__add")
#loc19 = loc("577|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|mark_tensor_3xla__mark_tensor")
#loc20 = loc("590|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_7xla__mark_tensor")
#loc21 = loc("592|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmulaten__view")
#loc22 = loc("591|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_1aten__permute")
#loc23 = loc("592|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmulaten__mm")
#loc24 = loc("593|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_5aten__add")
#loc25 = loc("600|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_1aten__view")
#loc26 = loc("601|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_4aten__permute")
#loc27 = loc("606|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_4xla__cast")
#loc28 = loc("609|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_4aten__mul")
#loc29 = loc("594|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_2aten__permute")
#loc30 = loc("595|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_1aten__mm")
#loc31 = loc("595|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_1aten__view")
#loc32 = loc("596|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_6aten__add")
#loc33 = loc("602|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_2aten__view")
#loc34 = loc("603|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_5aten__permute")
#loc35 = loc("607|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_5xla__cast")
#loc36 = loc("610|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_7aten__permute")
#loc37 = loc("611|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_5aten__mul")
#loc38 = loc("613|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmaxaten__einsum")
#loc39 = loc("614|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eqaten__eq")
#loc40 = loc("615|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_notaten__logical_not")
#loc42 = loc("or.2639")
#loc43 = loc("select.2640")
#loc44 = loc("617|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_1aten__logical_not")
#loc45 = loc("619|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|whereaten__expand")
#loc47 = loc("maximum.2604")
#loc48 = loc("add.2613")
#loc49 = loc("619|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|whereaten__where")
#loc50 = loc("597|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_3aten__permute")
#loc51 = loc("598|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_2aten__mm")
#loc52 = loc("598|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_2aten__view")
#loc53 = loc("599|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_7aten__add")
#loc54 = loc("604|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_3aten__view")
#loc55 = loc("605|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_6aten__permute")
#loc56 = loc("608|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_6xla__cast")
#loc57 = loc("621|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_8aten__einsum")
#loc58 = loc("621|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_8xla__cast")
#loc59 = loc("623|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|cloneaten__permute")
#loc60 = loc("626|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_3aten__view")
#loc61 = loc("625|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_9aten__permute")
#loc62 = loc("626|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_3aten__mm")
#loc63 = loc("627|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_8aten__add")
#loc64 = loc("628|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_9aten__add")
#loc65 = loc("641|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_11xla__mark_tensor")
#loc66 = loc("643|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_4aten__view")
#loc67 = loc("642|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_10aten__permute")
#loc68 = loc("643|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_4aten__mm")
#loc69 = loc("644|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_12aten__add")
#loc70 = loc("647|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[0].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_13xla__mark_tensor")
#loc71 = loc("649|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_5aten__view")
#loc72 = loc("648|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_11aten__permute")
#loc73 = loc("649|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_5aten__mm")
#loc74 = loc("650|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_13aten__add")
#loc75 = loc("651|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_14aten__add")
#loc76 = loc("664|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_17xla__mark_tensor")
#loc77 = loc("666|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_6aten__view")
#loc78 = loc("665|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_12aten__permute")
#loc79 = loc("666|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_6aten__mm")
#loc80 = loc("667|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_17aten__add")
#loc81 = loc("674|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_5aten__view")
#loc82 = loc("675|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_15aten__permute")
#loc83 = loc("680|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_13xla__cast")
#loc84 = loc("683|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_10aten__mul")
#loc85 = loc("668|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_13aten__permute")
#loc86 = loc("669|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_7aten__mm")
#loc87 = loc("669|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_7aten__view")
#loc88 = loc("670|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_18aten__add")
#loc89 = loc("676|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_6aten__view")
#loc90 = loc("677|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_16aten__permute")
#loc91 = loc("681|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_14xla__cast")
#loc92 = loc("684|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_18aten__permute")
#loc93 = loc("685|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_11aten__mul")
#loc94 = loc("687|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_1aten__einsum")
#loc95 = loc("688|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_1aten__eq")
#loc96 = loc("689|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_2aten__logical_not")
#loc98 = loc("or.2955")
#loc99 = loc("select.2956")
#loc100 = loc("691|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_3aten__logical_not")
#loc101 = loc("693|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_1aten__expand")
#loc103 = loc("maximum.2920")
#loc104 = loc("add.2929")
#loc105 = loc("693|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_1aten__where")
#loc106 = loc("671|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_14aten__permute")
#loc107 = loc("672|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_8aten__mm")
#loc108 = loc("672|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_8aten__view")
#loc109 = loc("673|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_19aten__add")
#loc110 = loc("678|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_7aten__view")
#loc111 = loc("679|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_17aten__permute")
#loc112 = loc("682|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_15xla__cast")
#loc113 = loc("695|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_17aten__einsum")
#loc114 = loc("695|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_17xla__cast")
#loc115 = loc("697|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_1aten__permute")
#loc116 = loc("700|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_9aten__view")
#loc117 = loc("699|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_20aten__permute")
#loc118 = loc("700|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_9aten__mm")
#loc119 = loc("701|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_20aten__add")
#loc120 = loc("702|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_21aten__add")
#loc121 = loc("715|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_21xla__mark_tensor")
#loc122 = loc("717|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_10aten__view")
#loc123 = loc("716|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_21aten__permute")
#loc124 = loc("717|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_10aten__mm")
#loc125 = loc("718|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_24aten__add")
#loc126 = loc("721|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[1].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_23xla__mark_tensor")
#loc127 = loc("723|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_11aten__view")
#loc128 = loc("722|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_22aten__permute")
#loc129 = loc("723|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_11aten__mm")
#loc130 = loc("724|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_25aten__add")
#loc131 = loc("725|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_26aten__add")
#loc132 = loc("738|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_27xla__mark_tensor")
#loc133 = loc("740|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_12aten__view")
#loc134 = loc("739|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_23aten__permute")
#loc135 = loc("740|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_12aten__mm")
#loc136 = loc("741|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_29aten__add")
#loc137 = loc("748|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_9aten__view")
#loc138 = loc("749|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_26aten__permute")
#loc139 = loc("754|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_22xla__cast")
#loc140 = loc("757|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_16aten__mul")
#loc141 = loc("742|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_24aten__permute")
#loc142 = loc("743|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_13aten__mm")
#loc143 = loc("743|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_13aten__view")
#loc144 = loc("744|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_30aten__add")
#loc145 = loc("750|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_10aten__view")
#loc146 = loc("751|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_27aten__permute")
#loc147 = loc("755|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_23xla__cast")
#loc148 = loc("758|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_29aten__permute")
#loc149 = loc("759|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_17aten__mul")
#loc150 = loc("761|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_2aten__einsum")
#loc151 = loc("762|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_2aten__eq")
#loc152 = loc("763|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_4aten__logical_not")
#loc154 = loc("or.3271")
#loc155 = loc("select.3272")
#loc156 = loc("765|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_5aten__logical_not")
#loc157 = loc("767|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_2aten__expand")
#loc159 = loc("maximum.3236")
#loc160 = loc("add.3245")
#loc161 = loc("767|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_2aten__where")
#loc162 = loc("745|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_25aten__permute")
#loc163 = loc("746|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_14aten__mm")
#loc164 = loc("746|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_14aten__view")
#loc165 = loc("747|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_31aten__add")
#loc166 = loc("752|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_11aten__view")
#loc167 = loc("753|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_28aten__permute")
#loc168 = loc("756|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_24xla__cast")
#loc169 = loc("769|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_26aten__einsum")
#loc170 = loc("769|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_26xla__cast")
#loc171 = loc("771|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_2aten__permute")
#loc172 = loc("774|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_15aten__view")
#loc173 = loc("773|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_31aten__permute")
#loc174 = loc("774|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_15aten__mm")
#loc175 = loc("775|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_32aten__add")
#loc176 = loc("776|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_33aten__add")
#loc177 = loc("789|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_31xla__mark_tensor")
#loc178 = loc("791|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_16aten__view")
#loc179 = loc("790|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_32aten__permute")
#loc180 = loc("791|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_16aten__mm")
#loc181 = loc("792|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_36aten__add")
#loc182 = loc("795|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[2].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_33xla__mark_tensor")
#loc183 = loc("797|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_17aten__view")
#loc184 = loc("796|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_33aten__permute")
#loc185 = loc("797|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_17aten__mm")
#loc186 = loc("798|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_37aten__add")
#loc187 = loc("799|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_38aten__add")
#loc188 = loc("812|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_37xla__mark_tensor")
#loc189 = loc("814|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_18aten__view")
#loc190 = loc("813|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_34aten__permute")
#loc191 = loc("814|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_18aten__mm")
#loc192 = loc("815|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_41aten__add")
#loc193 = loc("822|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_13aten__view")
#loc194 = loc("823|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_37aten__permute")
#loc195 = loc("828|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_31xla__cast")
#loc196 = loc("831|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_22aten__mul")
#loc197 = loc("816|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_35aten__permute")
#loc198 = loc("817|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_19aten__mm")
#loc199 = loc("817|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_19aten__view")
#loc200 = loc("818|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_42aten__add")
#loc201 = loc("824|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_14aten__view")
#loc202 = loc("825|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_38aten__permute")
#loc203 = loc("829|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_32xla__cast")
#loc204 = loc("832|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_40aten__permute")
#loc205 = loc("833|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_23aten__mul")
#loc206 = loc("835|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_3aten__einsum")
#loc207 = loc("836|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_3aten__eq")
#loc208 = loc("837|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_6aten__logical_not")
#loc210 = loc("or.3587")
#loc211 = loc("select.3588")
#loc212 = loc("839|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_7aten__logical_not")
#loc213 = loc("841|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_3aten__expand")
#loc215 = loc("maximum.3552")
#loc216 = loc("add.3561")
#loc217 = loc("841|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_3aten__where")
#loc218 = loc("819|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_36aten__permute")
#loc219 = loc("820|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_20aten__mm")
#loc220 = loc("820|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_20aten__view")
#loc221 = loc("821|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_43aten__add")
#loc222 = loc("826|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_15aten__view")
#loc223 = loc("827|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_39aten__permute")
#loc224 = loc("830|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_33xla__cast")
#loc225 = loc("843|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_35aten__einsum")
#loc226 = loc("843|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_35xla__cast")
#loc227 = loc("845|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_3aten__permute")
#loc228 = loc("848|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_21aten__view")
#loc229 = loc("847|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_42aten__permute")
#loc230 = loc("848|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_21aten__mm")
#loc231 = loc("849|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_44aten__add")
#loc232 = loc("850|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_45aten__add")
#loc233 = loc("863|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_41xla__mark_tensor")
#loc234 = loc("865|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_22aten__view")
#loc235 = loc("864|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_43aten__permute")
#loc236 = loc("865|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_22aten__mm")
#loc237 = loc("866|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_48aten__add")
#loc238 = loc("869|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[3].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_43xla__mark_tensor")
#loc239 = loc("871|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_23aten__view")
#loc240 = loc("870|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_44aten__permute")
#loc241 = loc("871|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_23aten__mm")
#loc242 = loc("872|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_49aten__add")
#loc243 = loc("873|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_50aten__add")
#loc244 = loc("886|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_47xla__mark_tensor")
#loc245 = loc("888|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_24aten__view")
#loc246 = loc("887|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_45aten__permute")
#loc247 = loc("888|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_24aten__mm")
#loc248 = loc("889|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_53aten__add")
#loc249 = loc("896|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_17aten__view")
#loc250 = loc("897|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_48aten__permute")
#loc251 = loc("902|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_40xla__cast")
#loc252 = loc("905|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_28aten__mul")
#loc253 = loc("890|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_46aten__permute")
#loc254 = loc("891|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_25aten__mm")
#loc255 = loc("891|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_25aten__view")
#loc256 = loc("892|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_54aten__add")
#loc257 = loc("898|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_18aten__view")
#loc258 = loc("899|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_49aten__permute")
#loc259 = loc("903|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_41xla__cast")
#loc260 = loc("906|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_51aten__permute")
#loc261 = loc("907|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_29aten__mul")
#loc262 = loc("909|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_4aten__einsum")
#loc263 = loc("910|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_4aten__eq")
#loc264 = loc("911|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_8aten__logical_not")
#loc266 = loc("or.3903")
#loc267 = loc("select.3904")
#loc268 = loc("913|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_9aten__logical_not")
#loc269 = loc("915|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_4aten__expand")
#loc271 = loc("maximum.3868")
#loc272 = loc("add.3877")
#loc273 = loc("915|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_4aten__where")
#loc274 = loc("893|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_47aten__permute")
#loc275 = loc("894|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_26aten__mm")
#loc276 = loc("894|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_26aten__view")
#loc277 = loc("895|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_55aten__add")
#loc278 = loc("900|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_19aten__view")
#loc279 = loc("901|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_50aten__permute")
#loc280 = loc("904|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_42xla__cast")
#loc281 = loc("917|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_44aten__einsum")
#loc282 = loc("917|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_44xla__cast")
#loc283 = loc("919|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_4aten__permute")
#loc284 = loc("922|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_27aten__view")
#loc285 = loc("921|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_53aten__permute")
#loc286 = loc("922|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_27aten__mm")
#loc287 = loc("923|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_56aten__add")
#loc288 = loc("924|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_57aten__add")
#loc289 = loc("937|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_51xla__mark_tensor")
#loc290 = loc("939|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_28aten__view")
#loc291 = loc("938|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_54aten__permute")
#loc292 = loc("939|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_28aten__mm")
#loc293 = loc("940|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_60aten__add")
#loc294 = loc("943|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[4].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_53xla__mark_tensor")
#loc295 = loc("945|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_29aten__view")
#loc296 = loc("944|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_55aten__permute")
#loc297 = loc("945|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_29aten__mm")
#loc298 = loc("946|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_61aten__add")
#loc299 = loc("947|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_62aten__add")
#loc300 = loc("960|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_57xla__mark_tensor")
#loc301 = loc("962|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_30aten__view")
#loc302 = loc("961|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_56aten__permute")
#loc303 = loc("962|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_30aten__mm")
#loc304 = loc("963|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_65aten__add")
#loc305 = loc("970|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_21aten__view")
#loc306 = loc("971|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_59aten__permute")
#loc307 = loc("976|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_49xla__cast")
#loc308 = loc("979|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_34aten__mul")
#loc309 = loc("964|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_57aten__permute")
#loc310 = loc("965|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_31aten__mm")
#loc311 = loc("965|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_31aten__view")
#loc312 = loc("966|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_66aten__add")
#loc313 = loc("972|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_22aten__view")
#loc314 = loc("973|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_60aten__permute")
#loc315 = loc("977|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_50xla__cast")
#loc316 = loc("980|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_62aten__permute")
#loc317 = loc("981|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_35aten__mul")
#loc318 = loc("983|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_5aten__einsum")
#loc319 = loc("984|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_5aten__eq")
#loc320 = loc("985|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_10aten__logical_not")
#loc322 = loc("or.4219")
#loc323 = loc("select.4220")
#loc324 = loc("987|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_11aten__logical_not")
#loc325 = loc("989|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_5aten__expand")
#loc327 = loc("maximum.4184")
#loc328 = loc("add.4193")
#loc329 = loc("989|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_5aten__where")
#loc330 = loc("967|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_58aten__permute")
#loc331 = loc("968|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_32aten__mm")
#loc332 = loc("968|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_32aten__view")
#loc333 = loc("969|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_67aten__add")
#loc334 = loc("974|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_23aten__view")
#loc335 = loc("975|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_61aten__permute")
#loc336 = loc("978|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_51xla__cast")
#loc337 = loc("991|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_53aten__einsum")
#loc338 = loc("991|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_53xla__cast")
#loc339 = loc("993|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_5aten__permute")
#loc340 = loc("996|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_33aten__view")
#loc341 = loc("995|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_64aten__permute")
#loc342 = loc("996|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_33aten__mm")
#loc343 = loc("997|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_68aten__add")
#loc344 = loc("998|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_69aten__add")
#loc345 = loc("1011|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_61xla__mark_tensor")
#loc346 = loc("1013|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_34aten__view")
#loc347 = loc("1012|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_65aten__permute")
#loc348 = loc("1013|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_34aten__mm")
#loc349 = loc("1014|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_72aten__add")
#loc350 = loc("1017|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[5].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_63xla__mark_tensor")
#loc351 = loc("1019|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_35aten__view")
#loc352 = loc("1018|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_66aten__permute")
#loc353 = loc("1019|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_35aten__mm")
#loc354 = loc("1020|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_73aten__add")
#loc355 = loc("1021|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_74aten__add")
#loc356 = loc("1034|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_67xla__mark_tensor")
#loc357 = loc("1036|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_36aten__view")
#loc358 = loc("1035|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_67aten__permute")
#loc359 = loc("1036|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_36aten__mm")
#loc360 = loc("1037|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_77aten__add")
#loc361 = loc("1044|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_25aten__view")
#loc362 = loc("1045|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_70aten__permute")
#loc363 = loc("1050|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_58xla__cast")
#loc364 = loc("1053|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_40aten__mul")
#loc365 = loc("1038|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_68aten__permute")
#loc366 = loc("1039|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_37aten__mm")
#loc367 = loc("1039|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_37aten__view")
#loc368 = loc("1040|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_78aten__add")
#loc369 = loc("1046|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_26aten__view")
#loc370 = loc("1047|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_71aten__permute")
#loc371 = loc("1051|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_59xla__cast")
#loc372 = loc("1054|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_73aten__permute")
#loc373 = loc("1055|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_41aten__mul")
#loc374 = loc("1057|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_6aten__einsum")
#loc375 = loc("1058|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_6aten__eq")
#loc376 = loc("1059|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_12aten__logical_not")
#loc378 = loc("or.4535")
#loc379 = loc("select.4536")
#loc380 = loc("1061|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_13aten__logical_not")
#loc381 = loc("1063|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_6aten__expand")
#loc383 = loc("maximum.4500")
#loc384 = loc("add.4509")
#loc385 = loc("1063|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_6aten__where")
#loc386 = loc("1041|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_69aten__permute")
#loc387 = loc("1042|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_38aten__mm")
#loc388 = loc("1042|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_38aten__view")
#loc389 = loc("1043|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_79aten__add")
#loc390 = loc("1048|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_27aten__view")
#loc391 = loc("1049|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_72aten__permute")
#loc392 = loc("1052|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_60xla__cast")
#loc393 = loc("1065|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_62aten__einsum")
#loc394 = loc("1065|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_62xla__cast")
#loc395 = loc("1067|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_6aten__permute")
#loc396 = loc("1070|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_39aten__view")
#loc397 = loc("1069|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_75aten__permute")
#loc398 = loc("1070|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_39aten__mm")
#loc399 = loc("1071|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_80aten__add")
#loc400 = loc("1072|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_81aten__add")
#loc401 = loc("1085|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_71xla__mark_tensor")
#loc402 = loc("1087|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_40aten__view")
#loc403 = loc("1086|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_76aten__permute")
#loc404 = loc("1087|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_40aten__mm")
#loc405 = loc("1088|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_84aten__add")
#loc406 = loc("1091|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[6].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_73xla__mark_tensor")
#loc407 = loc("1093|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_41aten__view")
#loc408 = loc("1092|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_77aten__permute")
#loc409 = loc("1093|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_41aten__mm")
#loc410 = loc("1094|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_85aten__add")
#loc411 = loc("1095|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_86aten__add")
#loc412 = loc("1108|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_77xla__mark_tensor")
#loc413 = loc("1110|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_42aten__view")
#loc414 = loc("1109|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_78aten__permute")
#loc415 = loc("1110|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_42aten__mm")
#loc416 = loc("1111|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_89aten__add")
#loc417 = loc("1118|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_29aten__view")
#loc418 = loc("1119|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_81aten__permute")
#loc419 = loc("1124|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_67xla__cast")
#loc420 = loc("1127|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_46aten__mul")
#loc421 = loc("1112|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_79aten__permute")
#loc422 = loc("1113|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_43aten__mm")
#loc423 = loc("1113|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_43aten__view")
#loc424 = loc("1114|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_90aten__add")
#loc425 = loc("1120|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_30aten__view")
#loc426 = loc("1121|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_82aten__permute")
#loc427 = loc("1125|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_68xla__cast")
#loc428 = loc("1128|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_84aten__permute")
#loc429 = loc("1129|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_47aten__mul")
#loc430 = loc("1131|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_7aten__einsum")
#loc431 = loc("1132|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_7aten__eq")
#loc432 = loc("1133|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_14aten__logical_not")
#loc434 = loc("or.4851")
#loc435 = loc("select.4852")
#loc436 = loc("1135|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_15aten__logical_not")
#loc437 = loc("1137|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_7aten__expand")
#loc439 = loc("maximum.4816")
#loc440 = loc("add.4825")
#loc441 = loc("1137|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_7aten__where")
#loc442 = loc("1115|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_80aten__permute")
#loc443 = loc("1116|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_44aten__mm")
#loc444 = loc("1116|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_44aten__view")
#loc445 = loc("1117|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_91aten__add")
#loc446 = loc("1122|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_31aten__view")
#loc447 = loc("1123|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_83aten__permute")
#loc448 = loc("1126|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_69xla__cast")
#loc449 = loc("1139|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_71aten__einsum")
#loc450 = loc("1139|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_71xla__cast")
#loc451 = loc("1141|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_7aten__permute")
#loc452 = loc("1144|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_45aten__view")
#loc453 = loc("1143|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_86aten__permute")
#loc454 = loc("1144|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_45aten__mm")
#loc455 = loc("1145|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_92aten__add")
#loc456 = loc("1146|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_93aten__add")
#loc457 = loc("1159|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_81xla__mark_tensor")
#loc458 = loc("1161|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_46aten__view")
#loc459 = loc("1160|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_87aten__permute")
#loc460 = loc("1161|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_46aten__mm")
#loc461 = loc("1162|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_96aten__add")
#loc462 = loc("1165|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[7].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_83xla__mark_tensor")
#loc463 = loc("1167|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_47aten__view")
#loc464 = loc("1166|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_88aten__permute")
#loc465 = loc("1167|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_47aten__mm")
#loc466 = loc("1168|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_97aten__add")
#loc467 = loc("1169|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_98aten__add")
#loc468 = loc("1182|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_87xla__mark_tensor")
#loc469 = loc("1184|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_48aten__view")
#loc470 = loc("1183|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_89aten__permute")
#loc471 = loc("1184|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_48aten__mm")
#loc472 = loc("1185|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_101aten__add")
#loc473 = loc("1192|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_33aten__view")
#loc474 = loc("1193|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_92aten__permute")
#loc475 = loc("1198|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_76xla__cast")
#loc476 = loc("1201|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_52aten__mul")
#loc477 = loc("1186|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_90aten__permute")
#loc478 = loc("1187|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_49aten__mm")
#loc479 = loc("1187|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_49aten__view")
#loc480 = loc("1188|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_102aten__add")
#loc481 = loc("1194|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_34aten__view")
#loc482 = loc("1195|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_93aten__permute")
#loc483 = loc("1199|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_77xla__cast")
#loc484 = loc("1202|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_95aten__permute")
#loc485 = loc("1203|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_53aten__mul")
#loc486 = loc("1205|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_8aten__einsum")
#loc487 = loc("1206|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_8aten__eq")
#loc488 = loc("1207|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_16aten__logical_not")
#loc490 = loc("or.5167")
#loc491 = loc("select.5168")
#loc492 = loc("1209|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_17aten__logical_not")
#loc493 = loc("1211|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_8aten__expand")
#loc495 = loc("maximum.5132")
#loc496 = loc("add.5141")
#loc497 = loc("1211|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_8aten__where")
#loc498 = loc("1189|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_91aten__permute")
#loc499 = loc("1190|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_50aten__mm")
#loc500 = loc("1190|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_50aten__view")
#loc501 = loc("1191|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_103aten__add")
#loc502 = loc("1196|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_35aten__view")
#loc503 = loc("1197|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_94aten__permute")
#loc504 = loc("1200|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_78xla__cast")
#loc505 = loc("1213|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_80aten__einsum")
#loc506 = loc("1213|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_80xla__cast")
#loc507 = loc("1215|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_8aten__permute")
#loc508 = loc("1218|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_51aten__view")
#loc509 = loc("1217|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_97aten__permute")
#loc510 = loc("1218|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_51aten__mm")
#loc511 = loc("1219|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_104aten__add")
#loc512 = loc("1220|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_105aten__add")
#loc513 = loc("1233|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_91xla__mark_tensor")
#loc514 = loc("1235|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_52aten__view")
#loc515 = loc("1234|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_98aten__permute")
#loc516 = loc("1235|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_52aten__mm")
#loc517 = loc("1236|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_108aten__add")
#loc518 = loc("1239|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[8].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_93xla__mark_tensor")
#loc519 = loc("1241|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_53aten__view")
#loc520 = loc("1240|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_99aten__permute")
#loc521 = loc("1241|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_53aten__mm")
#loc522 = loc("1242|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_109aten__add")
#loc523 = loc("1243|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_110aten__add")
#loc524 = loc("1256|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_97xla__mark_tensor")
#loc525 = loc("1258|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_54aten__view")
#loc526 = loc("1257|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_100aten__permute")
#loc527 = loc("1258|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_54aten__mm")
#loc528 = loc("1259|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_113aten__add")
#loc529 = loc("1266|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_37aten__view")
#loc530 = loc("1267|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_103aten__permute")
#loc531 = loc("1272|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_85xla__cast")
#loc532 = loc("1275|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_58aten__mul")
#loc533 = loc("1260|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_101aten__permute")
#loc534 = loc("1261|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_55aten__mm")
#loc535 = loc("1261|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_55aten__view")
#loc536 = loc("1262|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_114aten__add")
#loc537 = loc("1268|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_38aten__view")
#loc538 = loc("1269|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_104aten__permute")
#loc539 = loc("1273|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_86xla__cast")
#loc540 = loc("1276|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_106aten__permute")
#loc541 = loc("1277|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_59aten__mul")
#loc542 = loc("1279|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_9aten__einsum")
#loc543 = loc("1280|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_9aten__eq")
#loc544 = loc("1281|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_18aten__logical_not")
#loc546 = loc("or.5483")
#loc547 = loc("select.5484")
#loc548 = loc("1283|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_19aten__logical_not")
#loc549 = loc("1285|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_9aten__expand")
#loc551 = loc("maximum.5448")
#loc552 = loc("add.5457")
#loc553 = loc("1285|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_9aten__where")
#loc554 = loc("1263|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_102aten__permute")
#loc555 = loc("1264|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_56aten__mm")
#loc556 = loc("1264|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_56aten__view")
#loc557 = loc("1265|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_115aten__add")
#loc558 = loc("1270|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_39aten__view")
#loc559 = loc("1271|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_105aten__permute")
#loc560 = loc("1274|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_87xla__cast")
#loc561 = loc("1287|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_89aten__einsum")
#loc562 = loc("1287|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_89xla__cast")
#loc563 = loc("1289|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_9aten__permute")
#loc564 = loc("1292|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_57aten__view")
#loc565 = loc("1291|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_108aten__permute")
#loc566 = loc("1292|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_57aten__mm")
#loc567 = loc("1293|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_116aten__add")
#loc568 = loc("1294|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_117aten__add")
#loc569 = loc("1307|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_101xla__mark_tensor")
#loc570 = loc("1309|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_58aten__view")
#loc571 = loc("1308|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_109aten__permute")
#loc572 = loc("1309|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_58aten__mm")
#loc573 = loc("1310|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_120aten__add")
#loc574 = loc("1313|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[9].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_103xla__mark_tensor")
#loc575 = loc("1315|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_59aten__view")
#loc576 = loc("1314|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_110aten__permute")
#loc577 = loc("1315|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_59aten__mm")
#loc578 = loc("1316|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_121aten__add")
#loc579 = loc("1317|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_122aten__add")
#loc580 = loc("1330|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_107xla__mark_tensor")
#loc581 = loc("1332|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_60aten__view")
#loc582 = loc("1331|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_111aten__permute")
#loc583 = loc("1332|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_60aten__mm")
#loc584 = loc("1333|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_125aten__add")
#loc585 = loc("1340|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_41aten__view")
#loc586 = loc("1341|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_114aten__permute")
#loc587 = loc("1346|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_94xla__cast")
#loc588 = loc("1349|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_64aten__mul")
#loc589 = loc("1334|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_112aten__permute")
#loc590 = loc("1335|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_61aten__mm")
#loc591 = loc("1335|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_61aten__view")
#loc592 = loc("1336|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_126aten__add")
#loc593 = loc("1342|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_42aten__view")
#loc594 = loc("1343|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_115aten__permute")
#loc595 = loc("1347|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_95xla__cast")
#loc596 = loc("1350|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_117aten__permute")
#loc597 = loc("1351|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_65aten__mul")
#loc598 = loc("1353|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_10aten__einsum")
#loc599 = loc("1354|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_10aten__eq")
#loc600 = loc("1355|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_20aten__logical_not")
#loc602 = loc("or.5799")
#loc603 = loc("select.5800")
#loc604 = loc("1357|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_21aten__logical_not")
#loc605 = loc("1359|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_10aten__expand")
#loc607 = loc("maximum.5764")
#loc608 = loc("add.5773")
#loc609 = loc("1359|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_10aten__where")
#loc610 = loc("1337|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_113aten__permute")
#loc611 = loc("1338|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_62aten__mm")
#loc612 = loc("1338|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_62aten__view")
#loc613 = loc("1339|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_127aten__add")
#loc614 = loc("1344|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_43aten__view")
#loc615 = loc("1345|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_116aten__permute")
#loc616 = loc("1348|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_96xla__cast")
#loc617 = loc("1361|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_98aten__einsum")
#loc618 = loc("1361|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_98xla__cast")
#loc619 = loc("1363|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_10aten__permute")
#loc620 = loc("1366|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_63aten__view")
#loc621 = loc("1365|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_119aten__permute")
#loc622 = loc("1366|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_63aten__mm")
#loc623 = loc("1367|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_128aten__add")
#loc624 = loc("1368|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_129aten__add")
#loc625 = loc("1381|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_111xla__mark_tensor")
#loc626 = loc("1383|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_64aten__view")
#loc627 = loc("1382|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_120aten__permute")
#loc628 = loc("1383|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_64aten__mm")
#loc629 = loc("1384|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_132aten__add")
#loc630 = loc("1387|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[10].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_113xla__mark_tensor")
#loc631 = loc("1389|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_65aten__view")
#loc632 = loc("1388|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_121aten__permute")
#loc633 = loc("1389|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_65aten__mm")
#loc634 = loc("1390|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_133aten__add")
#loc635 = loc("1391|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_134aten__add")
#loc636 = loc("1404|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_117xla__mark_tensor")
#loc637 = loc("1406|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_66aten__view")
#loc638 = loc("1405|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_122aten__permute")
#loc639 = loc("1406|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_66aten__mm")
#loc640 = loc("1407|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_137aten__add")
#loc641 = loc("1414|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_45aten__view")
#loc642 = loc("1415|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_125aten__permute")
#loc643 = loc("1420|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_103xla__cast")
#loc644 = loc("1423|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_70aten__mul")
#loc645 = loc("1408|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_123aten__permute")
#loc646 = loc("1409|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_67aten__mm")
#loc647 = loc("1409|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_67aten__view")
#loc648 = loc("1410|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_138aten__add")
#loc649 = loc("1416|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_46aten__view")
#loc650 = loc("1417|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_126aten__permute")
#loc651 = loc("1421|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_104xla__cast")
#loc652 = loc("1424|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_128aten__permute")
#loc653 = loc("1425|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_71aten__mul")
#loc654 = loc("1427|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_11aten__einsum")
#loc655 = loc("1428|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_11aten__eq")
#loc656 = loc("1429|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_22aten__logical_not")
#loc658 = loc("or.6115")
#loc659 = loc("select.6116")
#loc660 = loc("1431|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_23aten__logical_not")
#loc661 = loc("1433|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_11aten__expand")
#loc663 = loc("maximum.6080")
#loc664 = loc("add.6089")
#loc665 = loc("1433|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_11aten__where")
#loc666 = loc("1411|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_124aten__permute")
#loc667 = loc("1412|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_68aten__mm")
#loc668 = loc("1412|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_68aten__view")
#loc669 = loc("1413|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_139aten__add")
#loc670 = loc("1418|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_47aten__view")
#loc671 = loc("1419|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_127aten__permute")
#loc672 = loc("1422|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_105xla__cast")
#loc673 = loc("1435|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_107aten__einsum")
#loc674 = loc("1435|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_107xla__cast")
#loc675 = loc("1437|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_11aten__permute")
#loc676 = loc("1440|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_69aten__view")
#loc677 = loc("1439|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_130aten__permute")
#loc678 = loc("1440|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_69aten__mm")
#loc679 = loc("1441|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_140aten__add")
#loc680 = loc("1442|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_141aten__add")
#loc681 = loc("1455|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_121xla__mark_tensor")
#loc682 = loc("1457|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_70aten__view")
#loc683 = loc("1456|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_131aten__permute")
#loc684 = loc("1457|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_70aten__mm")
#loc685 = loc("1458|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_144aten__add")
#loc686 = loc("1461|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[11].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_123xla__mark_tensor")
#loc687 = loc("1463|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_71aten__view")
#loc688 = loc("1462|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_132aten__permute")
#loc689 = loc("1463|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_71aten__mm")
#loc690 = loc("1464|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_145aten__add")
#loc691 = loc("1465|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_146aten__add")
#loc692 = loc("1478|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_127xla__mark_tensor")
#loc693 = loc("1480|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_72aten__view")
#loc694 = loc("1479|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_133aten__permute")
#loc695 = loc("1480|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_72aten__mm")
#loc696 = loc("1481|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_149aten__add")
#loc697 = loc("1488|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_49aten__view")
#loc698 = loc("1489|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_136aten__permute")
#loc699 = loc("1494|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_112xla__cast")
#loc700 = loc("1497|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_76aten__mul")
#loc701 = loc("1482|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_134aten__permute")
#loc702 = loc("1483|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_73aten__mm")
#loc703 = loc("1483|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_73aten__view")
#loc704 = loc("1484|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_150aten__add")
#loc705 = loc("1490|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_50aten__view")
#loc706 = loc("1491|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_137aten__permute")
#loc707 = loc("1495|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_113xla__cast")
#loc708 = loc("1498|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_139aten__permute")
#loc709 = loc("1499|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_77aten__mul")
#loc710 = loc("1501|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_12aten__einsum")
#loc711 = loc("1502|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_12aten__eq")
#loc712 = loc("1503|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_24aten__logical_not")
#loc714 = loc("or.6431")
#loc715 = loc("select.6432")
#loc716 = loc("1505|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_25aten__logical_not")
#loc717 = loc("1507|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_12aten__expand")
#loc719 = loc("maximum.6396")
#loc720 = loc("add.6405")
#loc721 = loc("1507|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_12aten__where")
#loc722 = loc("1485|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_135aten__permute")
#loc723 = loc("1486|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_74aten__mm")
#loc724 = loc("1486|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_74aten__view")
#loc725 = loc("1487|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_151aten__add")
#loc726 = loc("1492|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_51aten__view")
#loc727 = loc("1493|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_138aten__permute")
#loc728 = loc("1496|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_114xla__cast")
#loc729 = loc("1509|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_116aten__einsum")
#loc730 = loc("1509|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_116xla__cast")
#loc731 = loc("1511|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_12aten__permute")
#loc732 = loc("1514|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_75aten__view")
#loc733 = loc("1513|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_141aten__permute")
#loc734 = loc("1514|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_75aten__mm")
#loc735 = loc("1515|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_152aten__add")
#loc736 = loc("1516|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_153aten__add")
#loc737 = loc("1529|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_131xla__mark_tensor")
#loc738 = loc("1531|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_76aten__view")
#loc739 = loc("1530|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_142aten__permute")
#loc740 = loc("1531|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_76aten__mm")
#loc741 = loc("1532|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_156aten__add")
#loc742 = loc("1535|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[12].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_133xla__mark_tensor")
#loc743 = loc("1537|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_77aten__view")
#loc744 = loc("1536|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_143aten__permute")
#loc745 = loc("1537|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_77aten__mm")
#loc746 = loc("1538|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_157aten__add")
#loc747 = loc("1539|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_158aten__add")
#loc748 = loc("1552|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_137xla__mark_tensor")
#loc749 = loc("1554|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_78aten__view")
#loc750 = loc("1553|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_144aten__permute")
#loc751 = loc("1554|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_78aten__mm")
#loc752 = loc("1555|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_161aten__add")
#loc753 = loc("1562|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_53aten__view")
#loc754 = loc("1563|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_147aten__permute")
#loc755 = loc("1568|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_121xla__cast")
#loc756 = loc("1571|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_82aten__mul")
#loc757 = loc("1556|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_145aten__permute")
#loc758 = loc("1557|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_79aten__mm")
#loc759 = loc("1557|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_79aten__view")
#loc760 = loc("1558|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_162aten__add")
#loc761 = loc("1564|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_54aten__view")
#loc762 = loc("1565|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_148aten__permute")
#loc763 = loc("1569|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_122xla__cast")
#loc764 = loc("1572|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_150aten__permute")
#loc765 = loc("1573|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_83aten__mul")
#loc766 = loc("1575|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_13aten__einsum")
#loc767 = loc("1576|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_13aten__eq")
#loc768 = loc("1577|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_26aten__logical_not")
#loc770 = loc("or.6747")
#loc771 = loc("select.6748")
#loc772 = loc("1579|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_27aten__logical_not")
#loc773 = loc("1581|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_13aten__expand")
#loc775 = loc("maximum.6712")
#loc776 = loc("add.6721")
#loc777 = loc("1581|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_13aten__where")
#loc778 = loc("1559|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_146aten__permute")
#loc779 = loc("1560|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_80aten__mm")
#loc780 = loc("1560|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_80aten__view")
#loc781 = loc("1561|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_163aten__add")
#loc782 = loc("1566|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_55aten__view")
#loc783 = loc("1567|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_149aten__permute")
#loc784 = loc("1570|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_123xla__cast")
#loc785 = loc("1583|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_125aten__einsum")
#loc786 = loc("1583|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_125xla__cast")
#loc787 = loc("1585|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_13aten__permute")
#loc788 = loc("1588|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_81aten__view")
#loc789 = loc("1587|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_152aten__permute")
#loc790 = loc("1588|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_81aten__mm")
#loc791 = loc("1589|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_164aten__add")
#loc792 = loc("1590|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_165aten__add")
#loc793 = loc("1603|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_141xla__mark_tensor")
#loc794 = loc("1605|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_82aten__view")
#loc795 = loc("1604|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_153aten__permute")
#loc796 = loc("1605|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_82aten__mm")
#loc797 = loc("1606|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_168aten__add")
#loc798 = loc("1609|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[13].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_143xla__mark_tensor")
#loc799 = loc("1611|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_83aten__view")
#loc800 = loc("1610|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_154aten__permute")
#loc801 = loc("1611|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_83aten__mm")
#loc802 = loc("1612|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_169aten__add")
#loc803 = loc("1613|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_170aten__add")
#loc804 = loc("1626|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_147xla__mark_tensor")
#loc805 = loc("1628|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_84aten__view")
#loc806 = loc("1627|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_155aten__permute")
#loc807 = loc("1628|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_84aten__mm")
#loc808 = loc("1629|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_173aten__add")
#loc809 = loc("1636|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_57aten__view")
#loc810 = loc("1637|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_158aten__permute")
#loc811 = loc("1642|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_130xla__cast")
#loc812 = loc("1645|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_88aten__mul")
#loc813 = loc("1630|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_156aten__permute")
#loc814 = loc("1631|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_85aten__mm")
#loc815 = loc("1631|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_85aten__view")
#loc816 = loc("1632|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_174aten__add")
#loc817 = loc("1638|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_58aten__view")
#loc818 = loc("1639|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_159aten__permute")
#loc819 = loc("1643|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_131xla__cast")
#loc820 = loc("1646|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_161aten__permute")
#loc821 = loc("1647|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_89aten__mul")
#loc822 = loc("1649|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_14aten__einsum")
#loc823 = loc("1650|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_14aten__eq")
#loc824 = loc("1651|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_28aten__logical_not")
#loc826 = loc("or.7063")
#loc827 = loc("select.7064")
#loc828 = loc("1653|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_29aten__logical_not")
#loc829 = loc("1655|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_14aten__expand")
#loc831 = loc("maximum.7028")
#loc832 = loc("add.7037")
#loc833 = loc("1655|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_14aten__where")
#loc834 = loc("1633|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_157aten__permute")
#loc835 = loc("1634|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_86aten__mm")
#loc836 = loc("1634|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_86aten__view")
#loc837 = loc("1635|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_175aten__add")
#loc838 = loc("1640|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_59aten__view")
#loc839 = loc("1641|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_160aten__permute")
#loc840 = loc("1644|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_132xla__cast")
#loc841 = loc("1657|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_134aten__einsum")
#loc842 = loc("1657|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_134xla__cast")
#loc843 = loc("1659|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_14aten__permute")
#loc844 = loc("1662|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_87aten__view")
#loc845 = loc("1661|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_163aten__permute")
#loc846 = loc("1662|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_87aten__mm")
#loc847 = loc("1663|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_176aten__add")
#loc848 = loc("1664|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_177aten__add")
#loc849 = loc("1677|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_151xla__mark_tensor")
#loc850 = loc("1679|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_88aten__view")
#loc851 = loc("1678|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_164aten__permute")
#loc852 = loc("1679|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_88aten__mm")
#loc853 = loc("1680|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_180aten__add")
#loc854 = loc("1683|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[14].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_153xla__mark_tensor")
#loc855 = loc("1685|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_89aten__view")
#loc856 = loc("1684|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_165aten__permute")
#loc857 = loc("1685|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_89aten__mm")
#loc858 = loc("1686|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_181aten__add")
#loc859 = loc("1687|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_182aten__add")
#loc860 = loc("1700|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_157xla__mark_tensor")
#loc861 = loc("1702|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_90aten__view")
#loc862 = loc("1701|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_166aten__permute")
#loc863 = loc("1702|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_90aten__mm")
#loc864 = loc("1703|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_185aten__add")
#loc865 = loc("1710|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_61aten__view")
#loc866 = loc("1711|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_169aten__permute")
#loc867 = loc("1716|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_139xla__cast")
#loc868 = loc("1719|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_94aten__mul")
#loc869 = loc("1704|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_167aten__permute")
#loc870 = loc("1705|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_91aten__mm")
#loc871 = loc("1705|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_91aten__view")
#loc872 = loc("1706|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_186aten__add")
#loc873 = loc("1712|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_62aten__view")
#loc874 = loc("1713|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_170aten__permute")
#loc875 = loc("1717|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_140xla__cast")
#loc876 = loc("1720|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_172aten__permute")
#loc877 = loc("1721|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_95aten__mul")
#loc878 = loc("1723|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_15aten__einsum")
#loc879 = loc("1724|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_15aten__eq")
#loc880 = loc("1725|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_30aten__logical_not")
#loc882 = loc("or.7379")
#loc883 = loc("select.7380")
#loc884 = loc("1727|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_31aten__logical_not")
#loc885 = loc("1729|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_15aten__expand")
#loc887 = loc("maximum.7344")
#loc888 = loc("add.7353")
#loc889 = loc("1729|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_15aten__where")
#loc890 = loc("1707|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_168aten__permute")
#loc891 = loc("1708|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_92aten__mm")
#loc892 = loc("1708|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_92aten__view")
#loc893 = loc("1709|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_187aten__add")
#loc894 = loc("1714|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_63aten__view")
#loc895 = loc("1715|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_171aten__permute")
#loc896 = loc("1718|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_141xla__cast")
#loc897 = loc("1731|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_143aten__einsum")
#loc898 = loc("1731|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_143xla__cast")
#loc899 = loc("1733|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_15aten__permute")
#loc900 = loc("1736|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_93aten__view")
#loc901 = loc("1735|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_174aten__permute")
#loc902 = loc("1736|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_93aten__mm")
#loc903 = loc("1737|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_188aten__add")
#loc904 = loc("1738|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_189aten__add")
#loc905 = loc("1751|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_161xla__mark_tensor")
#loc906 = loc("1753|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_94aten__view")
#loc907 = loc("1752|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_175aten__permute")
#loc908 = loc("1753|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_94aten__mm")
#loc909 = loc("1754|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_192aten__add")
#loc910 = loc("1757|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[15].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_163xla__mark_tensor")
#loc911 = loc("1759|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_95aten__view")
#loc912 = loc("1758|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_176aten__permute")
#loc913 = loc("1759|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_95aten__mm")
#loc914 = loc("1760|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_193aten__add")
#loc915 = loc("1761|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_194aten__add")
#loc916 = loc("1774|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_167xla__mark_tensor")
#loc917 = loc("1776|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_96aten__view")
#loc918 = loc("1775|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_177aten__permute")
#loc919 = loc("1776|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_96aten__mm")
#loc920 = loc("1777|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_197aten__add")
#loc921 = loc("1784|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_65aten__view")
#loc922 = loc("1785|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_180aten__permute")
#loc923 = loc("1790|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_148xla__cast")
#loc924 = loc("1793|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_100aten__mul")
#loc925 = loc("1778|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_178aten__permute")
#loc926 = loc("1779|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_97aten__mm")
#loc927 = loc("1779|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_97aten__view")
#loc928 = loc("1780|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_198aten__add")
#loc929 = loc("1786|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_66aten__view")
#loc930 = loc("1787|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_181aten__permute")
#loc931 = loc("1791|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_149xla__cast")
#loc932 = loc("1794|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_183aten__permute")
#loc933 = loc("1795|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_101aten__mul")
#loc934 = loc("1797|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_16aten__einsum")
#loc935 = loc("1798|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_16aten__eq")
#loc936 = loc("1799|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_32aten__logical_not")
#loc938 = loc("or.7695")
#loc939 = loc("select.7696")
#loc940 = loc("1801|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_33aten__logical_not")
#loc941 = loc("1803|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_16aten__expand")
#loc943 = loc("maximum.7660")
#loc944 = loc("add.7669")
#loc945 = loc("1803|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_16aten__where")
#loc946 = loc("1781|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_179aten__permute")
#loc947 = loc("1782|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_98aten__mm")
#loc948 = loc("1782|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_98aten__view")
#loc949 = loc("1783|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_199aten__add")
#loc950 = loc("1788|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_67aten__view")
#loc951 = loc("1789|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_182aten__permute")
#loc952 = loc("1792|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_150xla__cast")
#loc953 = loc("1805|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_152aten__einsum")
#loc954 = loc("1805|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_152xla__cast")
#loc955 = loc("1807|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_16aten__permute")
#loc956 = loc("1810|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_99aten__view")
#loc957 = loc("1809|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_185aten__permute")
#loc958 = loc("1810|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_99aten__mm")
#loc959 = loc("1811|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_200aten__add")
#loc960 = loc("1812|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_201aten__add")
#loc961 = loc("1825|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_171xla__mark_tensor")
#loc962 = loc("1827|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_100aten__view")
#loc963 = loc("1826|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_186aten__permute")
#loc964 = loc("1827|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_100aten__mm")
#loc965 = loc("1828|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_204aten__add")
#loc966 = loc("1831|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[16].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_173xla__mark_tensor")
#loc967 = loc("1833|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_101aten__view")
#loc968 = loc("1832|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_187aten__permute")
#loc969 = loc("1833|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_101aten__mm")
#loc970 = loc("1834|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_205aten__add")
#loc971 = loc("1835|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_206aten__add")
#loc972 = loc("1848|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_177xla__mark_tensor")
#loc973 = loc("1850|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_102aten__view")
#loc974 = loc("1849|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_188aten__permute")
#loc975 = loc("1850|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_102aten__mm")
#loc976 = loc("1851|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_209aten__add")
#loc977 = loc("1858|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_69aten__view")
#loc978 = loc("1859|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_191aten__permute")
#loc979 = loc("1864|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_157xla__cast")
#loc980 = loc("1867|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_106aten__mul")
#loc981 = loc("1852|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_189aten__permute")
#loc982 = loc("1853|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_103aten__mm")
#loc983 = loc("1853|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_103aten__view")
#loc984 = loc("1854|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_210aten__add")
#loc985 = loc("1860|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_70aten__view")
#loc986 = loc("1861|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_192aten__permute")
#loc987 = loc("1865|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_158xla__cast")
#loc988 = loc("1868|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_194aten__permute")
#loc989 = loc("1869|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_107aten__mul")
#loc990 = loc("1871|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_17aten__einsum")
#loc991 = loc("1872|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_17aten__eq")
#loc992 = loc("1873|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_34aten__logical_not")
#loc994 = loc("or.8011")
#loc995 = loc("select.8012")
#loc996 = loc("1875|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_35aten__logical_not")
#loc997 = loc("1877|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_17aten__expand")
#loc999 = loc("maximum.7976")
#loc1000 = loc("add.7985")
#loc1001 = loc("1877|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_17aten__where")
#loc1002 = loc("1855|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_190aten__permute")
#loc1003 = loc("1856|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_104aten__mm")
#loc1004 = loc("1856|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_104aten__view")
#loc1005 = loc("1857|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_211aten__add")
#loc1006 = loc("1862|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_71aten__view")
#loc1007 = loc("1863|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_193aten__permute")
#loc1008 = loc("1866|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_159xla__cast")
#loc1009 = loc("1879|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_161aten__einsum")
#loc1010 = loc("1879|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_161xla__cast")
#loc1011 = loc("1881|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_17aten__permute")
#loc1012 = loc("1884|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_105aten__view")
#loc1013 = loc("1883|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_196aten__permute")
#loc1014 = loc("1884|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_105aten__mm")
#loc1015 = loc("1885|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_212aten__add")
#loc1016 = loc("1886|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_213aten__add")
#loc1017 = loc("1899|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_181xla__mark_tensor")
#loc1018 = loc("1901|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_106aten__view")
#loc1019 = loc("1900|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_197aten__permute")
#loc1020 = loc("1901|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_106aten__mm")
#loc1021 = loc("1902|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_216aten__add")
#loc1022 = loc("1905|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[17].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_183xla__mark_tensor")
#loc1023 = loc("1907|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_107aten__view")
#loc1024 = loc("1906|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_198aten__permute")
#loc1025 = loc("1907|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_107aten__mm")
#loc1026 = loc("1908|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_217aten__add")
#loc1027 = loc("1909|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_218aten__add")
#loc1028 = loc("1922|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_187xla__mark_tensor")
#loc1029 = loc("1924|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_108aten__view")
#loc1030 = loc("1923|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_199aten__permute")
#loc1031 = loc("1924|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_108aten__mm")
#loc1032 = loc("1925|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_221aten__add")
#loc1033 = loc("1932|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_73aten__view")
#loc1034 = loc("1933|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_202aten__permute")
#loc1035 = loc("1938|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_166xla__cast")
#loc1036 = loc("1941|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_112aten__mul")
#loc1037 = loc("1926|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_200aten__permute")
#loc1038 = loc("1927|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_109aten__mm")
#loc1039 = loc("1927|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_109aten__view")
#loc1040 = loc("1928|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_222aten__add")
#loc1041 = loc("1934|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_74aten__view")
#loc1042 = loc("1935|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_203aten__permute")
#loc1043 = loc("1939|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_167xla__cast")
#loc1044 = loc("1942|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_205aten__permute")
#loc1045 = loc("1943|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_113aten__mul")
#loc1046 = loc("1945|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_18aten__einsum")
#loc1047 = loc("1946|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_18aten__eq")
#loc1048 = loc("1947|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_36aten__logical_not")
#loc1050 = loc("or.8327")
#loc1051 = loc("select.8328")
#loc1052 = loc("1949|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_37aten__logical_not")
#loc1053 = loc("1951|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_18aten__expand")
#loc1055 = loc("maximum.8292")
#loc1056 = loc("add.8301")
#loc1057 = loc("1951|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_18aten__where")
#loc1058 = loc("1929|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_201aten__permute")
#loc1059 = loc("1930|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_110aten__mm")
#loc1060 = loc("1930|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_110aten__view")
#loc1061 = loc("1931|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_223aten__add")
#loc1062 = loc("1936|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_75aten__view")
#loc1063 = loc("1937|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_204aten__permute")
#loc1064 = loc("1940|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_168xla__cast")
#loc1065 = loc("1953|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_170aten__einsum")
#loc1066 = loc("1953|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_170xla__cast")
#loc1067 = loc("1955|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_18aten__permute")
#loc1068 = loc("1958|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_111aten__view")
#loc1069 = loc("1957|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_207aten__permute")
#loc1070 = loc("1958|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_111aten__mm")
#loc1071 = loc("1959|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_224aten__add")
#loc1072 = loc("1960|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_225aten__add")
#loc1073 = loc("1973|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_191xla__mark_tensor")
#loc1074 = loc("1975|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_112aten__view")
#loc1075 = loc("1974|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_208aten__permute")
#loc1076 = loc("1975|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_112aten__mm")
#loc1077 = loc("1976|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_228aten__add")
#loc1078 = loc("1979|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[18].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_193xla__mark_tensor")
#loc1079 = loc("1981|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_113aten__view")
#loc1080 = loc("1980|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_209aten__permute")
#loc1081 = loc("1981|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_113aten__mm")
#loc1082 = loc("1982|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_229aten__add")
#loc1083 = loc("1983|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_230aten__add")
#loc1084 = loc("1996|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_197xla__mark_tensor")
#loc1085 = loc("1998|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_114aten__view")
#loc1086 = loc("1997|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_210aten__permute")
#loc1087 = loc("1998|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_114aten__mm")
#loc1088 = loc("1999|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_233aten__add")
#loc1089 = loc("2006|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_77aten__view")
#loc1090 = loc("2007|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_213aten__permute")
#loc1091 = loc("2012|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_175xla__cast")
#loc1092 = loc("2015|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_118aten__mul")
#loc1093 = loc("2000|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_211aten__permute")
#loc1094 = loc("2001|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_115aten__mm")
#loc1095 = loc("2001|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_115aten__view")
#loc1096 = loc("2002|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_234aten__add")
#loc1097 = loc("2008|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_78aten__view")
#loc1098 = loc("2009|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_214aten__permute")
#loc1099 = loc("2013|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_176xla__cast")
#loc1100 = loc("2016|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_216aten__permute")
#loc1101 = loc("2017|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_119aten__mul")
#loc1102 = loc("2019|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_19aten__einsum")
#loc1103 = loc("2020|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_19aten__eq")
#loc1104 = loc("2021|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_38aten__logical_not")
#loc1106 = loc("or.8643")
#loc1107 = loc("select.8644")
#loc1108 = loc("2023|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_39aten__logical_not")
#loc1109 = loc("2025|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_19aten__expand")
#loc1111 = loc("maximum.8608")
#loc1112 = loc("add.8617")
#loc1113 = loc("2025|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_19aten__where")
#loc1114 = loc("2003|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_212aten__permute")
#loc1115 = loc("2004|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_116aten__mm")
#loc1116 = loc("2004|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_116aten__view")
#loc1117 = loc("2005|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_235aten__add")
#loc1118 = loc("2010|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_79aten__view")
#loc1119 = loc("2011|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_215aten__permute")
#loc1120 = loc("2014|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_177xla__cast")
#loc1121 = loc("2027|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_179aten__einsum")
#loc1122 = loc("2027|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_179xla__cast")
#loc1123 = loc("2029|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_19aten__permute")
#loc1124 = loc("2032|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_117aten__view")
#loc1125 = loc("2031|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_218aten__permute")
#loc1126 = loc("2032|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_117aten__mm")
#loc1127 = loc("2033|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_236aten__add")
#loc1128 = loc("2034|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_237aten__add")
#loc1129 = loc("2047|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_201xla__mark_tensor")
#loc1130 = loc("2049|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_118aten__view")
#loc1131 = loc("2048|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_219aten__permute")
#loc1132 = loc("2049|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_118aten__mm")
#loc1133 = loc("2050|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_240aten__add")
#loc1134 = loc("2053|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[19].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_203xla__mark_tensor")
#loc1135 = loc("2055|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_119aten__view")
#loc1136 = loc("2054|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_220aten__permute")
#loc1137 = loc("2055|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_119aten__mm")
#loc1138 = loc("2056|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_241aten__add")
#loc1139 = loc("2057|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_242aten__add")
#loc1140 = loc("2070|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_207xla__mark_tensor")
#loc1141 = loc("2072|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_120aten__view")
#loc1142 = loc("2071|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_221aten__permute")
#loc1143 = loc("2072|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_120aten__mm")
#loc1144 = loc("2073|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_245aten__add")
#loc1145 = loc("2080|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_81aten__view")
#loc1146 = loc("2081|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_224aten__permute")
#loc1147 = loc("2086|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_184xla__cast")
#loc1148 = loc("2089|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_124aten__mul")
#loc1149 = loc("2074|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_222aten__permute")
#loc1150 = loc("2075|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_121aten__mm")
#loc1151 = loc("2075|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_121aten__view")
#loc1152 = loc("2076|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_246aten__add")
#loc1153 = loc("2082|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_82aten__view")
#loc1154 = loc("2083|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_225aten__permute")
#loc1155 = loc("2087|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_185xla__cast")
#loc1156 = loc("2090|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_227aten__permute")
#loc1157 = loc("2091|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_125aten__mul")
#loc1158 = loc("2093|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_20aten__einsum")
#loc1159 = loc("2094|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_20aten__eq")
#loc1160 = loc("2095|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_40aten__logical_not")
#loc1162 = loc("or.8959")
#loc1163 = loc("select.8960")
#loc1164 = loc("2097|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_41aten__logical_not")
#loc1165 = loc("2099|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_20aten__expand")
#loc1167 = loc("maximum.8924")
#loc1168 = loc("add.8933")
#loc1169 = loc("2099|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_20aten__where")
#loc1170 = loc("2077|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_223aten__permute")
#loc1171 = loc("2078|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_122aten__mm")
#loc1172 = loc("2078|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_122aten__view")
#loc1173 = loc("2079|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_247aten__add")
#loc1174 = loc("2084|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_83aten__view")
#loc1175 = loc("2085|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_226aten__permute")
#loc1176 = loc("2088|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_186xla__cast")
#loc1177 = loc("2101|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_188aten__einsum")
#loc1178 = loc("2101|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_188xla__cast")
#loc1179 = loc("2103|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_20aten__permute")
#loc1180 = loc("2106|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_123aten__view")
#loc1181 = loc("2105|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_229aten__permute")
#loc1182 = loc("2106|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_123aten__mm")
#loc1183 = loc("2107|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_248aten__add")
#loc1184 = loc("2108|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_249aten__add")
#loc1185 = loc("2121|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_211xla__mark_tensor")
#loc1186 = loc("2123|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_124aten__view")
#loc1187 = loc("2122|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_230aten__permute")
#loc1188 = loc("2123|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_124aten__mm")
#loc1189 = loc("2124|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_252aten__add")
#loc1190 = loc("2127|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[20].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_213xla__mark_tensor")
#loc1191 = loc("2129|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_125aten__view")
#loc1192 = loc("2128|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_231aten__permute")
#loc1193 = loc("2129|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_125aten__mm")
#loc1194 = loc("2130|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_253aten__add")
#loc1195 = loc("2131|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_254aten__add")
#loc1196 = loc("2144|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_217xla__mark_tensor")
#loc1197 = loc("2146|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_126aten__view")
#loc1198 = loc("2145|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_232aten__permute")
#loc1199 = loc("2146|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_126aten__mm")
#loc1200 = loc("2147|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_257aten__add")
#loc1201 = loc("2154|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_85aten__view")
#loc1202 = loc("2155|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_235aten__permute")
#loc1203 = loc("2160|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_193xla__cast")
#loc1204 = loc("2163|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_130aten__mul")
#loc1205 = loc("2148|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_233aten__permute")
#loc1206 = loc("2149|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_127aten__mm")
#loc1207 = loc("2149|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_127aten__view")
#loc1208 = loc("2150|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_258aten__add")
#loc1209 = loc("2156|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_86aten__view")
#loc1210 = loc("2157|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_236aten__permute")
#loc1211 = loc("2161|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_194xla__cast")
#loc1212 = loc("2164|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_238aten__permute")
#loc1213 = loc("2165|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_131aten__mul")
#loc1214 = loc("2167|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_21aten__einsum")
#loc1215 = loc("2168|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_21aten__eq")
#loc1216 = loc("2169|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_42aten__logical_not")
#loc1218 = loc("or.9275")
#loc1219 = loc("select.9276")
#loc1220 = loc("2171|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_43aten__logical_not")
#loc1221 = loc("2173|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_21aten__expand")
#loc1223 = loc("maximum.9240")
#loc1224 = loc("add.9249")
#loc1225 = loc("2173|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_21aten__where")
#loc1226 = loc("2151|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_234aten__permute")
#loc1227 = loc("2152|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_128aten__mm")
#loc1228 = loc("2152|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_128aten__view")
#loc1229 = loc("2153|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_259aten__add")
#loc1230 = loc("2158|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_87aten__view")
#loc1231 = loc("2159|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_237aten__permute")
#loc1232 = loc("2162|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_195xla__cast")
#loc1233 = loc("2175|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_197aten__einsum")
#loc1234 = loc("2175|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_197xla__cast")
#loc1235 = loc("2177|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_21aten__permute")
#loc1236 = loc("2180|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_129aten__view")
#loc1237 = loc("2179|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_240aten__permute")
#loc1238 = loc("2180|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_129aten__mm")
#loc1239 = loc("2181|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_260aten__add")
#loc1240 = loc("2182|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_261aten__add")
#loc1241 = loc("2195|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_221xla__mark_tensor")
#loc1242 = loc("2197|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_130aten__view")
#loc1243 = loc("2196|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_241aten__permute")
#loc1244 = loc("2197|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_130aten__mm")
#loc1245 = loc("2198|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_264aten__add")
#loc1246 = loc("2201|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[21].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_223xla__mark_tensor")
#loc1247 = loc("2203|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_131aten__view")
#loc1248 = loc("2202|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_242aten__permute")
#loc1249 = loc("2203|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_131aten__mm")
#loc1250 = loc("2204|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_265aten__add")
#loc1251 = loc("2205|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_266aten__add")
#loc1252 = loc("2218|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_227xla__mark_tensor")
#loc1253 = loc("2220|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_132aten__view")
#loc1254 = loc("2219|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_243aten__permute")
#loc1255 = loc("2220|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_132aten__mm")
#loc1256 = loc("2221|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_269aten__add")
#loc1257 = loc("2228|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_89aten__view")
#loc1258 = loc("2229|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_246aten__permute")
#loc1259 = loc("2234|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_202xla__cast")
#loc1260 = loc("2237|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_136aten__mul")
#loc1261 = loc("2222|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_244aten__permute")
#loc1262 = loc("2223|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_133aten__mm")
#loc1263 = loc("2223|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_133aten__view")
#loc1264 = loc("2224|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_270aten__add")
#loc1265 = loc("2230|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_90aten__view")
#loc1266 = loc("2231|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_247aten__permute")
#loc1267 = loc("2235|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_203xla__cast")
#loc1268 = loc("2238|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_249aten__permute")
#loc1269 = loc("2239|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_137aten__mul")
#loc1270 = loc("2241|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_22aten__einsum")
#loc1271 = loc("2242|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_22aten__eq")
#loc1272 = loc("2243|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_44aten__logical_not")
#loc1274 = loc("or.9591")
#loc1275 = loc("select.9592")
#loc1276 = loc("2245|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_45aten__logical_not")
#loc1277 = loc("2247|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_22aten__expand")
#loc1279 = loc("maximum.9556")
#loc1280 = loc("add.9565")
#loc1281 = loc("2247|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_22aten__where")
#loc1282 = loc("2225|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_245aten__permute")
#loc1283 = loc("2226|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_134aten__mm")
#loc1284 = loc("2226|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_134aten__view")
#loc1285 = loc("2227|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_271aten__add")
#loc1286 = loc("2232|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_91aten__view")
#loc1287 = loc("2233|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_248aten__permute")
#loc1288 = loc("2236|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_204xla__cast")
#loc1289 = loc("2249|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_206aten__einsum")
#loc1290 = loc("2249|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_206xla__cast")
#loc1291 = loc("2251|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_22aten__permute")
#loc1292 = loc("2254|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_135aten__view")
#loc1293 = loc("2253|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_251aten__permute")
#loc1294 = loc("2254|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_135aten__mm")
#loc1295 = loc("2255|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_272aten__add")
#loc1296 = loc("2256|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_273aten__add")
#loc1297 = loc("2269|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_231xla__mark_tensor")
#loc1298 = loc("2271|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_136aten__view")
#loc1299 = loc("2270|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_252aten__permute")
#loc1300 = loc("2271|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_136aten__mm")
#loc1301 = loc("2272|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_276aten__add")
#loc1302 = loc("2275|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[22].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_233xla__mark_tensor")
#loc1303 = loc("2277|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_137aten__view")
#loc1304 = loc("2276|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_253aten__permute")
#loc1305 = loc("2277|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_137aten__mm")
#loc1306 = loc("2278|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_277aten__add")
#loc1307 = loc("2279|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_278aten__add")
#loc1308 = loc("2292|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_237xla__mark_tensor")
#loc1309 = loc("2294|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_138aten__view")
#loc1310 = loc("2293|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_254aten__permute")
#loc1311 = loc("2294|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_138aten__mm")
#loc1312 = loc("2295|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_281aten__add")
#loc1313 = loc("2302|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_93aten__view")
#loc1314 = loc("2303|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_257aten__permute")
#loc1315 = loc("2308|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_211xla__cast")
#loc1316 = loc("2311|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_142aten__mul")
#loc1317 = loc("2296|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_255aten__permute")
#loc1318 = loc("2297|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_139aten__mm")
#loc1319 = loc("2297|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_139aten__view")
#loc1320 = loc("2298|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_282aten__add")
#loc1321 = loc("2304|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_94aten__view")
#loc1322 = loc("2305|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_258aten__permute")
#loc1323 = loc("2309|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_212xla__cast")
#loc1324 = loc("2312|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_260aten__permute")
#loc1325 = loc("2313|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_143aten__mul")
#loc1326 = loc("2315|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_23aten__einsum")
#loc1327 = loc("2316|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_23aten__eq")
#loc1328 = loc("2317|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_46aten__logical_not")
#loc1330 = loc("or.9907")
#loc1331 = loc("select.9908")
#loc1332 = loc("2319|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_47aten__logical_not")
#loc1333 = loc("2321|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_23aten__expand")
#loc1335 = loc("maximum.9872")
#loc1336 = loc("add.9881")
#loc1337 = loc("2321|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_23aten__where")
#loc1338 = loc("2299|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_256aten__permute")
#loc1339 = loc("2300|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_140aten__mm")
#loc1340 = loc("2300|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_140aten__view")
#loc1341 = loc("2301|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_283aten__add")
#loc1342 = loc("2306|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_95aten__view")
#loc1343 = loc("2307|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_259aten__permute")
#loc1344 = loc("2310|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_213xla__cast")
#loc1345 = loc("2323|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_215aten__einsum")
#loc1346 = loc("2323|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_215xla__cast")
#loc1347 = loc("2325|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_23aten__permute")
#loc1348 = loc("2328|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_141aten__view")
#loc1349 = loc("2327|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_262aten__permute")
#loc1350 = loc("2328|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_141aten__mm")
#loc1351 = loc("2329|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_284aten__add")
#loc1352 = loc("2330|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_285aten__add")
#loc1353 = loc("2343|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_241xla__mark_tensor")
#loc1354 = loc("2345|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_142aten__view")
#loc1355 = loc("2344|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_263aten__permute")
#loc1356 = loc("2345|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_142aten__mm")
#loc1357 = loc("2346|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_288aten__add")
#loc1358 = loc("2349|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[23].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_243xla__mark_tensor")
#loc1359 = loc("2351|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_143aten__view")
#loc1360 = loc("2350|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_264aten__permute")
#loc1361 = loc("2351|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_143aten__mm")
#loc1362 = loc("2352|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_289aten__add")
#loc1363 = loc("2353|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_290aten__add")
#loc1364 = loc("2366|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_247xla__mark_tensor")
#loc1365 = loc("2368|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_144aten__view")
#loc1366 = loc("2367|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_265aten__permute")
#loc1367 = loc("2368|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_144aten__mm")
#loc1368 = loc("2369|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_293aten__add")
#loc1369 = loc("2376|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_97aten__view")
#loc1370 = loc("2377|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_268aten__permute")
#loc1371 = loc("2382|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_220xla__cast")
#loc1372 = loc("2385|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_148aten__mul")
#loc1373 = loc("2370|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_266aten__permute")
#loc1374 = loc("2371|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_145aten__mm")
#loc1375 = loc("2371|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_145aten__view")
#loc1376 = loc("2372|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_294aten__add")
#loc1377 = loc("2378|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_98aten__view")
#loc1378 = loc("2379|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_269aten__permute")
#loc1379 = loc("2383|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_221xla__cast")
#loc1380 = loc("2386|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_271aten__permute")
#loc1381 = loc("2387|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_149aten__mul")
#loc1382 = loc("2389|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_24aten__einsum")
#loc1383 = loc("2390|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_24aten__eq")
#loc1384 = loc("2391|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_48aten__logical_not")
#loc1386 = loc("or.10223")
#loc1387 = loc("select.10224")
#loc1388 = loc("2393|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_49aten__logical_not")
#loc1389 = loc("2395|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_24aten__expand")
#loc1391 = loc("maximum.10188")
#loc1392 = loc("add.10197")
#loc1393 = loc("2395|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_24aten__where")
#loc1394 = loc("2373|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_267aten__permute")
#loc1395 = loc("2374|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_146aten__mm")
#loc1396 = loc("2374|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_146aten__view")
#loc1397 = loc("2375|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_295aten__add")
#loc1398 = loc("2380|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_99aten__view")
#loc1399 = loc("2381|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_270aten__permute")
#loc1400 = loc("2384|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_222xla__cast")
#loc1401 = loc("2397|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_224aten__einsum")
#loc1402 = loc("2397|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_224xla__cast")
#loc1403 = loc("2399|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_24aten__permute")
#loc1404 = loc("2402|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_147aten__view")
#loc1405 = loc("2401|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_273aten__permute")
#loc1406 = loc("2402|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_147aten__mm")
#loc1407 = loc("2403|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_296aten__add")
#loc1408 = loc("2404|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_297aten__add")
#loc1409 = loc("2417|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_251xla__mark_tensor")
#loc1410 = loc("2419|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_148aten__view")
#loc1411 = loc("2418|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_274aten__permute")
#loc1412 = loc("2419|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_148aten__mm")
#loc1413 = loc("2420|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_300aten__add")
#loc1414 = loc("2423|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[24].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_253xla__mark_tensor")
#loc1415 = loc("2425|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_149aten__view")
#loc1416 = loc("2424|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_275aten__permute")
#loc1417 = loc("2425|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_149aten__mm")
#loc1418 = loc("2426|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_301aten__add")
#loc1419 = loc("2427|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_302aten__add")
#loc1420 = loc("2440|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_257xla__mark_tensor")
#loc1421 = loc("2442|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_150aten__view")
#loc1422 = loc("2441|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_276aten__permute")
#loc1423 = loc("2442|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_150aten__mm")
#loc1424 = loc("2443|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_305aten__add")
#loc1425 = loc("2450|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_101aten__view")
#loc1426 = loc("2451|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_279aten__permute")
#loc1427 = loc("2456|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_229xla__cast")
#loc1428 = loc("2459|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_154aten__mul")
#loc1429 = loc("2444|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_277aten__permute")
#loc1430 = loc("2445|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_151aten__mm")
#loc1431 = loc("2445|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_151aten__view")
#loc1432 = loc("2446|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_306aten__add")
#loc1433 = loc("2452|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_102aten__view")
#loc1434 = loc("2453|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_280aten__permute")
#loc1435 = loc("2457|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_230xla__cast")
#loc1436 = loc("2460|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_282aten__permute")
#loc1437 = loc("2461|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_155aten__mul")
#loc1438 = loc("2463|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_25aten__einsum")
#loc1439 = loc("2464|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_25aten__eq")
#loc1440 = loc("2465|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_50aten__logical_not")
#loc1442 = loc("or.10539")
#loc1443 = loc("select.10540")
#loc1444 = loc("2467|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_51aten__logical_not")
#loc1445 = loc("2469|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_25aten__expand")
#loc1447 = loc("maximum.10504")
#loc1448 = loc("add.10513")
#loc1449 = loc("2469|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_25aten__where")
#loc1450 = loc("2447|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_278aten__permute")
#loc1451 = loc("2448|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_152aten__mm")
#loc1452 = loc("2448|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_152aten__view")
#loc1453 = loc("2449|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_307aten__add")
#loc1454 = loc("2454|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_103aten__view")
#loc1455 = loc("2455|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_281aten__permute")
#loc1456 = loc("2458|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_231xla__cast")
#loc1457 = loc("2471|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_233aten__einsum")
#loc1458 = loc("2471|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_233xla__cast")
#loc1459 = loc("2473|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_25aten__permute")
#loc1460 = loc("2476|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_153aten__view")
#loc1461 = loc("2475|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_284aten__permute")
#loc1462 = loc("2476|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_153aten__mm")
#loc1463 = loc("2477|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_308aten__add")
#loc1464 = loc("2478|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_309aten__add")
#loc1465 = loc("2491|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_261xla__mark_tensor")
#loc1466 = loc("2493|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_154aten__view")
#loc1467 = loc("2492|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_285aten__permute")
#loc1468 = loc("2493|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_154aten__mm")
#loc1469 = loc("2494|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_312aten__add")
#loc1470 = loc("2497|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[25].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_263xla__mark_tensor")
#loc1471 = loc("2499|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_155aten__view")
#loc1472 = loc("2498|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_286aten__permute")
#loc1473 = loc("2499|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_155aten__mm")
#loc1474 = loc("2500|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_313aten__add")
#loc1475 = loc("2501|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_314aten__add")
#loc1476 = loc("2514|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_267xla__mark_tensor")
#loc1477 = loc("2516|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_156aten__view")
#loc1478 = loc("2515|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_287aten__permute")
#loc1479 = loc("2516|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_156aten__mm")
#loc1480 = loc("2517|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_317aten__add")
#loc1481 = loc("2524|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_105aten__view")
#loc1482 = loc("2525|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_290aten__permute")
#loc1483 = loc("2530|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_238xla__cast")
#loc1484 = loc("2533|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_160aten__mul")
#loc1485 = loc("2518|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_288aten__permute")
#loc1486 = loc("2519|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_157aten__mm")
#loc1487 = loc("2519|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_157aten__view")
#loc1488 = loc("2520|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_318aten__add")
#loc1489 = loc("2526|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_106aten__view")
#loc1490 = loc("2527|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_291aten__permute")
#loc1491 = loc("2531|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_239xla__cast")
#loc1492 = loc("2534|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_293aten__permute")
#loc1493 = loc("2535|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_161aten__mul")
#loc1494 = loc("2537|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_26aten__einsum")
#loc1495 = loc("2538|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_26aten__eq")
#loc1496 = loc("2539|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_52aten__logical_not")
#loc1498 = loc("or.10855")
#loc1499 = loc("select.10856")
#loc1500 = loc("2541|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_53aten__logical_not")
#loc1501 = loc("2543|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_26aten__expand")
#loc1503 = loc("maximum.10820")
#loc1504 = loc("add.10829")
#loc1505 = loc("2543|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_26aten__where")
#loc1506 = loc("2521|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_289aten__permute")
#loc1507 = loc("2522|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_158aten__mm")
#loc1508 = loc("2522|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_158aten__view")
#loc1509 = loc("2523|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_319aten__add")
#loc1510 = loc("2528|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_107aten__view")
#loc1511 = loc("2529|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_292aten__permute")
#loc1512 = loc("2532|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_240xla__cast")
#loc1513 = loc("2545|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_242aten__einsum")
#loc1514 = loc("2545|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_242xla__cast")
#loc1515 = loc("2547|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_26aten__permute")
#loc1516 = loc("2550|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_159aten__view")
#loc1517 = loc("2549|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_295aten__permute")
#loc1518 = loc("2550|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_159aten__mm")
#loc1519 = loc("2551|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_320aten__add")
#loc1520 = loc("2552|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_321aten__add")
#loc1521 = loc("2565|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_271xla__mark_tensor")
#loc1522 = loc("2567|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_160aten__view")
#loc1523 = loc("2566|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_296aten__permute")
#loc1524 = loc("2567|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_160aten__mm")
#loc1525 = loc("2568|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_324aten__add")
#loc1526 = loc("2571|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[26].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_273xla__mark_tensor")
#loc1527 = loc("2573|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_161aten__view")
#loc1528 = loc("2572|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_297aten__permute")
#loc1529 = loc("2573|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_161aten__mm")
#loc1530 = loc("2574|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_325aten__add")
#loc1531 = loc("2575|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_326aten__add")
#loc1532 = loc("2588|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_277xla__mark_tensor")
#loc1533 = loc("2590|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_162aten__view")
#loc1534 = loc("2589|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_298aten__permute")
#loc1535 = loc("2590|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_162aten__mm")
#loc1536 = loc("2591|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_329aten__add")
#loc1537 = loc("2598|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_109aten__view")
#loc1538 = loc("2599|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_301aten__permute")
#loc1539 = loc("2604|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_247xla__cast")
#loc1540 = loc("2607|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_166aten__mul")
#loc1541 = loc("2592|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_299aten__permute")
#loc1542 = loc("2593|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_163aten__mm")
#loc1543 = loc("2593|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_163aten__view")
#loc1544 = loc("2594|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_330aten__add")
#loc1545 = loc("2600|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_110aten__view")
#loc1546 = loc("2601|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_302aten__permute")
#loc1547 = loc("2605|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_248xla__cast")
#loc1548 = loc("2608|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_304aten__permute")
#loc1549 = loc("2609|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_167aten__mul")
#loc1550 = loc("2611|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_27aten__einsum")
#loc1551 = loc("2612|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_27aten__eq")
#loc1552 = loc("2613|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_54aten__logical_not")
#loc1554 = loc("or.11171")
#loc1555 = loc("select.11172")
#loc1556 = loc("2615|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_55aten__logical_not")
#loc1557 = loc("2617|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_27aten__expand")
#loc1559 = loc("maximum.11136")
#loc1560 = loc("add.11145")
#loc1561 = loc("2617|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_27aten__where")
#loc1562 = loc("2595|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_300aten__permute")
#loc1563 = loc("2596|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_164aten__mm")
#loc1564 = loc("2596|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_164aten__view")
#loc1565 = loc("2597|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_331aten__add")
#loc1566 = loc("2602|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_111aten__view")
#loc1567 = loc("2603|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_303aten__permute")
#loc1568 = loc("2606|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_249xla__cast")
#loc1569 = loc("2619|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_251aten__einsum")
#loc1570 = loc("2619|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_251xla__cast")
#loc1571 = loc("2621|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_27aten__permute")
#loc1572 = loc("2624|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_165aten__view")
#loc1573 = loc("2623|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_306aten__permute")
#loc1574 = loc("2624|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_165aten__mm")
#loc1575 = loc("2625|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_332aten__add")
#loc1576 = loc("2626|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_333aten__add")
#loc1577 = loc("2639|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_281xla__mark_tensor")
#loc1578 = loc("2641|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_166aten__view")
#loc1579 = loc("2640|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_307aten__permute")
#loc1580 = loc("2641|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_166aten__mm")
#loc1581 = loc("2642|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_336aten__add")
#loc1582 = loc("2645|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[27].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_283xla__mark_tensor")
#loc1583 = loc("2647|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_167aten__view")
#loc1584 = loc("2646|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_308aten__permute")
#loc1585 = loc("2647|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_167aten__mm")
#loc1586 = loc("2648|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_337aten__add")
#loc1587 = loc("2649|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_338aten__add")
#loc1588 = loc("2662|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_287xla__mark_tensor")
#loc1589 = loc("2664|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_168aten__view")
#loc1590 = loc("2663|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_309aten__permute")
#loc1591 = loc("2664|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_168aten__mm")
#loc1592 = loc("2665|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_341aten__add")
#loc1593 = loc("2672|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_113aten__view")
#loc1594 = loc("2673|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_312aten__permute")
#loc1595 = loc("2678|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_256xla__cast")
#loc1596 = loc("2681|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_172aten__mul")
#loc1597 = loc("2666|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_310aten__permute")
#loc1598 = loc("2667|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_169aten__mm")
#loc1599 = loc("2667|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_169aten__view")
#loc1600 = loc("2668|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_342aten__add")
#loc1601 = loc("2674|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_114aten__view")
#loc1602 = loc("2675|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_313aten__permute")
#loc1603 = loc("2679|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_257xla__cast")
#loc1604 = loc("2682|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_315aten__permute")
#loc1605 = loc("2683|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_173aten__mul")
#loc1606 = loc("2685|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_28aten__einsum")
#loc1607 = loc("2686|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_28aten__eq")
#loc1608 = loc("2687|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_56aten__logical_not")
#loc1610 = loc("or.11487")
#loc1611 = loc("select.11488")
#loc1612 = loc("2689|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_57aten__logical_not")
#loc1613 = loc("2691|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_28aten__expand")
#loc1615 = loc("maximum.11452")
#loc1616 = loc("add.11461")
#loc1617 = loc("2691|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_28aten__where")
#loc1618 = loc("2669|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_311aten__permute")
#loc1619 = loc("2670|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_170aten__mm")
#loc1620 = loc("2670|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_170aten__view")
#loc1621 = loc("2671|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_343aten__add")
#loc1622 = loc("2676|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_115aten__view")
#loc1623 = loc("2677|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_314aten__permute")
#loc1624 = loc("2680|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_258xla__cast")
#loc1625 = loc("2693|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_260aten__einsum")
#loc1626 = loc("2693|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_260xla__cast")
#loc1627 = loc("2695|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_28aten__permute")
#loc1628 = loc("2698|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_171aten__view")
#loc1629 = loc("2697|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_317aten__permute")
#loc1630 = loc("2698|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_171aten__mm")
#loc1631 = loc("2699|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_344aten__add")
#loc1632 = loc("2700|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_345aten__add")
#loc1633 = loc("2713|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_291xla__mark_tensor")
#loc1634 = loc("2715|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_172aten__view")
#loc1635 = loc("2714|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_318aten__permute")
#loc1636 = loc("2715|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_172aten__mm")
#loc1637 = loc("2716|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_348aten__add")
#loc1638 = loc("2719|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[28].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_293xla__mark_tensor")
#loc1639 = loc("2721|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_173aten__view")
#loc1640 = loc("2720|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_319aten__permute")
#loc1641 = loc("2721|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_173aten__mm")
#loc1642 = loc("2722|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_349aten__add")
#loc1643 = loc("2723|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_350aten__add")
#loc1644 = loc("2736|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_297xla__mark_tensor")
#loc1645 = loc("2738|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_174aten__view")
#loc1646 = loc("2737|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_320aten__permute")
#loc1647 = loc("2738|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_174aten__mm")
#loc1648 = loc("2739|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_353aten__add")
#loc1649 = loc("2746|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_117aten__view")
#loc1650 = loc("2747|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_323aten__permute")
#loc1651 = loc("2752|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_265xla__cast")
#loc1652 = loc("2755|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_178aten__mul")
#loc1653 = loc("2740|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_321aten__permute")
#loc1654 = loc("2741|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_175aten__mm")
#loc1655 = loc("2741|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_175aten__view")
#loc1656 = loc("2742|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_354aten__add")
#loc1657 = loc("2748|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_118aten__view")
#loc1658 = loc("2749|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_324aten__permute")
#loc1659 = loc("2753|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_266xla__cast")
#loc1660 = loc("2756|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_326aten__permute")
#loc1661 = loc("2757|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_179aten__mul")
#loc1662 = loc("2759|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_29aten__einsum")
#loc1663 = loc("2760|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_29aten__eq")
#loc1664 = loc("2761|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_58aten__logical_not")
#loc1666 = loc("or.11803")
#loc1667 = loc("select.11804")
#loc1668 = loc("2763|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_59aten__logical_not")
#loc1669 = loc("2765|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_29aten__expand")
#loc1671 = loc("maximum.11768")
#loc1672 = loc("add.11777")
#loc1673 = loc("2765|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_29aten__where")
#loc1674 = loc("2743|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_322aten__permute")
#loc1675 = loc("2744|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_176aten__mm")
#loc1676 = loc("2744|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_176aten__view")
#loc1677 = loc("2745|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_355aten__add")
#loc1678 = loc("2750|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_119aten__view")
#loc1679 = loc("2751|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_325aten__permute")
#loc1680 = loc("2754|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_267xla__cast")
#loc1681 = loc("2767|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_269aten__einsum")
#loc1682 = loc("2767|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_269xla__cast")
#loc1683 = loc("2769|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_29aten__permute")
#loc1684 = loc("2772|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_177aten__view")
#loc1685 = loc("2771|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_328aten__permute")
#loc1686 = loc("2772|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_177aten__mm")
#loc1687 = loc("2773|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_356aten__add")
#loc1688 = loc("2774|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_357aten__add")
#loc1689 = loc("2787|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_301xla__mark_tensor")
#loc1690 = loc("2789|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_178aten__view")
#loc1691 = loc("2788|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_329aten__permute")
#loc1692 = loc("2789|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_178aten__mm")
#loc1693 = loc("2790|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_360aten__add")
#loc1694 = loc("2793|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[29].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_303xla__mark_tensor")
#loc1695 = loc("2795|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_179aten__view")
#loc1696 = loc("2794|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_330aten__permute")
#loc1697 = loc("2795|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_179aten__mm")
#loc1698 = loc("2796|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_361aten__add")
#loc1699 = loc("2797|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_362aten__add")
#loc1700 = loc("2810|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_307xla__mark_tensor")
#loc1701 = loc("2812|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_180aten__view")
#loc1702 = loc("2811|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_331aten__permute")
#loc1703 = loc("2812|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_180aten__mm")
#loc1704 = loc("2813|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_365aten__add")
#loc1705 = loc("2820|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_121aten__view")
#loc1706 = loc("2821|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_334aten__permute")
#loc1707 = loc("2826|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_274xla__cast")
#loc1708 = loc("2829|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_184aten__mul")
#loc1709 = loc("2814|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_332aten__permute")
#loc1710 = loc("2815|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_181aten__mm")
#loc1711 = loc("2815|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_181aten__view")
#loc1712 = loc("2816|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_366aten__add")
#loc1713 = loc("2822|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_122aten__view")
#loc1714 = loc("2823|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_335aten__permute")
#loc1715 = loc("2827|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_275xla__cast")
#loc1716 = loc("2830|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_337aten__permute")
#loc1717 = loc("2831|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_185aten__mul")
#loc1718 = loc("2833|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_30aten__einsum")
#loc1719 = loc("2834|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_30aten__eq")
#loc1720 = loc("2835|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_60aten__logical_not")
#loc1722 = loc("or.12119")
#loc1723 = loc("select.12120")
#loc1724 = loc("2837|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_61aten__logical_not")
#loc1725 = loc("2839|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_30aten__expand")
#loc1727 = loc("maximum.12084")
#loc1728 = loc("add.12093")
#loc1729 = loc("2839|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_30aten__where")
#loc1730 = loc("2817|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_333aten__permute")
#loc1731 = loc("2818|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_182aten__mm")
#loc1732 = loc("2818|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_182aten__view")
#loc1733 = loc("2819|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_367aten__add")
#loc1734 = loc("2824|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_123aten__view")
#loc1735 = loc("2825|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_336aten__permute")
#loc1736 = loc("2828|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_276xla__cast")
#loc1737 = loc("2841|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_278aten__einsum")
#loc1738 = loc("2841|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_278xla__cast")
#loc1739 = loc("2843|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_30aten__permute")
#loc1740 = loc("2846|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_183aten__view")
#loc1741 = loc("2845|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_339aten__permute")
#loc1742 = loc("2846|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_183aten__mm")
#loc1743 = loc("2847|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_368aten__add")
#loc1744 = loc("2848|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_369aten__add")
#loc1745 = loc("2861|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_311xla__mark_tensor")
#loc1746 = loc("2863|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_184aten__view")
#loc1747 = loc("2862|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_340aten__permute")
#loc1748 = loc("2863|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_184aten__mm")
#loc1749 = loc("2864|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_372aten__add")
#loc1750 = loc("2867|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[30].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_313xla__mark_tensor")
#loc1751 = loc("2869|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_185aten__view")
#loc1752 = loc("2868|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_341aten__permute")
#loc1753 = loc("2869|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_185aten__mm")
#loc1754 = loc("2870|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_373aten__add")
#loc1755 = loc("2871|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_374aten__add")
#loc1756 = loc("2874|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_in]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2303|matmul_193aten__view")
#loc1757 = loc("2873|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_in]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2303|permute_354aten__permute")
#loc1758 = loc("2874|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_in]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2303|matmul_193aten__mm")
#loc1759 = loc("2875|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_in]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2303|add_389aten__add")
#loc1760 = loc("2888|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_331xla__mark_tensor")
#loc1761 = loc("2902|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2248|cat_1aten__cat")
#loc1762 = loc("2906|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_195aten__view")
#loc1763 = loc("2905|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|permute_356aten__permute")
#loc1764 = loc("2906|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_195aten__mm")
#loc1765 = loc("2911|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|view_130aten__view")
#loc1766 = loc("2912|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|permute_359aten__permute")
#loc1767 = loc("2916|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_297xla__cast")
#loc1768 = loc("2919|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|permute_361aten__permute")
#loc1769 = loc("2920|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_201aten__mul")
#loc1770 = loc("2922|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_32aten__einsum")
#loc1771 = loc("2923|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|eq_32aten__eq")
#loc1772 = loc("2924|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_64aten__logical_not")
#loc1774 = loc("or.12421")
#loc1775 = loc("select.12422")
#loc1776 = loc("2926|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_65aten__logical_not")
#loc1777 = loc("2928|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_32aten__expand")
#loc1779 = loc("maximum.12386")
#loc1780 = loc("add.12395")
#loc1781 = loc("2928|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_32aten__where")
#loc1782 = loc("2907|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|permute_357aten__permute")
#loc1783 = loc("2908|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|matmul_196aten__mm")
#loc1784 = loc("2913|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|view_131aten__view")
#loc1785 = loc("2914|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|permute_360aten__permute")
#loc1786 = loc("2917|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_298xla__cast")
#loc1787 = loc("2930|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_300aten__einsum")
#loc1788 = loc("2930|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_300xla__cast")
#loc1789 = loc("2932|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2769|clone_32aten__permute")
#loc1790 = loc("2935|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_197aten__view")
#loc1791 = loc("2934|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|permute_363aten__permute")
#loc1792 = loc("2935|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_197aten__mm")
#loc1793 = loc("2936|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Dropout[resampler.layers[0].attn.to_out[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2775|clone_33aten__view")
#loc1794 = loc("2937|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2783|divaten__div")
#loc1795 = loc("2938|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2249|add_394aten__add")
#loc1796 = loc("2951|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_339xla__mark_tensor")
#loc1797 = loc("2953|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|GELU[getattr(resampler.layers[0].ff, '1').net[0]]|Linear[getattr(resampler.layers[0].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_198aten__view")
#loc1798 = loc("2952|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|GELU[getattr(resampler.layers[0].ff, '1').net[0]]|Linear[getattr(resampler.layers[0].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|permute_364aten__permute")
#loc1799 = loc("2953|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|GELU[getattr(resampler.layers[0].ff, '1').net[0]]|Linear[getattr(resampler.layers[0].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_198aten__mm")
#loc1800 = loc("2957|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|Dropout[getattr(resampler.layers[0].ff, '1').net[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|clone_34xla__mark_tensor")
#loc1801 = loc("2959|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|Linear[getattr(resampler.layers[0].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_199aten__view")
#loc1802 = loc("2958|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|Linear[getattr(resampler.layers[0].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|permute_365aten__permute")
#loc1803 = loc("2959|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|Linear[getattr(resampler.layers[0].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_199aten__mm")
#loc1804 = loc("2960|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_397aten__add")
#loc1805 = loc("2986|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_349xla__mark_tensor")
#loc1806 = loc("2989|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_200aten__view")
#loc1807 = loc("2988|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|permute_366aten__permute")
#loc1808 = loc("2989|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_200aten__mm")
#loc1809 = loc("2994|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|view_133aten__view")
#loc1810 = loc("2995|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|permute_369aten__permute")
#loc1811 = loc("3000|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_307xla__cast")
#loc1812 = loc("3003|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_208aten__mul")
#loc1813 = loc("2973|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_345xla__mark_tensor")
#loc1814 = loc("2987|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2248|cat_2aten__cat")
#loc1815 = loc("2991|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_201aten__view")
#loc1816 = loc("2990|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|permute_367aten__permute")
#loc1817 = loc("2991|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_201aten__mm")
#loc1818 = loc("2996|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|view_134aten__view")
#loc1819 = loc("2997|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|permute_370aten__permute")
#loc1820 = loc("3001|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_308xla__cast")
#loc1821 = loc("3004|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|permute_372aten__permute")
#loc1822 = loc("3005|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_209aten__mul")
#loc1823 = loc("3007|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_33aten__einsum")
#loc1824 = loc("3008|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|eq_33aten__eq")
#loc1825 = loc("3009|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_66aten__logical_not")
#loc1827 = loc("or.12849")
#loc1828 = loc("select.12850")
#loc1829 = loc("3011|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_67aten__logical_not")
#loc1830 = loc("3013|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_33aten__expand")
#loc1832 = loc("maximum.12814")
#loc1833 = loc("add.12823")
#loc1834 = loc("3013|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_33aten__where")
#loc1835 = loc("2992|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|permute_368aten__permute")
#loc1836 = loc("2993|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|matmul_202aten__mm")
#loc1837 = loc("2998|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|view_135aten__view")
#loc1838 = loc("2999|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|permute_371aten__permute")
#loc1839 = loc("3002|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_309xla__cast")
#loc1840 = loc("3015|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_311aten__einsum")
#loc1841 = loc("3015|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_311xla__cast")
#loc1842 = loc("3017|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2769|clone_35aten__permute")
#loc1843 = loc("3020|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_203aten__view")
#loc1844 = loc("3019|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|permute_374aten__permute")
#loc1845 = loc("3020|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_203aten__mm")
#loc1846 = loc("3021|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Dropout[resampler.layers[1].attn.to_out[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2775|clone_36aten__view")
#loc1847 = loc("3022|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2783|div_1aten__div")
#loc1848 = loc("3023|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2249|add_402aten__add")
#loc1849 = loc("3036|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_353xla__mark_tensor")
#loc1850 = loc("3038|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|GELU[getattr(resampler.layers[1].ff, '1').net[0]]|Linear[getattr(resampler.layers[1].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_204aten__view")
#loc1851 = loc("3037|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|GELU[getattr(resampler.layers[1].ff, '1').net[0]]|Linear[getattr(resampler.layers[1].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|permute_375aten__permute")
#loc1852 = loc("3038|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|GELU[getattr(resampler.layers[1].ff, '1').net[0]]|Linear[getattr(resampler.layers[1].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_204aten__mm")
#loc1853 = loc("3042|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|Dropout[getattr(resampler.layers[1].ff, '1').net[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|clone_37xla__mark_tensor")
#loc1854 = loc("3044|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|Linear[getattr(resampler.layers[1].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_205aten__view")
#loc1855 = loc("3043|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|Linear[getattr(resampler.layers[1].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|permute_376aten__permute")
#loc1856 = loc("3044|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|Linear[getattr(resampler.layers[1].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_205aten__mm")
#loc1857 = loc("3045|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_405aten__add")
#loc1858 = loc("3071|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_363xla__mark_tensor")
#loc1859 = loc("3074|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_206aten__view")
#loc1860 = loc("3073|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|permute_377aten__permute")
#loc1861 = loc("3074|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_206aten__mm")
#loc1862 = loc("3079|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|view_137aten__view")
#loc1863 = loc("3080|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|permute_380aten__permute")
#loc1864 = loc("3085|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_318xla__cast")
#loc1865 = loc("3088|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_216aten__mul")
#loc1866 = loc("3058|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_359xla__mark_tensor")
#loc1867 = loc("3072|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2248|cat_3aten__cat")
#loc1868 = loc("3076|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_207aten__view")
#loc1869 = loc("3075|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|permute_378aten__permute")
#loc1870 = loc("3076|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_207aten__mm")
#loc1871 = loc("3081|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|view_138aten__view")
#loc1872 = loc("3082|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|permute_381aten__permute")
#loc1873 = loc("3086|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_319xla__cast")
#loc1874 = loc("3089|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|permute_383aten__permute")
#loc1875 = loc("3090|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_217aten__mul")
#loc1876 = loc("3092|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_34aten__einsum")
#loc1877 = loc("3093|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|eq_34aten__eq")
#loc1878 = loc("3094|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_68aten__logical_not")
#loc1880 = loc("or.13277")
#loc1881 = loc("select.13278")
#loc1882 = loc("3096|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_69aten__logical_not")
#loc1883 = loc("3098|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_34aten__expand")
#loc1885 = loc("maximum.13242")
#loc1886 = loc("add.13251")
#loc1887 = loc("3098|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_34aten__where")
#loc1888 = loc("3077|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|permute_379aten__permute")
#loc1889 = loc("3078|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|matmul_208aten__mm")
#loc1890 = loc("3083|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|view_139aten__view")
#loc1891 = loc("3084|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|permute_382aten__permute")
#loc1892 = loc("3087|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_320xla__cast")
#loc1893 = loc("3100|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_322aten__einsum")
#loc1894 = loc("3100|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_322xla__cast")
#loc1895 = loc("3102|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2769|clone_38aten__permute")
#loc1896 = loc("3105|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_209aten__view")
#loc1897 = loc("3104|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|permute_385aten__permute")
#loc1898 = loc("3105|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_209aten__mm")
#loc1899 = loc("3106|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Dropout[resampler.layers[2].attn.to_out[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2775|clone_39aten__view")
#loc1900 = loc("3107|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2783|div_2aten__div")
#loc1901 = loc("3108|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2249|add_410aten__add")
#loc1902 = loc("3121|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_367xla__mark_tensor")
#loc1903 = loc("3123|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|GELU[getattr(resampler.layers[2].ff, '1').net[0]]|Linear[getattr(resampler.layers[2].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_210aten__view")
#loc1904 = loc("3122|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|GELU[getattr(resampler.layers[2].ff, '1').net[0]]|Linear[getattr(resampler.layers[2].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|permute_386aten__permute")
#loc1905 = loc("3123|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|GELU[getattr(resampler.layers[2].ff, '1').net[0]]|Linear[getattr(resampler.layers[2].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_210aten__mm")
#loc1906 = loc("3127|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|Dropout[getattr(resampler.layers[2].ff, '1').net[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|clone_40xla__mark_tensor")
#loc1907 = loc("3129|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|Linear[getattr(resampler.layers[2].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_211aten__view")
#loc1908 = loc("3128|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|Linear[getattr(resampler.layers[2].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|permute_387aten__permute")
#loc1909 = loc("3129|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|Linear[getattr(resampler.layers[2].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_211aten__mm")
#loc1910 = loc("3130|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_413aten__add")
#loc1911 = loc("3156|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_377xla__mark_tensor")
#loc1912 = loc("3159|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_212aten__view")
#loc1913 = loc("3158|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|permute_388aten__permute")
#loc1914 = loc("3159|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_212aten__mm")
#loc1915 = loc("3164|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|view_141aten__view")
#loc1916 = loc("3165|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|permute_391aten__permute")
#loc1917 = loc("3170|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_329xla__cast")
#loc1918 = loc("3173|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_224aten__mul")
#loc1919 = loc("3143|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_373xla__mark_tensor")
#loc1920 = loc("3157|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2248|cat_4aten__cat")
#loc1921 = loc("3161|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_213aten__view")
#loc1922 = loc("3160|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|permute_389aten__permute")
#loc1923 = loc("3161|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_213aten__mm")
#loc1924 = loc("3166|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|view_142aten__view")
#loc1925 = loc("3167|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|permute_392aten__permute")
#loc1926 = loc("3171|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_330xla__cast")
#loc1927 = loc("3174|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|permute_394aten__permute")
#loc1928 = loc("3175|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_225aten__mul")
#loc1929 = loc("3177|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_35aten__einsum")
#loc1930 = loc("3178|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|eq_35aten__eq")
#loc1931 = loc("3179|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_70aten__logical_not")
#loc1933 = loc("or.13705")
#loc1934 = loc("select.13706")
#loc1935 = loc("3181|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_71aten__logical_not")
#loc1936 = loc("3183|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_35aten__expand")
#loc1938 = loc("maximum.13670")
#loc1939 = loc("add.13679")
#loc1940 = loc("3183|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_35aten__where")
#loc1941 = loc("3162|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|permute_390aten__permute")
#loc1942 = loc("3163|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|matmul_214aten__mm")
#loc1943 = loc("3168|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|view_143aten__view")
#loc1944 = loc("3169|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|permute_393aten__permute")
#loc1945 = loc("3172|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_331xla__cast")
#loc1946 = loc("3185|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_333aten__einsum")
#loc1947 = loc("3185|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_333xla__cast")
#loc1948 = loc("3187|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2769|clone_41aten__permute")
#loc1949 = loc("3190|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_215aten__view")
#loc1950 = loc("3189|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|permute_396aten__permute")
#loc1951 = loc("3190|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_215aten__mm")
#loc1952 = loc("3191|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Dropout[resampler.layers[3].attn.to_out[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2775|clone_42aten__view")
#loc1953 = loc("3192|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2783|div_3aten__div")
#loc1954 = loc("3193|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2249|add_418aten__add")
#loc1955 = loc("3206|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_381xla__mark_tensor")
#loc1956 = loc("3208|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|GELU[getattr(resampler.layers[3].ff, '1').net[0]]|Linear[getattr(resampler.layers[3].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_216aten__view")
#loc1957 = loc("3207|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|GELU[getattr(resampler.layers[3].ff, '1').net[0]]|Linear[getattr(resampler.layers[3].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|permute_397aten__permute")
#loc1958 = loc("3208|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|GELU[getattr(resampler.layers[3].ff, '1').net[0]]|Linear[getattr(resampler.layers[3].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_216aten__mm")
#loc1959 = loc("3212|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|Dropout[getattr(resampler.layers[3].ff, '1').net[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|clone_43xla__mark_tensor")
#loc1960 = loc("3214|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|Linear[getattr(resampler.layers[3].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_217aten__view")
#loc1961 = loc("3213|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|Linear[getattr(resampler.layers[3].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|permute_398aten__permute")
#loc1962 = loc("3214|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|Linear[getattr(resampler.layers[3].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_217aten__mm")
#loc1963 = loc("3215|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_421aten__add")
#loc1964 = loc("3217|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2309|matmul_218aten__view")
#loc1965 = loc("3216|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2309|permute_399aten__permute")
#loc1966 = loc("3217|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2309|matmul_218aten__mm")
#loc1967 = loc("3218|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2309|add_422aten__add")
#loc1968 = loc("3231|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|mark_tensor_387xla__mark_tensor")
#loc1972 = loc("3222|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|_to_copy_336xla__cast")
#loc1974 = loc("add.13910")
#loc1975 = loc("3226|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|sub_78aten__sub")
#loc1976 = loc("add.13893")
#loc1977 = loc("3224|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|add_423aten__add")
#loc1978 = loc("3225|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|rsqrt_78aten__rsqrt")
#loc1979 = loc("3227|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|mul_228aten__mul")
#loc1980 = loc("3228|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|mul_229xla__cast")
#loc1981 = loc("3228|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|mul_229aten__mul")
#loc1982 = loc("3229|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|add_424aten__add")
#loc1983 = loc("3230|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|_to_copy_337xla__cast")
#loc1987 = loc("3197|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|_to_copy_334xla__cast")
#loc1989 = loc("add.13801")
#loc1990 = loc("3201|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|sub_77aten__sub")
#loc1991 = loc("add.13784")
#loc1992 = loc("3199|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_419aten__add")
#loc1993 = loc("3200|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|rsqrt_77aten__rsqrt")
#loc1994 = loc("3202|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_226aten__mul")
#loc1995 = loc("3203|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_227xla__cast")
#loc1996 = loc("3203|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_227aten__mul")
#loc1997 = loc("3204|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_420aten__add")
#loc1998 = loc("3205|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|_to_copy_335xla__cast")
#loc2002 = loc("3134|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|_to_copy_325xla__cast")
#loc2004 = loc("add.13594")
#loc2005 = loc("3138|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|sub_75aten__sub")
#loc2006 = loc("add.13577")
#loc2007 = loc("3136|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|add_414aten__add")
#loc2008 = loc("3137|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|rsqrt_75aten__rsqrt")
#loc2009 = loc("3139|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_220aten__mul")
#loc2010 = loc("3140|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_221xla__cast")
#loc2011 = loc("3140|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_221aten__mul")
#loc2012 = loc("3141|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|add_415aten__add")
#loc2013 = loc("3142|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|_to_copy_326xla__cast")
#loc2017 = loc("3049|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|_to_copy_314xla__cast")
#loc2019 = loc("add.13166")
#loc2020 = loc("3053|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|sub_72aten__sub")
#loc2021 = loc("add.13149")
#loc2022 = loc("3051|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|add_406aten__add")
#loc2023 = loc("3052|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|rsqrt_72aten__rsqrt")
#loc2024 = loc("3054|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_212aten__mul")
#loc2025 = loc("3055|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_213xla__cast")
#loc2026 = loc("3055|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_213aten__mul")
#loc2027 = loc("3056|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|add_407aten__add")
#loc2028 = loc("3057|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|_to_copy_315xla__cast")
#loc2030 = loc("3040|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|GELU[getattr(resampler.layers[1].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|gelu_33aten__gelu")
#loc2032 = loc("2955|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|GELU[getattr(resampler.layers[0].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|gelu_32aten__gelu")
#loc2036 = loc("2942|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|_to_copy_301xla__cast")
#loc2038 = loc("add.12517")
#loc2039 = loc("2946|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|sub_68aten__sub")
#loc2040 = loc("add.12500")
#loc2041 = loc("2944|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_395aten__add")
#loc2042 = loc("2945|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|rsqrt_68aten__rsqrt")
#loc2043 = loc("2947|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_202aten__mul")
#loc2044 = loc("2948|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_203xla__cast")
#loc2045 = loc("2948|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_203aten__mul")
#loc2046 = loc("2949|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_396aten__add")
#loc2047 = loc("2950|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|_to_copy_302xla__cast")
#loc2049 = loc("2792|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[29].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_29aten__gelu")
#loc2053 = loc("2727|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_263xla__cast")
#loc2055 = loc("add.11671")
#loc2056 = loc("2731|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_59aten__sub")
#loc2057 = loc("add.11654")
#loc2058 = loc("2729|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_351aten__add")
#loc2059 = loc("2730|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_59aten__rsqrt")
#loc2060 = loc("2732|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_176aten__mul")
#loc2061 = loc("2733|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_177xla__cast")
#loc2062 = loc("2733|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_177aten__mul")
#loc2063 = loc("2734|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_352aten__add")
#loc2064 = loc("2735|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_264xla__cast")
#loc2066 = loc("2718|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[28].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_28aten__gelu")
#loc2070 = loc("2977|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|_to_copy_305xla__cast")
#loc2072 = loc("add.12644")
#loc2073 = loc("2981|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|sub_70aten__sub")
#loc2074 = loc("add.12627")
#loc2075 = loc("2979|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|add_400aten__add")
#loc2076 = loc("2980|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|rsqrt_70aten__rsqrt")
#loc2077 = loc("2982|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_206aten__mul")
#loc2078 = loc("2983|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_207xla__cast")
#loc2079 = loc("2983|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_207aten__mul")
#loc2080 = loc("2984|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|add_401aten__add")
#loc2081 = loc("2985|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|_to_copy_306xla__cast")
#loc2085 = loc("2704|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_261xla__cast")
#loc2087 = loc("add.11561")
#loc2088 = loc("2708|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_58aten__sub")
#loc2089 = loc("add.11544")
#loc2090 = loc("2706|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_346aten__add")
#loc2091 = loc("2707|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_58aten__rsqrt")
#loc2092 = loc("2709|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_174aten__mul")
#loc2093 = loc("2710|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_175xla__cast")
#loc2094 = loc("2710|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_175aten__mul")
#loc2095 = loc("2711|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_347aten__add")
#loc2096 = loc("2712|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_262xla__cast")
#loc2100 = loc("2653|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_254xla__cast")
#loc2102 = loc("add.11355")
#loc2103 = loc("2657|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_57aten__sub")
#loc2104 = loc("add.11338")
#loc2105 = loc("2655|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_339aten__add")
#loc2106 = loc("2656|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_57aten__rsqrt")
#loc2107 = loc("2658|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_170aten__mul")
#loc2108 = loc("2659|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_171xla__cast")
#loc2109 = loc("2659|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_171aten__mul")
#loc2110 = loc("2660|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_340aten__add")
#loc2111 = loc("2661|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_255xla__cast")
#loc2113 = loc("3125|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|GELU[getattr(resampler.layers[2].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|gelu_34aten__gelu")
#loc2117 = loc("2579|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_245xla__cast")
#loc2119 = loc("add.11039")
#loc2120 = loc("2583|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_55aten__sub")
#loc2121 = loc("add.11022")
#loc2122 = loc("2581|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_327aten__add")
#loc2123 = loc("2582|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_55aten__rsqrt")
#loc2124 = loc("2584|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_164aten__mul")
#loc2125 = loc("2585|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_165xla__cast")
#loc2126 = loc("2585|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_165aten__mul")
#loc2127 = loc("2586|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_328aten__add")
#loc2128 = loc("2587|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_246xla__cast")
#loc2132 = loc("2556|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_243xla__cast")
#loc2134 = loc("add.10929")
#loc2135 = loc("2560|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_54aten__sub")
#loc2136 = loc("add.10912")
#loc2137 = loc("2558|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_322aten__add")
#loc2138 = loc("2559|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_54aten__rsqrt")
#loc2139 = loc("2561|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_162aten__mul")
#loc2140 = loc("2562|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_163xla__cast")
#loc2141 = loc("2562|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_163aten__mul")
#loc2142 = loc("2563|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_323aten__add")
#loc2143 = loc("2564|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_244xla__cast")
#loc2147 = loc("2505|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_236xla__cast")
#loc2149 = loc("add.10723")
#loc2150 = loc("2509|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_53aten__sub")
#loc2151 = loc("add.10706")
#loc2152 = loc("2507|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_315aten__add")
#loc2153 = loc("2508|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_53aten__rsqrt")
#loc2154 = loc("2510|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_158aten__mul")
#loc2155 = loc("2511|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_159xla__cast")
#loc2156 = loc("2511|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_159aten__mul")
#loc2157 = loc("2512|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_316aten__add")
#loc2158 = loc("2513|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_237xla__cast")
#loc2160 = loc("2496|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[25].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_25aten__gelu")
#loc2164 = loc("2482|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_234xla__cast")
#loc2166 = loc("add.10613")
#loc2167 = loc("2486|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_52aten__sub")
#loc2168 = loc("add.10596")
#loc2169 = loc("2484|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_310aten__add")
#loc2170 = loc("2485|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_52aten__rsqrt")
#loc2171 = loc("2487|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_156aten__mul")
#loc2172 = loc("2488|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_157xla__cast")
#loc2173 = loc("2488|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_157aten__mul")
#loc2174 = loc("2489|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_311aten__add")
#loc2175 = loc("2490|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_235xla__cast")
#loc2179 = loc("2431|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_227xla__cast")
#loc2181 = loc("add.10407")
#loc2182 = loc("2435|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_51aten__sub")
#loc2183 = loc("add.10390")
#loc2184 = loc("2433|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_303aten__add")
#loc2185 = loc("2434|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_51aten__rsqrt")
#loc2186 = loc("2436|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_152aten__mul")
#loc2187 = loc("2437|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_153xla__cast")
#loc2188 = loc("2437|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_153aten__mul")
#loc2189 = loc("2438|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_304aten__add")
#loc2190 = loc("2439|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_228xla__cast")
#loc2192 = loc("2422|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[24].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_24aten__gelu")
#loc2196 = loc("2408|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_225xla__cast")
#loc2198 = loc("add.10297")
#loc2199 = loc("2412|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_50aten__sub")
#loc2200 = loc("add.10280")
#loc2201 = loc("2410|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_298aten__add")
#loc2202 = loc("2411|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_50aten__rsqrt")
#loc2203 = loc("2413|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_150aten__mul")
#loc2204 = loc("2414|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_151xla__cast")
#loc2205 = loc("2414|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_151aten__mul")
#loc2206 = loc("2415|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_299aten__add")
#loc2207 = loc("2416|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_226xla__cast")
#loc2211 = loc("2357|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_218xla__cast")
#loc2213 = loc("add.10091")
#loc2214 = loc("2361|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_49aten__sub")
#loc2215 = loc("add.10074")
#loc2216 = loc("2359|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_291aten__add")
#loc2217 = loc("2360|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_49aten__rsqrt")
#loc2218 = loc("2362|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_146aten__mul")
#loc2219 = loc("2363|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_147xla__cast")
#loc2220 = loc("2363|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_147aten__mul")
#loc2221 = loc("2364|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_292aten__add")
#loc2222 = loc("2365|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_219xla__cast")
#loc2224 = loc("2348|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[23].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_23aten__gelu")
#loc2228 = loc("2334|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_216xla__cast")
#loc2230 = loc("add.9981")
#loc2231 = loc("2338|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_48aten__sub")
#loc2232 = loc("add.9964")
#loc2233 = loc("2336|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_286aten__add")
#loc2234 = loc("2337|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_48aten__rsqrt")
#loc2235 = loc("2339|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_144aten__mul")
#loc2236 = loc("2340|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_145xla__cast")
#loc2237 = loc("2340|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_145aten__mul")
#loc2238 = loc("2341|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_287aten__add")
#loc2239 = loc("2342|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_217xla__cast")
#loc2243 = loc("2209|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_200xla__cast")
#loc2245 = loc("add.9459")
#loc2246 = loc("2213|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_45aten__sub")
#loc2247 = loc("add.9442")
#loc2248 = loc("2211|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_267aten__add")
#loc2249 = loc("2212|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_45aten__rsqrt")
#loc2250 = loc("2214|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_134aten__mul")
#loc2251 = loc("2215|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_135xla__cast")
#loc2252 = loc("2215|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_135aten__mul")
#loc2253 = loc("2216|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_268aten__add")
#loc2254 = loc("2217|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_201xla__cast")
#loc2256 = loc("2200|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[21].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_21aten__gelu")
#loc2260 = loc("3027|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|_to_copy_312xla__cast")
#loc2262 = loc("add.12945")
#loc2263 = loc("3031|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|sub_71aten__sub")
#loc2264 = loc("add.12928")
#loc2265 = loc("3029|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_403aten__add")
#loc2266 = loc("3030|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|rsqrt_71aten__rsqrt")
#loc2267 = loc("3032|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_210aten__mul")
#loc2268 = loc("3033|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_211xla__cast")
#loc2269 = loc("3033|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_211aten__mul")
#loc2270 = loc("3034|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_404aten__add")
#loc2271 = loc("3035|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|_to_copy_313xla__cast")
#loc2275 = loc("2186|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_198xla__cast")
#loc2277 = loc("add.9349")
#loc2278 = loc("2190|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_44aten__sub")
#loc2279 = loc("add.9332")
#loc2280 = loc("2188|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_262aten__add")
#loc2281 = loc("2189|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_44aten__rsqrt")
#loc2282 = loc("2191|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_132aten__mul")
#loc2283 = loc("2192|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_133xla__cast")
#loc2284 = loc("2192|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_133aten__mul")
#loc2285 = loc("2193|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_263aten__add")
#loc2286 = loc("2194|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_199xla__cast")
#loc2288 = loc("2126|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[20].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_20aten__gelu")
#loc2290 = loc("2570|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[26].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_26aten__gelu")
#loc2294 = loc("2061|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_182xla__cast")
#loc2296 = loc("add.8827")
#loc2297 = loc("2065|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_41aten__sub")
#loc2298 = loc("add.8810")
#loc2299 = loc("2063|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_243aten__add")
#loc2300 = loc("2064|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_41aten__rsqrt")
#loc2301 = loc("2066|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_122aten__mul")
#loc2302 = loc("2067|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_123xla__cast")
#loc2303 = loc("2067|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_123aten__mul")
#loc2304 = loc("2068|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_244aten__add")
#loc2305 = loc("2069|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_183xla__cast")
#loc2307 = loc("2052|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[19].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_19aten__gelu")
#loc2311 = loc("1987|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_173xla__cast")
#loc2313 = loc("add.8511")
#loc2314 = loc("1991|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_39aten__sub")
#loc2315 = loc("add.8494")
#loc2316 = loc("1989|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_231aten__add")
#loc2317 = loc("1990|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_39aten__rsqrt")
#loc2318 = loc("1992|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_116aten__mul")
#loc2319 = loc("1993|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_117xla__cast")
#loc2320 = loc("1993|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_117aten__mul")
#loc2321 = loc("1994|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_232aten__add")
#loc2322 = loc("1995|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_174xla__cast")
#loc2324 = loc("1238|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[8].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_8aten__gelu")
#loc2328 = loc("1321|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_92xla__cast")
#loc2330 = loc("add.5667")
#loc2331 = loc("1325|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_21aten__sub")
#loc2332 = loc("add.5650")
#loc2333 = loc("1323|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_123aten__add")
#loc2334 = loc("1324|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_21aten__rsqrt")
#loc2335 = loc("1326|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_62aten__mul")
#loc2336 = loc("1327|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_63xla__cast")
#loc2337 = loc("1327|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_63aten__mul")
#loc2338 = loc("1328|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_124aten__add")
#loc2339 = loc("1329|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_93xla__cast")
#loc2343 = loc("1173|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_74xla__cast")
#loc2345 = loc("add.5035")
#loc2346 = loc("1177|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_17aten__sub")
#loc2347 = loc("add.5018")
#loc2348 = loc("1175|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_99aten__add")
#loc2349 = loc("1176|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_17aten__rsqrt")
#loc2350 = loc("1178|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_50aten__mul")
#loc2351 = loc("1179|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_51xla__cast")
#loc2352 = loc("1179|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_51aten__mul")
#loc2353 = loc("1180|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_100aten__add")
#loc2354 = loc("1181|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_75xla__cast")
#loc2356 = loc("2866|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[30].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_30aten__gelu")
#loc2358 = loc("1164|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[7].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_7aten__gelu")
#loc2362 = loc("2852|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_279xla__cast")
#loc2364 = loc("add.12193")
#loc2365 = loc("2856|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_62aten__sub")
#loc2366 = loc("add.12176")
#loc2367 = loc("2854|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_370aten__add")
#loc2368 = loc("2855|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_62aten__rsqrt")
#loc2369 = loc("2857|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_186aten__mul")
#loc2370 = loc("2858|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_187xla__cast")
#loc2371 = loc("2858|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_187aten__mul")
#loc2372 = loc("2859|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_371aten__add")
#loc2373 = loc("2860|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_280xla__cast")
#loc2375 = loc("2274|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[22].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_22aten__gelu")
#loc2379 = loc("2260|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_207xla__cast")
#loc2381 = loc("add.9665")
#loc2382 = loc("2264|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_46aten__sub")
#loc2383 = loc("add.9648")
#loc2384 = loc("2262|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_274aten__add")
#loc2385 = loc("2263|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_46aten__rsqrt")
#loc2386 = loc("2265|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_138aten__mul")
#loc2387 = loc("2266|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_139xla__cast")
#loc2388 = loc("2266|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_139aten__mul")
#loc2389 = loc("2267|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_275aten__add")
#loc2390 = loc("2268|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_208xla__cast")
#loc2392 = loc("1090|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[6].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_6aten__gelu")
#loc2396 = loc("1076|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_63xla__cast")
#loc2398 = loc("add.4609")
#loc2399 = loc("1080|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_14aten__sub")
#loc2400 = loc("add.4592")
#loc2401 = loc("1078|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_82aten__add")
#loc2402 = loc("1079|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_14aten__rsqrt")
#loc2403 = loc("1081|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_42aten__mul")
#loc2404 = loc("1082|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_43xla__cast")
#loc2405 = loc("1082|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_43aten__mul")
#loc2406 = loc("1083|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_83aten__add")
#loc2407 = loc("1084|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_64xla__cast")
#loc2409 = loc("646|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[0].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|geluaten__gelu")
#loc2413 = loc("3112|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|_to_copy_323xla__cast")
#loc2415 = loc("add.13373")
#loc2416 = loc("3116|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|sub_74aten__sub")
#loc2417 = loc("add.13356")
#loc2418 = loc("3114|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_411aten__add")
#loc2419 = loc("3115|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|rsqrt_74aten__rsqrt")
#loc2420 = loc("3117|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_218aten__mul")
#loc2421 = loc("3118|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_219xla__cast")
#loc2422 = loc("3118|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_219aten__mul")
#loc2423 = loc("3119|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_412aten__add")
#loc2424 = loc("3120|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|_to_copy_324xla__cast")
#loc2428 = loc("1224|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_81xla__cast")
#loc2430 = loc("add.5241")
#loc2431 = loc("1228|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_18aten__sub")
#loc2432 = loc("add.5224")
#loc2433 = loc("1226|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_106aten__add")
#loc2434 = loc("1227|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_18aten__rsqrt")
#loc2435 = loc("1229|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_54aten__mul")
#loc2436 = loc("1230|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_55xla__cast")
#loc2437 = loc("1230|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_55aten__mul")
#loc2438 = loc("1231|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_107aten__add")
#loc2439 = loc("1232|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_82xla__cast")
#loc2443 = loc("729|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_20xla__cast")
#loc2445 = loc("add.3139")
#loc2446 = loc("733|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_5aten__sub")
#loc2447 = loc("add.3122")
#loc2448 = loc("731|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_27aten__add")
#loc2449 = loc("732|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_5aten__rsqrt")
#loc2450 = loc("734|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_14aten__mul")
#loc2451 = loc("735|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_15xla__cast")
#loc2452 = loc("735|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_15aten__mul")
#loc2453 = loc("736|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_28aten__add")
#loc2454 = loc("737|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_21xla__cast")
#loc2458 = loc("1150|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_72xla__cast")
#loc2460 = loc("add.4925")
#loc2461 = loc("1154|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_16aten__sub")
#loc2462 = loc("add.4908")
#loc2463 = loc("1152|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_94aten__add")
#loc2464 = loc("1153|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_16aten__rsqrt")
#loc2465 = loc("1155|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_48aten__mul")
#loc2466 = loc("1156|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_49xla__cast")
#loc2467 = loc("1156|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_49aten__mul")
#loc2468 = loc("1157|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_95aten__add")
#loc2469 = loc("1158|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_73xla__cast")
#loc2473 = loc("1839|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_155xla__cast")
#loc2475 = loc("add.7879")
#loc2476 = loc("1843|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_35aten__sub")
#loc2477 = loc("add.7862")
#loc2478 = loc("1841|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_207aten__add")
#loc2479 = loc("1842|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_35aten__rsqrt")
#loc2480 = loc("1844|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_104aten__mul")
#loc2481 = loc("1845|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_105xla__cast")
#loc2482 = loc("1845|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_105aten__mul")
#loc2483 = loc("1846|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_208aten__add")
#loc2484 = loc("1847|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_156xla__cast")
#loc2486 = loc("942|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[4].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_4aten__gelu")
#loc2490 = loc("2801|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_272xla__cast")
#loc2492 = loc("add.11987")
#loc2493 = loc("2805|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_61aten__sub")
#loc2494 = loc("add.11970")
#loc2495 = loc("2803|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_363aten__add")
#loc2496 = loc("2804|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_61aten__rsqrt")
#loc2497 = loc("2806|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_182aten__mul")
#loc2498 = loc("2807|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_183xla__cast")
#loc2499 = loc("2807|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_183aten__mul")
#loc2500 = loc("2808|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_364aten__add")
#loc2501 = loc("2809|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_273xla__cast")
#loc2505 = loc("780|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_27xla__cast")
#loc2507 = loc("add.3345")
#loc2508 = loc("784|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_6aten__sub")
#loc2509 = loc("add.3328")
#loc2510 = loc("782|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_34aten__add")
#loc2511 = loc("783|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_6aten__rsqrt")
#loc2512 = loc("785|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_18aten__mul")
#loc2513 = loc("786|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_19xla__cast")
#loc2514 = loc("786|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_19aten__mul")
#loc2515 = loc("787|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_35aten__add")
#loc2516 = loc("788|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_28xla__cast")
#loc2520 = loc("1668|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_135xla__cast")
#loc2522 = loc("add.7137")
#loc2523 = loc("1672|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_30aten__sub")
#loc2524 = loc("add.7120")
#loc2525 = loc("1670|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_178aten__add")
#loc2526 = loc("1671|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_30aten__rsqrt")
#loc2527 = loc("1673|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_90aten__mul")
#loc2528 = loc("1674|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_91xla__cast")
#loc2529 = loc("1674|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_91aten__mul")
#loc2530 = loc("1675|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_179aten__add")
#loc2531 = loc("1676|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_136xla__cast")
#loc2535 = loc("581|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_2xla__cast")
#loc2537 = loc("add.2507")
#loc2538 = loc("585|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_1aten__sub")
#loc2539 = loc("add.2490")
#loc2540 = loc("583|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_3aten__add")
#loc2541 = loc("584|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_1aten__rsqrt")
#loc2542 = loc("586|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_2aten__mul")
#loc2543 = loc("587|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_3xla__cast")
#loc2544 = loc("587|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_3aten__mul")
#loc2545 = loc("588|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_4aten__add")
#loc2546 = loc("589|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_3xla__cast")
#loc2548 = loc("1534|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[12].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_12aten__gelu")
#loc2552 = loc("632|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_9xla__cast")
#loc2554 = loc("add.2713")
#loc2555 = loc("636|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_2aten__sub")
#loc2556 = loc("add.2696")
#loc2557 = loc("634|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_10aten__add")
#loc2558 = loc("635|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_2aten__rsqrt")
#loc2559 = loc("637|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_6aten__mul")
#loc2560 = loc("638|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_7xla__cast")
#loc2561 = loc("638|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_7aten__mul")
#loc2562 = loc("639|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_11aten__add")
#loc2563 = loc("640|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_10xla__cast")
#loc2567 = loc("706|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_18xla__cast")
#loc2569 = loc("add.3029")
#loc2570 = loc("710|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_4aten__sub")
#loc2571 = loc("add.3012")
#loc2572 = loc("708|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_22aten__add")
#loc2573 = loc("709|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_4aten__rsqrt")
#loc2574 = loc("711|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_12aten__mul")
#loc2575 = loc("712|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_13xla__cast")
#loc2576 = loc("712|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_13aten__mul")
#loc2577 = loc("713|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_23aten__add")
#loc2578 = loc("714|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_19xla__cast")
#loc2580 = loc("720|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[1].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_1aten__gelu")
#loc2584 = loc("568|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|_to_copyxla__cast")
#loc2586 = loc("add.2427")
#loc2587 = loc("572|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|subaten__sub")
#loc2588 = loc("add.2410")
#loc2589 = loc("570|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|add_1aten__add")
#loc2590 = loc("571|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|rsqrtaten__rsqrt")
#loc2591 = loc("573|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|mulaten__mul")
#loc2592 = loc("574|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|mul_1xla__cast")
#loc2593 = loc("574|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|mul_1aten__mul")
#loc2594 = loc("575|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|add_2aten__add")
#loc2595 = loc("576|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|_to_copy_1xla__cast")
#loc2599 = loc("928|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_45xla__cast")
#loc2601 = loc("add.3977")
#loc2602 = loc("932|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_10aten__sub")
#loc2603 = loc("add.3960")
#loc2604 = loc("930|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_58aten__add")
#loc2605 = loc("931|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_10aten__rsqrt")
#loc2606 = loc("933|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_30aten__mul")
#loc2607 = loc("934|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_31xla__cast")
#loc2608 = loc("934|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_31aten__mul")
#loc2609 = loc("935|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_59aten__add")
#loc2610 = loc("936|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_46xla__cast")
#loc2614 = loc("655|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_11xla__cast")
#loc2616 = loc("add.2823")
#loc2617 = loc("659|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_3aten__sub")
#loc2618 = loc("add.2806")
#loc2619 = loc("657|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_15aten__add")
#loc2620 = loc("658|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_3aten__rsqrt")
#loc2621 = loc("660|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_8aten__mul")
#loc2622 = loc("661|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_9xla__cast")
#loc2623 = loc("661|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_9aten__mul")
#loc2624 = loc("662|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_16aten__add")
#loc2625 = loc("663|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_12xla__cast")
#loc2629 = loc("1691|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_137xla__cast")
#loc2631 = loc("add.7247")
#loc2632 = loc("1695|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_31aten__sub")
#loc2633 = loc("add.7230")
#loc2634 = loc("1693|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_183aten__add")
#loc2635 = loc("1694|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_31aten__rsqrt")
#loc2636 = loc("1696|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_92aten__mul")
#loc2637 = loc("1697|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_93xla__cast")
#loc2638 = loc("1697|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_93aten__mul")
#loc2639 = loc("1698|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_184aten__add")
#loc2640 = loc("1699|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_138xla__cast")
#loc2642 = loc("868|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[3].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_3aten__gelu")
#loc2646 = loc("1002|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_54xla__cast")
#loc2648 = loc("add.4293")
#loc2649 = loc("1006|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_12aten__sub")
#loc2650 = loc("add.4276")
#loc2651 = loc("1004|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_70aten__add")
#loc2652 = loc("1005|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_12aten__rsqrt")
#loc2653 = loc("1007|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_36aten__mul")
#loc2654 = loc("1008|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_37xla__cast")
#loc2655 = loc("1008|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_37aten__mul")
#loc2656 = loc("1009|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_71aten__add")
#loc2657 = loc("1010|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_55xla__cast")
#loc2659 = loc("794|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[2].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_2aten__gelu")
#loc2663 = loc("951|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_47xla__cast")
#loc2665 = loc("add.4087")
#loc2666 = loc("955|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_11aten__sub")
#loc2667 = loc("add.4070")
#loc2668 = loc("953|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_63aten__add")
#loc2669 = loc("954|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_11aten__rsqrt")
#loc2670 = loc("956|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_32aten__mul")
#loc2671 = loc("957|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_33xla__cast")
#loc2672 = loc("957|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_33aten__mul")
#loc2673 = loc("958|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_64aten__add")
#loc2674 = loc("959|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_48xla__cast")
#loc2678 = loc("803|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_29xla__cast")
#loc2680 = loc("add.3455")
#loc2681 = loc("807|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_7aten__sub")
#loc2682 = loc("add.3438")
#loc2683 = loc("805|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_39aten__add")
#loc2684 = loc("806|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_7aten__rsqrt")
#loc2685 = loc("808|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_20aten__mul")
#loc2686 = loc("809|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_21xla__cast")
#loc2687 = loc("809|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_21aten__mul")
#loc2688 = loc("810|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_40aten__add")
#loc2689 = loc("811|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_30xla__cast")
#loc2693 = loc("1765|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_146xla__cast")
#loc2695 = loc("add.7563")
#loc2696 = loc("1769|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_33aten__sub")
#loc2697 = loc("add.7546")
#loc2698 = loc("1767|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_195aten__add")
#loc2699 = loc("1768|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_33aten__rsqrt")
#loc2700 = loc("1770|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_98aten__mul")
#loc2701 = loc("1771|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_99xla__cast")
#loc2702 = loc("1771|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_99aten__mul")
#loc2703 = loc("1772|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_196aten__add")
#loc2704 = loc("1773|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_147xla__cast")
#loc2708 = loc("1298|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_90xla__cast")
#loc2710 = loc("add.5557")
#loc2711 = loc("1302|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_20aten__sub")
#loc2712 = loc("add.5540")
#loc2713 = loc("1300|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_118aten__add")
#loc2714 = loc("1301|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_20aten__rsqrt")
#loc2715 = loc("1303|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_60aten__mul")
#loc2716 = loc("1304|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_61xla__cast")
#loc2717 = loc("1304|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_61aten__mul")
#loc2718 = loc("1305|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_119aten__add")
#loc2719 = loc("1306|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_91xla__cast")
#loc2723 = loc("2630|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_252xla__cast")
#loc2725 = loc("add.11245")
#loc2726 = loc("2634|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_56aten__sub")
#loc2727 = loc("add.11228")
#loc2728 = loc("2632|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_334aten__add")
#loc2729 = loc("2633|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_56aten__rsqrt")
#loc2730 = loc("2635|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_168aten__mul")
#loc2731 = loc("2636|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_169xla__cast")
#loc2732 = loc("2636|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_169aten__mul")
#loc2733 = loc("2637|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_335aten__add")
#loc2734 = loc("2638|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_253xla__cast")
#loc2738 = loc("2112|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_189xla__cast")
#loc2740 = loc("add.9033")
#loc2741 = loc("2116|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_42aten__sub")
#loc2742 = loc("add.9016")
#loc2743 = loc("2114|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_250aten__add")
#loc2744 = loc("2115|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_42aten__rsqrt")
#loc2745 = loc("2117|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_126aten__mul")
#loc2746 = loc("2118|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_127xla__cast")
#loc2747 = loc("2118|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_127aten__mul")
#loc2748 = loc("2119|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_251aten__add")
#loc2749 = loc("2120|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_190xla__cast")
#loc2753 = loc("1742|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_144xla__cast")
#loc2755 = loc("add.7453")
#loc2756 = loc("1746|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_32aten__sub")
#loc2757 = loc("add.7436")
#loc2758 = loc("1744|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_190aten__add")
#loc2759 = loc("1745|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_32aten__rsqrt")
#loc2760 = loc("1747|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_96aten__mul")
#loc2761 = loc("1748|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_97xla__cast")
#loc2762 = loc("1748|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_97aten__mul")
#loc2763 = loc("1749|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_191aten__add")
#loc2764 = loc("1750|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_145xla__cast")
#loc2766 = loc("1312|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[9].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_9aten__gelu")
#loc2770 = loc("3062|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|_to_copy_316xla__cast")
#loc2772 = loc("add.13072")
#loc2773 = loc("3066|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|sub_73aten__sub")
#loc2774 = loc("add.13055")
#loc2775 = loc("3064|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|add_408aten__add")
#loc2776 = loc("3065|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|rsqrt_73aten__rsqrt")
#loc2777 = loc("3067|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_214aten__mul")
#loc2778 = loc("3068|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_215xla__cast")
#loc2779 = loc("3068|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_215aten__mul")
#loc2780 = loc("3069|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|add_409aten__add")
#loc2781 = loc("3070|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|_to_copy_317xla__cast")
#loc2785 = loc("1816|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_153xla__cast")
#loc2787 = loc("add.7769")
#loc2788 = loc("1820|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_34aten__sub")
#loc2789 = loc("add.7752")
#loc2790 = loc("1818|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_202aten__add")
#loc2791 = loc("1819|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_34aten__rsqrt")
#loc2792 = loc("1821|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_102aten__mul")
#loc2793 = loc("1822|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_103xla__cast")
#loc2794 = loc("1822|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_103aten__mul")
#loc2795 = loc("1823|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_203aten__add")
#loc2796 = loc("1824|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_154xla__cast")
#loc2800 = loc("2038|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_180xla__cast")
#loc2802 = loc("add.8717")
#loc2803 = loc("2042|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_40aten__sub")
#loc2804 = loc("add.8700")
#loc2805 = loc("2040|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_238aten__add")
#loc2806 = loc("2041|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_40aten__rsqrt")
#loc2807 = loc("2043|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_120aten__mul")
#loc2808 = loc("2044|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_121xla__cast")
#loc2809 = loc("2044|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_121aten__mul")
#loc2810 = loc("2045|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_239aten__add")
#loc2811 = loc("2046|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_181xla__cast")
#loc2815 = loc("854|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_36xla__cast")
#loc2817 = loc("add.3661")
#loc2818 = loc("858|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_8aten__sub")
#loc2819 = loc("add.3644")
#loc2820 = loc("856|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_46aten__add")
#loc2821 = loc("857|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_8aten__rsqrt")
#loc2822 = loc("859|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_24aten__mul")
#loc2823 = loc("860|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_25xla__cast")
#loc2824 = loc("860|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_25aten__mul")
#loc2825 = loc("861|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_47aten__add")
#loc2826 = loc("862|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_37xla__cast")
#loc2828 = loc("1978|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[18].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_18aten__gelu")
#loc2830 = loc("3210|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|GELU[getattr(resampler.layers[3].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|gelu_35aten__gelu")
#loc2834 = loc("1372|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_99xla__cast")
#loc2836 = loc("add.5873")
#loc2837 = loc("1376|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_22aten__sub")
#loc2838 = loc("add.5856")
#loc2839 = loc("1374|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_130aten__add")
#loc2840 = loc("1375|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_22aten__rsqrt")
#loc2841 = loc("1377|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_66aten__mul")
#loc2842 = loc("1378|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_67xla__cast")
#loc2843 = loc("1378|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_67aten__mul")
#loc2844 = loc("1379|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_131aten__add")
#loc2845 = loc("1380|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_100xla__cast")
#loc2849 = loc("2892|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|_to_copy_294xla__cast")
#loc2851 = loc("add.109")
#loc2852 = loc("2896|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|sub_67aten__sub")
#loc2853 = loc("add.92")
#loc2854 = loc("2894|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|add_392aten__add")
#loc2855 = loc("2895|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|rsqrt_67aten__rsqrt")
#loc2856 = loc("2897|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_198aten__mul")
#loc2857 = loc("2898|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_199xla__cast")
#loc2858 = loc("2898|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_199aten__mul")
#loc2859 = loc("2899|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|add_393aten__add")
#loc2860 = loc("2900|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|_to_copy_295xla__cast")
#loc2862 = loc("1386|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[10].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_10aten__gelu")
#loc2866 = loc("2135|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_191xla__cast")
#loc2868 = loc("add.9143")
#loc2869 = loc("2139|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_43aten__sub")
#loc2870 = loc("add.9126")
#loc2871 = loc("2137|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_255aten__add")
#loc2872 = loc("2138|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_43aten__rsqrt")
#loc2873 = loc("2140|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_128aten__mul")
#loc2874 = loc("2141|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_129xla__cast")
#loc2875 = loc("2141|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_129aten__mul")
#loc2876 = loc("2142|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_256aten__add")
#loc2877 = loc("2143|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_192xla__cast")
#loc2881 = loc("1395|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_101xla__cast")
#loc2883 = loc("add.5983")
#loc2884 = loc("1399|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_23aten__sub")
#loc2885 = loc("add.5966")
#loc2886 = loc("1397|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_135aten__add")
#loc2887 = loc("1398|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_23aten__rsqrt")
#loc2888 = loc("1400|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_68aten__mul")
#loc2889 = loc("1401|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_69xla__cast")
#loc2890 = loc("1401|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_69aten__mul")
#loc2891 = loc("1402|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_136aten__add")
#loc2892 = loc("1403|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_102xla__cast")
#loc2896 = loc("1446|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_108xla__cast")
#loc2898 = loc("add.6189")
#loc2899 = loc("1450|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_24aten__sub")
#loc2900 = loc("add.6172")
#loc2901 = loc("1448|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_142aten__add")
#loc2902 = loc("1449|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_24aten__rsqrt")
#loc2903 = loc("1451|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_72aten__mul")
#loc2904 = loc("1452|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_73xla__cast")
#loc2905 = loc("1452|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_73aten__mul")
#loc2906 = loc("1453|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_143aten__add")
#loc2907 = loc("1454|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_109xla__cast")
#loc2911 = loc("2283|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_209xla__cast")
#loc2913 = loc("add.9775")
#loc2914 = loc("2287|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_47aten__sub")
#loc2915 = loc("add.9758")
#loc2916 = loc("2285|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_279aten__add")
#loc2917 = loc("2286|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_47aten__rsqrt")
#loc2918 = loc("2288|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_140aten__mul")
#loc2919 = loc("2289|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_141xla__cast")
#loc2920 = loc("2289|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_141aten__mul")
#loc2921 = loc("2290|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_280aten__add")
#loc2922 = loc("2291|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_210xla__cast")
#loc2926 = loc("1247|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_83xla__cast")
#loc2928 = loc("add.5351")
#loc2929 = loc("1251|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_19aten__sub")
#loc2930 = loc("add.5334")
#loc2931 = loc("1249|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_111aten__add")
#loc2932 = loc("1250|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_19aten__rsqrt")
#loc2933 = loc("1252|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_56aten__mul")
#loc2934 = loc("1253|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_57xla__cast")
#loc2935 = loc("1253|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_57aten__mul")
#loc2936 = loc("1254|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_112aten__add")
#loc2937 = loc("1255|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_84xla__cast")
#loc2941 = loc("1520|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_117xla__cast")
#loc2943 = loc("add.6505")
#loc2944 = loc("1524|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_26aten__sub")
#loc2945 = loc("add.6488")
#loc2946 = loc("1522|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_154aten__add")
#loc2947 = loc("1523|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_26aten__rsqrt")
#loc2948 = loc("1525|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_78aten__mul")
#loc2949 = loc("1526|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_79xla__cast")
#loc2950 = loc("1526|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_79aten__mul")
#loc2951 = loc("1527|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_155aten__add")
#loc2952 = loc("1528|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_118xla__cast")
#loc2954 = loc("1016|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[5].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_5aten__gelu")
#loc2958 = loc("1964|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_171xla__cast")
#loc2960 = loc("add.8401")
#loc2961 = loc("1968|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_38aten__sub")
#loc2962 = loc("add.8384")
#loc2963 = loc("1966|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_226aten__add")
#loc2964 = loc("1967|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_38aten__rsqrt")
#loc2965 = loc("1969|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_114aten__mul")
#loc2966 = loc("1970|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_115xla__cast")
#loc2967 = loc("1970|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_115aten__mul")
#loc2968 = loc("1971|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_227aten__add")
#loc2969 = loc("1972|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_172xla__cast")
#loc2973 = loc("1594|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_126xla__cast")
#loc2975 = loc("add.6821")
#loc2976 = loc("1598|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_28aten__sub")
#loc2977 = loc("add.6804")
#loc2978 = loc("1596|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_166aten__add")
#loc2979 = loc("1597|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_28aten__rsqrt")
#loc2980 = loc("1599|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_84aten__mul")
#loc2981 = loc("1600|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_85xla__cast")
#loc2982 = loc("1600|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_85aten__mul")
#loc2983 = loc("1601|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_167aten__add")
#loc2984 = loc("1602|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_127xla__cast")
#loc2988 = loc("2879|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|_to_copy_292xla__cast")
#loc2990 = loc("add.12310")
#loc2991 = loc("2883|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|sub_66aten__sub")
#loc2992 = loc("add.12293")
#loc2993 = loc("2881|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|add_390aten__add")
#loc2994 = loc("2882|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|rsqrt_66aten__rsqrt")
#loc2995 = loc("2884|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_196aten__mul")
#loc2996 = loc("2885|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_197xla__cast")
#loc2997 = loc("2885|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_197aten__mul")
#loc2998 = loc("2886|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|add_391aten__add")
#loc2999 = loc("2887|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|_to_copy_293xla__cast")
#loc3003 = loc("1025|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_56xla__cast")
#loc3005 = loc("add.4403")
#loc3006 = loc("1029|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_13aten__sub")
#loc3007 = loc("add.4386")
#loc3008 = loc("1027|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_75aten__add")
#loc3009 = loc("1028|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_13aten__rsqrt")
#loc3010 = loc("1030|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_38aten__mul")
#loc3011 = loc("1031|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_39xla__cast")
#loc3012 = loc("1031|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_39aten__mul")
#loc3013 = loc("1032|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_76aten__add")
#loc3014 = loc("1033|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_57xla__cast")
#loc3016 = loc("1756|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[15].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_15aten__gelu")
#loc3018 = loc("1608|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[13].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_13aten__gelu")
#loc3020 = loc("2644|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[27].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_27aten__gelu")
#loc3024 = loc("1099|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_65xla__cast")
#loc3026 = loc("add.4719")
#loc3027 = loc("1103|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_15aten__sub")
#loc3028 = loc("add.4702")
#loc3029 = loc("1101|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_87aten__add")
#loc3030 = loc("1102|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_15aten__rsqrt")
#loc3031 = loc("1104|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_44aten__mul")
#loc3032 = loc("1105|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_45xla__cast")
#loc3033 = loc("1105|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_45aten__mul")
#loc3034 = loc("1106|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_88aten__add")
#loc3035 = loc("1107|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_66xla__cast")
#loc3039 = loc("1617|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_128xla__cast")
#loc3041 = loc("add.6931")
#loc3042 = loc("1621|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_29aten__sub")
#loc3043 = loc("add.6914")
#loc3044 = loc("1619|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_171aten__add")
#loc3045 = loc("1620|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_29aten__rsqrt")
#loc3046 = loc("1622|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_86aten__mul")
#loc3047 = loc("1623|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_87xla__cast")
#loc3048 = loc("1623|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_87aten__mul")
#loc3049 = loc("1624|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_172aten__add")
#loc3050 = loc("1625|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_129xla__cast")
#loc3054 = loc("2964|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|_to_copy_303xla__cast")
#loc3056 = loc("add.12738")
#loc3057 = loc("2968|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|sub_69aten__sub")
#loc3058 = loc("add.12721")
#loc3059 = loc("2966|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|add_398aten__add")
#loc3060 = loc("2967|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|rsqrt_69aten__rsqrt")
#loc3061 = loc("2969|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_204aten__mul")
#loc3062 = loc("2970|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_205xla__cast")
#loc3063 = loc("2970|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_205aten__mul")
#loc3064 = loc("2971|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|add_399aten__add")
#loc3065 = loc("2972|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|_to_copy_304xla__cast")
#loc3067 = loc("1460|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[11].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_11aten__gelu")
#loc3069 = loc("1682|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[14].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_14aten__gelu")
#loc3073 = loc("877|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_38xla__cast")
#loc3075 = loc("add.3771")
#loc3076 = loc("881|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_9aten__sub")
#loc3077 = loc("add.3754")
#loc3078 = loc("879|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_51aten__add")
#loc3079 = loc("880|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_9aten__rsqrt")
#loc3080 = loc("882|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_26aten__mul")
#loc3081 = loc("883|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_27xla__cast")
#loc3082 = loc("883|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_27aten__mul")
#loc3083 = loc("884|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_52aten__add")
#loc3084 = loc("885|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_39xla__cast")
#loc3088 = loc("1469|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_110xla__cast")
#loc3090 = loc("add.6299")
#loc3091 = loc("1473|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_25aten__sub")
#loc3092 = loc("add.6282")
#loc3093 = loc("1471|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_147aten__add")
#loc3094 = loc("1472|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_25aten__rsqrt")
#loc3095 = loc("1474|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_74aten__mul")
#loc3096 = loc("1475|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_75xla__cast")
#loc3097 = loc("1475|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_75aten__mul")
#loc3098 = loc("1476|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_148aten__add")
#loc3099 = loc("1477|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_111xla__cast")
#loc3101 = loc("1830|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[16].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_16aten__gelu")
#loc3105 = loc("1890|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_162xla__cast")
#loc3107 = loc("add.8085")
#loc3108 = loc("1894|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_36aten__sub")
#loc3109 = loc("add.8068")
#loc3110 = loc("1892|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_214aten__add")
#loc3111 = loc("1893|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_36aten__rsqrt")
#loc3112 = loc("1895|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_108aten__mul")
#loc3113 = loc("1896|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_109xla__cast")
#loc3114 = loc("1896|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_109aten__mul")
#loc3115 = loc("1897|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_215aten__add")
#loc3116 = loc("1898|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_163xla__cast")
#loc3120 = loc("3147|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|_to_copy_327xla__cast")
#loc3122 = loc("add.13500")
#loc3123 = loc("3151|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|sub_76aten__sub")
#loc3124 = loc("add.13483")
#loc3125 = loc("3149|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|add_416aten__add")
#loc3126 = loc("3150|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|rsqrt_76aten__rsqrt")
#loc3127 = loc("3152|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_222aten__mul")
#loc3128 = loc("3153|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_223xla__cast")
#loc3129 = loc("3153|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_223aten__mul")
#loc3130 = loc("3154|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|add_417aten__add")
#loc3131 = loc("3155|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|_to_copy_328xla__cast")
#loc3135 = loc("2778|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_270xla__cast")
#loc3137 = loc("add.11877")
#loc3138 = loc("2782|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_60aten__sub")
#loc3139 = loc("add.11860")
#loc3140 = loc("2780|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_358aten__add")
#loc3141 = loc("2781|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_60aten__rsqrt")
#loc3142 = loc("2783|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_180aten__mul")
#loc3143 = loc("2784|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_181xla__cast")
#loc3144 = loc("2784|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_181aten__mul")
#loc3145 = loc("2785|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_359aten__add")
#loc3146 = loc("2786|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_271xla__cast")
#loc3150 = loc("1543|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_119xla__cast")
#loc3152 = loc("add.6615")
#loc3153 = loc("1547|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_27aten__sub")
#loc3154 = loc("add.6598")
#loc3155 = loc("1545|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_159aten__add")
#loc3156 = loc("1546|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_27aten__rsqrt")
#loc3157 = loc("1548|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_80aten__mul")
#loc3158 = loc("1549|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_81xla__cast")
#loc3159 = loc("1549|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_81aten__mul")
#loc3160 = loc("1550|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_160aten__add")
#loc3161 = loc("1551|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_120xla__cast")
#loc3163 = loc("1904|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[17].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_17aten__gelu")
#loc3167 = loc("1913|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_164xla__cast")
#loc3169 = loc("add.8195")
#loc3170 = loc("1917|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_37aten__sub")
#loc3171 = loc("add.8178")
#loc3172 = loc("1915|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_219aten__add")
#loc3173 = loc("1916|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_37aten__rsqrt")
#loc3174 = loc("1918|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_110aten__mul")
#loc3175 = loc("1919|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_111xla__cast")
#loc3176 = loc("1919|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_111aten__mul")
#loc3177 = loc("1920|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_220aten__add")
#loc3178 = loc("1921|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_165xla__cast")
