#loc1 = loc("-1|unknown|unknown|-1|unknownxla__device_data")
#loc40 = loc("616|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_1aten__any")
#loc94 = loc("690|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_2aten__any")
#loc148 = loc("764|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_3aten__any")
#loc202 = loc("838|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_4aten__any")
#loc256 = loc("912|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_5aten__any")
#loc310 = loc("986|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_6aten__any")
#loc364 = loc("1060|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_7aten__any")
#loc418 = loc("1134|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_8aten__any")
#loc472 = loc("1208|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_9aten__any")
#loc526 = loc("1282|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_10aten__any")
#loc580 = loc("1356|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_11aten__any")
#loc634 = loc("1430|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_12aten__any")
#loc688 = loc("1504|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_13aten__any")
#loc742 = loc("1578|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_14aten__any")
#loc796 = loc("1652|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_15aten__any")
#loc850 = loc("1726|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_16aten__any")
#loc904 = loc("1800|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_17aten__any")
#loc958 = loc("1874|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_18aten__any")
#loc1012 = loc("1948|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_19aten__any")
#loc1066 = loc("2022|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_20aten__any")
#loc1120 = loc("2096|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_21aten__any")
#loc1174 = loc("2170|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_22aten__any")
#loc1228 = loc("2244|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_23aten__any")
#loc1282 = loc("2318|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_24aten__any")
#loc1336 = loc("2392|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_25aten__any")
#loc1390 = loc("2466|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_26aten__any")
#loc1444 = loc("2540|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_27aten__any")
#loc1498 = loc("2614|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_28aten__any")
#loc1552 = loc("2688|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_29aten__any")
#loc1606 = loc("2762|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_30aten__any")
#loc1660 = loc("2836|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_31aten__any")
#loc1710 = loc("2925|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_33aten__any")
#loc1761 = loc("3010|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_34aten__any")
#loc1812 = loc("3095|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_35aten__any")
#loc1863 = loc("3180|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_36aten__any")
#loc1898 = loc("3219|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|mark_tensor_384xla__mark_tensor")
#loc1899 = loc("3220|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|mark_tensor_385xla__mark_tensor")
#loc1900 = loc("3221|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|mark_tensor_386xla__mark_tensor")
#loc1911 = loc("3194|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_378xla__mark_tensor")
#loc1912 = loc("3195|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_379xla__mark_tensor")
#loc1913 = loc("3196|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_380xla__mark_tensor")
#loc1924 = loc("3131|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_370xla__mark_tensor")
#loc1925 = loc("3132|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_371xla__mark_tensor")
#loc1926 = loc("3133|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_372xla__mark_tensor")
#loc1937 = loc("3046|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_356xla__mark_tensor")
#loc1938 = loc("3047|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_357xla__mark_tensor")
#loc1939 = loc("3048|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_358xla__mark_tensor")
#loc1950 = loc("3039|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|GELU[getattr(resampler.layers[1].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|mark_tensor_354xla__mark_tensor")
#loc1952 = loc("2954|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|GELU[getattr(resampler.layers[0].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|mark_tensor_340xla__mark_tensor")
#loc1954 = loc("2939|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_336xla__mark_tensor")
#loc1955 = loc("2940|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_337xla__mark_tensor")
#loc1956 = loc("2941|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_338xla__mark_tensor")
#loc1967 = loc("2791|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[29].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_302xla__mark_tensor")
#loc1969 = loc("2724|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_294xla__mark_tensor")
#loc1970 = loc("2725|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_295xla__mark_tensor")
#loc1971 = loc("2726|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_296xla__mark_tensor")
#loc1982 = loc("2717|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[28].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_292xla__mark_tensor")
#loc1984 = loc("2974|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_346xla__mark_tensor")
#loc1985 = loc("2975|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_347xla__mark_tensor")
#loc1986 = loc("2976|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_348xla__mark_tensor")
#loc1997 = loc("2701|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_288xla__mark_tensor")
#loc1998 = loc("2702|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_289xla__mark_tensor")
#loc1999 = loc("2703|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_290xla__mark_tensor")
#loc2010 = loc("2650|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_284xla__mark_tensor")
#loc2011 = loc("2651|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_285xla__mark_tensor")
#loc2012 = loc("2652|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_286xla__mark_tensor")
#loc2023 = loc("3124|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|GELU[getattr(resampler.layers[2].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|mark_tensor_368xla__mark_tensor")
#loc2025 = loc("2576|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_274xla__mark_tensor")
#loc2026 = loc("2577|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_275xla__mark_tensor")
#loc2027 = loc("2578|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_276xla__mark_tensor")
#loc2038 = loc("2553|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_268xla__mark_tensor")
#loc2039 = loc("2554|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_269xla__mark_tensor")
#loc2040 = loc("2555|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_270xla__mark_tensor")
#loc2051 = loc("2502|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_264xla__mark_tensor")
#loc2052 = loc("2503|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_265xla__mark_tensor")
#loc2053 = loc("2504|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_266xla__mark_tensor")
#loc2064 = loc("2495|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[25].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_262xla__mark_tensor")
#loc2066 = loc("2479|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_258xla__mark_tensor")
#loc2067 = loc("2480|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_259xla__mark_tensor")
#loc2068 = loc("2481|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_260xla__mark_tensor")
#loc2079 = loc("2428|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_254xla__mark_tensor")
#loc2080 = loc("2429|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_255xla__mark_tensor")
#loc2081 = loc("2430|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_256xla__mark_tensor")
#loc2092 = loc("2421|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[24].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_252xla__mark_tensor")
#loc2094 = loc("2405|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_248xla__mark_tensor")
#loc2095 = loc("2406|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_249xla__mark_tensor")
#loc2096 = loc("2407|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_250xla__mark_tensor")
#loc2107 = loc("2354|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_244xla__mark_tensor")
#loc2108 = loc("2355|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_245xla__mark_tensor")
#loc2109 = loc("2356|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_246xla__mark_tensor")
#loc2120 = loc("2347|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[23].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_242xla__mark_tensor")
#loc2122 = loc("2331|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_238xla__mark_tensor")
#loc2123 = loc("2332|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_239xla__mark_tensor")
#loc2124 = loc("2333|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_240xla__mark_tensor")
#loc2135 = loc("2206|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_224xla__mark_tensor")
#loc2136 = loc("2207|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_225xla__mark_tensor")
#loc2137 = loc("2208|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_226xla__mark_tensor")
#loc2148 = loc("2199|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[21].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_222xla__mark_tensor")
#loc2150 = loc("3024|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_350xla__mark_tensor")
#loc2151 = loc("3025|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_351xla__mark_tensor")
#loc2152 = loc("3026|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_352xla__mark_tensor")
#loc2163 = loc("2183|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_218xla__mark_tensor")
#loc2164 = loc("2184|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_219xla__mark_tensor")
#loc2165 = loc("2185|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_220xla__mark_tensor")
#loc2176 = loc("2125|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[20].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_212xla__mark_tensor")
#loc2178 = loc("2569|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[26].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_272xla__mark_tensor")
#loc2180 = loc("2058|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_204xla__mark_tensor")
#loc2181 = loc("2059|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_205xla__mark_tensor")
#loc2182 = loc("2060|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_206xla__mark_tensor")
#loc2193 = loc("2051|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[19].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_202xla__mark_tensor")
#loc2195 = loc("1984|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_194xla__mark_tensor")
#loc2196 = loc("1985|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_195xla__mark_tensor")
#loc2197 = loc("1986|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_196xla__mark_tensor")
#loc2208 = loc("1237|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[8].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_92xla__mark_tensor")
#loc2210 = loc("1318|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_104xla__mark_tensor")
#loc2211 = loc("1319|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_105xla__mark_tensor")
#loc2212 = loc("1320|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_106xla__mark_tensor")
#loc2223 = loc("1170|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_84xla__mark_tensor")
#loc2224 = loc("1171|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_85xla__mark_tensor")
#loc2225 = loc("1172|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_86xla__mark_tensor")
#loc2236 = loc("2865|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[30].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_312xla__mark_tensor")
#loc2238 = loc("1163|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[7].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_82xla__mark_tensor")
#loc2240 = loc("2849|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_308xla__mark_tensor")
#loc2241 = loc("2850|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_309xla__mark_tensor")
#loc2242 = loc("2851|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_310xla__mark_tensor")
#loc2253 = loc("2273|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[22].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_232xla__mark_tensor")
#loc2255 = loc("2257|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_228xla__mark_tensor")
#loc2256 = loc("2258|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_229xla__mark_tensor")
#loc2257 = loc("2259|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_230xla__mark_tensor")
#loc2268 = loc("1089|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[6].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_72xla__mark_tensor")
#loc2270 = loc("1073|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_68xla__mark_tensor")
#loc2271 = loc("1074|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_69xla__mark_tensor")
#loc2272 = loc("1075|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_70xla__mark_tensor")
#loc2283 = loc("645|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[0].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_12xla__mark_tensor")
#loc2285 = loc("3109|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_364xla__mark_tensor")
#loc2286 = loc("3110|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_365xla__mark_tensor")
#loc2287 = loc("3111|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_366xla__mark_tensor")
#loc2298 = loc("1221|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_88xla__mark_tensor")
#loc2299 = loc("1222|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_89xla__mark_tensor")
#loc2300 = loc("1223|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_90xla__mark_tensor")
#loc2311 = loc("726|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_24xla__mark_tensor")
#loc2312 = loc("727|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_25xla__mark_tensor")
#loc2313 = loc("728|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_26xla__mark_tensor")
#loc2324 = loc("1147|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_78xla__mark_tensor")
#loc2325 = loc("1148|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_79xla__mark_tensor")
#loc2326 = loc("1149|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_80xla__mark_tensor")
#loc2337 = loc("1836|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_174xla__mark_tensor")
#loc2338 = loc("1837|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_175xla__mark_tensor")
#loc2339 = loc("1838|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_176xla__mark_tensor")
#loc2350 = loc("941|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[4].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_52xla__mark_tensor")
#loc2352 = loc("2798|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_304xla__mark_tensor")
#loc2353 = loc("2799|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_305xla__mark_tensor")
#loc2354 = loc("2800|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_306xla__mark_tensor")
#loc2365 = loc("777|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_28xla__mark_tensor")
#loc2366 = loc("778|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_29xla__mark_tensor")
#loc2367 = loc("779|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_30xla__mark_tensor")
#loc2378 = loc("1665|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_148xla__mark_tensor")
#loc2379 = loc("1666|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_149xla__mark_tensor")
#loc2380 = loc("1667|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_150xla__mark_tensor")
#loc2391 = loc("578|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_4xla__mark_tensor")
#loc2392 = loc("579|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_5xla__mark_tensor")
#loc2393 = loc("580|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_6xla__mark_tensor")
#loc2404 = loc("1533|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[12].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_132xla__mark_tensor")
#loc2406 = loc("629|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_8xla__mark_tensor")
#loc2407 = loc("630|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_9xla__mark_tensor")
#loc2408 = loc("631|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_10xla__mark_tensor")
#loc2419 = loc("703|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_18xla__mark_tensor")
#loc2420 = loc("704|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_19xla__mark_tensor")
#loc2421 = loc("705|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_20xla__mark_tensor")
#loc2432 = loc("719|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[1].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_22xla__mark_tensor")
#loc2434 = loc("565|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|mark_tensorxla__mark_tensor")
#loc2435 = loc("566|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|mark_tensor_1xla__mark_tensor")
#loc2436 = loc("567|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|mark_tensor_2xla__mark_tensor")
#loc2447 = loc("925|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_48xla__mark_tensor")
#loc2448 = loc("926|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_49xla__mark_tensor")
#loc2449 = loc("927|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_50xla__mark_tensor")
#loc2460 = loc("652|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_14xla__mark_tensor")
#loc2461 = loc("653|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_15xla__mark_tensor")
#loc2462 = loc("654|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_16xla__mark_tensor")
#loc2473 = loc("1688|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_154xla__mark_tensor")
#loc2474 = loc("1689|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_155xla__mark_tensor")
#loc2475 = loc("1690|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_156xla__mark_tensor")
#loc2486 = loc("867|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[3].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_42xla__mark_tensor")
#loc2488 = loc("999|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_58xla__mark_tensor")
#loc2489 = loc("1000|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_59xla__mark_tensor")
#loc2490 = loc("1001|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_60xla__mark_tensor")
#loc2501 = loc("793|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[2].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_32xla__mark_tensor")
#loc2503 = loc("948|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_54xla__mark_tensor")
#loc2504 = loc("949|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_55xla__mark_tensor")
#loc2505 = loc("950|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_56xla__mark_tensor")
#loc2516 = loc("800|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_34xla__mark_tensor")
#loc2517 = loc("801|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_35xla__mark_tensor")
#loc2518 = loc("802|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_36xla__mark_tensor")
#loc2529 = loc("1762|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_164xla__mark_tensor")
#loc2530 = loc("1763|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_165xla__mark_tensor")
#loc2531 = loc("1764|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_166xla__mark_tensor")
#loc2542 = loc("1295|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_98xla__mark_tensor")
#loc2543 = loc("1296|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_99xla__mark_tensor")
#loc2544 = loc("1297|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_100xla__mark_tensor")
#loc2555 = loc("2627|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_278xla__mark_tensor")
#loc2556 = loc("2628|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_279xla__mark_tensor")
#loc2557 = loc("2629|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_280xla__mark_tensor")
#loc2568 = loc("2109|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_208xla__mark_tensor")
#loc2569 = loc("2110|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_209xla__mark_tensor")
#loc2570 = loc("2111|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_210xla__mark_tensor")
#loc2581 = loc("1739|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_158xla__mark_tensor")
#loc2582 = loc("1740|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_159xla__mark_tensor")
#loc2583 = loc("1741|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_160xla__mark_tensor")
#loc2594 = loc("1311|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[9].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_102xla__mark_tensor")
#loc2596 = loc("3059|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_360xla__mark_tensor")
#loc2597 = loc("3060|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_361xla__mark_tensor")
#loc2598 = loc("3061|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_362xla__mark_tensor")
#loc2609 = loc("1813|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_168xla__mark_tensor")
#loc2610 = loc("1814|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_169xla__mark_tensor")
#loc2611 = loc("1815|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_170xla__mark_tensor")
#loc2622 = loc("2035|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_198xla__mark_tensor")
#loc2623 = loc("2036|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_199xla__mark_tensor")
#loc2624 = loc("2037|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_200xla__mark_tensor")
#loc2635 = loc("851|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_38xla__mark_tensor")
#loc2636 = loc("852|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_39xla__mark_tensor")
#loc2637 = loc("853|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_40xla__mark_tensor")
#loc2648 = loc("1977|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[18].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_192xla__mark_tensor")
#loc2650 = loc("3209|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|GELU[getattr(resampler.layers[3].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|mark_tensor_382xla__mark_tensor")
#loc2652 = loc("1369|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_108xla__mark_tensor")
#loc2653 = loc("1370|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_109xla__mark_tensor")
#loc2654 = loc("1371|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_110xla__mark_tensor")
#loc2665 = loc("2889|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_332xla__mark_tensor")
#loc2666 = loc("2890|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_333xla__mark_tensor")
#loc2667 = loc("2891|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_334xla__mark_tensor")
#loc2678 = loc("1385|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[10].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_112xla__mark_tensor")
#loc2680 = loc("2132|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_214xla__mark_tensor")
#loc2681 = loc("2133|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_215xla__mark_tensor")
#loc2682 = loc("2134|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_216xla__mark_tensor")
#loc2693 = loc("1392|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_114xla__mark_tensor")
#loc2694 = loc("1393|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_115xla__mark_tensor")
#loc2695 = loc("1394|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_116xla__mark_tensor")
#loc2706 = loc("1443|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_118xla__mark_tensor")
#loc2707 = loc("1444|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_119xla__mark_tensor")
#loc2708 = loc("1445|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_120xla__mark_tensor")
#loc2719 = loc("2280|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_234xla__mark_tensor")
#loc2720 = loc("2281|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_235xla__mark_tensor")
#loc2721 = loc("2282|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_236xla__mark_tensor")
#loc2732 = loc("1244|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_94xla__mark_tensor")
#loc2733 = loc("1245|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_95xla__mark_tensor")
#loc2734 = loc("1246|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_96xla__mark_tensor")
#loc2745 = loc("1517|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_128xla__mark_tensor")
#loc2746 = loc("1518|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_129xla__mark_tensor")
#loc2747 = loc("1519|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_130xla__mark_tensor")
#loc2758 = loc("1015|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[5].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_62xla__mark_tensor")
#loc2760 = loc("1961|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_188xla__mark_tensor")
#loc2761 = loc("1962|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_189xla__mark_tensor")
#loc2762 = loc("1963|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_190xla__mark_tensor")
#loc2773 = loc("1591|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_138xla__mark_tensor")
#loc2774 = loc("1592|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_139xla__mark_tensor")
#loc2775 = loc("1593|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_140xla__mark_tensor")
#loc2786 = loc("2876|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_328xla__mark_tensor")
#loc2787 = loc("2877|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_329xla__mark_tensor")
#loc2788 = loc("2878|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_330xla__mark_tensor")
#loc2799 = loc("1022|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_64xla__mark_tensor")
#loc2800 = loc("1023|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_65xla__mark_tensor")
#loc2801 = loc("1024|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_66xla__mark_tensor")
#loc2812 = loc("1755|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[15].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_162xla__mark_tensor")
#loc2814 = loc("1607|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[13].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_142xla__mark_tensor")
#loc2816 = loc("2643|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[27].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_282xla__mark_tensor")
#loc2818 = loc("1096|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_74xla__mark_tensor")
#loc2819 = loc("1097|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_75xla__mark_tensor")
#loc2820 = loc("1098|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_76xla__mark_tensor")
#loc2831 = loc("1614|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_144xla__mark_tensor")
#loc2832 = loc("1615|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_145xla__mark_tensor")
#loc2833 = loc("1616|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_146xla__mark_tensor")
#loc2844 = loc("2961|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_342xla__mark_tensor")
#loc2845 = loc("2962|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_343xla__mark_tensor")
#loc2846 = loc("2963|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_344xla__mark_tensor")
#loc2857 = loc("1459|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[11].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_122xla__mark_tensor")
#loc2859 = loc("1681|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[14].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_152xla__mark_tensor")
#loc2861 = loc("874|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_44xla__mark_tensor")
#loc2862 = loc("875|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_45xla__mark_tensor")
#loc2863 = loc("876|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_46xla__mark_tensor")
#loc2874 = loc("1466|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_124xla__mark_tensor")
#loc2875 = loc("1467|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_125xla__mark_tensor")
#loc2876 = loc("1468|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_126xla__mark_tensor")
#loc2887 = loc("1829|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[16].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_172xla__mark_tensor")
#loc2889 = loc("1887|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_178xla__mark_tensor")
#loc2890 = loc("1888|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_179xla__mark_tensor")
#loc2891 = loc("1889|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_180xla__mark_tensor")
#loc2902 = loc("3144|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_374xla__mark_tensor")
#loc2903 = loc("3145|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_375xla__mark_tensor")
#loc2904 = loc("3146|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_376xla__mark_tensor")
#loc2915 = loc("2775|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_298xla__mark_tensor")
#loc2916 = loc("2776|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_299xla__mark_tensor")
#loc2917 = loc("2777|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_300xla__mark_tensor")
#loc2928 = loc("1540|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_134xla__mark_tensor")
#loc2929 = loc("1541|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_135xla__mark_tensor")
#loc2930 = loc("1542|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_136xla__mark_tensor")
#loc2941 = loc("1903|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[17].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_182xla__mark_tensor")
#loc2943 = loc("1910|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_184xla__mark_tensor")
#loc2944 = loc("1911|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_185xla__mark_tensor")
#loc2945 = loc("1912|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_186xla__mark_tensor")
module @SyncTensorsGraph.13945 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_norm_out_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg1: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_norm_out_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_proj_out_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg3: tensor<2048x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_proj_out_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg4: tensor<1x16x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_latents"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg5: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_0_attn_to_out_0_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg6: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_0_attn_to_v_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg7: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_0_ln1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg8: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_0_ln1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg9: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_0_ln0_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg10: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_0_ln0_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg11: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_proj_in_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg12: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_proj_in_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg13: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg14: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg15: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg16: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg17: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg18: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg19: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg20: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg21: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg22: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg23: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg24: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg25: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg26: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg27: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg28: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg29: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg30: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg31: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg32: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg33: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg34: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg35: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg36: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg37: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg38: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg39: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg40: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg41: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg42: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg43: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg44: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg45: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg46: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg47: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg48: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg49: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg50: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg51: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg52: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg53: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg54: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg55: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg56: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg57: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg58: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg59: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg60: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg61: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg62: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg63: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg64: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg65: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg66: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg67: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg68: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg69: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg70: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg71: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg72: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg73: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg74: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg75: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg76: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg77: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg78: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg79: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg80: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg81: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg82: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg83: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg84: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg85: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg86: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg87: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg88: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg89: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg90: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg91: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg92: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg93: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg94: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg95: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg96: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg97: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg98: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg99: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg100: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg101: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg102: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg103: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg104: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg105: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg106: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg107: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg108: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg109: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg110: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg111: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg112: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg113: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg114: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg115: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg116: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg117: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg118: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg119: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg120: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg121: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg122: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg123: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg124: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg125: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg126: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg127: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg128: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg129: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg130: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg131: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg132: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg133: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg134: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg135: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg136: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg137: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg138: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg139: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg140: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg141: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg142: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg143: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg144: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg145: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg146: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg147: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg148: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg149: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg150: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg151: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg152: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg153: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg154: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg155: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg156: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg157: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg158: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg159: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg160: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg161: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg162: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg163: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg164: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg165: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg166: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg167: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg168: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg169: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg170: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg171: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg172: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg173: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg174: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg175: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg176: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg177: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg178: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg179: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg180: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg181: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg182: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg183: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg184: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg185: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg186: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg187: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg188: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg189: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg190: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg191: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg192: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg193: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg194: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg195: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg196: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg197: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg198: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg199: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg200: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg201: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg202: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg203: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg204: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg205: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg206: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg207: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg208: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg209: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg210: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg211: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg212: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg213: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg214: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg215: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg216: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg217: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg218: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg219: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg220: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg221: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg222: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg223: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg224: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg225: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg226: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg227: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg228: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg229: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg230: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg231: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg232: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg233: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg234: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg235: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg236: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg237: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg238: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg239: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg240: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg241: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg242: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg243: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg244: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg245: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg246: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg247: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg248: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg249: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg250: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg251: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg252: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg253: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg254: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg255: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg256: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg257: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg258: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg259: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg260: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg261: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg262: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg263: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg264: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg265: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg266: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg267: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg268: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg269: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg270: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg271: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg272: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg273: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg274: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg275: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg276: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg277: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg278: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg279: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg280: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg281: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg282: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg283: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg284: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg285: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg286: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg287: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg288: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg289: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg290: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg291: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg292: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg293: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg294: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg295: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg296: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg297: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg298: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg299: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg300: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg301: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg302: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg303: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg304: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg305: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg306: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg307: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg308: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg309: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg310: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg311: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg312: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg313: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg314: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg315: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg316: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg317: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg318: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg319: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg320: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg321: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg322: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg323: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg324: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg325: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg326: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg327: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg328: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg329: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg330: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg331: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg332: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg333: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg334: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg335: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg336: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg337: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg338: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg339: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg340: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg341: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg342: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg343: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg344: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg345: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg346: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg347: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg348: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg349: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg350: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg351: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg352: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg353: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg354: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg355: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg356: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg357: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg358: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg359: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg360: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg361: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg362: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg363: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg364: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg365: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg366: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg367: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg368: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg369: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg370: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg371: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg372: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg373: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_mlp_fc2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg374: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_mlp_fc2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg375: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_mlp_fc1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg376: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_mlp_fc1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg377: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_layer_norm2_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg378: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_layer_norm2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg379: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_out_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg380: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_out_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg381: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_v_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg382: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_v_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg383: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_layer_norm1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg384: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_layer_norm1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg385: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_pre_layrnorm_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg386: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_pre_layrnorm_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg387: tensor<1x257xi64> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "l__self___image_encoder_vision_model_embeddings_position_ids"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg388: tensor<257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_embeddings_position_embedding_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg389: tensor<1280x3x14x14xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_embeddings_patch_embedding_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg390: tensor<1x3x224x224xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg391: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_embeddings_class_embedding"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg392: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg393: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg394: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg395: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_0_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg396: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg397: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg398: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg399: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_1_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg400: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg401: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg402: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg403: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_2_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg404: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg405: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg406: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg407: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_3_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg408: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg409: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg410: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg411: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_4_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg412: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg413: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg414: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg415: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_5_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg416: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg417: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg418: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg419: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_6_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg420: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg421: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg422: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg423: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_7_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg424: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg425: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg426: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg427: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_8_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg428: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg429: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg430: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg431: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_9_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg432: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg433: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg434: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg435: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_10_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg436: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg437: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg438: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg439: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_11_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg440: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg441: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg442: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg443: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_12_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg444: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg445: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg446: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg447: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_13_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg448: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg449: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg450: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg451: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_14_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg452: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg453: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg454: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg455: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_15_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg456: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg457: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg458: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg459: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_16_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg460: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg461: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg462: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg463: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_17_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg464: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg465: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg466: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg467: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_18_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg468: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg469: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg470: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg471: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_19_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg472: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg473: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg474: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg475: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_20_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg476: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg477: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg478: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg479: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_21_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg480: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg481: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg482: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg483: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_22_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg484: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg485: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg486: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg487: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_23_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg488: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg489: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg490: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg491: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_24_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg492: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg493: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg494: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg495: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_25_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg496: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg497: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg498: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg499: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_26_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg500: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg501: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg502: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg503: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_27_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg504: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg505: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg506: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg507: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_28_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg508: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg509: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg510: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg511: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_29_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg512: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_k_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg513: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_k_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg514: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_q_proj_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg515: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___image_encoder_vision_model_encoder_layers_30_self_attn_q_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg516: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_0_attn_to_k_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg517: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_0_attn_to_q_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg518: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "getattr_l__self___resampler_layers_0_ff___1___net_2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg519: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "getattr_l__self___resampler_layers_0_ff___1___net_0_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg520: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_0_ff_0_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg521: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_0_ff_0_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg522: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_1_attn_to_out_0_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg523: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_1_attn_to_v_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg524: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_1_ln1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg525: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_1_ln1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg526: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_1_ln0_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg527: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_1_ln0_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg528: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_1_attn_to_k_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg529: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_1_attn_to_q_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg530: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "getattr_l__self___resampler_layers_1_ff___1___net_2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg531: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "getattr_l__self___resampler_layers_1_ff___1___net_0_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg532: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_1_ff_0_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg533: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_1_ff_0_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg534: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_2_attn_to_out_0_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg535: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_2_attn_to_v_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg536: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_2_ln1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg537: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_2_ln1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg538: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_2_ln0_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg539: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_2_ln0_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg540: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_2_attn_to_k_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg541: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_2_attn_to_q_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg542: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "getattr_l__self___resampler_layers_2_ff___1___net_2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg543: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "getattr_l__self___resampler_layers_2_ff___1___net_0_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg544: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_2_ff_0_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg545: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_2_ff_0_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg546: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_3_attn_to_out_0_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg547: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_3_attn_to_v_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg548: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_3_ln1_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg549: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_3_ln1_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg550: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_3_ln0_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg551: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_3_ln0_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg552: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_3_attn_to_k_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg553: tensor<1280x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_3_attn_to_q_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg554: tensor<1280x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "getattr_l__self___resampler_layers_3_ff___1___net_2_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg555: tensor<5120x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "getattr_l__self___resampler_layers_3_ff___1___net_0_proj_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg556: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_3_ff_0_bias"} loc("-1|unknown|unknown|-1|unknownxla__device_data"), %arg557: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___resampler_layers_3_ff_0_weight"} loc("-1|unknown|unknown|-1|unknownxla__device_data")) -> tensor<1x16x2048xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x16x1280xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<1x20x16x273xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0xFFF0000000000000> : tensor<1x20x16x273xf64> loc(#loc)
    %cst_2 = stablehlo.constant dense<0.353553385> : tensor<1x20x64x273xf32> loc(#loc)
    %cst_3 = stablehlo.constant dense<0.000000e+00> : tensor<1x16x257x257xf32> loc(#loc)
    %cst_4 = stablehlo.constant dense<0xFFF0000000000000> : tensor<1x16x257x257xf64> loc(#loc)
    %cst_5 = stablehlo.constant dense<0.334370166> : tensor<1x16x80x257xf32> loc(#loc)
    %cst_6 = stablehlo.constant dense<0.334370166> : tensor<1x16x257x80xf32> loc(#loc)
    %cst_7 = stablehlo.constant dense<0.353553385> : tensor<1x20x16x64xf32> loc(#loc)
    %cst_8 = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)
    %c = stablehlo.constant dense<true> : tensor<i1> loc(#loc)
    %c_9 = stablehlo.constant dense<false> : tensor<i1> loc(#loc)
    %cst_10 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.reshape %arg8 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1 = stablehlo.reshape %0 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2 = stablehlo.reshape %arg7 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3 = stablehlo.reshape %2 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %4 = stablehlo.composite "tenstorrent.layer_norm" %arg4, %1, %3 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_54} : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc3)
    %5 = stablehlo.reshape %4 : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc4)
    %6 = stablehlo.reshape %arg517 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %7 = stablehlo.reshape %6 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %8 = stablehlo.transpose %7, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc5)
    %9 = stablehlo.dot_general %5, %8, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc6)
    %10 = stablehlo.reshape %9 : (tensor<16x1280xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc7)
    %11 = stablehlo.transpose %10, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,16,64]{3,1,2,0}"} : (tensor<1x16x20x64xbf16>) -> tensor<1x20x16x64xbf16> loc(#loc8)
    %12 = stablehlo.convert %11 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,16,64]{3,1,2,0}"} : (tensor<1x20x16x64xbf16>) -> tensor<1x20x16x64xf32> loc(#loc9)
    %13 = stablehlo.multiply %12, %cst_7 : tensor<1x20x16x64xf32> loc(#loc10)
    %14 = stablehlo.reshape %arg391 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %15 = stablehlo.convolution(%arg390, %arg389) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [14, 14]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x3x224x224xbf16>, tensor<1280x3x14x14xbf16>) -> tensor<1x1280x16x16xbf16> loc(#loc11)
    %16 = stablehlo.reshape %15 : (tensor<1x1280x16x16xbf16>) -> tensor<1x1280x256xbf16> loc(#loc12)
    %17 = stablehlo.transpose %16, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "bf16[1,256,1280]{1,2,0}"} : (tensor<1x1280x256xbf16>) -> tensor<1x256x1280xbf16> loc(#loc13)
    %18 = stablehlo.concatenate %14, %17, dim = 1 : (tensor<1x1x1280xbf16>, tensor<1x256x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc14)
    %19 = stablehlo.reshape %arg388 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc2)
    %20 = stablehlo.reshape %19 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc2)
    %21 = stablehlo.reshape %arg387 : (tensor<1x257xi64>) -> tensor<1x1x257xi64> loc(#loc2)
    %22 = stablehlo.reshape %21 : (tensor<1x1x257xi64>) -> tensor<257xi64> loc(#loc15)
    %23 = stablehlo.convert %22 : (tensor<257xi64>) -> tensor<257xui32> loc(#loc16)
    %24 = "stablehlo.gather"(%20, %23) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 1280>}> : (tensor<257x1280xbf16>, tensor<257xui32>) -> tensor<257x1280xbf16> loc(#loc16)
    %25 = stablehlo.reshape %24 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc15)
    %26 = stablehlo.add %18, %25 : tensor<1x257x1280xbf16> loc(#loc17)
    %27 = stablehlo.reshape %arg386 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %28 = stablehlo.reshape %27 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %29 = stablehlo.reshape %arg385 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %30 = stablehlo.reshape %29 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %31 = stablehlo.composite "tenstorrent.layer_norm" %26, %28, %30 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_37} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc18)
    %32 = stablehlo.reshape %arg384 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %33 = stablehlo.reshape %32 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %34 = stablehlo.reshape %arg383 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %35 = stablehlo.reshape %34 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %36 = stablehlo.composite "tenstorrent.layer_norm" %31, %33, %35 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_34} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc19)
    %37 = stablehlo.reshape %36 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc20)
    %38 = stablehlo.reshape %arg395 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %39 = stablehlo.reshape %38 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %40 = stablehlo.transpose %39, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc21)
    %41 = stablehlo.dot_general %37, %40, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc22)
    %42 = stablehlo.reshape %41 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc20)
    %43 = stablehlo.reshape %arg394 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %44 = stablehlo.reshape %43 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %45 = stablehlo.broadcast_in_dim %44, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc23)
    %46 = stablehlo.add %42, %45 : tensor<1x257x1280xbf16> loc(#loc23)
    %47 = stablehlo.reshape %46 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc24)
    %48 = stablehlo.transpose %47, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc25)
    %49 = stablehlo.convert %48 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc26)
    %50 = stablehlo.multiply %49, %cst_6 : tensor<1x16x257x80xf32> loc(#loc27)
    %51 = stablehlo.reshape %arg393 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %52 = stablehlo.reshape %51 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %53 = stablehlo.transpose %52, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc28)
    %54 = stablehlo.dot_general %37, %53, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc29)
    %55 = stablehlo.reshape %54 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc30)
    %56 = stablehlo.reshape %arg392 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %57 = stablehlo.reshape %56 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %58 = stablehlo.broadcast_in_dim %57, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc31)
    %59 = stablehlo.add %55, %58 : tensor<1x257x1280xbf16> loc(#loc31)
    %60 = stablehlo.reshape %59 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc32)
    %61 = stablehlo.transpose %60, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc33)
    %62 = stablehlo.convert %61 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc34)
    %63 = stablehlo.transpose %62, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc35)
    %64 = stablehlo.multiply %63, %cst_5 : tensor<1x16x80x257xf32> loc(#loc36)
    %65 = stablehlo.dot_general %50, %64, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc37)
    %66 = stablehlo.convert %65 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc38)
    %67 = stablehlo.compare  EQ, %66, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc38)
    %68 = stablehlo.not %67 : tensor<1x16x257x257xi1> loc(#loc39)
    %69 = stablehlo.reduce(%68 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("616|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_1aten__any"), %arg559: tensor<i1> loc("616|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_1aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc41)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc42)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc40)
    %70 = stablehlo.reshape %69 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc40)
    %71 = stablehlo.not %70 : tensor<1x16x257x1xi1> loc(#loc43)
    %72 = stablehlo.reshape %71 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc44)
    %73 = stablehlo.broadcast_in_dim %72, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc44)
    %74 = stablehlo.reduce(%65 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc45)
    %75 = stablehlo.broadcast_in_dim %74, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc45)
    %76 = stablehlo.subtract %65, %75 : tensor<1x16x257x257xf32> loc(#loc45)
    %77 = stablehlo.exponential %76 : tensor<1x16x257x257xf32> loc(#loc45)
    %78 = stablehlo.reduce(%77 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc45)
    %79 = stablehlo.broadcast_in_dim %78, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc45)
    %80 = stablehlo.divide %77, %79 : tensor<1x16x257x257xf32> loc(#loc45)
    %81 = stablehlo.select %73, %cst_3, %80 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc46)
    %82 = stablehlo.reshape %arg382 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %83 = stablehlo.reshape %82 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %84 = stablehlo.transpose %83, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc47)
    %85 = stablehlo.dot_general %37, %84, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc48)
    %86 = stablehlo.reshape %85 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc49)
    %87 = stablehlo.reshape %arg381 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %88 = stablehlo.reshape %87 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %89 = stablehlo.broadcast_in_dim %88, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc50)
    %90 = stablehlo.add %86, %89 : tensor<1x257x1280xbf16> loc(#loc50)
    %91 = stablehlo.reshape %90 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc51)
    %92 = stablehlo.transpose %91, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc52)
    %93 = stablehlo.convert %92 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc53)
    %94 = stablehlo.dot_general %81, %93, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc54)
    %95 = stablehlo.convert %94 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc55)
    %96 = stablehlo.transpose %95, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc56)
    %97 = stablehlo.reshape %96 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc57)
    %98 = stablehlo.reshape %arg380 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %99 = stablehlo.reshape %98 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %100 = stablehlo.transpose %99, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc58)
    %101 = stablehlo.dot_general %97, %100, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc59)
    %102 = stablehlo.reshape %101 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc57)
    %103 = stablehlo.reshape %arg379 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %104 = stablehlo.reshape %103 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %105 = stablehlo.broadcast_in_dim %104, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc60)
    %106 = stablehlo.add %102, %105 : tensor<1x257x1280xbf16> loc(#loc60)
    %107 = stablehlo.add %31, %106 : tensor<1x257x1280xbf16> loc(#loc61)
    %108 = stablehlo.reshape %arg378 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %109 = stablehlo.reshape %108 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %110 = stablehlo.reshape %arg377 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %111 = stablehlo.reshape %110 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %112 = stablehlo.composite "tenstorrent.layer_norm" %107, %109, %111 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_35} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc62)
    %113 = stablehlo.reshape %112 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc63)
    %114 = stablehlo.reshape %arg376 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %115 = stablehlo.reshape %114 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %116 = stablehlo.transpose %115, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc64)
    %117 = stablehlo.dot_general %113, %116, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc65)
    %118 = stablehlo.reshape %117 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc63)
    %119 = stablehlo.reshape %arg375 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %120 = stablehlo.reshape %119 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
    %121 = stablehlo.broadcast_in_dim %120, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc66)
    %122 = stablehlo.add %118, %121 : tensor<1x257x5120xbf16> loc(#loc66)
    %123 = stablehlo.composite "tenstorrent.gelu" %122 {decomposition = @tenstorrent.gelu.impl_16} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc67)
    %124 = stablehlo.reshape %123 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc68)
    %125 = stablehlo.reshape %arg374 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %126 = stablehlo.reshape %125 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %127 = stablehlo.transpose %126, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc69)
    %128 = stablehlo.dot_general %124, %127, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc70)
    %129 = stablehlo.reshape %128 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc68)
    %130 = stablehlo.reshape %arg373 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %131 = stablehlo.reshape %130 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %132 = stablehlo.broadcast_in_dim %131, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc71)
    %133 = stablehlo.add %129, %132 : tensor<1x257x1280xbf16> loc(#loc71)
    %134 = stablehlo.add %107, %133 : tensor<1x257x1280xbf16> loc(#loc72)
    %135 = stablehlo.reshape %arg372 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %136 = stablehlo.reshape %135 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %137 = stablehlo.reshape %arg371 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %138 = stablehlo.reshape %137 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %139 = stablehlo.composite "tenstorrent.layer_norm" %134, %136, %138 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_39} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc73)
    %140 = stablehlo.reshape %139 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc74)
    %141 = stablehlo.reshape %arg399 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %142 = stablehlo.reshape %141 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %143 = stablehlo.transpose %142, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc75)
    %144 = stablehlo.dot_general %140, %143, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc76)
    %145 = stablehlo.reshape %144 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc74)
    %146 = stablehlo.reshape %arg398 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %147 = stablehlo.reshape %146 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %148 = stablehlo.broadcast_in_dim %147, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc77)
    %149 = stablehlo.add %145, %148 : tensor<1x257x1280xbf16> loc(#loc77)
    %150 = stablehlo.reshape %149 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc78)
    %151 = stablehlo.transpose %150, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc79)
    %152 = stablehlo.convert %151 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc80)
    %153 = stablehlo.multiply %152, %cst_6 : tensor<1x16x257x80xf32> loc(#loc81)
    %154 = stablehlo.reshape %arg397 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %155 = stablehlo.reshape %154 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %156 = stablehlo.transpose %155, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc82)
    %157 = stablehlo.dot_general %140, %156, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc83)
    %158 = stablehlo.reshape %157 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc84)
    %159 = stablehlo.reshape %arg396 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %160 = stablehlo.reshape %159 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %161 = stablehlo.broadcast_in_dim %160, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc85)
    %162 = stablehlo.add %158, %161 : tensor<1x257x1280xbf16> loc(#loc85)
    %163 = stablehlo.reshape %162 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc86)
    %164 = stablehlo.transpose %163, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc87)
    %165 = stablehlo.convert %164 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc88)
    %166 = stablehlo.transpose %165, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc89)
    %167 = stablehlo.multiply %166, %cst_5 : tensor<1x16x80x257xf32> loc(#loc90)
    %168 = stablehlo.dot_general %153, %167, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc91)
    %169 = stablehlo.convert %168 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc92)
    %170 = stablehlo.compare  EQ, %169, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc92)
    %171 = stablehlo.not %170 : tensor<1x16x257x257xi1> loc(#loc93)
    %172 = stablehlo.reduce(%171 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("690|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_2aten__any"), %arg559: tensor<i1> loc("690|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_2aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc95)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc96)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc94)
    %173 = stablehlo.reshape %172 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc94)
    %174 = stablehlo.not %173 : tensor<1x16x257x1xi1> loc(#loc97)
    %175 = stablehlo.reshape %174 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc98)
    %176 = stablehlo.broadcast_in_dim %175, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc98)
    %177 = stablehlo.reduce(%168 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc99)
    %178 = stablehlo.broadcast_in_dim %177, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc99)
    %179 = stablehlo.subtract %168, %178 : tensor<1x16x257x257xf32> loc(#loc99)
    %180 = stablehlo.exponential %179 : tensor<1x16x257x257xf32> loc(#loc99)
    %181 = stablehlo.reduce(%180 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc99)
    %182 = stablehlo.broadcast_in_dim %181, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc99)
    %183 = stablehlo.divide %180, %182 : tensor<1x16x257x257xf32> loc(#loc99)
    %184 = stablehlo.select %176, %cst_3, %183 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc100)
    %185 = stablehlo.reshape %arg370 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %186 = stablehlo.reshape %185 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %187 = stablehlo.transpose %186, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc101)
    %188 = stablehlo.dot_general %140, %187, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc102)
    %189 = stablehlo.reshape %188 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc103)
    %190 = stablehlo.reshape %arg369 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %191 = stablehlo.reshape %190 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %192 = stablehlo.broadcast_in_dim %191, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc104)
    %193 = stablehlo.add %189, %192 : tensor<1x257x1280xbf16> loc(#loc104)
    %194 = stablehlo.reshape %193 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc105)
    %195 = stablehlo.transpose %194, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc106)
    %196 = stablehlo.convert %195 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc107)
    %197 = stablehlo.dot_general %184, %196, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc108)
    %198 = stablehlo.convert %197 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc109)
    %199 = stablehlo.transpose %198, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc110)
    %200 = stablehlo.reshape %199 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc111)
    %201 = stablehlo.reshape %arg368 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %202 = stablehlo.reshape %201 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %203 = stablehlo.transpose %202, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc112)
    %204 = stablehlo.dot_general %200, %203, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc113)
    %205 = stablehlo.reshape %204 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc111)
    %206 = stablehlo.reshape %arg367 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %207 = stablehlo.reshape %206 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %208 = stablehlo.broadcast_in_dim %207, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc114)
    %209 = stablehlo.add %205, %208 : tensor<1x257x1280xbf16> loc(#loc114)
    %210 = stablehlo.add %134, %209 : tensor<1x257x1280xbf16> loc(#loc115)
    %211 = stablehlo.reshape %arg366 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %212 = stablehlo.reshape %211 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %213 = stablehlo.reshape %arg365 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %214 = stablehlo.reshape %213 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %215 = stablehlo.composite "tenstorrent.layer_norm" %210, %212, %214 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_36} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc116)
    %216 = stablehlo.reshape %215 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc117)
    %217 = stablehlo.reshape %arg364 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %218 = stablehlo.reshape %217 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %219 = stablehlo.transpose %218, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc118)
    %220 = stablehlo.dot_general %216, %219, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc119)
    %221 = stablehlo.reshape %220 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc117)
    %222 = stablehlo.reshape %arg363 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %223 = stablehlo.reshape %222 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
    %224 = stablehlo.broadcast_in_dim %223, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc120)
    %225 = stablehlo.add %221, %224 : tensor<1x257x5120xbf16> loc(#loc120)
    %226 = stablehlo.composite "tenstorrent.gelu" %225 {decomposition = @tenstorrent.gelu.impl_19} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc121)
    %227 = stablehlo.reshape %226 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc122)
    %228 = stablehlo.reshape %arg362 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %229 = stablehlo.reshape %228 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %230 = stablehlo.transpose %229, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc123)
    %231 = stablehlo.dot_general %227, %230, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc124)
    %232 = stablehlo.reshape %231 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc122)
    %233 = stablehlo.reshape %arg361 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %234 = stablehlo.reshape %233 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %235 = stablehlo.broadcast_in_dim %234, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc125)
    %236 = stablehlo.add %232, %235 : tensor<1x257x1280xbf16> loc(#loc125)
    %237 = stablehlo.add %210, %236 : tensor<1x257x1280xbf16> loc(#loc126)
    %238 = stablehlo.reshape %arg360 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %239 = stablehlo.reshape %238 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %240 = stablehlo.reshape %arg359 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %241 = stablehlo.reshape %240 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %242 = stablehlo.composite "tenstorrent.layer_norm" %237, %239, %241 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_28} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc127)
    %243 = stablehlo.reshape %242 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc128)
    %244 = stablehlo.reshape %arg403 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %245 = stablehlo.reshape %244 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %246 = stablehlo.transpose %245, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc129)
    %247 = stablehlo.dot_general %243, %246, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc130)
    %248 = stablehlo.reshape %247 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc128)
    %249 = stablehlo.reshape %arg402 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %250 = stablehlo.reshape %249 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %251 = stablehlo.broadcast_in_dim %250, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc131)
    %252 = stablehlo.add %248, %251 : tensor<1x257x1280xbf16> loc(#loc131)
    %253 = stablehlo.reshape %252 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc132)
    %254 = stablehlo.transpose %253, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc133)
    %255 = stablehlo.convert %254 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc134)
    %256 = stablehlo.multiply %255, %cst_6 : tensor<1x16x257x80xf32> loc(#loc135)
    %257 = stablehlo.reshape %arg401 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %258 = stablehlo.reshape %257 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %259 = stablehlo.transpose %258, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc136)
    %260 = stablehlo.dot_general %243, %259, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc137)
    %261 = stablehlo.reshape %260 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc138)
    %262 = stablehlo.reshape %arg400 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %263 = stablehlo.reshape %262 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %264 = stablehlo.broadcast_in_dim %263, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc139)
    %265 = stablehlo.add %261, %264 : tensor<1x257x1280xbf16> loc(#loc139)
    %266 = stablehlo.reshape %265 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc140)
    %267 = stablehlo.transpose %266, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc141)
    %268 = stablehlo.convert %267 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc142)
    %269 = stablehlo.transpose %268, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc143)
    %270 = stablehlo.multiply %269, %cst_5 : tensor<1x16x80x257xf32> loc(#loc144)
    %271 = stablehlo.dot_general %256, %270, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc145)
    %272 = stablehlo.convert %271 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc146)
    %273 = stablehlo.compare  EQ, %272, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc146)
    %274 = stablehlo.not %273 : tensor<1x16x257x257xi1> loc(#loc147)
    %275 = stablehlo.reduce(%274 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("764|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_3aten__any"), %arg559: tensor<i1> loc("764|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_3aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc149)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc150)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc148)
    %276 = stablehlo.reshape %275 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc148)
    %277 = stablehlo.not %276 : tensor<1x16x257x1xi1> loc(#loc151)
    %278 = stablehlo.reshape %277 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc152)
    %279 = stablehlo.broadcast_in_dim %278, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc152)
    %280 = stablehlo.reduce(%271 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc153)
    %281 = stablehlo.broadcast_in_dim %280, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc153)
    %282 = stablehlo.subtract %271, %281 : tensor<1x16x257x257xf32> loc(#loc153)
    %283 = stablehlo.exponential %282 : tensor<1x16x257x257xf32> loc(#loc153)
    %284 = stablehlo.reduce(%283 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc153)
    %285 = stablehlo.broadcast_in_dim %284, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc153)
    %286 = stablehlo.divide %283, %285 : tensor<1x16x257x257xf32> loc(#loc153)
    %287 = stablehlo.select %279, %cst_3, %286 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc154)
    %288 = stablehlo.reshape %arg358 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %289 = stablehlo.reshape %288 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %290 = stablehlo.transpose %289, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc155)
    %291 = stablehlo.dot_general %243, %290, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc156)
    %292 = stablehlo.reshape %291 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc157)
    %293 = stablehlo.reshape %arg357 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %294 = stablehlo.reshape %293 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %295 = stablehlo.broadcast_in_dim %294, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc158)
    %296 = stablehlo.add %292, %295 : tensor<1x257x1280xbf16> loc(#loc158)
    %297 = stablehlo.reshape %296 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc159)
    %298 = stablehlo.transpose %297, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc160)
    %299 = stablehlo.convert %298 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc161)
    %300 = stablehlo.dot_general %287, %299, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc162)
    %301 = stablehlo.convert %300 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc163)
    %302 = stablehlo.transpose %301, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc164)
    %303 = stablehlo.reshape %302 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc165)
    %304 = stablehlo.reshape %arg356 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %305 = stablehlo.reshape %304 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %306 = stablehlo.transpose %305, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc166)
    %307 = stablehlo.dot_general %303, %306, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc167)
    %308 = stablehlo.reshape %307 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc165)
    %309 = stablehlo.reshape %arg355 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %310 = stablehlo.reshape %309 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %311 = stablehlo.broadcast_in_dim %310, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc168)
    %312 = stablehlo.add %308, %311 : tensor<1x257x1280xbf16> loc(#loc168)
    %313 = stablehlo.add %237, %312 : tensor<1x257x1280xbf16> loc(#loc169)
    %314 = stablehlo.reshape %arg354 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %315 = stablehlo.reshape %314 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %316 = stablehlo.reshape %arg353 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %317 = stablehlo.reshape %316 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %318 = stablehlo.composite "tenstorrent.layer_norm" %313, %315, %317 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_32} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc170)
    %319 = stablehlo.reshape %318 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc171)
    %320 = stablehlo.reshape %arg352 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %321 = stablehlo.reshape %320 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %322 = stablehlo.transpose %321, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc172)
    %323 = stablehlo.dot_general %319, %322, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc173)
    %324 = stablehlo.reshape %323 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc171)
    %325 = stablehlo.reshape %arg351 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %326 = stablehlo.reshape %325 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
    %327 = stablehlo.broadcast_in_dim %326, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc174)
    %328 = stablehlo.add %324, %327 : tensor<1x257x5120xbf16> loc(#loc174)
    %329 = stablehlo.composite "tenstorrent.gelu" %328 {decomposition = @tenstorrent.gelu.impl_21} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc175)
    %330 = stablehlo.reshape %329 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc176)
    %331 = stablehlo.reshape %arg350 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %332 = stablehlo.reshape %331 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %333 = stablehlo.transpose %332, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc177)
    %334 = stablehlo.dot_general %330, %333, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc178)
    %335 = stablehlo.reshape %334 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc176)
    %336 = stablehlo.reshape %arg349 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %337 = stablehlo.reshape %336 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %338 = stablehlo.broadcast_in_dim %337, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc179)
    %339 = stablehlo.add %335, %338 : tensor<1x257x1280xbf16> loc(#loc179)
    %340 = stablehlo.add %313, %339 : tensor<1x257x1280xbf16> loc(#loc180)
    %341 = stablehlo.reshape %arg348 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %342 = stablehlo.reshape %341 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %343 = stablehlo.reshape %arg347 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %344 = stablehlo.reshape %343 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %345 = stablehlo.composite "tenstorrent.layer_norm" %340, %342, %344 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_43} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc181)
    %346 = stablehlo.reshape %345 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc182)
    %347 = stablehlo.reshape %arg407 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %348 = stablehlo.reshape %347 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %349 = stablehlo.transpose %348, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc183)
    %350 = stablehlo.dot_general %346, %349, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc184)
    %351 = stablehlo.reshape %350 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc182)
    %352 = stablehlo.reshape %arg406 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %353 = stablehlo.reshape %352 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %354 = stablehlo.broadcast_in_dim %353, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc185)
    %355 = stablehlo.add %351, %354 : tensor<1x257x1280xbf16> loc(#loc185)
    %356 = stablehlo.reshape %355 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc186)
    %357 = stablehlo.transpose %356, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc187)
    %358 = stablehlo.convert %357 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc188)
    %359 = stablehlo.multiply %358, %cst_6 : tensor<1x16x257x80xf32> loc(#loc189)
    %360 = stablehlo.reshape %arg405 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %361 = stablehlo.reshape %360 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %362 = stablehlo.transpose %361, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc190)
    %363 = stablehlo.dot_general %346, %362, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc191)
    %364 = stablehlo.reshape %363 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc192)
    %365 = stablehlo.reshape %arg404 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %366 = stablehlo.reshape %365 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %367 = stablehlo.broadcast_in_dim %366, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc193)
    %368 = stablehlo.add %364, %367 : tensor<1x257x1280xbf16> loc(#loc193)
    %369 = stablehlo.reshape %368 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc194)
    %370 = stablehlo.transpose %369, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc195)
    %371 = stablehlo.convert %370 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc196)
    %372 = stablehlo.transpose %371, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc197)
    %373 = stablehlo.multiply %372, %cst_5 : tensor<1x16x80x257xf32> loc(#loc198)
    %374 = stablehlo.dot_general %359, %373, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc199)
    %375 = stablehlo.convert %374 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc200)
    %376 = stablehlo.compare  EQ, %375, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc200)
    %377 = stablehlo.not %376 : tensor<1x16x257x257xi1> loc(#loc201)
    %378 = stablehlo.reduce(%377 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("838|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_4aten__any"), %arg559: tensor<i1> loc("838|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_4aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc203)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc204)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc202)
    %379 = stablehlo.reshape %378 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc202)
    %380 = stablehlo.not %379 : tensor<1x16x257x1xi1> loc(#loc205)
    %381 = stablehlo.reshape %380 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc206)
    %382 = stablehlo.broadcast_in_dim %381, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc206)
    %383 = stablehlo.reduce(%374 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc207)
    %384 = stablehlo.broadcast_in_dim %383, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc207)
    %385 = stablehlo.subtract %374, %384 : tensor<1x16x257x257xf32> loc(#loc207)
    %386 = stablehlo.exponential %385 : tensor<1x16x257x257xf32> loc(#loc207)
    %387 = stablehlo.reduce(%386 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc207)
    %388 = stablehlo.broadcast_in_dim %387, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc207)
    %389 = stablehlo.divide %386, %388 : tensor<1x16x257x257xf32> loc(#loc207)
    %390 = stablehlo.select %382, %cst_3, %389 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc208)
    %391 = stablehlo.reshape %arg346 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %392 = stablehlo.reshape %391 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %393 = stablehlo.transpose %392, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc209)
    %394 = stablehlo.dot_general %346, %393, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc210)
    %395 = stablehlo.reshape %394 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc211)
    %396 = stablehlo.reshape %arg345 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %397 = stablehlo.reshape %396 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %398 = stablehlo.broadcast_in_dim %397, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc212)
    %399 = stablehlo.add %395, %398 : tensor<1x257x1280xbf16> loc(#loc212)
    %400 = stablehlo.reshape %399 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc213)
    %401 = stablehlo.transpose %400, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc214)
    %402 = stablehlo.convert %401 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc215)
    %403 = stablehlo.dot_general %390, %402, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc216)
    %404 = stablehlo.convert %403 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc217)
    %405 = stablehlo.transpose %404, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc218)
    %406 = stablehlo.reshape %405 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc219)
    %407 = stablehlo.reshape %arg344 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %408 = stablehlo.reshape %407 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %409 = stablehlo.transpose %408, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc220)
    %410 = stablehlo.dot_general %406, %409, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc221)
    %411 = stablehlo.reshape %410 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc219)
    %412 = stablehlo.reshape %arg343 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %413 = stablehlo.reshape %412 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %414 = stablehlo.broadcast_in_dim %413, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc222)
    %415 = stablehlo.add %411, %414 : tensor<1x257x1280xbf16> loc(#loc222)
    %416 = stablehlo.add %340, %415 : tensor<1x257x1280xbf16> loc(#loc223)
    %417 = stablehlo.reshape %arg342 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %418 = stablehlo.reshape %417 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %419 = stablehlo.reshape %arg341 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %420 = stablehlo.reshape %419 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %421 = stablehlo.composite "tenstorrent.layer_norm" %416, %418, %420 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_52} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc224)
    %422 = stablehlo.reshape %421 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc225)
    %423 = stablehlo.reshape %arg340 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %424 = stablehlo.reshape %423 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %425 = stablehlo.transpose %424, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc226)
    %426 = stablehlo.dot_general %422, %425, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc227)
    %427 = stablehlo.reshape %426 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc225)
    %428 = stablehlo.reshape %arg339 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %429 = stablehlo.reshape %428 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
    %430 = stablehlo.broadcast_in_dim %429, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc228)
    %431 = stablehlo.add %427, %430 : tensor<1x257x5120xbf16> loc(#loc228)
    %432 = stablehlo.composite "tenstorrent.gelu" %431 {decomposition = @tenstorrent.gelu.impl_20} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc229)
    %433 = stablehlo.reshape %432 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc230)
    %434 = stablehlo.reshape %arg338 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %435 = stablehlo.reshape %434 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %436 = stablehlo.transpose %435, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc231)
    %437 = stablehlo.dot_general %433, %436, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc232)
    %438 = stablehlo.reshape %437 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc230)
    %439 = stablehlo.reshape %arg337 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %440 = stablehlo.reshape %439 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %441 = stablehlo.broadcast_in_dim %440, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc233)
    %442 = stablehlo.add %438, %441 : tensor<1x257x1280xbf16> loc(#loc233)
    %443 = stablehlo.add %416, %442 : tensor<1x257x1280xbf16> loc(#loc234)
    %444 = stablehlo.reshape %arg336 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %445 = stablehlo.reshape %444 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %446 = stablehlo.reshape %arg335 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %447 = stablehlo.reshape %446 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %448 = stablehlo.composite "tenstorrent.layer_norm" %443, %445, %447 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_68} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc235)
    %449 = stablehlo.reshape %448 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc236)
    %450 = stablehlo.reshape %arg411 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %451 = stablehlo.reshape %450 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %452 = stablehlo.transpose %451, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc237)
    %453 = stablehlo.dot_general %449, %452, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc238)
    %454 = stablehlo.reshape %453 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc236)
    %455 = stablehlo.reshape %arg410 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %456 = stablehlo.reshape %455 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %457 = stablehlo.broadcast_in_dim %456, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc239)
    %458 = stablehlo.add %454, %457 : tensor<1x257x1280xbf16> loc(#loc239)
    %459 = stablehlo.reshape %458 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc240)
    %460 = stablehlo.transpose %459, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc241)
    %461 = stablehlo.convert %460 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc242)
    %462 = stablehlo.multiply %461, %cst_6 : tensor<1x16x257x80xf32> loc(#loc243)
    %463 = stablehlo.reshape %arg409 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %464 = stablehlo.reshape %463 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %465 = stablehlo.transpose %464, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc244)
    %466 = stablehlo.dot_general %449, %465, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc245)
    %467 = stablehlo.reshape %466 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc246)
    %468 = stablehlo.reshape %arg408 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %469 = stablehlo.reshape %468 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %470 = stablehlo.broadcast_in_dim %469, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc247)
    %471 = stablehlo.add %467, %470 : tensor<1x257x1280xbf16> loc(#loc247)
    %472 = stablehlo.reshape %471 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc248)
    %473 = stablehlo.transpose %472, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc249)
    %474 = stablehlo.convert %473 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc250)
    %475 = stablehlo.transpose %474, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc251)
    %476 = stablehlo.multiply %475, %cst_5 : tensor<1x16x80x257xf32> loc(#loc252)
    %477 = stablehlo.dot_general %462, %476, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc253)
    %478 = stablehlo.convert %477 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc254)
    %479 = stablehlo.compare  EQ, %478, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc254)
    %480 = stablehlo.not %479 : tensor<1x16x257x257xi1> loc(#loc255)
    %481 = stablehlo.reduce(%480 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("912|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_5aten__any"), %arg559: tensor<i1> loc("912|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_5aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc257)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc258)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc256)
    %482 = stablehlo.reshape %481 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc256)
    %483 = stablehlo.not %482 : tensor<1x16x257x1xi1> loc(#loc259)
    %484 = stablehlo.reshape %483 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc260)
    %485 = stablehlo.broadcast_in_dim %484, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc260)
    %486 = stablehlo.reduce(%477 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc261)
    %487 = stablehlo.broadcast_in_dim %486, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc261)
    %488 = stablehlo.subtract %477, %487 : tensor<1x16x257x257xf32> loc(#loc261)
    %489 = stablehlo.exponential %488 : tensor<1x16x257x257xf32> loc(#loc261)
    %490 = stablehlo.reduce(%489 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc261)
    %491 = stablehlo.broadcast_in_dim %490, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc261)
    %492 = stablehlo.divide %489, %491 : tensor<1x16x257x257xf32> loc(#loc261)
    %493 = stablehlo.select %485, %cst_3, %492 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc262)
    %494 = stablehlo.reshape %arg334 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %495 = stablehlo.reshape %494 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %496 = stablehlo.transpose %495, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc263)
    %497 = stablehlo.dot_general %449, %496, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc264)
    %498 = stablehlo.reshape %497 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc265)
    %499 = stablehlo.reshape %arg333 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %500 = stablehlo.reshape %499 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %501 = stablehlo.broadcast_in_dim %500, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc266)
    %502 = stablehlo.add %498, %501 : tensor<1x257x1280xbf16> loc(#loc266)
    %503 = stablehlo.reshape %502 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc267)
    %504 = stablehlo.transpose %503, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc268)
    %505 = stablehlo.convert %504 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc269)
    %506 = stablehlo.dot_general %493, %505, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc270)
    %507 = stablehlo.convert %506 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc271)
    %508 = stablehlo.transpose %507, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc272)
    %509 = stablehlo.reshape %508 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc273)
    %510 = stablehlo.reshape %arg332 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %511 = stablehlo.reshape %510 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %512 = stablehlo.transpose %511, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc274)
    %513 = stablehlo.dot_general %509, %512, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc275)
    %514 = stablehlo.reshape %513 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc273)
    %515 = stablehlo.reshape %arg331 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %516 = stablehlo.reshape %515 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %517 = stablehlo.broadcast_in_dim %516, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc276)
    %518 = stablehlo.add %514, %517 : tensor<1x257x1280xbf16> loc(#loc276)
    %519 = stablehlo.add %443, %518 : tensor<1x257x1280xbf16> loc(#loc277)
    %520 = stablehlo.reshape %arg330 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %521 = stablehlo.reshape %520 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %522 = stablehlo.reshape %arg329 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %523 = stablehlo.reshape %522 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %524 = stablehlo.composite "tenstorrent.layer_norm" %519, %521, %523 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_38} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc278)
    %525 = stablehlo.reshape %524 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc279)
    %526 = stablehlo.reshape %arg328 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %527 = stablehlo.reshape %526 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %528 = stablehlo.transpose %527, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc280)
    %529 = stablehlo.dot_general %525, %528, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc281)
    %530 = stablehlo.reshape %529 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc279)
    %531 = stablehlo.reshape %arg327 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %532 = stablehlo.reshape %531 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
    %533 = stablehlo.broadcast_in_dim %532, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc282)
    %534 = stablehlo.add %530, %533 : tensor<1x257x5120xbf16> loc(#loc282)
    %535 = stablehlo.composite "tenstorrent.gelu" %534 {decomposition = @tenstorrent.gelu.impl_17} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc283)
    %536 = stablehlo.reshape %535 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc284)
    %537 = stablehlo.reshape %arg326 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %538 = stablehlo.reshape %537 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %539 = stablehlo.transpose %538, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc285)
    %540 = stablehlo.dot_general %536, %539, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc286)
    %541 = stablehlo.reshape %540 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc284)
    %542 = stablehlo.reshape %arg325 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %543 = stablehlo.reshape %542 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %544 = stablehlo.broadcast_in_dim %543, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc287)
    %545 = stablehlo.add %541, %544 : tensor<1x257x1280xbf16> loc(#loc287)
    %546 = stablehlo.add %519, %545 : tensor<1x257x1280xbf16> loc(#loc288)
    %547 = stablehlo.reshape %arg324 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %548 = stablehlo.reshape %547 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %549 = stablehlo.reshape %arg323 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %550 = stablehlo.reshape %549 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %551 = stablehlo.composite "tenstorrent.layer_norm" %546, %548, %550 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_42} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc289)
    %552 = stablehlo.reshape %551 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc290)
    %553 = stablehlo.reshape %arg415 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %554 = stablehlo.reshape %553 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %555 = stablehlo.transpose %554, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc291)
    %556 = stablehlo.dot_general %552, %555, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc292)
    %557 = stablehlo.reshape %556 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc290)
    %558 = stablehlo.reshape %arg414 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %559 = stablehlo.reshape %558 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %560 = stablehlo.broadcast_in_dim %559, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc293)
    %561 = stablehlo.add %557, %560 : tensor<1x257x1280xbf16> loc(#loc293)
    %562 = stablehlo.reshape %561 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc294)
    %563 = stablehlo.transpose %562, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc295)
    %564 = stablehlo.convert %563 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc296)
    %565 = stablehlo.multiply %564, %cst_6 : tensor<1x16x257x80xf32> loc(#loc297)
    %566 = stablehlo.reshape %arg413 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %567 = stablehlo.reshape %566 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %568 = stablehlo.transpose %567, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc298)
    %569 = stablehlo.dot_general %552, %568, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc299)
    %570 = stablehlo.reshape %569 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc300)
    %571 = stablehlo.reshape %arg412 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %572 = stablehlo.reshape %571 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %573 = stablehlo.broadcast_in_dim %572, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc301)
    %574 = stablehlo.add %570, %573 : tensor<1x257x1280xbf16> loc(#loc301)
    %575 = stablehlo.reshape %574 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc302)
    %576 = stablehlo.transpose %575, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc303)
    %577 = stablehlo.convert %576 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc304)
    %578 = stablehlo.transpose %577, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc305)
    %579 = stablehlo.multiply %578, %cst_5 : tensor<1x16x80x257xf32> loc(#loc306)
    %580 = stablehlo.dot_general %565, %579, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc307)
    %581 = stablehlo.convert %580 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc308)
    %582 = stablehlo.compare  EQ, %581, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc308)
    %583 = stablehlo.not %582 : tensor<1x16x257x257xi1> loc(#loc309)
    %584 = stablehlo.reduce(%583 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("986|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_6aten__any"), %arg559: tensor<i1> loc("986|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_6aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc311)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc312)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc310)
    %585 = stablehlo.reshape %584 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc310)
    %586 = stablehlo.not %585 : tensor<1x16x257x1xi1> loc(#loc313)
    %587 = stablehlo.reshape %586 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc314)
    %588 = stablehlo.broadcast_in_dim %587, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc314)
    %589 = stablehlo.reduce(%580 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc315)
    %590 = stablehlo.broadcast_in_dim %589, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc315)
    %591 = stablehlo.subtract %580, %590 : tensor<1x16x257x257xf32> loc(#loc315)
    %592 = stablehlo.exponential %591 : tensor<1x16x257x257xf32> loc(#loc315)
    %593 = stablehlo.reduce(%592 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc315)
    %594 = stablehlo.broadcast_in_dim %593, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc315)
    %595 = stablehlo.divide %592, %594 : tensor<1x16x257x257xf32> loc(#loc315)
    %596 = stablehlo.select %588, %cst_3, %595 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc316)
    %597 = stablehlo.reshape %arg322 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %598 = stablehlo.reshape %597 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %599 = stablehlo.transpose %598, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc317)
    %600 = stablehlo.dot_general %552, %599, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc318)
    %601 = stablehlo.reshape %600 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc319)
    %602 = stablehlo.reshape %arg321 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %603 = stablehlo.reshape %602 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %604 = stablehlo.broadcast_in_dim %603, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc320)
    %605 = stablehlo.add %601, %604 : tensor<1x257x1280xbf16> loc(#loc320)
    %606 = stablehlo.reshape %605 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc321)
    %607 = stablehlo.transpose %606, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc322)
    %608 = stablehlo.convert %607 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc323)
    %609 = stablehlo.dot_general %596, %608, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc324)
    %610 = stablehlo.convert %609 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc325)
    %611 = stablehlo.transpose %610, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc326)
    %612 = stablehlo.reshape %611 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc327)
    %613 = stablehlo.reshape %arg320 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %614 = stablehlo.reshape %613 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %615 = stablehlo.transpose %614, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc328)
    %616 = stablehlo.dot_general %612, %615, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc329)
    %617 = stablehlo.reshape %616 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc327)
    %618 = stablehlo.reshape %arg319 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %619 = stablehlo.reshape %618 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %620 = stablehlo.broadcast_in_dim %619, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc330)
    %621 = stablehlo.add %617, %620 : tensor<1x257x1280xbf16> loc(#loc330)
    %622 = stablehlo.add %546, %621 : tensor<1x257x1280xbf16> loc(#loc331)
    %623 = stablehlo.reshape %arg318 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %624 = stablehlo.reshape %623 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %625 = stablehlo.reshape %arg317 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %626 = stablehlo.reshape %625 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %627 = stablehlo.composite "tenstorrent.layer_norm" %622, %624, %626 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_41} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc332)
    %628 = stablehlo.reshape %627 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc333)
    %629 = stablehlo.reshape %arg316 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %630 = stablehlo.reshape %629 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %631 = stablehlo.transpose %630, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc334)
    %632 = stablehlo.dot_general %628, %631, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc335)
    %633 = stablehlo.reshape %632 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc333)
    %634 = stablehlo.reshape %arg315 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %635 = stablehlo.reshape %634 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
    %636 = stablehlo.broadcast_in_dim %635, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc336)
    %637 = stablehlo.add %633, %636 : tensor<1x257x5120xbf16> loc(#loc336)
    %638 = stablehlo.composite "tenstorrent.gelu" %637 {decomposition = @tenstorrent.gelu.impl_26} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc337)
    %639 = stablehlo.reshape %638 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc338)
    %640 = stablehlo.reshape %arg314 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %641 = stablehlo.reshape %640 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %642 = stablehlo.transpose %641, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc339)
    %643 = stablehlo.dot_general %639, %642, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc340)
    %644 = stablehlo.reshape %643 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc338)
    %645 = stablehlo.reshape %arg313 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %646 = stablehlo.reshape %645 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %647 = stablehlo.broadcast_in_dim %646, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc341)
    %648 = stablehlo.add %644, %647 : tensor<1x257x1280xbf16> loc(#loc341)
    %649 = stablehlo.add %622, %648 : tensor<1x257x1280xbf16> loc(#loc342)
    %650 = stablehlo.reshape %arg312 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %651 = stablehlo.reshape %650 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %652 = stablehlo.reshape %arg311 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %653 = stablehlo.reshape %652 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %654 = stablehlo.composite "tenstorrent.layer_norm" %649, %651, %653 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_64} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc343)
    %655 = stablehlo.reshape %654 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc344)
    %656 = stablehlo.reshape %arg419 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %657 = stablehlo.reshape %656 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %658 = stablehlo.transpose %657, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc345)
    %659 = stablehlo.dot_general %655, %658, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc346)
    %660 = stablehlo.reshape %659 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc344)
    %661 = stablehlo.reshape %arg418 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %662 = stablehlo.reshape %661 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %663 = stablehlo.broadcast_in_dim %662, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc347)
    %664 = stablehlo.add %660, %663 : tensor<1x257x1280xbf16> loc(#loc347)
    %665 = stablehlo.reshape %664 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc348)
    %666 = stablehlo.transpose %665, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc349)
    %667 = stablehlo.convert %666 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc350)
    %668 = stablehlo.multiply %667, %cst_6 : tensor<1x16x257x80xf32> loc(#loc351)
    %669 = stablehlo.reshape %arg417 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %670 = stablehlo.reshape %669 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %671 = stablehlo.transpose %670, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc352)
    %672 = stablehlo.dot_general %655, %671, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc353)
    %673 = stablehlo.reshape %672 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc354)
    %674 = stablehlo.reshape %arg416 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %675 = stablehlo.reshape %674 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %676 = stablehlo.broadcast_in_dim %675, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc355)
    %677 = stablehlo.add %673, %676 : tensor<1x257x1280xbf16> loc(#loc355)
    %678 = stablehlo.reshape %677 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc356)
    %679 = stablehlo.transpose %678, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc357)
    %680 = stablehlo.convert %679 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc358)
    %681 = stablehlo.transpose %680, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc359)
    %682 = stablehlo.multiply %681, %cst_5 : tensor<1x16x80x257xf32> loc(#loc360)
    %683 = stablehlo.dot_general %668, %682, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc361)
    %684 = stablehlo.convert %683 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc362)
    %685 = stablehlo.compare  EQ, %684, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc362)
    %686 = stablehlo.not %685 : tensor<1x16x257x257xi1> loc(#loc363)
    %687 = stablehlo.reduce(%686 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("1060|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_7aten__any"), %arg559: tensor<i1> loc("1060|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_7aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc365)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc366)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc364)
    %688 = stablehlo.reshape %687 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc364)
    %689 = stablehlo.not %688 : tensor<1x16x257x1xi1> loc(#loc367)
    %690 = stablehlo.reshape %689 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc368)
    %691 = stablehlo.broadcast_in_dim %690, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc368)
    %692 = stablehlo.reduce(%683 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc369)
    %693 = stablehlo.broadcast_in_dim %692, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc369)
    %694 = stablehlo.subtract %683, %693 : tensor<1x16x257x257xf32> loc(#loc369)
    %695 = stablehlo.exponential %694 : tensor<1x16x257x257xf32> loc(#loc369)
    %696 = stablehlo.reduce(%695 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc369)
    %697 = stablehlo.broadcast_in_dim %696, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc369)
    %698 = stablehlo.divide %695, %697 : tensor<1x16x257x257xf32> loc(#loc369)
    %699 = stablehlo.select %691, %cst_3, %698 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc370)
    %700 = stablehlo.reshape %arg310 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %701 = stablehlo.reshape %700 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %702 = stablehlo.transpose %701, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc371)
    %703 = stablehlo.dot_general %655, %702, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc372)
    %704 = stablehlo.reshape %703 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc373)
    %705 = stablehlo.reshape %arg309 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %706 = stablehlo.reshape %705 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %707 = stablehlo.broadcast_in_dim %706, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc374)
    %708 = stablehlo.add %704, %707 : tensor<1x257x1280xbf16> loc(#loc374)
    %709 = stablehlo.reshape %708 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc375)
    %710 = stablehlo.transpose %709, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc376)
    %711 = stablehlo.convert %710 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc377)
    %712 = stablehlo.dot_general %699, %711, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc378)
    %713 = stablehlo.convert %712 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc379)
    %714 = stablehlo.transpose %713, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc380)
    %715 = stablehlo.reshape %714 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc381)
    %716 = stablehlo.reshape %arg308 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %717 = stablehlo.reshape %716 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %718 = stablehlo.transpose %717, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc382)
    %719 = stablehlo.dot_general %715, %718, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc383)
    %720 = stablehlo.reshape %719 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc381)
    %721 = stablehlo.reshape %arg307 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %722 = stablehlo.reshape %721 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %723 = stablehlo.broadcast_in_dim %722, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc384)
    %724 = stablehlo.add %720, %723 : tensor<1x257x1280xbf16> loc(#loc384)
    %725 = stablehlo.add %649, %724 : tensor<1x257x1280xbf16> loc(#loc385)
    %726 = stablehlo.reshape %arg306 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %727 = stablehlo.reshape %726 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %728 = stablehlo.reshape %arg305 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %729 = stablehlo.reshape %728 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %730 = stablehlo.composite "tenstorrent.layer_norm" %725, %727, %729 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_25} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc386)
    %731 = stablehlo.reshape %730 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc387)
    %732 = stablehlo.reshape %arg304 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %733 = stablehlo.reshape %732 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %734 = stablehlo.transpose %733, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc388)
    %735 = stablehlo.dot_general %731, %734, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc389)
    %736 = stablehlo.reshape %735 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc387)
    %737 = stablehlo.reshape %arg303 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %738 = stablehlo.reshape %737 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
    %739 = stablehlo.broadcast_in_dim %738, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc390)
    %740 = stablehlo.add %736, %739 : tensor<1x257x5120xbf16> loc(#loc390)
    %741 = stablehlo.composite "tenstorrent.gelu" %740 {decomposition = @tenstorrent.gelu.impl_15} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc391)
    %742 = stablehlo.reshape %741 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc392)
    %743 = stablehlo.reshape %arg302 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %744 = stablehlo.reshape %743 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %745 = stablehlo.transpose %744, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc393)
    %746 = stablehlo.dot_general %742, %745, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc394)
    %747 = stablehlo.reshape %746 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc392)
    %748 = stablehlo.reshape %arg301 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %749 = stablehlo.reshape %748 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %750 = stablehlo.broadcast_in_dim %749, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc395)
    %751 = stablehlo.add %747, %750 : tensor<1x257x1280xbf16> loc(#loc395)
    %752 = stablehlo.add %725, %751 : tensor<1x257x1280xbf16> loc(#loc396)
    %753 = stablehlo.reshape %arg300 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %754 = stablehlo.reshape %753 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %755 = stablehlo.reshape %arg299 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %756 = stablehlo.reshape %755 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %757 = stablehlo.composite "tenstorrent.layer_norm" %752, %754, %756 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_65} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc397)
    %758 = stablehlo.reshape %757 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc398)
    %759 = stablehlo.reshape %arg423 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %760 = stablehlo.reshape %759 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %761 = stablehlo.transpose %760, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc399)
    %762 = stablehlo.dot_general %758, %761, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc400)
    %763 = stablehlo.reshape %762 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc398)
    %764 = stablehlo.reshape %arg422 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %765 = stablehlo.reshape %764 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %766 = stablehlo.broadcast_in_dim %765, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc401)
    %767 = stablehlo.add %763, %766 : tensor<1x257x1280xbf16> loc(#loc401)
    %768 = stablehlo.reshape %767 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc402)
    %769 = stablehlo.transpose %768, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc403)
    %770 = stablehlo.convert %769 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc404)
    %771 = stablehlo.multiply %770, %cst_6 : tensor<1x16x257x80xf32> loc(#loc405)
    %772 = stablehlo.reshape %arg421 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %773 = stablehlo.reshape %772 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %774 = stablehlo.transpose %773, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc406)
    %775 = stablehlo.dot_general %758, %774, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc407)
    %776 = stablehlo.reshape %775 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc408)
    %777 = stablehlo.reshape %arg420 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %778 = stablehlo.reshape %777 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %779 = stablehlo.broadcast_in_dim %778, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc409)
    %780 = stablehlo.add %776, %779 : tensor<1x257x1280xbf16> loc(#loc409)
    %781 = stablehlo.reshape %780 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc410)
    %782 = stablehlo.transpose %781, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc411)
    %783 = stablehlo.convert %782 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc412)
    %784 = stablehlo.transpose %783, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc413)
    %785 = stablehlo.multiply %784, %cst_5 : tensor<1x16x80x257xf32> loc(#loc414)
    %786 = stablehlo.dot_general %771, %785, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc415)
    %787 = stablehlo.convert %786 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc416)
    %788 = stablehlo.compare  EQ, %787, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc416)
    %789 = stablehlo.not %788 : tensor<1x16x257x257xi1> loc(#loc417)
    %790 = stablehlo.reduce(%789 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("1134|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_8aten__any"), %arg559: tensor<i1> loc("1134|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_8aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc419)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc420)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc418)
    %791 = stablehlo.reshape %790 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc418)
    %792 = stablehlo.not %791 : tensor<1x16x257x1xi1> loc(#loc421)
    %793 = stablehlo.reshape %792 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc422)
    %794 = stablehlo.broadcast_in_dim %793, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc422)
    %795 = stablehlo.reduce(%786 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc423)
    %796 = stablehlo.broadcast_in_dim %795, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc423)
    %797 = stablehlo.subtract %786, %796 : tensor<1x16x257x257xf32> loc(#loc423)
    %798 = stablehlo.exponential %797 : tensor<1x16x257x257xf32> loc(#loc423)
    %799 = stablehlo.reduce(%798 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc423)
    %800 = stablehlo.broadcast_in_dim %799, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc423)
    %801 = stablehlo.divide %798, %800 : tensor<1x16x257x257xf32> loc(#loc423)
    %802 = stablehlo.select %794, %cst_3, %801 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc424)
    %803 = stablehlo.reshape %arg298 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %804 = stablehlo.reshape %803 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %805 = stablehlo.transpose %804, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc425)
    %806 = stablehlo.dot_general %758, %805, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc426)
    %807 = stablehlo.reshape %806 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc427)
    %808 = stablehlo.reshape %arg297 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %809 = stablehlo.reshape %808 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %810 = stablehlo.broadcast_in_dim %809, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc428)
    %811 = stablehlo.add %807, %810 : tensor<1x257x1280xbf16> loc(#loc428)
    %812 = stablehlo.reshape %811 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc429)
    %813 = stablehlo.transpose %812, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc430)
    %814 = stablehlo.convert %813 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc431)
    %815 = stablehlo.dot_general %802, %814, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc432)
    %816 = stablehlo.convert %815 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc433)
    %817 = stablehlo.transpose %816, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc434)
    %818 = stablehlo.reshape %817 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc435)
    %819 = stablehlo.reshape %arg296 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %820 = stablehlo.reshape %819 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %821 = stablehlo.transpose %820, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc436)
    %822 = stablehlo.dot_general %818, %821, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc437)
    %823 = stablehlo.reshape %822 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc435)
    %824 = stablehlo.reshape %arg295 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %825 = stablehlo.reshape %824 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %826 = stablehlo.broadcast_in_dim %825, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc438)
    %827 = stablehlo.add %823, %826 : tensor<1x257x1280xbf16> loc(#loc438)
    %828 = stablehlo.add %752, %827 : tensor<1x257x1280xbf16> loc(#loc439)
    %829 = stablehlo.reshape %arg294 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %830 = stablehlo.reshape %829 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %831 = stablehlo.reshape %arg293 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %832 = stablehlo.reshape %831 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %833 = stablehlo.composite "tenstorrent.layer_norm" %828, %830, %832 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_29} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc440)
    %834 = stablehlo.reshape %833 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc441)
    %835 = stablehlo.reshape %arg292 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %836 = stablehlo.reshape %835 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %837 = stablehlo.transpose %836, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc442)
    %838 = stablehlo.dot_general %834, %837, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc443)
    %839 = stablehlo.reshape %838 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc441)
    %840 = stablehlo.reshape %arg291 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %841 = stablehlo.reshape %840 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
    %842 = stablehlo.broadcast_in_dim %841, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc444)
    %843 = stablehlo.add %839, %842 : tensor<1x257x5120xbf16> loc(#loc444)
    %844 = stablehlo.composite "tenstorrent.gelu" %843 {decomposition = @tenstorrent.gelu.impl_13} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc445)
    %845 = stablehlo.reshape %844 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc446)
    %846 = stablehlo.reshape %arg290 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %847 = stablehlo.reshape %846 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %848 = stablehlo.transpose %847, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc447)
    %849 = stablehlo.dot_general %845, %848, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc448)
    %850 = stablehlo.reshape %849 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc446)
    %851 = stablehlo.reshape %arg289 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %852 = stablehlo.reshape %851 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %853 = stablehlo.broadcast_in_dim %852, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc449)
    %854 = stablehlo.add %850, %853 : tensor<1x257x1280xbf16> loc(#loc449)
    %855 = stablehlo.add %828, %854 : tensor<1x257x1280xbf16> loc(#loc450)
    %856 = stablehlo.reshape %arg288 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %857 = stablehlo.reshape %856 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %858 = stablehlo.reshape %arg287 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %859 = stablehlo.reshape %858 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %860 = stablehlo.composite "tenstorrent.layer_norm" %855, %857, %859 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_22} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc451)
    %861 = stablehlo.reshape %860 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc452)
    %862 = stablehlo.reshape %arg427 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %863 = stablehlo.reshape %862 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %864 = stablehlo.transpose %863, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc453)
    %865 = stablehlo.dot_general %861, %864, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc454)
    %866 = stablehlo.reshape %865 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc452)
    %867 = stablehlo.reshape %arg426 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %868 = stablehlo.reshape %867 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %869 = stablehlo.broadcast_in_dim %868, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc455)
    %870 = stablehlo.add %866, %869 : tensor<1x257x1280xbf16> loc(#loc455)
    %871 = stablehlo.reshape %870 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc456)
    %872 = stablehlo.transpose %871, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc457)
    %873 = stablehlo.convert %872 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc458)
    %874 = stablehlo.multiply %873, %cst_6 : tensor<1x16x257x80xf32> loc(#loc459)
    %875 = stablehlo.reshape %arg425 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %876 = stablehlo.reshape %875 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %877 = stablehlo.transpose %876, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc460)
    %878 = stablehlo.dot_general %861, %877, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc461)
    %879 = stablehlo.reshape %878 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc462)
    %880 = stablehlo.reshape %arg424 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %881 = stablehlo.reshape %880 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %882 = stablehlo.broadcast_in_dim %881, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc463)
    %883 = stablehlo.add %879, %882 : tensor<1x257x1280xbf16> loc(#loc463)
    %884 = stablehlo.reshape %883 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc464)
    %885 = stablehlo.transpose %884, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc465)
    %886 = stablehlo.convert %885 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc466)
    %887 = stablehlo.transpose %886, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc467)
    %888 = stablehlo.multiply %887, %cst_5 : tensor<1x16x80x257xf32> loc(#loc468)
    %889 = stablehlo.dot_general %874, %888, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc469)
    %890 = stablehlo.convert %889 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc470)
    %891 = stablehlo.compare  EQ, %890, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc470)
    %892 = stablehlo.not %891 : tensor<1x16x257x257xi1> loc(#loc471)
    %893 = stablehlo.reduce(%892 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("1208|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_9aten__any"), %arg559: tensor<i1> loc("1208|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_9aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc473)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc474)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc472)
    %894 = stablehlo.reshape %893 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc472)
    %895 = stablehlo.not %894 : tensor<1x16x257x1xi1> loc(#loc475)
    %896 = stablehlo.reshape %895 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc476)
    %897 = stablehlo.broadcast_in_dim %896, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc476)
    %898 = stablehlo.reduce(%889 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc477)
    %899 = stablehlo.broadcast_in_dim %898, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc477)
    %900 = stablehlo.subtract %889, %899 : tensor<1x16x257x257xf32> loc(#loc477)
    %901 = stablehlo.exponential %900 : tensor<1x16x257x257xf32> loc(#loc477)
    %902 = stablehlo.reduce(%901 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc477)
    %903 = stablehlo.broadcast_in_dim %902, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc477)
    %904 = stablehlo.divide %901, %903 : tensor<1x16x257x257xf32> loc(#loc477)
    %905 = stablehlo.select %897, %cst_3, %904 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc478)
    %906 = stablehlo.reshape %arg286 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %907 = stablehlo.reshape %906 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %908 = stablehlo.transpose %907, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc479)
    %909 = stablehlo.dot_general %861, %908, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc480)
    %910 = stablehlo.reshape %909 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc481)
    %911 = stablehlo.reshape %arg285 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %912 = stablehlo.reshape %911 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %913 = stablehlo.broadcast_in_dim %912, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc482)
    %914 = stablehlo.add %910, %913 : tensor<1x257x1280xbf16> loc(#loc482)
    %915 = stablehlo.reshape %914 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc483)
    %916 = stablehlo.transpose %915, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc484)
    %917 = stablehlo.convert %916 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc485)
    %918 = stablehlo.dot_general %905, %917, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc486)
    %919 = stablehlo.convert %918 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc487)
    %920 = stablehlo.transpose %919, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc488)
    %921 = stablehlo.reshape %920 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc489)
    %922 = stablehlo.reshape %arg284 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %923 = stablehlo.reshape %922 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %924 = stablehlo.transpose %923, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc490)
    %925 = stablehlo.dot_general %921, %924, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc491)
    %926 = stablehlo.reshape %925 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc489)
    %927 = stablehlo.reshape %arg283 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %928 = stablehlo.reshape %927 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %929 = stablehlo.broadcast_in_dim %928, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc492)
    %930 = stablehlo.add %926, %929 : tensor<1x257x1280xbf16> loc(#loc492)
    %931 = stablehlo.add %855, %930 : tensor<1x257x1280xbf16> loc(#loc493)
    %932 = stablehlo.reshape %arg282 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %933 = stablehlo.reshape %932 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %934 = stablehlo.reshape %arg281 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %935 = stablehlo.reshape %934 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %936 = stablehlo.composite "tenstorrent.layer_norm" %931, %933, %935 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_27} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc494)
    %937 = stablehlo.reshape %936 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc495)
    %938 = stablehlo.reshape %arg280 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %939 = stablehlo.reshape %938 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %940 = stablehlo.transpose %939, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc496)
    %941 = stablehlo.dot_general %937, %940, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc497)
    %942 = stablehlo.reshape %941 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc495)
    %943 = stablehlo.reshape %arg279 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %944 = stablehlo.reshape %943 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
    %945 = stablehlo.broadcast_in_dim %944, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc498)
    %946 = stablehlo.add %942, %945 : tensor<1x257x5120xbf16> loc(#loc498)
    %947 = stablehlo.composite "tenstorrent.gelu" %946 {decomposition = @tenstorrent.gelu.impl_11} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc499)
    %948 = stablehlo.reshape %947 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc500)
    %949 = stablehlo.reshape %arg278 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %950 = stablehlo.reshape %949 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %951 = stablehlo.transpose %950, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc501)
    %952 = stablehlo.dot_general %948, %951, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc502)
    %953 = stablehlo.reshape %952 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc500)
    %954 = stablehlo.reshape %arg277 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %955 = stablehlo.reshape %954 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %956 = stablehlo.broadcast_in_dim %955, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc503)
    %957 = stablehlo.add %953, %956 : tensor<1x257x1280xbf16> loc(#loc503)
    %958 = stablehlo.add %931, %957 : tensor<1x257x1280xbf16> loc(#loc504)
    %959 = stablehlo.reshape %arg276 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %960 = stablehlo.reshape %959 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %961 = stablehlo.reshape %arg275 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %962 = stablehlo.reshape %961 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %963 = stablehlo.composite "tenstorrent.layer_norm" %958, %960, %962 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_59} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc505)
    %964 = stablehlo.reshape %963 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc506)
    %965 = stablehlo.reshape %arg431 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %966 = stablehlo.reshape %965 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %967 = stablehlo.transpose %966, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc507)
    %968 = stablehlo.dot_general %964, %967, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc508)
    %969 = stablehlo.reshape %968 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc506)
    %970 = stablehlo.reshape %arg430 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %971 = stablehlo.reshape %970 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %972 = stablehlo.broadcast_in_dim %971, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc509)
    %973 = stablehlo.add %969, %972 : tensor<1x257x1280xbf16> loc(#loc509)
    %974 = stablehlo.reshape %973 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc510)
    %975 = stablehlo.transpose %974, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc511)
    %976 = stablehlo.convert %975 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc512)
    %977 = stablehlo.multiply %976, %cst_6 : tensor<1x16x257x80xf32> loc(#loc513)
    %978 = stablehlo.reshape %arg429 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %979 = stablehlo.reshape %978 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %980 = stablehlo.transpose %979, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc514)
    %981 = stablehlo.dot_general %964, %980, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc515)
    %982 = stablehlo.reshape %981 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc516)
    %983 = stablehlo.reshape %arg428 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %984 = stablehlo.reshape %983 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %985 = stablehlo.broadcast_in_dim %984, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc517)
    %986 = stablehlo.add %982, %985 : tensor<1x257x1280xbf16> loc(#loc517)
    %987 = stablehlo.reshape %986 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc518)
    %988 = stablehlo.transpose %987, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc519)
    %989 = stablehlo.convert %988 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc520)
    %990 = stablehlo.transpose %989, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc521)
    %991 = stablehlo.multiply %990, %cst_5 : tensor<1x16x80x257xf32> loc(#loc522)
    %992 = stablehlo.dot_general %977, %991, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc523)
    %993 = stablehlo.convert %992 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc524)
    %994 = stablehlo.compare  EQ, %993, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc524)
    %995 = stablehlo.not %994 : tensor<1x16x257x257xi1> loc(#loc525)
    %996 = stablehlo.reduce(%995 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("1282|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_10aten__any"), %arg559: tensor<i1> loc("1282|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_10aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc527)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc528)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc526)
    %997 = stablehlo.reshape %996 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc526)
    %998 = stablehlo.not %997 : tensor<1x16x257x1xi1> loc(#loc529)
    %999 = stablehlo.reshape %998 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc530)
    %1000 = stablehlo.broadcast_in_dim %999, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc530)
    %1001 = stablehlo.reduce(%992 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc531)
    %1002 = stablehlo.broadcast_in_dim %1001, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc531)
    %1003 = stablehlo.subtract %992, %1002 : tensor<1x16x257x257xf32> loc(#loc531)
    %1004 = stablehlo.exponential %1003 : tensor<1x16x257x257xf32> loc(#loc531)
    %1005 = stablehlo.reduce(%1004 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc531)
    %1006 = stablehlo.broadcast_in_dim %1005, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc531)
    %1007 = stablehlo.divide %1004, %1006 : tensor<1x16x257x257xf32> loc(#loc531)
    %1008 = stablehlo.select %1000, %cst_3, %1007 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc532)
    %1009 = stablehlo.reshape %arg274 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1010 = stablehlo.reshape %1009 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1011 = stablehlo.transpose %1010, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc533)
    %1012 = stablehlo.dot_general %964, %1011, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc534)
    %1013 = stablehlo.reshape %1012 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc535)
    %1014 = stablehlo.reshape %arg273 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1015 = stablehlo.reshape %1014 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1016 = stablehlo.broadcast_in_dim %1015, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc536)
    %1017 = stablehlo.add %1013, %1016 : tensor<1x257x1280xbf16> loc(#loc536)
    %1018 = stablehlo.reshape %1017 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc537)
    %1019 = stablehlo.transpose %1018, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc538)
    %1020 = stablehlo.convert %1019 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc539)
    %1021 = stablehlo.dot_general %1008, %1020, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc540)
    %1022 = stablehlo.convert %1021 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc541)
    %1023 = stablehlo.transpose %1022, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc542)
    %1024 = stablehlo.reshape %1023 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc543)
    %1025 = stablehlo.reshape %arg272 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1026 = stablehlo.reshape %1025 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1027 = stablehlo.transpose %1026, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc544)
    %1028 = stablehlo.dot_general %1024, %1027, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc545)
    %1029 = stablehlo.reshape %1028 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc543)
    %1030 = stablehlo.reshape %arg271 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1031 = stablehlo.reshape %1030 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1032 = stablehlo.broadcast_in_dim %1031, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc546)
    %1033 = stablehlo.add %1029, %1032 : tensor<1x257x1280xbf16> loc(#loc546)
    %1034 = stablehlo.add %958, %1033 : tensor<1x257x1280xbf16> loc(#loc547)
    %1035 = stablehlo.reshape %arg270 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1036 = stablehlo.reshape %1035 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1037 = stablehlo.reshape %arg269 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1038 = stablehlo.reshape %1037 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1039 = stablehlo.composite "tenstorrent.layer_norm" %1034, %1036, %1038 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_45} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc548)
    %1040 = stablehlo.reshape %1039 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc549)
    %1041 = stablehlo.reshape %arg268 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %1042 = stablehlo.reshape %1041 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %1043 = stablehlo.transpose %1042, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc550)
    %1044 = stablehlo.dot_general %1040, %1043, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc551)
    %1045 = stablehlo.reshape %1044 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc549)
    %1046 = stablehlo.reshape %arg267 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %1047 = stablehlo.reshape %1046 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
    %1048 = stablehlo.broadcast_in_dim %1047, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc552)
    %1049 = stablehlo.add %1045, %1048 : tensor<1x257x5120xbf16> loc(#loc552)
    %1050 = stablehlo.composite "tenstorrent.gelu" %1049 {decomposition = @tenstorrent.gelu.impl_22} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc553)
    %1051 = stablehlo.reshape %1050 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc554)
    %1052 = stablehlo.reshape %arg266 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %1053 = stablehlo.reshape %1052 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %1054 = stablehlo.transpose %1053, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc555)
    %1055 = stablehlo.dot_general %1051, %1054, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc556)
    %1056 = stablehlo.reshape %1055 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc554)
    %1057 = stablehlo.reshape %arg265 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1058 = stablehlo.reshape %1057 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1059 = stablehlo.broadcast_in_dim %1058, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc557)
    %1060 = stablehlo.add %1056, %1059 : tensor<1x257x1280xbf16> loc(#loc557)
    %1061 = stablehlo.add %1034, %1060 : tensor<1x257x1280xbf16> loc(#loc558)
    %1062 = stablehlo.reshape %arg264 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1063 = stablehlo.reshape %1062 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1064 = stablehlo.reshape %arg263 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1065 = stablehlo.reshape %1064 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1066 = stablehlo.composite "tenstorrent.layer_norm" %1061, %1063, %1065 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_21} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc559)
    %1067 = stablehlo.reshape %1066 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc560)
    %1068 = stablehlo.reshape %arg435 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1069 = stablehlo.reshape %1068 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1070 = stablehlo.transpose %1069, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc561)
    %1071 = stablehlo.dot_general %1067, %1070, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc562)
    %1072 = stablehlo.reshape %1071 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc560)
    %1073 = stablehlo.reshape %arg434 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1074 = stablehlo.reshape %1073 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1075 = stablehlo.broadcast_in_dim %1074, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc563)
    %1076 = stablehlo.add %1072, %1075 : tensor<1x257x1280xbf16> loc(#loc563)
    %1077 = stablehlo.reshape %1076 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc564)
    %1078 = stablehlo.transpose %1077, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc565)
    %1079 = stablehlo.convert %1078 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc566)
    %1080 = stablehlo.multiply %1079, %cst_6 : tensor<1x16x257x80xf32> loc(#loc567)
    %1081 = stablehlo.reshape %arg433 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1082 = stablehlo.reshape %1081 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1083 = stablehlo.transpose %1082, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc568)
    %1084 = stablehlo.dot_general %1067, %1083, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc569)
    %1085 = stablehlo.reshape %1084 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc570)
    %1086 = stablehlo.reshape %arg432 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1087 = stablehlo.reshape %1086 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1088 = stablehlo.broadcast_in_dim %1087, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc571)
    %1089 = stablehlo.add %1085, %1088 : tensor<1x257x1280xbf16> loc(#loc571)
    %1090 = stablehlo.reshape %1089 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc572)
    %1091 = stablehlo.transpose %1090, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc573)
    %1092 = stablehlo.convert %1091 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc574)
    %1093 = stablehlo.transpose %1092, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc575)
    %1094 = stablehlo.multiply %1093, %cst_5 : tensor<1x16x80x257xf32> loc(#loc576)
    %1095 = stablehlo.dot_general %1080, %1094, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc577)
    %1096 = stablehlo.convert %1095 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc578)
    %1097 = stablehlo.compare  EQ, %1096, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc578)
    %1098 = stablehlo.not %1097 : tensor<1x16x257x257xi1> loc(#loc579)
    %1099 = stablehlo.reduce(%1098 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("1356|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_11aten__any"), %arg559: tensor<i1> loc("1356|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_11aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc581)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc582)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc580)
    %1100 = stablehlo.reshape %1099 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc580)
    %1101 = stablehlo.not %1100 : tensor<1x16x257x1xi1> loc(#loc583)
    %1102 = stablehlo.reshape %1101 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc584)
    %1103 = stablehlo.broadcast_in_dim %1102, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc584)
    %1104 = stablehlo.reduce(%1095 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc585)
    %1105 = stablehlo.broadcast_in_dim %1104, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc585)
    %1106 = stablehlo.subtract %1095, %1105 : tensor<1x16x257x257xf32> loc(#loc585)
    %1107 = stablehlo.exponential %1106 : tensor<1x16x257x257xf32> loc(#loc585)
    %1108 = stablehlo.reduce(%1107 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc585)
    %1109 = stablehlo.broadcast_in_dim %1108, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc585)
    %1110 = stablehlo.divide %1107, %1109 : tensor<1x16x257x257xf32> loc(#loc585)
    %1111 = stablehlo.select %1103, %cst_3, %1110 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc586)
    %1112 = stablehlo.reshape %arg262 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1113 = stablehlo.reshape %1112 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1114 = stablehlo.transpose %1113, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc587)
    %1115 = stablehlo.dot_general %1067, %1114, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc588)
    %1116 = stablehlo.reshape %1115 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc589)
    %1117 = stablehlo.reshape %arg261 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1118 = stablehlo.reshape %1117 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1119 = stablehlo.broadcast_in_dim %1118, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc590)
    %1120 = stablehlo.add %1116, %1119 : tensor<1x257x1280xbf16> loc(#loc590)
    %1121 = stablehlo.reshape %1120 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc591)
    %1122 = stablehlo.transpose %1121, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc592)
    %1123 = stablehlo.convert %1122 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc593)
    %1124 = stablehlo.dot_general %1111, %1123, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc594)
    %1125 = stablehlo.convert %1124 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc595)
    %1126 = stablehlo.transpose %1125, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc596)
    %1127 = stablehlo.reshape %1126 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc597)
    %1128 = stablehlo.reshape %arg260 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1129 = stablehlo.reshape %1128 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1130 = stablehlo.transpose %1129, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc598)
    %1131 = stablehlo.dot_general %1127, %1130, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc599)
    %1132 = stablehlo.reshape %1131 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc597)
    %1133 = stablehlo.reshape %arg259 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1134 = stablehlo.reshape %1133 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1135 = stablehlo.broadcast_in_dim %1134, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc600)
    %1136 = stablehlo.add %1132, %1135 : tensor<1x257x1280xbf16> loc(#loc600)
    %1137 = stablehlo.add %1061, %1136 : tensor<1x257x1280xbf16> loc(#loc601)
    %1138 = stablehlo.reshape %arg258 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1139 = stablehlo.reshape %1138 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1140 = stablehlo.reshape %arg257 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1141 = stablehlo.reshape %1140 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1142 = stablehlo.composite "tenstorrent.layer_norm" %1137, %1139, %1141 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_53} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc602)
    %1143 = stablehlo.reshape %1142 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc603)
    %1144 = stablehlo.reshape %arg256 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %1145 = stablehlo.reshape %1144 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %1146 = stablehlo.transpose %1145, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc604)
    %1147 = stablehlo.dot_general %1143, %1146, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc605)
    %1148 = stablehlo.reshape %1147 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc603)
    %1149 = stablehlo.reshape %arg255 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %1150 = stablehlo.reshape %1149 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
    %1151 = stablehlo.broadcast_in_dim %1150, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc606)
    %1152 = stablehlo.add %1148, %1151 : tensor<1x257x5120xbf16> loc(#loc606)
    %1153 = stablehlo.composite "tenstorrent.gelu" %1152 {decomposition = @tenstorrent.gelu.impl_25} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc607)
    %1154 = stablehlo.reshape %1153 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc608)
    %1155 = stablehlo.reshape %arg254 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %1156 = stablehlo.reshape %1155 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %1157 = stablehlo.transpose %1156, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc609)
    %1158 = stablehlo.dot_general %1154, %1157, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc610)
    %1159 = stablehlo.reshape %1158 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc608)
    %1160 = stablehlo.reshape %arg253 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1161 = stablehlo.reshape %1160 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1162 = stablehlo.broadcast_in_dim %1161, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc611)
    %1163 = stablehlo.add %1159, %1162 : tensor<1x257x1280xbf16> loc(#loc611)
    %1164 = stablehlo.add %1137, %1163 : tensor<1x257x1280xbf16> loc(#loc612)
    %1165 = stablehlo.reshape %arg252 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1166 = stablehlo.reshape %1165 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1167 = stablehlo.reshape %arg251 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1168 = stablehlo.reshape %1167 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1169 = stablehlo.composite "tenstorrent.layer_norm" %1164, %1166, %1168 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_56} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc613)
    %1170 = stablehlo.reshape %1169 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc614)
    %1171 = stablehlo.reshape %arg439 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1172 = stablehlo.reshape %1171 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1173 = stablehlo.transpose %1172, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc615)
    %1174 = stablehlo.dot_general %1170, %1173, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc616)
    %1175 = stablehlo.reshape %1174 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc614)
    %1176 = stablehlo.reshape %arg438 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1177 = stablehlo.reshape %1176 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1178 = stablehlo.broadcast_in_dim %1177, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc617)
    %1179 = stablehlo.add %1175, %1178 : tensor<1x257x1280xbf16> loc(#loc617)
    %1180 = stablehlo.reshape %1179 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc618)
    %1181 = stablehlo.transpose %1180, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc619)
    %1182 = stablehlo.convert %1181 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc620)
    %1183 = stablehlo.multiply %1182, %cst_6 : tensor<1x16x257x80xf32> loc(#loc621)
    %1184 = stablehlo.reshape %arg437 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1185 = stablehlo.reshape %1184 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1186 = stablehlo.transpose %1185, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc622)
    %1187 = stablehlo.dot_general %1170, %1186, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc623)
    %1188 = stablehlo.reshape %1187 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc624)
    %1189 = stablehlo.reshape %arg436 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1190 = stablehlo.reshape %1189 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1191 = stablehlo.broadcast_in_dim %1190, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc625)
    %1192 = stablehlo.add %1188, %1191 : tensor<1x257x1280xbf16> loc(#loc625)
    %1193 = stablehlo.reshape %1192 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc626)
    %1194 = stablehlo.transpose %1193, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc627)
    %1195 = stablehlo.convert %1194 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc628)
    %1196 = stablehlo.transpose %1195, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc629)
    %1197 = stablehlo.multiply %1196, %cst_5 : tensor<1x16x80x257xf32> loc(#loc630)
    %1198 = stablehlo.dot_general %1183, %1197, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc631)
    %1199 = stablehlo.convert %1198 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc632)
    %1200 = stablehlo.compare  EQ, %1199, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc632)
    %1201 = stablehlo.not %1200 : tensor<1x16x257x257xi1> loc(#loc633)
    %1202 = stablehlo.reduce(%1201 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("1430|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_12aten__any"), %arg559: tensor<i1> loc("1430|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_12aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc635)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc636)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc634)
    %1203 = stablehlo.reshape %1202 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc634)
    %1204 = stablehlo.not %1203 : tensor<1x16x257x1xi1> loc(#loc637)
    %1205 = stablehlo.reshape %1204 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc638)
    %1206 = stablehlo.broadcast_in_dim %1205, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc638)
    %1207 = stablehlo.reduce(%1198 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc639)
    %1208 = stablehlo.broadcast_in_dim %1207, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc639)
    %1209 = stablehlo.subtract %1198, %1208 : tensor<1x16x257x257xf32> loc(#loc639)
    %1210 = stablehlo.exponential %1209 : tensor<1x16x257x257xf32> loc(#loc639)
    %1211 = stablehlo.reduce(%1210 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc639)
    %1212 = stablehlo.broadcast_in_dim %1211, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc639)
    %1213 = stablehlo.divide %1210, %1212 : tensor<1x16x257x257xf32> loc(#loc639)
    %1214 = stablehlo.select %1206, %cst_3, %1213 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc640)
    %1215 = stablehlo.reshape %arg250 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1216 = stablehlo.reshape %1215 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1217 = stablehlo.transpose %1216, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc641)
    %1218 = stablehlo.dot_general %1170, %1217, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc642)
    %1219 = stablehlo.reshape %1218 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc643)
    %1220 = stablehlo.reshape %arg249 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1221 = stablehlo.reshape %1220 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1222 = stablehlo.broadcast_in_dim %1221, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc644)
    %1223 = stablehlo.add %1219, %1222 : tensor<1x257x1280xbf16> loc(#loc644)
    %1224 = stablehlo.reshape %1223 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc645)
    %1225 = stablehlo.transpose %1224, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc646)
    %1226 = stablehlo.convert %1225 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc647)
    %1227 = stablehlo.dot_general %1214, %1226, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc648)
    %1228 = stablehlo.convert %1227 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc649)
    %1229 = stablehlo.transpose %1228, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc650)
    %1230 = stablehlo.reshape %1229 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc651)
    %1231 = stablehlo.reshape %arg248 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1232 = stablehlo.reshape %1231 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1233 = stablehlo.transpose %1232, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc652)
    %1234 = stablehlo.dot_general %1230, %1233, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc653)
    %1235 = stablehlo.reshape %1234 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc651)
    %1236 = stablehlo.reshape %arg247 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1237 = stablehlo.reshape %1236 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1238 = stablehlo.broadcast_in_dim %1237, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc654)
    %1239 = stablehlo.add %1235, %1238 : tensor<1x257x1280xbf16> loc(#loc654)
    %1240 = stablehlo.add %1164, %1239 : tensor<1x257x1280xbf16> loc(#loc655)
    %1241 = stablehlo.reshape %arg246 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1242 = stablehlo.reshape %1241 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1243 = stablehlo.reshape %arg245 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1244 = stablehlo.reshape %1243 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1245 = stablehlo.composite "tenstorrent.layer_norm" %1240, %1242, %1244 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_57} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc656)
    %1246 = stablehlo.reshape %1245 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc657)
    %1247 = stablehlo.reshape %arg244 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %1248 = stablehlo.reshape %1247 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %1249 = stablehlo.transpose %1248, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc658)
    %1250 = stablehlo.dot_general %1246, %1249, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc659)
    %1251 = stablehlo.reshape %1250 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc657)
    %1252 = stablehlo.reshape %arg243 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %1253 = stablehlo.reshape %1252 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
    %1254 = stablehlo.broadcast_in_dim %1253, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc660)
    %1255 = stablehlo.add %1251, %1254 : tensor<1x257x5120xbf16> loc(#loc660)
    %1256 = stablehlo.composite "tenstorrent.gelu" %1255 {decomposition = @tenstorrent.gelu.impl_30} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc661)
    %1257 = stablehlo.reshape %1256 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc662)
    %1258 = stablehlo.reshape %arg242 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %1259 = stablehlo.reshape %1258 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %1260 = stablehlo.transpose %1259, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc663)
    %1261 = stablehlo.dot_general %1257, %1260, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc664)
    %1262 = stablehlo.reshape %1261 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc662)
    %1263 = stablehlo.reshape %arg241 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1264 = stablehlo.reshape %1263 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1265 = stablehlo.broadcast_in_dim %1264, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc665)
    %1266 = stablehlo.add %1262, %1265 : tensor<1x257x1280xbf16> loc(#loc665)
    %1267 = stablehlo.add %1240, %1266 : tensor<1x257x1280xbf16> loc(#loc666)
    %1268 = stablehlo.reshape %arg240 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1269 = stablehlo.reshape %1268 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1270 = stablehlo.reshape %arg239 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1271 = stablehlo.reshape %1270 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1272 = stablehlo.composite "tenstorrent.layer_norm" %1267, %1269, %1271 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_69} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc667)
    %1273 = stablehlo.reshape %1272 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc668)
    %1274 = stablehlo.reshape %arg443 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1275 = stablehlo.reshape %1274 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1276 = stablehlo.transpose %1275, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc669)
    %1277 = stablehlo.dot_general %1273, %1276, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc670)
    %1278 = stablehlo.reshape %1277 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc668)
    %1279 = stablehlo.reshape %arg442 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1280 = stablehlo.reshape %1279 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1281 = stablehlo.broadcast_in_dim %1280, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc671)
    %1282 = stablehlo.add %1278, %1281 : tensor<1x257x1280xbf16> loc(#loc671)
    %1283 = stablehlo.reshape %1282 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc672)
    %1284 = stablehlo.transpose %1283, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc673)
    %1285 = stablehlo.convert %1284 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc674)
    %1286 = stablehlo.multiply %1285, %cst_6 : tensor<1x16x257x80xf32> loc(#loc675)
    %1287 = stablehlo.reshape %arg441 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1288 = stablehlo.reshape %1287 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1289 = stablehlo.transpose %1288, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc676)
    %1290 = stablehlo.dot_general %1273, %1289, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc677)
    %1291 = stablehlo.reshape %1290 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc678)
    %1292 = stablehlo.reshape %arg440 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1293 = stablehlo.reshape %1292 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1294 = stablehlo.broadcast_in_dim %1293, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc679)
    %1295 = stablehlo.add %1291, %1294 : tensor<1x257x1280xbf16> loc(#loc679)
    %1296 = stablehlo.reshape %1295 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc680)
    %1297 = stablehlo.transpose %1296, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc681)
    %1298 = stablehlo.convert %1297 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc682)
    %1299 = stablehlo.transpose %1298, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc683)
    %1300 = stablehlo.multiply %1299, %cst_5 : tensor<1x16x80x257xf32> loc(#loc684)
    %1301 = stablehlo.dot_general %1286, %1300, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc685)
    %1302 = stablehlo.convert %1301 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc686)
    %1303 = stablehlo.compare  EQ, %1302, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc686)
    %1304 = stablehlo.not %1303 : tensor<1x16x257x257xi1> loc(#loc687)
    %1305 = stablehlo.reduce(%1304 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("1504|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_13aten__any"), %arg559: tensor<i1> loc("1504|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_13aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc689)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc690)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc688)
    %1306 = stablehlo.reshape %1305 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc688)
    %1307 = stablehlo.not %1306 : tensor<1x16x257x1xi1> loc(#loc691)
    %1308 = stablehlo.reshape %1307 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc692)
    %1309 = stablehlo.broadcast_in_dim %1308, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc692)
    %1310 = stablehlo.reduce(%1301 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc693)
    %1311 = stablehlo.broadcast_in_dim %1310, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc693)
    %1312 = stablehlo.subtract %1301, %1311 : tensor<1x16x257x257xf32> loc(#loc693)
    %1313 = stablehlo.exponential %1312 : tensor<1x16x257x257xf32> loc(#loc693)
    %1314 = stablehlo.reduce(%1313 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc693)
    %1315 = stablehlo.broadcast_in_dim %1314, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc693)
    %1316 = stablehlo.divide %1313, %1315 : tensor<1x16x257x257xf32> loc(#loc693)
    %1317 = stablehlo.select %1309, %cst_3, %1316 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc694)
    %1318 = stablehlo.reshape %arg238 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1319 = stablehlo.reshape %1318 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1320 = stablehlo.transpose %1319, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc695)
    %1321 = stablehlo.dot_general %1273, %1320, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc696)
    %1322 = stablehlo.reshape %1321 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc697)
    %1323 = stablehlo.reshape %arg237 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1324 = stablehlo.reshape %1323 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1325 = stablehlo.broadcast_in_dim %1324, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc698)
    %1326 = stablehlo.add %1322, %1325 : tensor<1x257x1280xbf16> loc(#loc698)
    %1327 = stablehlo.reshape %1326 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc699)
    %1328 = stablehlo.transpose %1327, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc700)
    %1329 = stablehlo.convert %1328 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc701)
    %1330 = stablehlo.dot_general %1317, %1329, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc702)
    %1331 = stablehlo.convert %1330 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc703)
    %1332 = stablehlo.transpose %1331, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc704)
    %1333 = stablehlo.reshape %1332 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc705)
    %1334 = stablehlo.reshape %arg236 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1335 = stablehlo.reshape %1334 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1336 = stablehlo.transpose %1335, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc706)
    %1337 = stablehlo.dot_general %1333, %1336, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc707)
    %1338 = stablehlo.reshape %1337 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc705)
    %1339 = stablehlo.reshape %arg235 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1340 = stablehlo.reshape %1339 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1341 = stablehlo.broadcast_in_dim %1340, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc708)
    %1342 = stablehlo.add %1338, %1341 : tensor<1x257x1280xbf16> loc(#loc708)
    %1343 = stablehlo.add %1267, %1342 : tensor<1x257x1280xbf16> loc(#loc709)
    %1344 = stablehlo.reshape %arg234 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1345 = stablehlo.reshape %1344 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1346 = stablehlo.reshape %arg233 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1347 = stablehlo.reshape %1346 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1348 = stablehlo.composite "tenstorrent.layer_norm" %1343, %1345, %1347 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_60} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc710)
    %1349 = stablehlo.reshape %1348 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc711)
    %1350 = stablehlo.reshape %arg232 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %1351 = stablehlo.reshape %1350 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %1352 = stablehlo.transpose %1351, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc712)
    %1353 = stablehlo.dot_general %1349, %1352, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc713)
    %1354 = stablehlo.reshape %1353 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc711)
    %1355 = stablehlo.reshape %arg231 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %1356 = stablehlo.reshape %1355 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
    %1357 = stablehlo.broadcast_in_dim %1356, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc714)
    %1358 = stablehlo.add %1354, %1357 : tensor<1x257x5120xbf16> loc(#loc714)
    %1359 = stablehlo.composite "tenstorrent.gelu" %1358 {decomposition = @tenstorrent.gelu.impl_18} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc715)
    %1360 = stablehlo.reshape %1359 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc716)
    %1361 = stablehlo.reshape %arg230 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %1362 = stablehlo.reshape %1361 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %1363 = stablehlo.transpose %1362, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc717)
    %1364 = stablehlo.dot_general %1360, %1363, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc718)
    %1365 = stablehlo.reshape %1364 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc716)
    %1366 = stablehlo.reshape %arg229 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1367 = stablehlo.reshape %1366 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1368 = stablehlo.broadcast_in_dim %1367, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc719)
    %1369 = stablehlo.add %1365, %1368 : tensor<1x257x1280xbf16> loc(#loc719)
    %1370 = stablehlo.add %1343, %1369 : tensor<1x257x1280xbf16> loc(#loc720)
    %1371 = stablehlo.reshape %arg228 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1372 = stablehlo.reshape %1371 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1373 = stablehlo.reshape %arg227 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1374 = stablehlo.reshape %1373 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1375 = stablehlo.composite "tenstorrent.layer_norm" %1370, %1372, %1374 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_73} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc721)
    %1376 = stablehlo.reshape %1375 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc722)
    %1377 = stablehlo.reshape %arg447 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1378 = stablehlo.reshape %1377 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1379 = stablehlo.transpose %1378, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc723)
    %1380 = stablehlo.dot_general %1376, %1379, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc724)
    %1381 = stablehlo.reshape %1380 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc722)
    %1382 = stablehlo.reshape %arg446 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1383 = stablehlo.reshape %1382 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1384 = stablehlo.broadcast_in_dim %1383, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc725)
    %1385 = stablehlo.add %1381, %1384 : tensor<1x257x1280xbf16> loc(#loc725)
    %1386 = stablehlo.reshape %1385 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc726)
    %1387 = stablehlo.transpose %1386, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc727)
    %1388 = stablehlo.convert %1387 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc728)
    %1389 = stablehlo.multiply %1388, %cst_6 : tensor<1x16x257x80xf32> loc(#loc729)
    %1390 = stablehlo.reshape %arg445 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1391 = stablehlo.reshape %1390 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1392 = stablehlo.transpose %1391, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc730)
    %1393 = stablehlo.dot_general %1376, %1392, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc731)
    %1394 = stablehlo.reshape %1393 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc732)
    %1395 = stablehlo.reshape %arg444 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1396 = stablehlo.reshape %1395 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1397 = stablehlo.broadcast_in_dim %1396, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc733)
    %1398 = stablehlo.add %1394, %1397 : tensor<1x257x1280xbf16> loc(#loc733)
    %1399 = stablehlo.reshape %1398 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc734)
    %1400 = stablehlo.transpose %1399, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc735)
    %1401 = stablehlo.convert %1400 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc736)
    %1402 = stablehlo.transpose %1401, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc737)
    %1403 = stablehlo.multiply %1402, %cst_5 : tensor<1x16x80x257xf32> loc(#loc738)
    %1404 = stablehlo.dot_general %1389, %1403, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc739)
    %1405 = stablehlo.convert %1404 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc740)
    %1406 = stablehlo.compare  EQ, %1405, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc740)
    %1407 = stablehlo.not %1406 : tensor<1x16x257x257xi1> loc(#loc741)
    %1408 = stablehlo.reduce(%1407 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("1578|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_14aten__any"), %arg559: tensor<i1> loc("1578|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_14aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc743)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc744)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc742)
    %1409 = stablehlo.reshape %1408 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc742)
    %1410 = stablehlo.not %1409 : tensor<1x16x257x1xi1> loc(#loc745)
    %1411 = stablehlo.reshape %1410 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc746)
    %1412 = stablehlo.broadcast_in_dim %1411, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc746)
    %1413 = stablehlo.reduce(%1404 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc747)
    %1414 = stablehlo.broadcast_in_dim %1413, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc747)
    %1415 = stablehlo.subtract %1404, %1414 : tensor<1x16x257x257xf32> loc(#loc747)
    %1416 = stablehlo.exponential %1415 : tensor<1x16x257x257xf32> loc(#loc747)
    %1417 = stablehlo.reduce(%1416 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc747)
    %1418 = stablehlo.broadcast_in_dim %1417, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc747)
    %1419 = stablehlo.divide %1416, %1418 : tensor<1x16x257x257xf32> loc(#loc747)
    %1420 = stablehlo.select %1412, %cst_3, %1419 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc748)
    %1421 = stablehlo.reshape %arg226 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1422 = stablehlo.reshape %1421 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1423 = stablehlo.transpose %1422, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc749)
    %1424 = stablehlo.dot_general %1376, %1423, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc750)
    %1425 = stablehlo.reshape %1424 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc751)
    %1426 = stablehlo.reshape %arg225 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1427 = stablehlo.reshape %1426 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1428 = stablehlo.broadcast_in_dim %1427, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc752)
    %1429 = stablehlo.add %1425, %1428 : tensor<1x257x1280xbf16> loc(#loc752)
    %1430 = stablehlo.reshape %1429 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc753)
    %1431 = stablehlo.transpose %1430, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc754)
    %1432 = stablehlo.convert %1431 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc755)
    %1433 = stablehlo.dot_general %1420, %1432, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc756)
    %1434 = stablehlo.convert %1433 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc757)
    %1435 = stablehlo.transpose %1434, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc758)
    %1436 = stablehlo.reshape %1435 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc759)
    %1437 = stablehlo.reshape %arg224 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1438 = stablehlo.reshape %1437 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1439 = stablehlo.transpose %1438, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc760)
    %1440 = stablehlo.dot_general %1436, %1439, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc761)
    %1441 = stablehlo.reshape %1440 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc759)
    %1442 = stablehlo.reshape %arg223 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1443 = stablehlo.reshape %1442 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1444 = stablehlo.broadcast_in_dim %1443, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc762)
    %1445 = stablehlo.add %1441, %1444 : tensor<1x257x1280xbf16> loc(#loc762)
    %1446 = stablehlo.add %1370, %1445 : tensor<1x257x1280xbf16> loc(#loc763)
    %1447 = stablehlo.reshape %arg222 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1448 = stablehlo.reshape %1447 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1449 = stablehlo.reshape %arg221 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1450 = stablehlo.reshape %1449 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1451 = stablehlo.composite "tenstorrent.layer_norm" %1446, %1448, %1450 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_62} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc764)
    %1452 = stablehlo.reshape %1451 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc765)
    %1453 = stablehlo.reshape %arg220 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %1454 = stablehlo.reshape %1453 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %1455 = stablehlo.transpose %1454, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc766)
    %1456 = stablehlo.dot_general %1452, %1455, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc767)
    %1457 = stablehlo.reshape %1456 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc765)
    %1458 = stablehlo.reshape %arg219 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %1459 = stablehlo.reshape %1458 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
    %1460 = stablehlo.broadcast_in_dim %1459, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc768)
    %1461 = stablehlo.add %1457, %1460 : tensor<1x257x5120xbf16> loc(#loc768)
    %1462 = stablehlo.composite "tenstorrent.gelu" %1461 {decomposition = @tenstorrent.gelu.impl_28} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc769)
    %1463 = stablehlo.reshape %1462 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc770)
    %1464 = stablehlo.reshape %arg218 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %1465 = stablehlo.reshape %1464 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %1466 = stablehlo.transpose %1465, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc771)
    %1467 = stablehlo.dot_general %1463, %1466, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc772)
    %1468 = stablehlo.reshape %1467 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc770)
    %1469 = stablehlo.reshape %arg217 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1470 = stablehlo.reshape %1469 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1471 = stablehlo.broadcast_in_dim %1470, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc773)
    %1472 = stablehlo.add %1468, %1471 : tensor<1x257x1280xbf16> loc(#loc773)
    %1473 = stablehlo.add %1446, %1472 : tensor<1x257x1280xbf16> loc(#loc774)
    %1474 = stablehlo.reshape %arg216 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1475 = stablehlo.reshape %1474 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1476 = stablehlo.reshape %arg215 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1477 = stablehlo.reshape %1476 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1478 = stablehlo.composite "tenstorrent.layer_norm" %1473, %1475, %1477 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_66} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc775)
    %1479 = stablehlo.reshape %1478 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc776)
    %1480 = stablehlo.reshape %arg451 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1481 = stablehlo.reshape %1480 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1482 = stablehlo.transpose %1481, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc777)
    %1483 = stablehlo.dot_general %1479, %1482, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc778)
    %1484 = stablehlo.reshape %1483 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc776)
    %1485 = stablehlo.reshape %arg450 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1486 = stablehlo.reshape %1485 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1487 = stablehlo.broadcast_in_dim %1486, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc779)
    %1488 = stablehlo.add %1484, %1487 : tensor<1x257x1280xbf16> loc(#loc779)
    %1489 = stablehlo.reshape %1488 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc780)
    %1490 = stablehlo.transpose %1489, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc781)
    %1491 = stablehlo.convert %1490 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc782)
    %1492 = stablehlo.multiply %1491, %cst_6 : tensor<1x16x257x80xf32> loc(#loc783)
    %1493 = stablehlo.reshape %arg449 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1494 = stablehlo.reshape %1493 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1495 = stablehlo.transpose %1494, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc784)
    %1496 = stablehlo.dot_general %1479, %1495, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc785)
    %1497 = stablehlo.reshape %1496 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc786)
    %1498 = stablehlo.reshape %arg448 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1499 = stablehlo.reshape %1498 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1500 = stablehlo.broadcast_in_dim %1499, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc787)
    %1501 = stablehlo.add %1497, %1500 : tensor<1x257x1280xbf16> loc(#loc787)
    %1502 = stablehlo.reshape %1501 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc788)
    %1503 = stablehlo.transpose %1502, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc789)
    %1504 = stablehlo.convert %1503 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc790)
    %1505 = stablehlo.transpose %1504, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc791)
    %1506 = stablehlo.multiply %1505, %cst_5 : tensor<1x16x80x257xf32> loc(#loc792)
    %1507 = stablehlo.dot_general %1492, %1506, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc793)
    %1508 = stablehlo.convert %1507 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc794)
    %1509 = stablehlo.compare  EQ, %1508, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc794)
    %1510 = stablehlo.not %1509 : tensor<1x16x257x257xi1> loc(#loc795)
    %1511 = stablehlo.reduce(%1510 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("1652|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_15aten__any"), %arg559: tensor<i1> loc("1652|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_15aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc797)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc798)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc796)
    %1512 = stablehlo.reshape %1511 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc796)
    %1513 = stablehlo.not %1512 : tensor<1x16x257x1xi1> loc(#loc799)
    %1514 = stablehlo.reshape %1513 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc800)
    %1515 = stablehlo.broadcast_in_dim %1514, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc800)
    %1516 = stablehlo.reduce(%1507 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc801)
    %1517 = stablehlo.broadcast_in_dim %1516, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc801)
    %1518 = stablehlo.subtract %1507, %1517 : tensor<1x16x257x257xf32> loc(#loc801)
    %1519 = stablehlo.exponential %1518 : tensor<1x16x257x257xf32> loc(#loc801)
    %1520 = stablehlo.reduce(%1519 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc801)
    %1521 = stablehlo.broadcast_in_dim %1520, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc801)
    %1522 = stablehlo.divide %1519, %1521 : tensor<1x16x257x257xf32> loc(#loc801)
    %1523 = stablehlo.select %1515, %cst_3, %1522 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc802)
    %1524 = stablehlo.reshape %arg214 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1525 = stablehlo.reshape %1524 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1526 = stablehlo.transpose %1525, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc803)
    %1527 = stablehlo.dot_general %1479, %1526, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc804)
    %1528 = stablehlo.reshape %1527 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc805)
    %1529 = stablehlo.reshape %arg213 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1530 = stablehlo.reshape %1529 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1531 = stablehlo.broadcast_in_dim %1530, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc806)
    %1532 = stablehlo.add %1528, %1531 : tensor<1x257x1280xbf16> loc(#loc806)
    %1533 = stablehlo.reshape %1532 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc807)
    %1534 = stablehlo.transpose %1533, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc808)
    %1535 = stablehlo.convert %1534 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc809)
    %1536 = stablehlo.dot_general %1523, %1535, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc810)
    %1537 = stablehlo.convert %1536 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc811)
    %1538 = stablehlo.transpose %1537, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc812)
    %1539 = stablehlo.reshape %1538 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc813)
    %1540 = stablehlo.reshape %arg212 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1541 = stablehlo.reshape %1540 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1542 = stablehlo.transpose %1541, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc814)
    %1543 = stablehlo.dot_general %1539, %1542, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc815)
    %1544 = stablehlo.reshape %1543 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc813)
    %1545 = stablehlo.reshape %arg211 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1546 = stablehlo.reshape %1545 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1547 = stablehlo.broadcast_in_dim %1546, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc816)
    %1548 = stablehlo.add %1544, %1547 : tensor<1x257x1280xbf16> loc(#loc816)
    %1549 = stablehlo.add %1473, %1548 : tensor<1x257x1280xbf16> loc(#loc817)
    %1550 = stablehlo.reshape %arg210 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1551 = stablehlo.reshape %1550 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1552 = stablehlo.reshape %arg209 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1553 = stablehlo.reshape %1552 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1554 = stablehlo.composite "tenstorrent.layer_norm" %1549, %1551, %1553 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_33} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc818)
    %1555 = stablehlo.reshape %1554 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc819)
    %1556 = stablehlo.reshape %arg208 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %1557 = stablehlo.reshape %1556 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %1558 = stablehlo.transpose %1557, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc820)
    %1559 = stablehlo.dot_general %1555, %1558, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc821)
    %1560 = stablehlo.reshape %1559 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc819)
    %1561 = stablehlo.reshape %arg207 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %1562 = stablehlo.reshape %1561 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
    %1563 = stablehlo.broadcast_in_dim %1562, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc822)
    %1564 = stablehlo.add %1560, %1563 : tensor<1x257x5120xbf16> loc(#loc822)
    %1565 = stablehlo.composite "tenstorrent.gelu" %1564 {decomposition = @tenstorrent.gelu.impl_31} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc823)
    %1566 = stablehlo.reshape %1565 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc824)
    %1567 = stablehlo.reshape %arg206 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %1568 = stablehlo.reshape %1567 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %1569 = stablehlo.transpose %1568, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc825)
    %1570 = stablehlo.dot_general %1566, %1569, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc826)
    %1571 = stablehlo.reshape %1570 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc824)
    %1572 = stablehlo.reshape %arg205 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1573 = stablehlo.reshape %1572 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1574 = stablehlo.broadcast_in_dim %1573, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc827)
    %1575 = stablehlo.add %1571, %1574 : tensor<1x257x1280xbf16> loc(#loc827)
    %1576 = stablehlo.add %1549, %1575 : tensor<1x257x1280xbf16> loc(#loc828)
    %1577 = stablehlo.reshape %arg204 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1578 = stablehlo.reshape %1577 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1579 = stablehlo.reshape %arg203 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1580 = stablehlo.reshape %1579 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1581 = stablehlo.composite "tenstorrent.layer_norm" %1576, %1578, %1580 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_40} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc829)
    %1582 = stablehlo.reshape %1581 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc830)
    %1583 = stablehlo.reshape %arg455 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1584 = stablehlo.reshape %1583 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1585 = stablehlo.transpose %1584, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc831)
    %1586 = stablehlo.dot_general %1582, %1585, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc832)
    %1587 = stablehlo.reshape %1586 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc830)
    %1588 = stablehlo.reshape %arg454 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1589 = stablehlo.reshape %1588 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1590 = stablehlo.broadcast_in_dim %1589, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc833)
    %1591 = stablehlo.add %1587, %1590 : tensor<1x257x1280xbf16> loc(#loc833)
    %1592 = stablehlo.reshape %1591 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc834)
    %1593 = stablehlo.transpose %1592, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc835)
    %1594 = stablehlo.convert %1593 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc836)
    %1595 = stablehlo.multiply %1594, %cst_6 : tensor<1x16x257x80xf32> loc(#loc837)
    %1596 = stablehlo.reshape %arg453 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1597 = stablehlo.reshape %1596 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1598 = stablehlo.transpose %1597, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc838)
    %1599 = stablehlo.dot_general %1582, %1598, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc839)
    %1600 = stablehlo.reshape %1599 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc840)
    %1601 = stablehlo.reshape %arg452 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1602 = stablehlo.reshape %1601 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1603 = stablehlo.broadcast_in_dim %1602, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc841)
    %1604 = stablehlo.add %1600, %1603 : tensor<1x257x1280xbf16> loc(#loc841)
    %1605 = stablehlo.reshape %1604 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc842)
    %1606 = stablehlo.transpose %1605, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc843)
    %1607 = stablehlo.convert %1606 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc844)
    %1608 = stablehlo.transpose %1607, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc845)
    %1609 = stablehlo.multiply %1608, %cst_5 : tensor<1x16x80x257xf32> loc(#loc846)
    %1610 = stablehlo.dot_general %1595, %1609, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc847)
    %1611 = stablehlo.convert %1610 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc848)
    %1612 = stablehlo.compare  EQ, %1611, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc848)
    %1613 = stablehlo.not %1612 : tensor<1x16x257x257xi1> loc(#loc849)
    %1614 = stablehlo.reduce(%1613 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("1726|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_16aten__any"), %arg559: tensor<i1> loc("1726|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_16aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc851)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc852)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc850)
    %1615 = stablehlo.reshape %1614 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc850)
    %1616 = stablehlo.not %1615 : tensor<1x16x257x1xi1> loc(#loc853)
    %1617 = stablehlo.reshape %1616 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc854)
    %1618 = stablehlo.broadcast_in_dim %1617, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc854)
    %1619 = stablehlo.reduce(%1610 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc855)
    %1620 = stablehlo.broadcast_in_dim %1619, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc855)
    %1621 = stablehlo.subtract %1610, %1620 : tensor<1x16x257x257xf32> loc(#loc855)
    %1622 = stablehlo.exponential %1621 : tensor<1x16x257x257xf32> loc(#loc855)
    %1623 = stablehlo.reduce(%1622 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc855)
    %1624 = stablehlo.broadcast_in_dim %1623, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc855)
    %1625 = stablehlo.divide %1622, %1624 : tensor<1x16x257x257xf32> loc(#loc855)
    %1626 = stablehlo.select %1618, %cst_3, %1625 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc856)
    %1627 = stablehlo.reshape %arg202 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1628 = stablehlo.reshape %1627 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1629 = stablehlo.transpose %1628, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc857)
    %1630 = stablehlo.dot_general %1582, %1629, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc858)
    %1631 = stablehlo.reshape %1630 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc859)
    %1632 = stablehlo.reshape %arg201 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1633 = stablehlo.reshape %1632 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1634 = stablehlo.broadcast_in_dim %1633, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc860)
    %1635 = stablehlo.add %1631, %1634 : tensor<1x257x1280xbf16> loc(#loc860)
    %1636 = stablehlo.reshape %1635 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc861)
    %1637 = stablehlo.transpose %1636, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc862)
    %1638 = stablehlo.convert %1637 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc863)
    %1639 = stablehlo.dot_general %1626, %1638, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc864)
    %1640 = stablehlo.convert %1639 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc865)
    %1641 = stablehlo.transpose %1640, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc866)
    %1642 = stablehlo.reshape %1641 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc867)
    %1643 = stablehlo.reshape %arg200 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1644 = stablehlo.reshape %1643 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1645 = stablehlo.transpose %1644, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc868)
    %1646 = stablehlo.dot_general %1642, %1645, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc869)
    %1647 = stablehlo.reshape %1646 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc867)
    %1648 = stablehlo.reshape %arg199 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1649 = stablehlo.reshape %1648 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1650 = stablehlo.broadcast_in_dim %1649, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc870)
    %1651 = stablehlo.add %1647, %1650 : tensor<1x257x1280xbf16> loc(#loc870)
    %1652 = stablehlo.add %1576, %1651 : tensor<1x257x1280xbf16> loc(#loc871)
    %1653 = stablehlo.reshape %arg198 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1654 = stablehlo.reshape %1653 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1655 = stablehlo.reshape %arg197 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1656 = stablehlo.reshape %1655 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1657 = stablehlo.composite "tenstorrent.layer_norm" %1652, %1654, %1656 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_48} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc872)
    %1658 = stablehlo.reshape %1657 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc873)
    %1659 = stablehlo.reshape %arg196 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %1660 = stablehlo.reshape %1659 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %1661 = stablehlo.transpose %1660, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc874)
    %1662 = stablehlo.dot_general %1658, %1661, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc875)
    %1663 = stablehlo.reshape %1662 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc873)
    %1664 = stablehlo.reshape %arg195 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %1665 = stablehlo.reshape %1664 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
    %1666 = stablehlo.broadcast_in_dim %1665, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc876)
    %1667 = stablehlo.add %1663, %1666 : tensor<1x257x5120xbf16> loc(#loc876)
    %1668 = stablehlo.composite "tenstorrent.gelu" %1667 {decomposition = @tenstorrent.gelu.impl_27} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc877)
    %1669 = stablehlo.reshape %1668 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc878)
    %1670 = stablehlo.reshape %arg194 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %1671 = stablehlo.reshape %1670 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %1672 = stablehlo.transpose %1671, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc879)
    %1673 = stablehlo.dot_general %1669, %1672, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc880)
    %1674 = stablehlo.reshape %1673 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc878)
    %1675 = stablehlo.reshape %arg193 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1676 = stablehlo.reshape %1675 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1677 = stablehlo.broadcast_in_dim %1676, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc881)
    %1678 = stablehlo.add %1674, %1677 : tensor<1x257x1280xbf16> loc(#loc881)
    %1679 = stablehlo.add %1652, %1678 : tensor<1x257x1280xbf16> loc(#loc882)
    %1680 = stablehlo.reshape %arg192 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1681 = stablehlo.reshape %1680 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1682 = stablehlo.reshape %arg191 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1683 = stablehlo.reshape %1682 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1684 = stablehlo.composite "tenstorrent.layer_norm" %1679, %1681, %1683 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_44} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc883)
    %1685 = stablehlo.reshape %1684 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc884)
    %1686 = stablehlo.reshape %arg459 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1687 = stablehlo.reshape %1686 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1688 = stablehlo.transpose %1687, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc885)
    %1689 = stablehlo.dot_general %1685, %1688, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc886)
    %1690 = stablehlo.reshape %1689 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc884)
    %1691 = stablehlo.reshape %arg458 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1692 = stablehlo.reshape %1691 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1693 = stablehlo.broadcast_in_dim %1692, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc887)
    %1694 = stablehlo.add %1690, %1693 : tensor<1x257x1280xbf16> loc(#loc887)
    %1695 = stablehlo.reshape %1694 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc888)
    %1696 = stablehlo.transpose %1695, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc889)
    %1697 = stablehlo.convert %1696 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc890)
    %1698 = stablehlo.multiply %1697, %cst_6 : tensor<1x16x257x80xf32> loc(#loc891)
    %1699 = stablehlo.reshape %arg457 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1700 = stablehlo.reshape %1699 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1701 = stablehlo.transpose %1700, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc892)
    %1702 = stablehlo.dot_general %1685, %1701, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc893)
    %1703 = stablehlo.reshape %1702 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc894)
    %1704 = stablehlo.reshape %arg456 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1705 = stablehlo.reshape %1704 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1706 = stablehlo.broadcast_in_dim %1705, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc895)
    %1707 = stablehlo.add %1703, %1706 : tensor<1x257x1280xbf16> loc(#loc895)
    %1708 = stablehlo.reshape %1707 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc896)
    %1709 = stablehlo.transpose %1708, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc897)
    %1710 = stablehlo.convert %1709 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc898)
    %1711 = stablehlo.transpose %1710, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc899)
    %1712 = stablehlo.multiply %1711, %cst_5 : tensor<1x16x80x257xf32> loc(#loc900)
    %1713 = stablehlo.dot_general %1698, %1712, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc901)
    %1714 = stablehlo.convert %1713 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc902)
    %1715 = stablehlo.compare  EQ, %1714, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc902)
    %1716 = stablehlo.not %1715 : tensor<1x16x257x257xi1> loc(#loc903)
    %1717 = stablehlo.reduce(%1716 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("1800|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_17aten__any"), %arg559: tensor<i1> loc("1800|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_17aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc905)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc906)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc904)
    %1718 = stablehlo.reshape %1717 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc904)
    %1719 = stablehlo.not %1718 : tensor<1x16x257x1xi1> loc(#loc907)
    %1720 = stablehlo.reshape %1719 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc908)
    %1721 = stablehlo.broadcast_in_dim %1720, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc908)
    %1722 = stablehlo.reduce(%1713 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc909)
    %1723 = stablehlo.broadcast_in_dim %1722, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc909)
    %1724 = stablehlo.subtract %1713, %1723 : tensor<1x16x257x257xf32> loc(#loc909)
    %1725 = stablehlo.exponential %1724 : tensor<1x16x257x257xf32> loc(#loc909)
    %1726 = stablehlo.reduce(%1725 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc909)
    %1727 = stablehlo.broadcast_in_dim %1726, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc909)
    %1728 = stablehlo.divide %1725, %1727 : tensor<1x16x257x257xf32> loc(#loc909)
    %1729 = stablehlo.select %1721, %cst_3, %1728 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc910)
    %1730 = stablehlo.reshape %arg190 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1731 = stablehlo.reshape %1730 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1732 = stablehlo.transpose %1731, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc911)
    %1733 = stablehlo.dot_general %1685, %1732, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc912)
    %1734 = stablehlo.reshape %1733 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc913)
    %1735 = stablehlo.reshape %arg189 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1736 = stablehlo.reshape %1735 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1737 = stablehlo.broadcast_in_dim %1736, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc914)
    %1738 = stablehlo.add %1734, %1737 : tensor<1x257x1280xbf16> loc(#loc914)
    %1739 = stablehlo.reshape %1738 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc915)
    %1740 = stablehlo.transpose %1739, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc916)
    %1741 = stablehlo.convert %1740 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc917)
    %1742 = stablehlo.dot_general %1729, %1741, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc918)
    %1743 = stablehlo.convert %1742 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc919)
    %1744 = stablehlo.transpose %1743, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc920)
    %1745 = stablehlo.reshape %1744 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc921)
    %1746 = stablehlo.reshape %arg188 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1747 = stablehlo.reshape %1746 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1748 = stablehlo.transpose %1747, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc922)
    %1749 = stablehlo.dot_general %1745, %1748, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc923)
    %1750 = stablehlo.reshape %1749 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc921)
    %1751 = stablehlo.reshape %arg187 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1752 = stablehlo.reshape %1751 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1753 = stablehlo.broadcast_in_dim %1752, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc924)
    %1754 = stablehlo.add %1750, %1753 : tensor<1x257x1280xbf16> loc(#loc924)
    %1755 = stablehlo.add %1679, %1754 : tensor<1x257x1280xbf16> loc(#loc925)
    %1756 = stablehlo.reshape %arg186 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1757 = stablehlo.reshape %1756 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1758 = stablehlo.reshape %arg185 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1759 = stablehlo.reshape %1758 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1760 = stablehlo.composite "tenstorrent.layer_norm" %1755, %1757, %1759 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_50} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc926)
    %1761 = stablehlo.reshape %1760 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc927)
    %1762 = stablehlo.reshape %arg184 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %1763 = stablehlo.reshape %1762 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %1764 = stablehlo.transpose %1763, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc928)
    %1765 = stablehlo.dot_general %1761, %1764, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc929)
    %1766 = stablehlo.reshape %1765 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc927)
    %1767 = stablehlo.reshape %arg183 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %1768 = stablehlo.reshape %1767 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
    %1769 = stablehlo.broadcast_in_dim %1768, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc930)
    %1770 = stablehlo.add %1766, %1769 : tensor<1x257x5120xbf16> loc(#loc930)
    %1771 = stablehlo.composite "tenstorrent.gelu" %1770 {decomposition = @tenstorrent.gelu.impl_32} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc931)
    %1772 = stablehlo.reshape %1771 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc932)
    %1773 = stablehlo.reshape %arg182 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %1774 = stablehlo.reshape %1773 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %1775 = stablehlo.transpose %1774, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc933)
    %1776 = stablehlo.dot_general %1772, %1775, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc934)
    %1777 = stablehlo.reshape %1776 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc932)
    %1778 = stablehlo.reshape %arg181 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1779 = stablehlo.reshape %1778 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1780 = stablehlo.broadcast_in_dim %1779, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc935)
    %1781 = stablehlo.add %1777, %1780 : tensor<1x257x1280xbf16> loc(#loc935)
    %1782 = stablehlo.add %1755, %1781 : tensor<1x257x1280xbf16> loc(#loc936)
    %1783 = stablehlo.reshape %arg180 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1784 = stablehlo.reshape %1783 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1785 = stablehlo.reshape %arg179 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1786 = stablehlo.reshape %1785 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1787 = stablehlo.composite "tenstorrent.layer_norm" %1782, %1784, %1786 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_30} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc937)
    %1788 = stablehlo.reshape %1787 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc938)
    %1789 = stablehlo.reshape %arg463 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1790 = stablehlo.reshape %1789 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1791 = stablehlo.transpose %1790, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc939)
    %1792 = stablehlo.dot_general %1788, %1791, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc940)
    %1793 = stablehlo.reshape %1792 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc938)
    %1794 = stablehlo.reshape %arg462 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1795 = stablehlo.reshape %1794 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1796 = stablehlo.broadcast_in_dim %1795, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc941)
    %1797 = stablehlo.add %1793, %1796 : tensor<1x257x1280xbf16> loc(#loc941)
    %1798 = stablehlo.reshape %1797 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc942)
    %1799 = stablehlo.transpose %1798, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc943)
    %1800 = stablehlo.convert %1799 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc944)
    %1801 = stablehlo.multiply %1800, %cst_6 : tensor<1x16x257x80xf32> loc(#loc945)
    %1802 = stablehlo.reshape %arg461 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1803 = stablehlo.reshape %1802 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1804 = stablehlo.transpose %1803, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc946)
    %1805 = stablehlo.dot_general %1788, %1804, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc947)
    %1806 = stablehlo.reshape %1805 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc948)
    %1807 = stablehlo.reshape %arg460 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1808 = stablehlo.reshape %1807 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1809 = stablehlo.broadcast_in_dim %1808, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc949)
    %1810 = stablehlo.add %1806, %1809 : tensor<1x257x1280xbf16> loc(#loc949)
    %1811 = stablehlo.reshape %1810 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc950)
    %1812 = stablehlo.transpose %1811, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc951)
    %1813 = stablehlo.convert %1812 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc952)
    %1814 = stablehlo.transpose %1813, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc953)
    %1815 = stablehlo.multiply %1814, %cst_5 : tensor<1x16x80x257xf32> loc(#loc954)
    %1816 = stablehlo.dot_general %1801, %1815, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc955)
    %1817 = stablehlo.convert %1816 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc956)
    %1818 = stablehlo.compare  EQ, %1817, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc956)
    %1819 = stablehlo.not %1818 : tensor<1x16x257x257xi1> loc(#loc957)
    %1820 = stablehlo.reduce(%1819 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("1874|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_18aten__any"), %arg559: tensor<i1> loc("1874|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_18aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc959)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc960)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc958)
    %1821 = stablehlo.reshape %1820 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc958)
    %1822 = stablehlo.not %1821 : tensor<1x16x257x1xi1> loc(#loc961)
    %1823 = stablehlo.reshape %1822 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc962)
    %1824 = stablehlo.broadcast_in_dim %1823, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc962)
    %1825 = stablehlo.reduce(%1816 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc963)
    %1826 = stablehlo.broadcast_in_dim %1825, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc963)
    %1827 = stablehlo.subtract %1816, %1826 : tensor<1x16x257x257xf32> loc(#loc963)
    %1828 = stablehlo.exponential %1827 : tensor<1x16x257x257xf32> loc(#loc963)
    %1829 = stablehlo.reduce(%1828 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc963)
    %1830 = stablehlo.broadcast_in_dim %1829, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc963)
    %1831 = stablehlo.divide %1828, %1830 : tensor<1x16x257x257xf32> loc(#loc963)
    %1832 = stablehlo.select %1824, %cst_3, %1831 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc964)
    %1833 = stablehlo.reshape %arg178 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1834 = stablehlo.reshape %1833 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1835 = stablehlo.transpose %1834, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc965)
    %1836 = stablehlo.dot_general %1788, %1835, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc966)
    %1837 = stablehlo.reshape %1836 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc967)
    %1838 = stablehlo.reshape %arg177 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1839 = stablehlo.reshape %1838 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1840 = stablehlo.broadcast_in_dim %1839, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc968)
    %1841 = stablehlo.add %1837, %1840 : tensor<1x257x1280xbf16> loc(#loc968)
    %1842 = stablehlo.reshape %1841 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc969)
    %1843 = stablehlo.transpose %1842, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc970)
    %1844 = stablehlo.convert %1843 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc971)
    %1845 = stablehlo.dot_general %1832, %1844, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc972)
    %1846 = stablehlo.convert %1845 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc973)
    %1847 = stablehlo.transpose %1846, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc974)
    %1848 = stablehlo.reshape %1847 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc975)
    %1849 = stablehlo.reshape %arg176 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1850 = stablehlo.reshape %1849 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1851 = stablehlo.transpose %1850, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc976)
    %1852 = stablehlo.dot_general %1848, %1851, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc977)
    %1853 = stablehlo.reshape %1852 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc975)
    %1854 = stablehlo.reshape %arg175 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1855 = stablehlo.reshape %1854 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1856 = stablehlo.broadcast_in_dim %1855, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc978)
    %1857 = stablehlo.add %1853, %1856 : tensor<1x257x1280xbf16> loc(#loc978)
    %1858 = stablehlo.add %1782, %1857 : tensor<1x257x1280xbf16> loc(#loc979)
    %1859 = stablehlo.reshape %arg174 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1860 = stablehlo.reshape %1859 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1861 = stablehlo.reshape %arg173 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1862 = stablehlo.reshape %1861 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1863 = stablehlo.composite "tenstorrent.layer_norm" %1858, %1860, %1862 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_70} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc980)
    %1864 = stablehlo.reshape %1863 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc981)
    %1865 = stablehlo.reshape %arg172 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %1866 = stablehlo.reshape %1865 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %1867 = stablehlo.transpose %1866, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc982)
    %1868 = stablehlo.dot_general %1864, %1867, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc983)
    %1869 = stablehlo.reshape %1868 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc981)
    %1870 = stablehlo.reshape %arg171 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %1871 = stablehlo.reshape %1870 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
    %1872 = stablehlo.broadcast_in_dim %1871, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc984)
    %1873 = stablehlo.add %1869, %1872 : tensor<1x257x5120xbf16> loc(#loc984)
    %1874 = stablehlo.composite "tenstorrent.gelu" %1873 {decomposition = @tenstorrent.gelu.impl_33} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc985)
    %1875 = stablehlo.reshape %1874 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc986)
    %1876 = stablehlo.reshape %arg170 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %1877 = stablehlo.reshape %1876 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %1878 = stablehlo.transpose %1877, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc987)
    %1879 = stablehlo.dot_general %1875, %1878, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc988)
    %1880 = stablehlo.reshape %1879 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc986)
    %1881 = stablehlo.reshape %arg169 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1882 = stablehlo.reshape %1881 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1883 = stablehlo.broadcast_in_dim %1882, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc989)
    %1884 = stablehlo.add %1880, %1883 : tensor<1x257x1280xbf16> loc(#loc989)
    %1885 = stablehlo.add %1858, %1884 : tensor<1x257x1280xbf16> loc(#loc990)
    %1886 = stablehlo.reshape %arg168 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1887 = stablehlo.reshape %1886 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1888 = stablehlo.reshape %arg167 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1889 = stablehlo.reshape %1888 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1890 = stablehlo.composite "tenstorrent.layer_norm" %1885, %1887, %1889 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_74} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc991)
    %1891 = stablehlo.reshape %1890 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc992)
    %1892 = stablehlo.reshape %arg467 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1893 = stablehlo.reshape %1892 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1894 = stablehlo.transpose %1893, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc993)
    %1895 = stablehlo.dot_general %1891, %1894, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc994)
    %1896 = stablehlo.reshape %1895 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc992)
    %1897 = stablehlo.reshape %arg466 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1898 = stablehlo.reshape %1897 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1899 = stablehlo.broadcast_in_dim %1898, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc995)
    %1900 = stablehlo.add %1896, %1899 : tensor<1x257x1280xbf16> loc(#loc995)
    %1901 = stablehlo.reshape %1900 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc996)
    %1902 = stablehlo.transpose %1901, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc997)
    %1903 = stablehlo.convert %1902 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc998)
    %1904 = stablehlo.multiply %1903, %cst_6 : tensor<1x16x257x80xf32> loc(#loc999)
    %1905 = stablehlo.reshape %arg465 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1906 = stablehlo.reshape %1905 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1907 = stablehlo.transpose %1906, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1000)
    %1908 = stablehlo.dot_general %1891, %1907, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1001)
    %1909 = stablehlo.reshape %1908 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1002)
    %1910 = stablehlo.reshape %arg464 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1911 = stablehlo.reshape %1910 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1912 = stablehlo.broadcast_in_dim %1911, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1003)
    %1913 = stablehlo.add %1909, %1912 : tensor<1x257x1280xbf16> loc(#loc1003)
    %1914 = stablehlo.reshape %1913 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1004)
    %1915 = stablehlo.transpose %1914, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1005)
    %1916 = stablehlo.convert %1915 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1006)
    %1917 = stablehlo.transpose %1916, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1007)
    %1918 = stablehlo.multiply %1917, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1008)
    %1919 = stablehlo.dot_general %1904, %1918, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1009)
    %1920 = stablehlo.convert %1919 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1010)
    %1921 = stablehlo.compare  EQ, %1920, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1010)
    %1922 = stablehlo.not %1921 : tensor<1x16x257x257xi1> loc(#loc1011)
    %1923 = stablehlo.reduce(%1922 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("1948|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_19aten__any"), %arg559: tensor<i1> loc("1948|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_19aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1013)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1014)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc1012)
    %1924 = stablehlo.reshape %1923 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1012)
    %1925 = stablehlo.not %1924 : tensor<1x16x257x1xi1> loc(#loc1015)
    %1926 = stablehlo.reshape %1925 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1016)
    %1927 = stablehlo.broadcast_in_dim %1926, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1016)
    %1928 = stablehlo.reduce(%1919 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1017)
    %1929 = stablehlo.broadcast_in_dim %1928, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1017)
    %1930 = stablehlo.subtract %1919, %1929 : tensor<1x16x257x257xf32> loc(#loc1017)
    %1931 = stablehlo.exponential %1930 : tensor<1x16x257x257xf32> loc(#loc1017)
    %1932 = stablehlo.reduce(%1931 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1017)
    %1933 = stablehlo.broadcast_in_dim %1932, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1017)
    %1934 = stablehlo.divide %1931, %1933 : tensor<1x16x257x257xf32> loc(#loc1017)
    %1935 = stablehlo.select %1927, %cst_3, %1934 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1018)
    %1936 = stablehlo.reshape %arg166 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1937 = stablehlo.reshape %1936 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1938 = stablehlo.transpose %1937, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1019)
    %1939 = stablehlo.dot_general %1891, %1938, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1020)
    %1940 = stablehlo.reshape %1939 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1021)
    %1941 = stablehlo.reshape %arg165 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1942 = stablehlo.reshape %1941 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1943 = stablehlo.broadcast_in_dim %1942, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1022)
    %1944 = stablehlo.add %1940, %1943 : tensor<1x257x1280xbf16> loc(#loc1022)
    %1945 = stablehlo.reshape %1944 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1023)
    %1946 = stablehlo.transpose %1945, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1024)
    %1947 = stablehlo.convert %1946 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1025)
    %1948 = stablehlo.dot_general %1935, %1947, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1026)
    %1949 = stablehlo.convert %1948 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1027)
    %1950 = stablehlo.transpose %1949, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1028)
    %1951 = stablehlo.reshape %1950 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1029)
    %1952 = stablehlo.reshape %arg164 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1953 = stablehlo.reshape %1952 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1954 = stablehlo.transpose %1953, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1030)
    %1955 = stablehlo.dot_general %1951, %1954, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1031)
    %1956 = stablehlo.reshape %1955 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1029)
    %1957 = stablehlo.reshape %arg163 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1958 = stablehlo.reshape %1957 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1959 = stablehlo.broadcast_in_dim %1958, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1032)
    %1960 = stablehlo.add %1956, %1959 : tensor<1x257x1280xbf16> loc(#loc1032)
    %1961 = stablehlo.add %1885, %1960 : tensor<1x257x1280xbf16> loc(#loc1033)
    %1962 = stablehlo.reshape %arg162 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1963 = stablehlo.reshape %1962 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1964 = stablehlo.reshape %arg161 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1965 = stablehlo.reshape %1964 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1966 = stablehlo.composite "tenstorrent.layer_norm" %1961, %1963, %1965 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_61} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1034)
    %1967 = stablehlo.reshape %1966 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1035)
    %1968 = stablehlo.reshape %arg160 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %1969 = stablehlo.reshape %1968 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %1970 = stablehlo.transpose %1969, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1036)
    %1971 = stablehlo.dot_general %1967, %1970, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1037)
    %1972 = stablehlo.reshape %1971 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1035)
    %1973 = stablehlo.reshape %arg159 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %1974 = stablehlo.reshape %1973 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
    %1975 = stablehlo.broadcast_in_dim %1974, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1038)
    %1976 = stablehlo.add %1972, %1975 : tensor<1x257x5120xbf16> loc(#loc1038)
    %1977 = stablehlo.composite "tenstorrent.gelu" %1976 {decomposition = @tenstorrent.gelu.impl_23} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1039)
    %1978 = stablehlo.reshape %1977 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1040)
    %1979 = stablehlo.reshape %arg158 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %1980 = stablehlo.reshape %1979 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %1981 = stablehlo.transpose %1980, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1041)
    %1982 = stablehlo.dot_general %1978, %1981, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1042)
    %1983 = stablehlo.reshape %1982 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1040)
    %1984 = stablehlo.reshape %arg157 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1985 = stablehlo.reshape %1984 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1986 = stablehlo.broadcast_in_dim %1985, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1043)
    %1987 = stablehlo.add %1983, %1986 : tensor<1x257x1280xbf16> loc(#loc1043)
    %1988 = stablehlo.add %1961, %1987 : tensor<1x257x1280xbf16> loc(#loc1044)
    %1989 = stablehlo.reshape %arg156 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1990 = stablehlo.reshape %1989 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1991 = stablehlo.reshape %arg155 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %1992 = stablehlo.reshape %1991 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %1993 = stablehlo.composite "tenstorrent.layer_norm" %1988, %1990, %1992 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_20} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1045)
    %1994 = stablehlo.reshape %1993 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1046)
    %1995 = stablehlo.reshape %arg471 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %1996 = stablehlo.reshape %1995 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %1997 = stablehlo.transpose %1996, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1047)
    %1998 = stablehlo.dot_general %1994, %1997, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1048)
    %1999 = stablehlo.reshape %1998 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1046)
    %2000 = stablehlo.reshape %arg470 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2001 = stablehlo.reshape %2000 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2002 = stablehlo.broadcast_in_dim %2001, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1049)
    %2003 = stablehlo.add %1999, %2002 : tensor<1x257x1280xbf16> loc(#loc1049)
    %2004 = stablehlo.reshape %2003 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1050)
    %2005 = stablehlo.transpose %2004, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1051)
    %2006 = stablehlo.convert %2005 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1052)
    %2007 = stablehlo.multiply %2006, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1053)
    %2008 = stablehlo.reshape %arg469 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2009 = stablehlo.reshape %2008 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2010 = stablehlo.transpose %2009, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1054)
    %2011 = stablehlo.dot_general %1994, %2010, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1055)
    %2012 = stablehlo.reshape %2011 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1056)
    %2013 = stablehlo.reshape %arg468 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2014 = stablehlo.reshape %2013 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2015 = stablehlo.broadcast_in_dim %2014, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1057)
    %2016 = stablehlo.add %2012, %2015 : tensor<1x257x1280xbf16> loc(#loc1057)
    %2017 = stablehlo.reshape %2016 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1058)
    %2018 = stablehlo.transpose %2017, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1059)
    %2019 = stablehlo.convert %2018 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1060)
    %2020 = stablehlo.transpose %2019, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1061)
    %2021 = stablehlo.multiply %2020, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1062)
    %2022 = stablehlo.dot_general %2007, %2021, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1063)
    %2023 = stablehlo.convert %2022 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1064)
    %2024 = stablehlo.compare  EQ, %2023, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1064)
    %2025 = stablehlo.not %2024 : tensor<1x16x257x257xi1> loc(#loc1065)
    %2026 = stablehlo.reduce(%2025 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("2022|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_20aten__any"), %arg559: tensor<i1> loc("2022|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_20aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1067)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1068)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc1066)
    %2027 = stablehlo.reshape %2026 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1066)
    %2028 = stablehlo.not %2027 : tensor<1x16x257x1xi1> loc(#loc1069)
    %2029 = stablehlo.reshape %2028 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1070)
    %2030 = stablehlo.broadcast_in_dim %2029, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1070)
    %2031 = stablehlo.reduce(%2022 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1071)
    %2032 = stablehlo.broadcast_in_dim %2031, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1071)
    %2033 = stablehlo.subtract %2022, %2032 : tensor<1x16x257x257xf32> loc(#loc1071)
    %2034 = stablehlo.exponential %2033 : tensor<1x16x257x257xf32> loc(#loc1071)
    %2035 = stablehlo.reduce(%2034 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1071)
    %2036 = stablehlo.broadcast_in_dim %2035, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1071)
    %2037 = stablehlo.divide %2034, %2036 : tensor<1x16x257x257xf32> loc(#loc1071)
    %2038 = stablehlo.select %2030, %cst_3, %2037 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1072)
    %2039 = stablehlo.reshape %arg154 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2040 = stablehlo.reshape %2039 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2041 = stablehlo.transpose %2040, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1073)
    %2042 = stablehlo.dot_general %1994, %2041, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1074)
    %2043 = stablehlo.reshape %2042 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1075)
    %2044 = stablehlo.reshape %arg153 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2045 = stablehlo.reshape %2044 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2046 = stablehlo.broadcast_in_dim %2045, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1076)
    %2047 = stablehlo.add %2043, %2046 : tensor<1x257x1280xbf16> loc(#loc1076)
    %2048 = stablehlo.reshape %2047 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1077)
    %2049 = stablehlo.transpose %2048, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1078)
    %2050 = stablehlo.convert %2049 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1079)
    %2051 = stablehlo.dot_general %2038, %2050, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1080)
    %2052 = stablehlo.convert %2051 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1081)
    %2053 = stablehlo.transpose %2052, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1082)
    %2054 = stablehlo.reshape %2053 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1083)
    %2055 = stablehlo.reshape %arg152 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2056 = stablehlo.reshape %2055 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2057 = stablehlo.transpose %2056, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1084)
    %2058 = stablehlo.dot_general %2054, %2057, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1085)
    %2059 = stablehlo.reshape %2058 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1083)
    %2060 = stablehlo.reshape %arg151 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2061 = stablehlo.reshape %2060 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2062 = stablehlo.broadcast_in_dim %2061, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1086)
    %2063 = stablehlo.add %2059, %2062 : tensor<1x257x1280xbf16> loc(#loc1086)
    %2064 = stablehlo.add %1988, %2063 : tensor<1x257x1280xbf16> loc(#loc1087)
    %2065 = stablehlo.reshape %arg150 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2066 = stablehlo.reshape %2065 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2067 = stablehlo.reshape %arg149 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2068 = stablehlo.reshape %2067 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2069 = stablehlo.composite "tenstorrent.layer_norm" %2064, %2066, %2068 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_51} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1088)
    %2070 = stablehlo.reshape %2069 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1089)
    %2071 = stablehlo.reshape %arg148 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %2072 = stablehlo.reshape %2071 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %2073 = stablehlo.transpose %2072, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1090)
    %2074 = stablehlo.dot_general %2070, %2073, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1091)
    %2075 = stablehlo.reshape %2074 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1089)
    %2076 = stablehlo.reshape %arg147 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %2077 = stablehlo.reshape %2076 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
    %2078 = stablehlo.broadcast_in_dim %2077, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1092)
    %2079 = stablehlo.add %2075, %2078 : tensor<1x257x5120xbf16> loc(#loc1092)
    %2080 = stablehlo.composite "tenstorrent.gelu" %2079 {decomposition = @tenstorrent.gelu.impl_10} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1093)
    %2081 = stablehlo.reshape %2080 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1094)
    %2082 = stablehlo.reshape %arg146 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %2083 = stablehlo.reshape %2082 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %2084 = stablehlo.transpose %2083, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1095)
    %2085 = stablehlo.dot_general %2081, %2084, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1096)
    %2086 = stablehlo.reshape %2085 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1094)
    %2087 = stablehlo.reshape %arg145 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2088 = stablehlo.reshape %2087 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2089 = stablehlo.broadcast_in_dim %2088, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1097)
    %2090 = stablehlo.add %2086, %2089 : tensor<1x257x1280xbf16> loc(#loc1097)
    %2091 = stablehlo.add %2064, %2090 : tensor<1x257x1280xbf16> loc(#loc1098)
    %2092 = stablehlo.reshape %arg144 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2093 = stablehlo.reshape %2092 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2094 = stablehlo.reshape %arg143 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2095 = stablehlo.reshape %2094 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2096 = stablehlo.composite "tenstorrent.layer_norm" %2091, %2093, %2095 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_19} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1099)
    %2097 = stablehlo.reshape %2096 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1100)
    %2098 = stablehlo.reshape %arg475 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2099 = stablehlo.reshape %2098 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2100 = stablehlo.transpose %2099, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1101)
    %2101 = stablehlo.dot_general %2097, %2100, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1102)
    %2102 = stablehlo.reshape %2101 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1100)
    %2103 = stablehlo.reshape %arg474 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2104 = stablehlo.reshape %2103 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2105 = stablehlo.broadcast_in_dim %2104, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1103)
    %2106 = stablehlo.add %2102, %2105 : tensor<1x257x1280xbf16> loc(#loc1103)
    %2107 = stablehlo.reshape %2106 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1104)
    %2108 = stablehlo.transpose %2107, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1105)
    %2109 = stablehlo.convert %2108 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1106)
    %2110 = stablehlo.multiply %2109, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1107)
    %2111 = stablehlo.reshape %arg473 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2112 = stablehlo.reshape %2111 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2113 = stablehlo.transpose %2112, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1108)
    %2114 = stablehlo.dot_general %2097, %2113, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1109)
    %2115 = stablehlo.reshape %2114 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1110)
    %2116 = stablehlo.reshape %arg472 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2117 = stablehlo.reshape %2116 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2118 = stablehlo.broadcast_in_dim %2117, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1111)
    %2119 = stablehlo.add %2115, %2118 : tensor<1x257x1280xbf16> loc(#loc1111)
    %2120 = stablehlo.reshape %2119 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1112)
    %2121 = stablehlo.transpose %2120, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1113)
    %2122 = stablehlo.convert %2121 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1114)
    %2123 = stablehlo.transpose %2122, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1115)
    %2124 = stablehlo.multiply %2123, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1116)
    %2125 = stablehlo.dot_general %2110, %2124, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1117)
    %2126 = stablehlo.convert %2125 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1118)
    %2127 = stablehlo.compare  EQ, %2126, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1118)
    %2128 = stablehlo.not %2127 : tensor<1x16x257x257xi1> loc(#loc1119)
    %2129 = stablehlo.reduce(%2128 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("2096|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_21aten__any"), %arg559: tensor<i1> loc("2096|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_21aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1121)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1122)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc1120)
    %2130 = stablehlo.reshape %2129 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1120)
    %2131 = stablehlo.not %2130 : tensor<1x16x257x1xi1> loc(#loc1123)
    %2132 = stablehlo.reshape %2131 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1124)
    %2133 = stablehlo.broadcast_in_dim %2132, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1124)
    %2134 = stablehlo.reduce(%2125 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1125)
    %2135 = stablehlo.broadcast_in_dim %2134, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1125)
    %2136 = stablehlo.subtract %2125, %2135 : tensor<1x16x257x257xf32> loc(#loc1125)
    %2137 = stablehlo.exponential %2136 : tensor<1x16x257x257xf32> loc(#loc1125)
    %2138 = stablehlo.reduce(%2137 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1125)
    %2139 = stablehlo.broadcast_in_dim %2138, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1125)
    %2140 = stablehlo.divide %2137, %2139 : tensor<1x16x257x257xf32> loc(#loc1125)
    %2141 = stablehlo.select %2133, %cst_3, %2140 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1126)
    %2142 = stablehlo.reshape %arg142 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2143 = stablehlo.reshape %2142 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2144 = stablehlo.transpose %2143, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1127)
    %2145 = stablehlo.dot_general %2097, %2144, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1128)
    %2146 = stablehlo.reshape %2145 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1129)
    %2147 = stablehlo.reshape %arg141 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2148 = stablehlo.reshape %2147 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2149 = stablehlo.broadcast_in_dim %2148, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1130)
    %2150 = stablehlo.add %2146, %2149 : tensor<1x257x1280xbf16> loc(#loc1130)
    %2151 = stablehlo.reshape %2150 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1131)
    %2152 = stablehlo.transpose %2151, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1132)
    %2153 = stablehlo.convert %2152 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1133)
    %2154 = stablehlo.dot_general %2141, %2153, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1134)
    %2155 = stablehlo.convert %2154 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1135)
    %2156 = stablehlo.transpose %2155, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1136)
    %2157 = stablehlo.reshape %2156 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1137)
    %2158 = stablehlo.reshape %arg140 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2159 = stablehlo.reshape %2158 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2160 = stablehlo.transpose %2159, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1138)
    %2161 = stablehlo.dot_general %2157, %2160, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1139)
    %2162 = stablehlo.reshape %2161 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1137)
    %2163 = stablehlo.reshape %arg139 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2164 = stablehlo.reshape %2163 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2165 = stablehlo.broadcast_in_dim %2164, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1140)
    %2166 = stablehlo.add %2162, %2165 : tensor<1x257x1280xbf16> loc(#loc1140)
    %2167 = stablehlo.add %2091, %2166 : tensor<1x257x1280xbf16> loc(#loc1141)
    %2168 = stablehlo.reshape %arg138 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2169 = stablehlo.reshape %2168 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2170 = stablehlo.reshape %arg137 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2171 = stablehlo.reshape %2170 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2172 = stablehlo.composite "tenstorrent.layer_norm" %2167, %2169, %2171 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_47} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1142)
    %2173 = stablehlo.reshape %2172 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1143)
    %2174 = stablehlo.reshape %arg136 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %2175 = stablehlo.reshape %2174 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %2176 = stablehlo.transpose %2175, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1144)
    %2177 = stablehlo.dot_general %2173, %2176, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1145)
    %2178 = stablehlo.reshape %2177 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1143)
    %2179 = stablehlo.reshape %arg135 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %2180 = stablehlo.reshape %2179 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
    %2181 = stablehlo.broadcast_in_dim %2180, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1146)
    %2182 = stablehlo.add %2178, %2181 : tensor<1x257x5120xbf16> loc(#loc1146)
    %2183 = stablehlo.composite "tenstorrent.gelu" %2182 {decomposition = @tenstorrent.gelu.impl_8} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1147)
    %2184 = stablehlo.reshape %2183 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1148)
    %2185 = stablehlo.reshape %arg134 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %2186 = stablehlo.reshape %2185 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %2187 = stablehlo.transpose %2186, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1149)
    %2188 = stablehlo.dot_general %2184, %2187, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1150)
    %2189 = stablehlo.reshape %2188 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1148)
    %2190 = stablehlo.reshape %arg133 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2191 = stablehlo.reshape %2190 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2192 = stablehlo.broadcast_in_dim %2191, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1151)
    %2193 = stablehlo.add %2189, %2192 : tensor<1x257x1280xbf16> loc(#loc1151)
    %2194 = stablehlo.add %2167, %2193 : tensor<1x257x1280xbf16> loc(#loc1152)
    %2195 = stablehlo.reshape %arg132 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2196 = stablehlo.reshape %2195 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2197 = stablehlo.reshape %arg131 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2198 = stablehlo.reshape %2197 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2199 = stablehlo.composite "tenstorrent.layer_norm" %2194, %2196, %2198 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_55} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1153)
    %2200 = stablehlo.reshape %2199 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1154)
    %2201 = stablehlo.reshape %arg479 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2202 = stablehlo.reshape %2201 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2203 = stablehlo.transpose %2202, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1155)
    %2204 = stablehlo.dot_general %2200, %2203, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1156)
    %2205 = stablehlo.reshape %2204 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1154)
    %2206 = stablehlo.reshape %arg478 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2207 = stablehlo.reshape %2206 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2208 = stablehlo.broadcast_in_dim %2207, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1157)
    %2209 = stablehlo.add %2205, %2208 : tensor<1x257x1280xbf16> loc(#loc1157)
    %2210 = stablehlo.reshape %2209 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1158)
    %2211 = stablehlo.transpose %2210, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1159)
    %2212 = stablehlo.convert %2211 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1160)
    %2213 = stablehlo.multiply %2212, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1161)
    %2214 = stablehlo.reshape %arg477 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2215 = stablehlo.reshape %2214 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2216 = stablehlo.transpose %2215, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1162)
    %2217 = stablehlo.dot_general %2200, %2216, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1163)
    %2218 = stablehlo.reshape %2217 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1164)
    %2219 = stablehlo.reshape %arg476 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2220 = stablehlo.reshape %2219 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2221 = stablehlo.broadcast_in_dim %2220, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1165)
    %2222 = stablehlo.add %2218, %2221 : tensor<1x257x1280xbf16> loc(#loc1165)
    %2223 = stablehlo.reshape %2222 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1166)
    %2224 = stablehlo.transpose %2223, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1167)
    %2225 = stablehlo.convert %2224 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1168)
    %2226 = stablehlo.transpose %2225, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1169)
    %2227 = stablehlo.multiply %2226, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1170)
    %2228 = stablehlo.dot_general %2213, %2227, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1171)
    %2229 = stablehlo.convert %2228 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1172)
    %2230 = stablehlo.compare  EQ, %2229, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1172)
    %2231 = stablehlo.not %2230 : tensor<1x16x257x257xi1> loc(#loc1173)
    %2232 = stablehlo.reduce(%2231 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("2170|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_22aten__any"), %arg559: tensor<i1> loc("2170|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_22aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1175)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1176)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc1174)
    %2233 = stablehlo.reshape %2232 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1174)
    %2234 = stablehlo.not %2233 : tensor<1x16x257x1xi1> loc(#loc1177)
    %2235 = stablehlo.reshape %2234 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1178)
    %2236 = stablehlo.broadcast_in_dim %2235, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1178)
    %2237 = stablehlo.reduce(%2228 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1179)
    %2238 = stablehlo.broadcast_in_dim %2237, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1179)
    %2239 = stablehlo.subtract %2228, %2238 : tensor<1x16x257x257xf32> loc(#loc1179)
    %2240 = stablehlo.exponential %2239 : tensor<1x16x257x257xf32> loc(#loc1179)
    %2241 = stablehlo.reduce(%2240 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1179)
    %2242 = stablehlo.broadcast_in_dim %2241, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1179)
    %2243 = stablehlo.divide %2240, %2242 : tensor<1x16x257x257xf32> loc(#loc1179)
    %2244 = stablehlo.select %2236, %cst_3, %2243 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1180)
    %2245 = stablehlo.reshape %arg130 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2246 = stablehlo.reshape %2245 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2247 = stablehlo.transpose %2246, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1181)
    %2248 = stablehlo.dot_general %2200, %2247, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1182)
    %2249 = stablehlo.reshape %2248 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1183)
    %2250 = stablehlo.reshape %arg129 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2251 = stablehlo.reshape %2250 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2252 = stablehlo.broadcast_in_dim %2251, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1184)
    %2253 = stablehlo.add %2249, %2252 : tensor<1x257x1280xbf16> loc(#loc1184)
    %2254 = stablehlo.reshape %2253 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1185)
    %2255 = stablehlo.transpose %2254, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1186)
    %2256 = stablehlo.convert %2255 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1187)
    %2257 = stablehlo.dot_general %2244, %2256, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1188)
    %2258 = stablehlo.convert %2257 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1189)
    %2259 = stablehlo.transpose %2258, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1190)
    %2260 = stablehlo.reshape %2259 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1191)
    %2261 = stablehlo.reshape %arg128 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2262 = stablehlo.reshape %2261 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2263 = stablehlo.transpose %2262, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1192)
    %2264 = stablehlo.dot_general %2260, %2263, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1193)
    %2265 = stablehlo.reshape %2264 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1191)
    %2266 = stablehlo.reshape %arg127 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2267 = stablehlo.reshape %2266 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2268 = stablehlo.broadcast_in_dim %2267, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1194)
    %2269 = stablehlo.add %2265, %2268 : tensor<1x257x1280xbf16> loc(#loc1194)
    %2270 = stablehlo.add %2194, %2269 : tensor<1x257x1280xbf16> loc(#loc1195)
    %2271 = stablehlo.reshape %arg126 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2272 = stablehlo.reshape %2271 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2273 = stablehlo.reshape %arg125 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2274 = stablehlo.reshape %2273 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2275 = stablehlo.composite "tenstorrent.layer_norm" %2270, %2272, %2274 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_18} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1196)
    %2276 = stablehlo.reshape %2275 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1197)
    %2277 = stablehlo.reshape %arg124 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %2278 = stablehlo.reshape %2277 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %2279 = stablehlo.transpose %2278, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1198)
    %2280 = stablehlo.dot_general %2276, %2279, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1199)
    %2281 = stablehlo.reshape %2280 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1197)
    %2282 = stablehlo.reshape %arg123 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %2283 = stablehlo.reshape %2282 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
    %2284 = stablehlo.broadcast_in_dim %2283, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1200)
    %2285 = stablehlo.add %2281, %2284 : tensor<1x257x5120xbf16> loc(#loc1200)
    %2286 = stablehlo.composite "tenstorrent.gelu" %2285 {decomposition = @tenstorrent.gelu.impl_7} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1201)
    %2287 = stablehlo.reshape %2286 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1202)
    %2288 = stablehlo.reshape %arg122 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %2289 = stablehlo.reshape %2288 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %2290 = stablehlo.transpose %2289, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1203)
    %2291 = stablehlo.dot_general %2287, %2290, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1204)
    %2292 = stablehlo.reshape %2291 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1202)
    %2293 = stablehlo.reshape %arg121 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2294 = stablehlo.reshape %2293 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2295 = stablehlo.broadcast_in_dim %2294, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1205)
    %2296 = stablehlo.add %2292, %2295 : tensor<1x257x1280xbf16> loc(#loc1205)
    %2297 = stablehlo.add %2270, %2296 : tensor<1x257x1280xbf16> loc(#loc1206)
    %2298 = stablehlo.reshape %arg120 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2299 = stablehlo.reshape %2298 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2300 = stablehlo.reshape %arg119 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2301 = stablehlo.reshape %2300 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2302 = stablehlo.composite "tenstorrent.layer_norm" %2297, %2299, %2301 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_16} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1207)
    %2303 = stablehlo.reshape %2302 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1208)
    %2304 = stablehlo.reshape %arg483 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2305 = stablehlo.reshape %2304 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2306 = stablehlo.transpose %2305, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1209)
    %2307 = stablehlo.dot_general %2303, %2306, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1210)
    %2308 = stablehlo.reshape %2307 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1208)
    %2309 = stablehlo.reshape %arg482 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2310 = stablehlo.reshape %2309 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2311 = stablehlo.broadcast_in_dim %2310, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1211)
    %2312 = stablehlo.add %2308, %2311 : tensor<1x257x1280xbf16> loc(#loc1211)
    %2313 = stablehlo.reshape %2312 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1212)
    %2314 = stablehlo.transpose %2313, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1213)
    %2315 = stablehlo.convert %2314 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1214)
    %2316 = stablehlo.multiply %2315, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1215)
    %2317 = stablehlo.reshape %arg481 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2318 = stablehlo.reshape %2317 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2319 = stablehlo.transpose %2318, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1216)
    %2320 = stablehlo.dot_general %2303, %2319, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1217)
    %2321 = stablehlo.reshape %2320 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1218)
    %2322 = stablehlo.reshape %arg480 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2323 = stablehlo.reshape %2322 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2324 = stablehlo.broadcast_in_dim %2323, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1219)
    %2325 = stablehlo.add %2321, %2324 : tensor<1x257x1280xbf16> loc(#loc1219)
    %2326 = stablehlo.reshape %2325 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1220)
    %2327 = stablehlo.transpose %2326, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1221)
    %2328 = stablehlo.convert %2327 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1222)
    %2329 = stablehlo.transpose %2328, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1223)
    %2330 = stablehlo.multiply %2329, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1224)
    %2331 = stablehlo.dot_general %2316, %2330, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1225)
    %2332 = stablehlo.convert %2331 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1226)
    %2333 = stablehlo.compare  EQ, %2332, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1226)
    %2334 = stablehlo.not %2333 : tensor<1x16x257x257xi1> loc(#loc1227)
    %2335 = stablehlo.reduce(%2334 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("2244|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_23aten__any"), %arg559: tensor<i1> loc("2244|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_23aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1229)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1230)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc1228)
    %2336 = stablehlo.reshape %2335 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1228)
    %2337 = stablehlo.not %2336 : tensor<1x16x257x1xi1> loc(#loc1231)
    %2338 = stablehlo.reshape %2337 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1232)
    %2339 = stablehlo.broadcast_in_dim %2338, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1232)
    %2340 = stablehlo.reduce(%2331 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1233)
    %2341 = stablehlo.broadcast_in_dim %2340, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1233)
    %2342 = stablehlo.subtract %2331, %2341 : tensor<1x16x257x257xf32> loc(#loc1233)
    %2343 = stablehlo.exponential %2342 : tensor<1x16x257x257xf32> loc(#loc1233)
    %2344 = stablehlo.reduce(%2343 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1233)
    %2345 = stablehlo.broadcast_in_dim %2344, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1233)
    %2346 = stablehlo.divide %2343, %2345 : tensor<1x16x257x257xf32> loc(#loc1233)
    %2347 = stablehlo.select %2339, %cst_3, %2346 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1234)
    %2348 = stablehlo.reshape %arg118 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2349 = stablehlo.reshape %2348 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2350 = stablehlo.transpose %2349, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1235)
    %2351 = stablehlo.dot_general %2303, %2350, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1236)
    %2352 = stablehlo.reshape %2351 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1237)
    %2353 = stablehlo.reshape %arg117 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2354 = stablehlo.reshape %2353 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2355 = stablehlo.broadcast_in_dim %2354, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1238)
    %2356 = stablehlo.add %2352, %2355 : tensor<1x257x1280xbf16> loc(#loc1238)
    %2357 = stablehlo.reshape %2356 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1239)
    %2358 = stablehlo.transpose %2357, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1240)
    %2359 = stablehlo.convert %2358 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1241)
    %2360 = stablehlo.dot_general %2347, %2359, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1242)
    %2361 = stablehlo.convert %2360 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1243)
    %2362 = stablehlo.transpose %2361, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1244)
    %2363 = stablehlo.reshape %2362 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1245)
    %2364 = stablehlo.reshape %arg116 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2365 = stablehlo.reshape %2364 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2366 = stablehlo.transpose %2365, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1246)
    %2367 = stablehlo.dot_general %2363, %2366, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1247)
    %2368 = stablehlo.reshape %2367 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1245)
    %2369 = stablehlo.reshape %arg115 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2370 = stablehlo.reshape %2369 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2371 = stablehlo.broadcast_in_dim %2370, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1248)
    %2372 = stablehlo.add %2368, %2371 : tensor<1x257x1280xbf16> loc(#loc1248)
    %2373 = stablehlo.add %2297, %2372 : tensor<1x257x1280xbf16> loc(#loc1249)
    %2374 = stablehlo.reshape %arg114 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2375 = stablehlo.reshape %2374 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2376 = stablehlo.reshape %arg113 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2377 = stablehlo.reshape %2376 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2378 = stablehlo.composite "tenstorrent.layer_norm" %2373, %2375, %2377 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_24} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1250)
    %2379 = stablehlo.reshape %2378 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1251)
    %2380 = stablehlo.reshape %arg112 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %2381 = stablehlo.reshape %2380 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %2382 = stablehlo.transpose %2381, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1252)
    %2383 = stablehlo.dot_general %2379, %2382, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1253)
    %2384 = stablehlo.reshape %2383 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1251)
    %2385 = stablehlo.reshape %arg111 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %2386 = stablehlo.reshape %2385 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
    %2387 = stablehlo.broadcast_in_dim %2386, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1254)
    %2388 = stablehlo.add %2384, %2387 : tensor<1x257x5120xbf16> loc(#loc1254)
    %2389 = stablehlo.composite "tenstorrent.gelu" %2388 {decomposition = @tenstorrent.gelu.impl_14} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1255)
    %2390 = stablehlo.reshape %2389 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1256)
    %2391 = stablehlo.reshape %arg110 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %2392 = stablehlo.reshape %2391 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %2393 = stablehlo.transpose %2392, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1257)
    %2394 = stablehlo.dot_general %2390, %2393, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1258)
    %2395 = stablehlo.reshape %2394 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1256)
    %2396 = stablehlo.reshape %arg109 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2397 = stablehlo.reshape %2396 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2398 = stablehlo.broadcast_in_dim %2397, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1259)
    %2399 = stablehlo.add %2395, %2398 : tensor<1x257x1280xbf16> loc(#loc1259)
    %2400 = stablehlo.add %2373, %2399 : tensor<1x257x1280xbf16> loc(#loc1260)
    %2401 = stablehlo.reshape %arg108 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2402 = stablehlo.reshape %2401 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2403 = stablehlo.reshape %arg107 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2404 = stablehlo.reshape %2403 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2405 = stablehlo.composite "tenstorrent.layer_norm" %2400, %2402, %2404 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_58} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1261)
    %2406 = stablehlo.reshape %2405 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1262)
    %2407 = stablehlo.reshape %arg487 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2408 = stablehlo.reshape %2407 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2409 = stablehlo.transpose %2408, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1263)
    %2410 = stablehlo.dot_general %2406, %2409, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1264)
    %2411 = stablehlo.reshape %2410 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1262)
    %2412 = stablehlo.reshape %arg486 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2413 = stablehlo.reshape %2412 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2414 = stablehlo.broadcast_in_dim %2413, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1265)
    %2415 = stablehlo.add %2411, %2414 : tensor<1x257x1280xbf16> loc(#loc1265)
    %2416 = stablehlo.reshape %2415 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1266)
    %2417 = stablehlo.transpose %2416, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1267)
    %2418 = stablehlo.convert %2417 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1268)
    %2419 = stablehlo.multiply %2418, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1269)
    %2420 = stablehlo.reshape %arg485 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2421 = stablehlo.reshape %2420 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2422 = stablehlo.transpose %2421, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1270)
    %2423 = stablehlo.dot_general %2406, %2422, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1271)
    %2424 = stablehlo.reshape %2423 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1272)
    %2425 = stablehlo.reshape %arg484 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2426 = stablehlo.reshape %2425 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2427 = stablehlo.broadcast_in_dim %2426, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1273)
    %2428 = stablehlo.add %2424, %2427 : tensor<1x257x1280xbf16> loc(#loc1273)
    %2429 = stablehlo.reshape %2428 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1274)
    %2430 = stablehlo.transpose %2429, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1275)
    %2431 = stablehlo.convert %2430 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1276)
    %2432 = stablehlo.transpose %2431, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1277)
    %2433 = stablehlo.multiply %2432, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1278)
    %2434 = stablehlo.dot_general %2419, %2433, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1279)
    %2435 = stablehlo.convert %2434 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1280)
    %2436 = stablehlo.compare  EQ, %2435, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1280)
    %2437 = stablehlo.not %2436 : tensor<1x16x257x257xi1> loc(#loc1281)
    %2438 = stablehlo.reduce(%2437 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("2318|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_24aten__any"), %arg559: tensor<i1> loc("2318|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_24aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1283)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1284)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc1282)
    %2439 = stablehlo.reshape %2438 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1282)
    %2440 = stablehlo.not %2439 : tensor<1x16x257x1xi1> loc(#loc1285)
    %2441 = stablehlo.reshape %2440 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1286)
    %2442 = stablehlo.broadcast_in_dim %2441, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1286)
    %2443 = stablehlo.reduce(%2434 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1287)
    %2444 = stablehlo.broadcast_in_dim %2443, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1287)
    %2445 = stablehlo.subtract %2434, %2444 : tensor<1x16x257x257xf32> loc(#loc1287)
    %2446 = stablehlo.exponential %2445 : tensor<1x16x257x257xf32> loc(#loc1287)
    %2447 = stablehlo.reduce(%2446 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1287)
    %2448 = stablehlo.broadcast_in_dim %2447, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1287)
    %2449 = stablehlo.divide %2446, %2448 : tensor<1x16x257x257xf32> loc(#loc1287)
    %2450 = stablehlo.select %2442, %cst_3, %2449 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1288)
    %2451 = stablehlo.reshape %arg106 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2452 = stablehlo.reshape %2451 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2453 = stablehlo.transpose %2452, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1289)
    %2454 = stablehlo.dot_general %2406, %2453, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1290)
    %2455 = stablehlo.reshape %2454 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1291)
    %2456 = stablehlo.reshape %arg105 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2457 = stablehlo.reshape %2456 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2458 = stablehlo.broadcast_in_dim %2457, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1292)
    %2459 = stablehlo.add %2455, %2458 : tensor<1x257x1280xbf16> loc(#loc1292)
    %2460 = stablehlo.reshape %2459 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1293)
    %2461 = stablehlo.transpose %2460, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1294)
    %2462 = stablehlo.convert %2461 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1295)
    %2463 = stablehlo.dot_general %2450, %2462, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1296)
    %2464 = stablehlo.convert %2463 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1297)
    %2465 = stablehlo.transpose %2464, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1298)
    %2466 = stablehlo.reshape %2465 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1299)
    %2467 = stablehlo.reshape %arg104 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2468 = stablehlo.reshape %2467 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2469 = stablehlo.transpose %2468, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1300)
    %2470 = stablehlo.dot_general %2466, %2469, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1301)
    %2471 = stablehlo.reshape %2470 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1299)
    %2472 = stablehlo.reshape %arg103 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2473 = stablehlo.reshape %2472 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2474 = stablehlo.broadcast_in_dim %2473, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1302)
    %2475 = stablehlo.add %2471, %2474 : tensor<1x257x1280xbf16> loc(#loc1302)
    %2476 = stablehlo.add %2400, %2475 : tensor<1x257x1280xbf16> loc(#loc1303)
    %2477 = stablehlo.reshape %arg102 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2478 = stablehlo.reshape %2477 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2479 = stablehlo.reshape %arg101 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2480 = stablehlo.reshape %2479 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2481 = stablehlo.composite "tenstorrent.layer_norm" %2476, %2478, %2480 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_15} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1304)
    %2482 = stablehlo.reshape %2481 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1305)
    %2483 = stablehlo.reshape %arg100 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %2484 = stablehlo.reshape %2483 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %2485 = stablehlo.transpose %2484, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1306)
    %2486 = stablehlo.dot_general %2482, %2485, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1307)
    %2487 = stablehlo.reshape %2486 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1305)
    %2488 = stablehlo.reshape %arg99 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %2489 = stablehlo.reshape %2488 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
    %2490 = stablehlo.broadcast_in_dim %2489, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1308)
    %2491 = stablehlo.add %2487, %2490 : tensor<1x257x5120xbf16> loc(#loc1308)
    %2492 = stablehlo.composite "tenstorrent.gelu" %2491 {decomposition = @tenstorrent.gelu.impl_6} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1309)
    %2493 = stablehlo.reshape %2492 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1310)
    %2494 = stablehlo.reshape %arg98 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %2495 = stablehlo.reshape %2494 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %2496 = stablehlo.transpose %2495, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1311)
    %2497 = stablehlo.dot_general %2493, %2496, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1312)
    %2498 = stablehlo.reshape %2497 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1310)
    %2499 = stablehlo.reshape %arg97 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2500 = stablehlo.reshape %2499 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2501 = stablehlo.broadcast_in_dim %2500, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1313)
    %2502 = stablehlo.add %2498, %2501 : tensor<1x257x1280xbf16> loc(#loc1313)
    %2503 = stablehlo.add %2476, %2502 : tensor<1x257x1280xbf16> loc(#loc1314)
    %2504 = stablehlo.reshape %arg96 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2505 = stablehlo.reshape %2504 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2506 = stablehlo.reshape %arg95 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2507 = stablehlo.reshape %2506 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2508 = stablehlo.composite "tenstorrent.layer_norm" %2503, %2505, %2507 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_14} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1315)
    %2509 = stablehlo.reshape %2508 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1316)
    %2510 = stablehlo.reshape %arg491 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2511 = stablehlo.reshape %2510 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2512 = stablehlo.transpose %2511, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1317)
    %2513 = stablehlo.dot_general %2509, %2512, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1318)
    %2514 = stablehlo.reshape %2513 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1316)
    %2515 = stablehlo.reshape %arg490 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2516 = stablehlo.reshape %2515 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2517 = stablehlo.broadcast_in_dim %2516, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1319)
    %2518 = stablehlo.add %2514, %2517 : tensor<1x257x1280xbf16> loc(#loc1319)
    %2519 = stablehlo.reshape %2518 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1320)
    %2520 = stablehlo.transpose %2519, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1321)
    %2521 = stablehlo.convert %2520 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1322)
    %2522 = stablehlo.multiply %2521, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1323)
    %2523 = stablehlo.reshape %arg489 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2524 = stablehlo.reshape %2523 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2525 = stablehlo.transpose %2524, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1324)
    %2526 = stablehlo.dot_general %2509, %2525, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1325)
    %2527 = stablehlo.reshape %2526 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1326)
    %2528 = stablehlo.reshape %arg488 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2529 = stablehlo.reshape %2528 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2530 = stablehlo.broadcast_in_dim %2529, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1327)
    %2531 = stablehlo.add %2527, %2530 : tensor<1x257x1280xbf16> loc(#loc1327)
    %2532 = stablehlo.reshape %2531 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1328)
    %2533 = stablehlo.transpose %2532, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1329)
    %2534 = stablehlo.convert %2533 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1330)
    %2535 = stablehlo.transpose %2534, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1331)
    %2536 = stablehlo.multiply %2535, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1332)
    %2537 = stablehlo.dot_general %2522, %2536, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1333)
    %2538 = stablehlo.convert %2537 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1334)
    %2539 = stablehlo.compare  EQ, %2538, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1334)
    %2540 = stablehlo.not %2539 : tensor<1x16x257x257xi1> loc(#loc1335)
    %2541 = stablehlo.reduce(%2540 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("2392|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_25aten__any"), %arg559: tensor<i1> loc("2392|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_25aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1337)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1338)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc1336)
    %2542 = stablehlo.reshape %2541 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1336)
    %2543 = stablehlo.not %2542 : tensor<1x16x257x1xi1> loc(#loc1339)
    %2544 = stablehlo.reshape %2543 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1340)
    %2545 = stablehlo.broadcast_in_dim %2544, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1340)
    %2546 = stablehlo.reduce(%2537 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1341)
    %2547 = stablehlo.broadcast_in_dim %2546, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1341)
    %2548 = stablehlo.subtract %2537, %2547 : tensor<1x16x257x257xf32> loc(#loc1341)
    %2549 = stablehlo.exponential %2548 : tensor<1x16x257x257xf32> loc(#loc1341)
    %2550 = stablehlo.reduce(%2549 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1341)
    %2551 = stablehlo.broadcast_in_dim %2550, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1341)
    %2552 = stablehlo.divide %2549, %2551 : tensor<1x16x257x257xf32> loc(#loc1341)
    %2553 = stablehlo.select %2545, %cst_3, %2552 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1342)
    %2554 = stablehlo.reshape %arg94 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2555 = stablehlo.reshape %2554 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2556 = stablehlo.transpose %2555, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1343)
    %2557 = stablehlo.dot_general %2509, %2556, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1344)
    %2558 = stablehlo.reshape %2557 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1345)
    %2559 = stablehlo.reshape %arg93 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2560 = stablehlo.reshape %2559 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2561 = stablehlo.broadcast_in_dim %2560, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1346)
    %2562 = stablehlo.add %2558, %2561 : tensor<1x257x1280xbf16> loc(#loc1346)
    %2563 = stablehlo.reshape %2562 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1347)
    %2564 = stablehlo.transpose %2563, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1348)
    %2565 = stablehlo.convert %2564 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1349)
    %2566 = stablehlo.dot_general %2553, %2565, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1350)
    %2567 = stablehlo.convert %2566 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1351)
    %2568 = stablehlo.transpose %2567, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1352)
    %2569 = stablehlo.reshape %2568 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1353)
    %2570 = stablehlo.reshape %arg92 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2571 = stablehlo.reshape %2570 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2572 = stablehlo.transpose %2571, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1354)
    %2573 = stablehlo.dot_general %2569, %2572, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1355)
    %2574 = stablehlo.reshape %2573 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1353)
    %2575 = stablehlo.reshape %arg91 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2576 = stablehlo.reshape %2575 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2577 = stablehlo.broadcast_in_dim %2576, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1356)
    %2578 = stablehlo.add %2574, %2577 : tensor<1x257x1280xbf16> loc(#loc1356)
    %2579 = stablehlo.add %2503, %2578 : tensor<1x257x1280xbf16> loc(#loc1357)
    %2580 = stablehlo.reshape %arg90 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2581 = stablehlo.reshape %2580 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2582 = stablehlo.reshape %arg89 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2583 = stablehlo.reshape %2582 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2584 = stablehlo.composite "tenstorrent.layer_norm" %2579, %2581, %2583 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_13} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1358)
    %2585 = stablehlo.reshape %2584 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1359)
    %2586 = stablehlo.reshape %arg88 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %2587 = stablehlo.reshape %2586 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %2588 = stablehlo.transpose %2587, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1360)
    %2589 = stablehlo.dot_general %2585, %2588, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1361)
    %2590 = stablehlo.reshape %2589 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1359)
    %2591 = stablehlo.reshape %arg87 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %2592 = stablehlo.reshape %2591 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
    %2593 = stablehlo.broadcast_in_dim %2592, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1362)
    %2594 = stablehlo.add %2590, %2593 : tensor<1x257x5120xbf16> loc(#loc1362)
    %2595 = stablehlo.composite "tenstorrent.gelu" %2594 {decomposition = @tenstorrent.gelu.impl_5} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1363)
    %2596 = stablehlo.reshape %2595 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1364)
    %2597 = stablehlo.reshape %arg86 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %2598 = stablehlo.reshape %2597 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %2599 = stablehlo.transpose %2598, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1365)
    %2600 = stablehlo.dot_general %2596, %2599, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1366)
    %2601 = stablehlo.reshape %2600 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1364)
    %2602 = stablehlo.reshape %arg85 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2603 = stablehlo.reshape %2602 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2604 = stablehlo.broadcast_in_dim %2603, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1367)
    %2605 = stablehlo.add %2601, %2604 : tensor<1x257x1280xbf16> loc(#loc1367)
    %2606 = stablehlo.add %2579, %2605 : tensor<1x257x1280xbf16> loc(#loc1368)
    %2607 = stablehlo.reshape %arg84 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2608 = stablehlo.reshape %2607 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2609 = stablehlo.reshape %arg83 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2610 = stablehlo.reshape %2609 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2611 = stablehlo.composite "tenstorrent.layer_norm" %2606, %2608, %2610 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_12} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1369)
    %2612 = stablehlo.reshape %2611 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1370)
    %2613 = stablehlo.reshape %arg495 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2614 = stablehlo.reshape %2613 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2615 = stablehlo.transpose %2614, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1371)
    %2616 = stablehlo.dot_general %2612, %2615, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1372)
    %2617 = stablehlo.reshape %2616 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1370)
    %2618 = stablehlo.reshape %arg494 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2619 = stablehlo.reshape %2618 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2620 = stablehlo.broadcast_in_dim %2619, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1373)
    %2621 = stablehlo.add %2617, %2620 : tensor<1x257x1280xbf16> loc(#loc1373)
    %2622 = stablehlo.reshape %2621 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1374)
    %2623 = stablehlo.transpose %2622, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1375)
    %2624 = stablehlo.convert %2623 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1376)
    %2625 = stablehlo.multiply %2624, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1377)
    %2626 = stablehlo.reshape %arg493 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2627 = stablehlo.reshape %2626 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2628 = stablehlo.transpose %2627, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1378)
    %2629 = stablehlo.dot_general %2612, %2628, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1379)
    %2630 = stablehlo.reshape %2629 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1380)
    %2631 = stablehlo.reshape %arg492 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2632 = stablehlo.reshape %2631 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2633 = stablehlo.broadcast_in_dim %2632, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1381)
    %2634 = stablehlo.add %2630, %2633 : tensor<1x257x1280xbf16> loc(#loc1381)
    %2635 = stablehlo.reshape %2634 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1382)
    %2636 = stablehlo.transpose %2635, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1383)
    %2637 = stablehlo.convert %2636 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1384)
    %2638 = stablehlo.transpose %2637, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1385)
    %2639 = stablehlo.multiply %2638, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1386)
    %2640 = stablehlo.dot_general %2625, %2639, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1387)
    %2641 = stablehlo.convert %2640 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1388)
    %2642 = stablehlo.compare  EQ, %2641, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1388)
    %2643 = stablehlo.not %2642 : tensor<1x16x257x257xi1> loc(#loc1389)
    %2644 = stablehlo.reduce(%2643 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("2466|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_26aten__any"), %arg559: tensor<i1> loc("2466|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_26aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1391)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1392)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc1390)
    %2645 = stablehlo.reshape %2644 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1390)
    %2646 = stablehlo.not %2645 : tensor<1x16x257x1xi1> loc(#loc1393)
    %2647 = stablehlo.reshape %2646 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1394)
    %2648 = stablehlo.broadcast_in_dim %2647, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1394)
    %2649 = stablehlo.reduce(%2640 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1395)
    %2650 = stablehlo.broadcast_in_dim %2649, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1395)
    %2651 = stablehlo.subtract %2640, %2650 : tensor<1x16x257x257xf32> loc(#loc1395)
    %2652 = stablehlo.exponential %2651 : tensor<1x16x257x257xf32> loc(#loc1395)
    %2653 = stablehlo.reduce(%2652 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1395)
    %2654 = stablehlo.broadcast_in_dim %2653, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1395)
    %2655 = stablehlo.divide %2652, %2654 : tensor<1x16x257x257xf32> loc(#loc1395)
    %2656 = stablehlo.select %2648, %cst_3, %2655 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1396)
    %2657 = stablehlo.reshape %arg82 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2658 = stablehlo.reshape %2657 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2659 = stablehlo.transpose %2658, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1397)
    %2660 = stablehlo.dot_general %2612, %2659, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1398)
    %2661 = stablehlo.reshape %2660 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1399)
    %2662 = stablehlo.reshape %arg81 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2663 = stablehlo.reshape %2662 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2664 = stablehlo.broadcast_in_dim %2663, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1400)
    %2665 = stablehlo.add %2661, %2664 : tensor<1x257x1280xbf16> loc(#loc1400)
    %2666 = stablehlo.reshape %2665 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1401)
    %2667 = stablehlo.transpose %2666, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1402)
    %2668 = stablehlo.convert %2667 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1403)
    %2669 = stablehlo.dot_general %2656, %2668, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1404)
    %2670 = stablehlo.convert %2669 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1405)
    %2671 = stablehlo.transpose %2670, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1406)
    %2672 = stablehlo.reshape %2671 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1407)
    %2673 = stablehlo.reshape %arg80 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2674 = stablehlo.reshape %2673 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2675 = stablehlo.transpose %2674, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1408)
    %2676 = stablehlo.dot_general %2672, %2675, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1409)
    %2677 = stablehlo.reshape %2676 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1407)
    %2678 = stablehlo.reshape %arg79 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2679 = stablehlo.reshape %2678 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2680 = stablehlo.broadcast_in_dim %2679, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1410)
    %2681 = stablehlo.add %2677, %2680 : tensor<1x257x1280xbf16> loc(#loc1410)
    %2682 = stablehlo.add %2606, %2681 : tensor<1x257x1280xbf16> loc(#loc1411)
    %2683 = stablehlo.reshape %arg78 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2684 = stablehlo.reshape %2683 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2685 = stablehlo.reshape %arg77 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2686 = stablehlo.reshape %2685 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2687 = stablehlo.composite "tenstorrent.layer_norm" %2682, %2684, %2686 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_11} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1412)
    %2688 = stablehlo.reshape %2687 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1413)
    %2689 = stablehlo.reshape %arg76 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %2690 = stablehlo.reshape %2689 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %2691 = stablehlo.transpose %2690, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1414)
    %2692 = stablehlo.dot_general %2688, %2691, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1415)
    %2693 = stablehlo.reshape %2692 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1413)
    %2694 = stablehlo.reshape %arg75 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %2695 = stablehlo.reshape %2694 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
    %2696 = stablehlo.broadcast_in_dim %2695, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1416)
    %2697 = stablehlo.add %2693, %2696 : tensor<1x257x5120xbf16> loc(#loc1416)
    %2698 = stablehlo.composite "tenstorrent.gelu" %2697 {decomposition = @tenstorrent.gelu.impl_4} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1417)
    %2699 = stablehlo.reshape %2698 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1418)
    %2700 = stablehlo.reshape %arg74 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %2701 = stablehlo.reshape %2700 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %2702 = stablehlo.transpose %2701, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1419)
    %2703 = stablehlo.dot_general %2699, %2702, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1420)
    %2704 = stablehlo.reshape %2703 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1418)
    %2705 = stablehlo.reshape %arg73 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2706 = stablehlo.reshape %2705 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2707 = stablehlo.broadcast_in_dim %2706, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1421)
    %2708 = stablehlo.add %2704, %2707 : tensor<1x257x1280xbf16> loc(#loc1421)
    %2709 = stablehlo.add %2682, %2708 : tensor<1x257x1280xbf16> loc(#loc1422)
    %2710 = stablehlo.reshape %arg72 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2711 = stablehlo.reshape %2710 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2712 = stablehlo.reshape %arg71 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2713 = stablehlo.reshape %2712 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2714 = stablehlo.composite "tenstorrent.layer_norm" %2709, %2711, %2713 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_10} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1423)
    %2715 = stablehlo.reshape %2714 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1424)
    %2716 = stablehlo.reshape %arg499 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2717 = stablehlo.reshape %2716 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2718 = stablehlo.transpose %2717, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1425)
    %2719 = stablehlo.dot_general %2715, %2718, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1426)
    %2720 = stablehlo.reshape %2719 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1424)
    %2721 = stablehlo.reshape %arg498 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2722 = stablehlo.reshape %2721 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2723 = stablehlo.broadcast_in_dim %2722, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1427)
    %2724 = stablehlo.add %2720, %2723 : tensor<1x257x1280xbf16> loc(#loc1427)
    %2725 = stablehlo.reshape %2724 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1428)
    %2726 = stablehlo.transpose %2725, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1429)
    %2727 = stablehlo.convert %2726 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1430)
    %2728 = stablehlo.multiply %2727, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1431)
    %2729 = stablehlo.reshape %arg497 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2730 = stablehlo.reshape %2729 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2731 = stablehlo.transpose %2730, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1432)
    %2732 = stablehlo.dot_general %2715, %2731, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1433)
    %2733 = stablehlo.reshape %2732 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1434)
    %2734 = stablehlo.reshape %arg496 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2735 = stablehlo.reshape %2734 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2736 = stablehlo.broadcast_in_dim %2735, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1435)
    %2737 = stablehlo.add %2733, %2736 : tensor<1x257x1280xbf16> loc(#loc1435)
    %2738 = stablehlo.reshape %2737 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1436)
    %2739 = stablehlo.transpose %2738, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1437)
    %2740 = stablehlo.convert %2739 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1438)
    %2741 = stablehlo.transpose %2740, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1439)
    %2742 = stablehlo.multiply %2741, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1440)
    %2743 = stablehlo.dot_general %2728, %2742, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1441)
    %2744 = stablehlo.convert %2743 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1442)
    %2745 = stablehlo.compare  EQ, %2744, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1442)
    %2746 = stablehlo.not %2745 : tensor<1x16x257x257xi1> loc(#loc1443)
    %2747 = stablehlo.reduce(%2746 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("2540|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_27aten__any"), %arg559: tensor<i1> loc("2540|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_27aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1445)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1446)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc1444)
    %2748 = stablehlo.reshape %2747 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1444)
    %2749 = stablehlo.not %2748 : tensor<1x16x257x1xi1> loc(#loc1447)
    %2750 = stablehlo.reshape %2749 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1448)
    %2751 = stablehlo.broadcast_in_dim %2750, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1448)
    %2752 = stablehlo.reduce(%2743 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1449)
    %2753 = stablehlo.broadcast_in_dim %2752, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1449)
    %2754 = stablehlo.subtract %2743, %2753 : tensor<1x16x257x257xf32> loc(#loc1449)
    %2755 = stablehlo.exponential %2754 : tensor<1x16x257x257xf32> loc(#loc1449)
    %2756 = stablehlo.reduce(%2755 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1449)
    %2757 = stablehlo.broadcast_in_dim %2756, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1449)
    %2758 = stablehlo.divide %2755, %2757 : tensor<1x16x257x257xf32> loc(#loc1449)
    %2759 = stablehlo.select %2751, %cst_3, %2758 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1450)
    %2760 = stablehlo.reshape %arg70 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2761 = stablehlo.reshape %2760 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2762 = stablehlo.transpose %2761, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1451)
    %2763 = stablehlo.dot_general %2715, %2762, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1452)
    %2764 = stablehlo.reshape %2763 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1453)
    %2765 = stablehlo.reshape %arg69 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2766 = stablehlo.reshape %2765 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2767 = stablehlo.broadcast_in_dim %2766, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1454)
    %2768 = stablehlo.add %2764, %2767 : tensor<1x257x1280xbf16> loc(#loc1454)
    %2769 = stablehlo.reshape %2768 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1455)
    %2770 = stablehlo.transpose %2769, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1456)
    %2771 = stablehlo.convert %2770 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1457)
    %2772 = stablehlo.dot_general %2759, %2771, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1458)
    %2773 = stablehlo.convert %2772 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1459)
    %2774 = stablehlo.transpose %2773, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1460)
    %2775 = stablehlo.reshape %2774 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1461)
    %2776 = stablehlo.reshape %arg68 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2777 = stablehlo.reshape %2776 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2778 = stablehlo.transpose %2777, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1462)
    %2779 = stablehlo.dot_general %2775, %2778, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1463)
    %2780 = stablehlo.reshape %2779 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1461)
    %2781 = stablehlo.reshape %arg67 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2782 = stablehlo.reshape %2781 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2783 = stablehlo.broadcast_in_dim %2782, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1464)
    %2784 = stablehlo.add %2780, %2783 : tensor<1x257x1280xbf16> loc(#loc1464)
    %2785 = stablehlo.add %2709, %2784 : tensor<1x257x1280xbf16> loc(#loc1465)
    %2786 = stablehlo.reshape %arg66 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2787 = stablehlo.reshape %2786 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2788 = stablehlo.reshape %arg65 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2789 = stablehlo.reshape %2788 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2790 = stablehlo.composite "tenstorrent.layer_norm" %2785, %2787, %2789 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_9} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1466)
    %2791 = stablehlo.reshape %2790 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1467)
    %2792 = stablehlo.reshape %arg64 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %2793 = stablehlo.reshape %2792 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %2794 = stablehlo.transpose %2793, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1468)
    %2795 = stablehlo.dot_general %2791, %2794, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1469)
    %2796 = stablehlo.reshape %2795 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1467)
    %2797 = stablehlo.reshape %arg63 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %2798 = stablehlo.reshape %2797 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
    %2799 = stablehlo.broadcast_in_dim %2798, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1470)
    %2800 = stablehlo.add %2796, %2799 : tensor<1x257x5120xbf16> loc(#loc1470)
    %2801 = stablehlo.composite "tenstorrent.gelu" %2800 {decomposition = @tenstorrent.gelu.impl_9} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1471)
    %2802 = stablehlo.reshape %2801 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1472)
    %2803 = stablehlo.reshape %arg62 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %2804 = stablehlo.reshape %2803 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %2805 = stablehlo.transpose %2804, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1473)
    %2806 = stablehlo.dot_general %2802, %2805, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1474)
    %2807 = stablehlo.reshape %2806 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1472)
    %2808 = stablehlo.reshape %arg61 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2809 = stablehlo.reshape %2808 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2810 = stablehlo.broadcast_in_dim %2809, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1475)
    %2811 = stablehlo.add %2807, %2810 : tensor<1x257x1280xbf16> loc(#loc1475)
    %2812 = stablehlo.add %2785, %2811 : tensor<1x257x1280xbf16> loc(#loc1476)
    %2813 = stablehlo.reshape %arg60 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2814 = stablehlo.reshape %2813 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2815 = stablehlo.reshape %arg59 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2816 = stablehlo.reshape %2815 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2817 = stablehlo.composite "tenstorrent.layer_norm" %2812, %2814, %2816 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_8} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1477)
    %2818 = stablehlo.reshape %2817 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1478)
    %2819 = stablehlo.reshape %arg503 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2820 = stablehlo.reshape %2819 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2821 = stablehlo.transpose %2820, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1479)
    %2822 = stablehlo.dot_general %2818, %2821, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1480)
    %2823 = stablehlo.reshape %2822 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1478)
    %2824 = stablehlo.reshape %arg502 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2825 = stablehlo.reshape %2824 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2826 = stablehlo.broadcast_in_dim %2825, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1481)
    %2827 = stablehlo.add %2823, %2826 : tensor<1x257x1280xbf16> loc(#loc1481)
    %2828 = stablehlo.reshape %2827 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1482)
    %2829 = stablehlo.transpose %2828, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1483)
    %2830 = stablehlo.convert %2829 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1484)
    %2831 = stablehlo.multiply %2830, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1485)
    %2832 = stablehlo.reshape %arg501 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2833 = stablehlo.reshape %2832 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2834 = stablehlo.transpose %2833, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1486)
    %2835 = stablehlo.dot_general %2818, %2834, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1487)
    %2836 = stablehlo.reshape %2835 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1488)
    %2837 = stablehlo.reshape %arg500 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2838 = stablehlo.reshape %2837 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2839 = stablehlo.broadcast_in_dim %2838, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1489)
    %2840 = stablehlo.add %2836, %2839 : tensor<1x257x1280xbf16> loc(#loc1489)
    %2841 = stablehlo.reshape %2840 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1490)
    %2842 = stablehlo.transpose %2841, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1491)
    %2843 = stablehlo.convert %2842 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1492)
    %2844 = stablehlo.transpose %2843, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1493)
    %2845 = stablehlo.multiply %2844, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1494)
    %2846 = stablehlo.dot_general %2831, %2845, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1495)
    %2847 = stablehlo.convert %2846 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1496)
    %2848 = stablehlo.compare  EQ, %2847, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1496)
    %2849 = stablehlo.not %2848 : tensor<1x16x257x257xi1> loc(#loc1497)
    %2850 = stablehlo.reduce(%2849 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("2614|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_28aten__any"), %arg559: tensor<i1> loc("2614|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_28aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1499)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1500)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc1498)
    %2851 = stablehlo.reshape %2850 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1498)
    %2852 = stablehlo.not %2851 : tensor<1x16x257x1xi1> loc(#loc1501)
    %2853 = stablehlo.reshape %2852 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1502)
    %2854 = stablehlo.broadcast_in_dim %2853, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1502)
    %2855 = stablehlo.reduce(%2846 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1503)
    %2856 = stablehlo.broadcast_in_dim %2855, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1503)
    %2857 = stablehlo.subtract %2846, %2856 : tensor<1x16x257x257xf32> loc(#loc1503)
    %2858 = stablehlo.exponential %2857 : tensor<1x16x257x257xf32> loc(#loc1503)
    %2859 = stablehlo.reduce(%2858 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1503)
    %2860 = stablehlo.broadcast_in_dim %2859, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1503)
    %2861 = stablehlo.divide %2858, %2860 : tensor<1x16x257x257xf32> loc(#loc1503)
    %2862 = stablehlo.select %2854, %cst_3, %2861 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1504)
    %2863 = stablehlo.reshape %arg58 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2864 = stablehlo.reshape %2863 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2865 = stablehlo.transpose %2864, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1505)
    %2866 = stablehlo.dot_general %2818, %2865, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1506)
    %2867 = stablehlo.reshape %2866 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1507)
    %2868 = stablehlo.reshape %arg57 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2869 = stablehlo.reshape %2868 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2870 = stablehlo.broadcast_in_dim %2869, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1508)
    %2871 = stablehlo.add %2867, %2870 : tensor<1x257x1280xbf16> loc(#loc1508)
    %2872 = stablehlo.reshape %2871 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1509)
    %2873 = stablehlo.transpose %2872, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1510)
    %2874 = stablehlo.convert %2873 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1511)
    %2875 = stablehlo.dot_general %2862, %2874, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1512)
    %2876 = stablehlo.convert %2875 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1513)
    %2877 = stablehlo.transpose %2876, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1514)
    %2878 = stablehlo.reshape %2877 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1515)
    %2879 = stablehlo.reshape %arg56 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2880 = stablehlo.reshape %2879 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2881 = stablehlo.transpose %2880, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1516)
    %2882 = stablehlo.dot_general %2878, %2881, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1517)
    %2883 = stablehlo.reshape %2882 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1515)
    %2884 = stablehlo.reshape %arg55 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2885 = stablehlo.reshape %2884 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2886 = stablehlo.broadcast_in_dim %2885, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1518)
    %2887 = stablehlo.add %2883, %2886 : tensor<1x257x1280xbf16> loc(#loc1518)
    %2888 = stablehlo.add %2812, %2887 : tensor<1x257x1280xbf16> loc(#loc1519)
    %2889 = stablehlo.reshape %arg54 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2890 = stablehlo.reshape %2889 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2891 = stablehlo.reshape %arg53 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2892 = stablehlo.reshape %2891 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2893 = stablehlo.composite "tenstorrent.layer_norm" %2888, %2890, %2892 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_46} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1520)
    %2894 = stablehlo.reshape %2893 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1521)
    %2895 = stablehlo.reshape %arg52 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %2896 = stablehlo.reshape %2895 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %2897 = stablehlo.transpose %2896, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1522)
    %2898 = stablehlo.dot_general %2894, %2897, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1523)
    %2899 = stablehlo.reshape %2898 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1521)
    %2900 = stablehlo.reshape %arg51 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %2901 = stablehlo.reshape %2900 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
    %2902 = stablehlo.broadcast_in_dim %2901, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1524)
    %2903 = stablehlo.add %2899, %2902 : tensor<1x257x5120xbf16> loc(#loc1524)
    %2904 = stablehlo.composite "tenstorrent.gelu" %2903 {decomposition = @tenstorrent.gelu.impl_29} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1525)
    %2905 = stablehlo.reshape %2904 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1526)
    %2906 = stablehlo.reshape %arg50 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %2907 = stablehlo.reshape %2906 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %2908 = stablehlo.transpose %2907, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1527)
    %2909 = stablehlo.dot_general %2905, %2908, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1528)
    %2910 = stablehlo.reshape %2909 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1526)
    %2911 = stablehlo.reshape %arg49 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2912 = stablehlo.reshape %2911 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2913 = stablehlo.broadcast_in_dim %2912, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1529)
    %2914 = stablehlo.add %2910, %2913 : tensor<1x257x1280xbf16> loc(#loc1529)
    %2915 = stablehlo.add %2888, %2914 : tensor<1x257x1280xbf16> loc(#loc1530)
    %2916 = stablehlo.reshape %arg48 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2917 = stablehlo.reshape %2916 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2918 = stablehlo.reshape %arg47 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2919 = stablehlo.reshape %2918 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2920 = stablehlo.composite "tenstorrent.layer_norm" %2915, %2917, %2919 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_7} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1531)
    %2921 = stablehlo.reshape %2920 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1532)
    %2922 = stablehlo.reshape %arg507 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2923 = stablehlo.reshape %2922 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2924 = stablehlo.transpose %2923, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1533)
    %2925 = stablehlo.dot_general %2921, %2924, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1534)
    %2926 = stablehlo.reshape %2925 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1532)
    %2927 = stablehlo.reshape %arg506 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2928 = stablehlo.reshape %2927 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2929 = stablehlo.broadcast_in_dim %2928, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1535)
    %2930 = stablehlo.add %2926, %2929 : tensor<1x257x1280xbf16> loc(#loc1535)
    %2931 = stablehlo.reshape %2930 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1536)
    %2932 = stablehlo.transpose %2931, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1537)
    %2933 = stablehlo.convert %2932 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1538)
    %2934 = stablehlo.multiply %2933, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1539)
    %2935 = stablehlo.reshape %arg505 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2936 = stablehlo.reshape %2935 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2937 = stablehlo.transpose %2936, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1540)
    %2938 = stablehlo.dot_general %2921, %2937, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1541)
    %2939 = stablehlo.reshape %2938 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1542)
    %2940 = stablehlo.reshape %arg504 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2941 = stablehlo.reshape %2940 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2942 = stablehlo.broadcast_in_dim %2941, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1543)
    %2943 = stablehlo.add %2939, %2942 : tensor<1x257x1280xbf16> loc(#loc1543)
    %2944 = stablehlo.reshape %2943 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1544)
    %2945 = stablehlo.transpose %2944, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1545)
    %2946 = stablehlo.convert %2945 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1546)
    %2947 = stablehlo.transpose %2946, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1547)
    %2948 = stablehlo.multiply %2947, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1548)
    %2949 = stablehlo.dot_general %2934, %2948, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1549)
    %2950 = stablehlo.convert %2949 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1550)
    %2951 = stablehlo.compare  EQ, %2950, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1550)
    %2952 = stablehlo.not %2951 : tensor<1x16x257x257xi1> loc(#loc1551)
    %2953 = stablehlo.reduce(%2952 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("2688|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_29aten__any"), %arg559: tensor<i1> loc("2688|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_29aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1553)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1554)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc1552)
    %2954 = stablehlo.reshape %2953 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1552)
    %2955 = stablehlo.not %2954 : tensor<1x16x257x1xi1> loc(#loc1555)
    %2956 = stablehlo.reshape %2955 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1556)
    %2957 = stablehlo.broadcast_in_dim %2956, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1556)
    %2958 = stablehlo.reduce(%2949 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1557)
    %2959 = stablehlo.broadcast_in_dim %2958, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1557)
    %2960 = stablehlo.subtract %2949, %2959 : tensor<1x16x257x257xf32> loc(#loc1557)
    %2961 = stablehlo.exponential %2960 : tensor<1x16x257x257xf32> loc(#loc1557)
    %2962 = stablehlo.reduce(%2961 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1557)
    %2963 = stablehlo.broadcast_in_dim %2962, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1557)
    %2964 = stablehlo.divide %2961, %2963 : tensor<1x16x257x257xf32> loc(#loc1557)
    %2965 = stablehlo.select %2957, %cst_3, %2964 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1558)
    %2966 = stablehlo.reshape %arg46 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2967 = stablehlo.reshape %2966 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2968 = stablehlo.transpose %2967, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1559)
    %2969 = stablehlo.dot_general %2921, %2968, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1560)
    %2970 = stablehlo.reshape %2969 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1561)
    %2971 = stablehlo.reshape %arg45 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2972 = stablehlo.reshape %2971 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2973 = stablehlo.broadcast_in_dim %2972, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1562)
    %2974 = stablehlo.add %2970, %2973 : tensor<1x257x1280xbf16> loc(#loc1562)
    %2975 = stablehlo.reshape %2974 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1563)
    %2976 = stablehlo.transpose %2975, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1564)
    %2977 = stablehlo.convert %2976 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1565)
    %2978 = stablehlo.dot_general %2965, %2977, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1566)
    %2979 = stablehlo.convert %2978 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1567)
    %2980 = stablehlo.transpose %2979, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1568)
    %2981 = stablehlo.reshape %2980 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1569)
    %2982 = stablehlo.reshape %arg44 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %2983 = stablehlo.reshape %2982 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %2984 = stablehlo.transpose %2983, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1570)
    %2985 = stablehlo.dot_general %2981, %2984, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1571)
    %2986 = stablehlo.reshape %2985 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1569)
    %2987 = stablehlo.reshape %arg43 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2988 = stablehlo.reshape %2987 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2989 = stablehlo.broadcast_in_dim %2988, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1572)
    %2990 = stablehlo.add %2986, %2989 : tensor<1x257x1280xbf16> loc(#loc1572)
    %2991 = stablehlo.add %2915, %2990 : tensor<1x257x1280xbf16> loc(#loc1573)
    %2992 = stablehlo.reshape %arg42 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2993 = stablehlo.reshape %2992 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2994 = stablehlo.reshape %arg41 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %2995 = stablehlo.reshape %2994 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %2996 = stablehlo.composite "tenstorrent.layer_norm" %2991, %2993, %2995 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_6} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1574)
    %2997 = stablehlo.reshape %2996 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1575)
    %2998 = stablehlo.reshape %arg40 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %2999 = stablehlo.reshape %2998 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %3000 = stablehlo.transpose %2999, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1576)
    %3001 = stablehlo.dot_general %2997, %3000, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1577)
    %3002 = stablehlo.reshape %3001 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1575)
    %3003 = stablehlo.reshape %arg39 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %3004 = stablehlo.reshape %3003 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
    %3005 = stablehlo.broadcast_in_dim %3004, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1578)
    %3006 = stablehlo.add %3002, %3005 : tensor<1x257x5120xbf16> loc(#loc1578)
    %3007 = stablehlo.composite "tenstorrent.gelu" %3006 {decomposition = @tenstorrent.gelu.impl_2} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1579)
    %3008 = stablehlo.reshape %3007 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1580)
    %3009 = stablehlo.reshape %arg38 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %3010 = stablehlo.reshape %3009 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %3011 = stablehlo.transpose %3010, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1581)
    %3012 = stablehlo.dot_general %3008, %3011, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1582)
    %3013 = stablehlo.reshape %3012 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1580)
    %3014 = stablehlo.reshape %arg37 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3015 = stablehlo.reshape %3014 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3016 = stablehlo.broadcast_in_dim %3015, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1583)
    %3017 = stablehlo.add %3013, %3016 : tensor<1x257x1280xbf16> loc(#loc1583)
    %3018 = stablehlo.add %2991, %3017 : tensor<1x257x1280xbf16> loc(#loc1584)
    %3019 = stablehlo.reshape %arg36 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3020 = stablehlo.reshape %3019 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3021 = stablehlo.reshape %arg35 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3022 = stablehlo.reshape %3021 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3023 = stablehlo.composite "tenstorrent.layer_norm" %3018, %3020, %3022 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_4} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1585)
    %3024 = stablehlo.reshape %3023 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1586)
    %3025 = stablehlo.reshape %arg511 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3026 = stablehlo.reshape %3025 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %3027 = stablehlo.transpose %3026, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1587)
    %3028 = stablehlo.dot_general %3024, %3027, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1588)
    %3029 = stablehlo.reshape %3028 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1586)
    %3030 = stablehlo.reshape %arg510 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3031 = stablehlo.reshape %3030 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3032 = stablehlo.broadcast_in_dim %3031, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1589)
    %3033 = stablehlo.add %3029, %3032 : tensor<1x257x1280xbf16> loc(#loc1589)
    %3034 = stablehlo.reshape %3033 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1590)
    %3035 = stablehlo.transpose %3034, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1591)
    %3036 = stablehlo.convert %3035 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1592)
    %3037 = stablehlo.multiply %3036, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1593)
    %3038 = stablehlo.reshape %arg509 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3039 = stablehlo.reshape %3038 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %3040 = stablehlo.transpose %3039, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1594)
    %3041 = stablehlo.dot_general %3024, %3040, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1595)
    %3042 = stablehlo.reshape %3041 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1596)
    %3043 = stablehlo.reshape %arg508 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3044 = stablehlo.reshape %3043 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3045 = stablehlo.broadcast_in_dim %3044, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1597)
    %3046 = stablehlo.add %3042, %3045 : tensor<1x257x1280xbf16> loc(#loc1597)
    %3047 = stablehlo.reshape %3046 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1598)
    %3048 = stablehlo.transpose %3047, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1599)
    %3049 = stablehlo.convert %3048 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1600)
    %3050 = stablehlo.transpose %3049, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1601)
    %3051 = stablehlo.multiply %3050, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1602)
    %3052 = stablehlo.dot_general %3037, %3051, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1603)
    %3053 = stablehlo.convert %3052 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1604)
    %3054 = stablehlo.compare  EQ, %3053, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1604)
    %3055 = stablehlo.not %3054 : tensor<1x16x257x257xi1> loc(#loc1605)
    %3056 = stablehlo.reduce(%3055 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("2762|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_30aten__any"), %arg559: tensor<i1> loc("2762|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_30aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1607)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1608)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc1606)
    %3057 = stablehlo.reshape %3056 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1606)
    %3058 = stablehlo.not %3057 : tensor<1x16x257x1xi1> loc(#loc1609)
    %3059 = stablehlo.reshape %3058 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1610)
    %3060 = stablehlo.broadcast_in_dim %3059, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1610)
    %3061 = stablehlo.reduce(%3052 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1611)
    %3062 = stablehlo.broadcast_in_dim %3061, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1611)
    %3063 = stablehlo.subtract %3052, %3062 : tensor<1x16x257x257xf32> loc(#loc1611)
    %3064 = stablehlo.exponential %3063 : tensor<1x16x257x257xf32> loc(#loc1611)
    %3065 = stablehlo.reduce(%3064 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1611)
    %3066 = stablehlo.broadcast_in_dim %3065, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1611)
    %3067 = stablehlo.divide %3064, %3066 : tensor<1x16x257x257xf32> loc(#loc1611)
    %3068 = stablehlo.select %3060, %cst_3, %3067 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1612)
    %3069 = stablehlo.reshape %arg34 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3070 = stablehlo.reshape %3069 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %3071 = stablehlo.transpose %3070, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1613)
    %3072 = stablehlo.dot_general %3024, %3071, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1614)
    %3073 = stablehlo.reshape %3072 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1615)
    %3074 = stablehlo.reshape %arg33 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3075 = stablehlo.reshape %3074 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3076 = stablehlo.broadcast_in_dim %3075, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1616)
    %3077 = stablehlo.add %3073, %3076 : tensor<1x257x1280xbf16> loc(#loc1616)
    %3078 = stablehlo.reshape %3077 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1617)
    %3079 = stablehlo.transpose %3078, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1618)
    %3080 = stablehlo.convert %3079 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1619)
    %3081 = stablehlo.dot_general %3068, %3080, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1620)
    %3082 = stablehlo.convert %3081 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1621)
    %3083 = stablehlo.transpose %3082, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1622)
    %3084 = stablehlo.reshape %3083 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1623)
    %3085 = stablehlo.reshape %arg32 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3086 = stablehlo.reshape %3085 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %3087 = stablehlo.transpose %3086, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1624)
    %3088 = stablehlo.dot_general %3084, %3087, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1625)
    %3089 = stablehlo.reshape %3088 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1623)
    %3090 = stablehlo.reshape %arg31 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3091 = stablehlo.reshape %3090 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3092 = stablehlo.broadcast_in_dim %3091, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1626)
    %3093 = stablehlo.add %3089, %3092 : tensor<1x257x1280xbf16> loc(#loc1626)
    %3094 = stablehlo.add %3018, %3093 : tensor<1x257x1280xbf16> loc(#loc1627)
    %3095 = stablehlo.reshape %arg30 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3096 = stablehlo.reshape %3095 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3097 = stablehlo.reshape %arg29 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3098 = stablehlo.reshape %3097 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3099 = stablehlo.composite "tenstorrent.layer_norm" %3094, %3096, %3098 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_72} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1628)
    %3100 = stablehlo.reshape %3099 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1629)
    %3101 = stablehlo.reshape %arg28 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %3102 = stablehlo.reshape %3101 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %3103 = stablehlo.transpose %3102, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1630)
    %3104 = stablehlo.dot_general %3100, %3103, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1631)
    %3105 = stablehlo.reshape %3104 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1629)
    %3106 = stablehlo.reshape %arg27 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %3107 = stablehlo.reshape %3106 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
    %3108 = stablehlo.broadcast_in_dim %3107, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1632)
    %3109 = stablehlo.add %3105, %3108 : tensor<1x257x5120xbf16> loc(#loc1632)
    %3110 = stablehlo.composite "tenstorrent.gelu" %3109 {decomposition = @tenstorrent.gelu.impl_1} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1633)
    %3111 = stablehlo.reshape %3110 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1634)
    %3112 = stablehlo.reshape %arg26 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %3113 = stablehlo.reshape %3112 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %3114 = stablehlo.transpose %3113, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1635)
    %3115 = stablehlo.dot_general %3111, %3114, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1636)
    %3116 = stablehlo.reshape %3115 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1634)
    %3117 = stablehlo.reshape %arg25 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3118 = stablehlo.reshape %3117 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3119 = stablehlo.broadcast_in_dim %3118, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1637)
    %3120 = stablehlo.add %3116, %3119 : tensor<1x257x1280xbf16> loc(#loc1637)
    %3121 = stablehlo.add %3094, %3120 : tensor<1x257x1280xbf16> loc(#loc1638)
    %3122 = stablehlo.reshape %arg24 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3123 = stablehlo.reshape %3122 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3124 = stablehlo.reshape %arg23 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3125 = stablehlo.reshape %3124 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3126 = stablehlo.composite "tenstorrent.layer_norm" %3121, %3123, %3125 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_31} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1639)
    %3127 = stablehlo.reshape %3126 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1640)
    %3128 = stablehlo.reshape %arg515 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3129 = stablehlo.reshape %3128 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %3130 = stablehlo.transpose %3129, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1641)
    %3131 = stablehlo.dot_general %3127, %3130, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1642)
    %3132 = stablehlo.reshape %3131 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1640)
    %3133 = stablehlo.reshape %arg514 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3134 = stablehlo.reshape %3133 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3135 = stablehlo.broadcast_in_dim %3134, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1643)
    %3136 = stablehlo.add %3132, %3135 : tensor<1x257x1280xbf16> loc(#loc1643)
    %3137 = stablehlo.reshape %3136 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1644)
    %3138 = stablehlo.transpose %3137, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1645)
    %3139 = stablehlo.convert %3138 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1646)
    %3140 = stablehlo.multiply %3139, %cst_6 : tensor<1x16x257x80xf32> loc(#loc1647)
    %3141 = stablehlo.reshape %arg513 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3142 = stablehlo.reshape %3141 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %3143 = stablehlo.transpose %3142, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1648)
    %3144 = stablehlo.dot_general %3127, %3143, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1649)
    %3145 = stablehlo.reshape %3144 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1650)
    %3146 = stablehlo.reshape %arg512 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3147 = stablehlo.reshape %3146 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3148 = stablehlo.broadcast_in_dim %3147, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1651)
    %3149 = stablehlo.add %3145, %3148 : tensor<1x257x1280xbf16> loc(#loc1651)
    %3150 = stablehlo.reshape %3149 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1652)
    %3151 = stablehlo.transpose %3150, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1653)
    %3152 = stablehlo.convert %3151 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1654)
    %3153 = stablehlo.transpose %3152, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,80,257]{2,1,3,0}"} : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32> loc(#loc1655)
    %3154 = stablehlo.multiply %3153, %cst_5 : tensor<1x16x80x257xf32> loc(#loc1656)
    %3155 = stablehlo.dot_general %3140, %3154, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1657)
    %3156 = stablehlo.convert %3155 : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64> loc(#loc1658)
    %3157 = stablehlo.compare  EQ, %3156, %cst_4 : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1> loc(#loc1658)
    %3158 = stablehlo.not %3157 : tensor<1x16x257x257xi1> loc(#loc1659)
    %3159 = stablehlo.reduce(%3158 init: %c_9) across dimensions = [3] : (tensor<1x16x257x257xi1>, tensor<i1>) -> tensor<1x16x257xi1>
     reducer(%arg558: tensor<i1> loc("2836|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_31aten__any"), %arg559: tensor<i1> loc("2836|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|any_31aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1661)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1662)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc1660)
    %3160 = stablehlo.reshape %3159 : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1> loc(#loc1660)
    %3161 = stablehlo.not %3160 : tensor<1x16x257x1xi1> loc(#loc1663)
    %3162 = stablehlo.reshape %3161 : (tensor<1x16x257x1xi1>) -> tensor<1x16x257xi1> loc(#loc1664)
    %3163 = stablehlo.broadcast_in_dim %3162, dims = [0, 1, 2] : (tensor<1x16x257xi1>) -> tensor<1x16x257x257xi1> loc(#loc1664)
    %3164 = stablehlo.reduce(%3155 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1665)
    %3165 = stablehlo.broadcast_in_dim %3164, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1665)
    %3166 = stablehlo.subtract %3155, %3165 : tensor<1x16x257x257xf32> loc(#loc1665)
    %3167 = stablehlo.exponential %3166 : tensor<1x16x257x257xf32> loc(#loc1665)
    %3168 = stablehlo.reduce(%3167 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x16x257x257xf32>, tensor<f32>) -> tensor<1x16x257xf32> loc(#loc1665)
    %3169 = stablehlo.broadcast_in_dim %3168, dims = [0, 1, 2] : (tensor<1x16x257xf32>) -> tensor<1x16x257x257xf32> loc(#loc1665)
    %3170 = stablehlo.divide %3167, %3169 : tensor<1x16x257x257xf32> loc(#loc1665)
    %3171 = stablehlo.select %3163, %cst_3, %3170 : tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32> loc(#loc1666)
    %3172 = stablehlo.reshape %arg22 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3173 = stablehlo.reshape %3172 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %3174 = stablehlo.transpose %3173, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1667)
    %3175 = stablehlo.dot_general %3127, %3174, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1668)
    %3176 = stablehlo.reshape %3175 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1669)
    %3177 = stablehlo.reshape %arg21 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3178 = stablehlo.reshape %3177 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3179 = stablehlo.broadcast_in_dim %3178, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1670)
    %3180 = stablehlo.add %3176, %3179 : tensor<1x257x1280xbf16> loc(#loc1670)
    %3181 = stablehlo.reshape %3180 : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1671)
    %3182 = stablehlo.transpose %3181, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,257,80]{3,1,2,0}"} : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16> loc(#loc1672)
    %3183 = stablehlo.convert %3182 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,16,257,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32> loc(#loc1673)
    %3184 = stablehlo.dot_general %3171, %3183, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32> loc(#loc1674)
    %3185 = stablehlo.convert %3184 : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16> loc(#loc1675)
    %3186 = stablehlo.transpose %3185, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,257,16,80]{3,1,2,0}"} : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16> loc(#loc1676)
    %3187 = stablehlo.reshape %3186 : (tensor<1x257x16x80xbf16>) -> tensor<257x1280xbf16> loc(#loc1677)
    %3188 = stablehlo.reshape %arg20 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3189 = stablehlo.reshape %3188 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %3190 = stablehlo.transpose %3189, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1678)
    %3191 = stablehlo.dot_general %3187, %3190, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1679)
    %3192 = stablehlo.reshape %3191 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1677)
    %3193 = stablehlo.reshape %arg19 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3194 = stablehlo.reshape %3193 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3195 = stablehlo.broadcast_in_dim %3194, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1680)
    %3196 = stablehlo.add %3192, %3195 : tensor<1x257x1280xbf16> loc(#loc1680)
    %3197 = stablehlo.add %3121, %3196 : tensor<1x257x1280xbf16> loc(#loc1681)
    %3198 = stablehlo.reshape %arg18 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3199 = stablehlo.reshape %3198 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3200 = stablehlo.reshape %arg17 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3201 = stablehlo.reshape %3200 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3202 = stablehlo.composite "tenstorrent.layer_norm" %3197, %3199, %3201 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_23} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1682)
    %3203 = stablehlo.reshape %3202 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1683)
    %3204 = stablehlo.reshape %arg16 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %3205 = stablehlo.reshape %3204 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %3206 = stablehlo.transpose %3205, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1684)
    %3207 = stablehlo.dot_general %3203, %3206, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1685)
    %3208 = stablehlo.reshape %3207 : (tensor<257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1683)
    %3209 = stablehlo.reshape %arg15 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2)
    %3210 = stablehlo.reshape %3209 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2)
    %3211 = stablehlo.broadcast_in_dim %3210, dims = [2] : (tensor<5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1686)
    %3212 = stablehlo.add %3208, %3211 : tensor<1x257x5120xbf16> loc(#loc1686)
    %3213 = stablehlo.composite "tenstorrent.gelu" %3212 {decomposition = @tenstorrent.gelu.impl_12} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1687)
    %3214 = stablehlo.reshape %3213 : (tensor<1x257x5120xbf16>) -> tensor<257x5120xbf16> loc(#loc1688)
    %3215 = stablehlo.reshape %arg14 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %3216 = stablehlo.reshape %3215 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %3217 = stablehlo.transpose %3216, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1689)
    %3218 = stablehlo.dot_general %3214, %3217, contracting_dims = [1] x [0] : (tensor<257x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1690)
    %3219 = stablehlo.reshape %3218 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1688)
    %3220 = stablehlo.reshape %arg13 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3221 = stablehlo.reshape %3220 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3222 = stablehlo.broadcast_in_dim %3221, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1691)
    %3223 = stablehlo.add %3219, %3222 : tensor<1x257x1280xbf16> loc(#loc1691)
    %3224 = stablehlo.add %3197, %3223 : tensor<1x257x1280xbf16> loc(#loc1692)
    %3225 = stablehlo.reshape %3224 : (tensor<1x257x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1693)
    %3226 = stablehlo.reshape %arg12 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3227 = stablehlo.reshape %3226 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %3228 = stablehlo.transpose %3227, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1694)
    %3229 = stablehlo.dot_general %3225, %3228, contracting_dims = [1] x [0] : (tensor<257x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<257x1280xbf16> loc(#loc1695)
    %3230 = stablehlo.reshape %3229 : (tensor<257x1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1693)
    %3231 = stablehlo.reshape %arg11 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3232 = stablehlo.reshape %3231 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3233 = stablehlo.broadcast_in_dim %3232, dims = [2] : (tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1696)
    %3234 = stablehlo.add %3230, %3233 : tensor<1x257x1280xbf16> loc(#loc1696)
    %3235 = stablehlo.reshape %arg10 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3236 = stablehlo.reshape %3235 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3237 = stablehlo.reshape %arg9 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3238 = stablehlo.reshape %3237 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3239 = stablehlo.composite "tenstorrent.layer_norm" %3234, %3236, %3238 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_63} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1697)
    %3240 = stablehlo.concatenate %3239, %4, dim = 1 : (tensor<1x257x1280xbf16>, tensor<1x16x1280xbf16>) -> tensor<1x273x1280xbf16> loc(#loc1698)
    %3241 = stablehlo.reshape %3240 : (tensor<1x273x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1699)
    %3242 = stablehlo.reshape %arg516 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3243 = stablehlo.reshape %3242 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %3244 = stablehlo.transpose %3243, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1700)
    %3245 = stablehlo.dot_general %3241, %3244, contracting_dims = [1] x [0] : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1701)
    %3246 = stablehlo.reshape %3245 : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc1702)
    %3247 = stablehlo.transpose %3246, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,273,64]{3,1,2,0}"} : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc1703)
    %3248 = stablehlo.convert %3247 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,273,64]{3,1,2,0}"} : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc1704)
    %3249 = stablehlo.transpose %3248, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,64,273]{2,1,3,0}"} : (tensor<1x20x273x64xf32>) -> tensor<1x20x64x273xf32> loc(#loc1705)
    %3250 = stablehlo.multiply %3249, %cst_2 : tensor<1x20x64x273xf32> loc(#loc1706)
    %3251 = stablehlo.dot_general %13, %3250, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x20x16x64xf32>, tensor<1x20x64x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc1707)
    %3252 = stablehlo.convert %3251 : (tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf64> loc(#loc1708)
    %3253 = stablehlo.compare  EQ, %3252, %cst_1 : (tensor<1x20x16x273xf64>, tensor<1x20x16x273xf64>) -> tensor<1x20x16x273xi1> loc(#loc1708)
    %3254 = stablehlo.not %3253 : tensor<1x20x16x273xi1> loc(#loc1709)
    %3255 = stablehlo.reduce(%3254 init: %c_9) across dimensions = [3] : (tensor<1x20x16x273xi1>, tensor<i1>) -> tensor<1x20x16xi1>
     reducer(%arg558: tensor<i1> loc("2925|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_33aten__any"), %arg559: tensor<i1> loc("2925|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_33aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1711)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1712)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc1710)
    %3256 = stablehlo.reshape %3255 : (tensor<1x20x16xi1>) -> tensor<1x20x16x1xi1> loc(#loc1710)
    %3257 = stablehlo.not %3256 : tensor<1x20x16x1xi1> loc(#loc1713)
    %3258 = stablehlo.reshape %3257 : (tensor<1x20x16x1xi1>) -> tensor<1x20x16xi1> loc(#loc1714)
    %3259 = stablehlo.broadcast_in_dim %3258, dims = [0, 1, 2] : (tensor<1x20x16xi1>) -> tensor<1x20x16x273xi1> loc(#loc1714)
    %3260 = stablehlo.reduce(%3251 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x20x16x273xf32>, tensor<f32>) -> tensor<1x20x16xf32> loc(#loc1715)
    %3261 = stablehlo.broadcast_in_dim %3260, dims = [0, 1, 2] : (tensor<1x20x16xf32>) -> tensor<1x20x16x273xf32> loc(#loc1715)
    %3262 = stablehlo.subtract %3251, %3261 : tensor<1x20x16x273xf32> loc(#loc1715)
    %3263 = stablehlo.exponential %3262 : tensor<1x20x16x273xf32> loc(#loc1715)
    %3264 = stablehlo.reduce(%3263 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x20x16x273xf32>, tensor<f32>) -> tensor<1x20x16xf32> loc(#loc1715)
    %3265 = stablehlo.broadcast_in_dim %3264, dims = [0, 1, 2] : (tensor<1x20x16xf32>) -> tensor<1x20x16x273xf32> loc(#loc1715)
    %3266 = stablehlo.divide %3263, %3265 : tensor<1x20x16x273xf32> loc(#loc1715)
    %3267 = stablehlo.select %3259, %cst_0, %3266 : tensor<1x20x16x273xi1>, tensor<1x20x16x273xf32> loc(#loc1716)
    %3268 = stablehlo.reshape %arg6 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3269 = stablehlo.reshape %3268 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %3270 = stablehlo.transpose %3269, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1717)
    %3271 = stablehlo.dot_general %3241, %3270, contracting_dims = [1] x [0] : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1718)
    %3272 = stablehlo.reshape %3271 : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc1719)
    %3273 = stablehlo.transpose %3272, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,273,64]{3,1,2,0}"} : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc1720)
    %3274 = stablehlo.convert %3273 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,273,64]{3,1,2,0}"} : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc1721)
    %3275 = stablehlo.dot_general %3267, %3274, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x20x16x273xf32>, tensor<1x20x273x64xf32>) -> tensor<1x20x16x64xf32> loc(#loc1722)
    %3276 = stablehlo.convert %3275 : (tensor<1x20x16x64xf32>) -> tensor<1x20x16x64xbf16> loc(#loc1723)
    %3277 = stablehlo.transpose %3276, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,20,64]{3,1,2,0}"} : (tensor<1x20x16x64xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc1724)
    %3278 = stablehlo.reshape %3277 : (tensor<1x16x20x64xbf16>) -> tensor<16x1280xbf16> loc(#loc1725)
    %3279 = stablehlo.reshape %arg5 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3280 = stablehlo.reshape %3279 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %3281 = stablehlo.transpose %3280, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1726)
    %3282 = stablehlo.dot_general %3278, %3281, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1727)
    %3283 = stablehlo.reshape %3282 : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1728)
    %3284 = stablehlo.divide %3283, %cst : tensor<1x16x1280xbf16> loc(#loc1729)
    %3285 = stablehlo.add %3284, %arg4 : tensor<1x16x1280xbf16> loc(#loc1730)
    %3286 = stablehlo.reshape %arg521 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3287 = stablehlo.reshape %3286 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3288 = stablehlo.reshape %arg520 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3289 = stablehlo.reshape %3288 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3290 = stablehlo.composite "tenstorrent.layer_norm" %3285, %3287, %3289 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_3} : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1731)
    %3291 = stablehlo.reshape %3290 : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1732)
    %3292 = stablehlo.reshape %arg519 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %3293 = stablehlo.reshape %3292 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %3294 = stablehlo.transpose %3293, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1733)
    %3295 = stablehlo.dot_general %3291, %3294, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc1734)
    %3296 = stablehlo.reshape %3295 : (tensor<16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc1732)
    %3297 = stablehlo.composite "tenstorrent.gelu" %3296 {decomposition = @tenstorrent.gelu.impl_0} : (tensor<1x16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc1735)
    %3298 = stablehlo.reshape %3297 : (tensor<1x16x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc1736)
    %3299 = stablehlo.reshape %arg518 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %3300 = stablehlo.reshape %3299 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %3301 = stablehlo.transpose %3300, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1737)
    %3302 = stablehlo.dot_general %3298, %3301, contracting_dims = [1] x [0] : (tensor<16x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1738)
    %3303 = stablehlo.reshape %3302 : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1736)
    %3304 = stablehlo.add %3303, %3285 : tensor<1x16x1280xbf16> loc(#loc1739)
    %3305 = stablehlo.reshape %arg525 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3306 = stablehlo.reshape %3305 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3307 = stablehlo.reshape %arg524 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3308 = stablehlo.reshape %3307 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3309 = stablehlo.composite "tenstorrent.layer_norm" %3304, %3306, %3308 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_5} : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1740)
    %3310 = stablehlo.reshape %3309 : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1741)
    %3311 = stablehlo.reshape %arg529 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3312 = stablehlo.reshape %3311 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %3313 = stablehlo.transpose %3312, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1742)
    %3314 = stablehlo.dot_general %3310, %3313, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1743)
    %3315 = stablehlo.reshape %3314 : (tensor<16x1280xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc1744)
    %3316 = stablehlo.transpose %3315, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,16,64]{3,1,2,0}"} : (tensor<1x16x20x64xbf16>) -> tensor<1x20x16x64xbf16> loc(#loc1745)
    %3317 = stablehlo.convert %3316 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,16,64]{3,1,2,0}"} : (tensor<1x20x16x64xbf16>) -> tensor<1x20x16x64xf32> loc(#loc1746)
    %3318 = stablehlo.multiply %3317, %cst_7 : tensor<1x20x16x64xf32> loc(#loc1747)
    %3319 = stablehlo.reshape %arg527 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3320 = stablehlo.reshape %3319 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3321 = stablehlo.reshape %arg526 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3322 = stablehlo.reshape %3321 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3323 = stablehlo.composite "tenstorrent.layer_norm" %3234, %3320, %3322 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_67} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1748)
    %3324 = stablehlo.concatenate %3323, %3309, dim = 1 : (tensor<1x257x1280xbf16>, tensor<1x16x1280xbf16>) -> tensor<1x273x1280xbf16> loc(#loc1749)
    %3325 = stablehlo.reshape %3324 : (tensor<1x273x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1750)
    %3326 = stablehlo.reshape %arg528 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3327 = stablehlo.reshape %3326 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %3328 = stablehlo.transpose %3327, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1751)
    %3329 = stablehlo.dot_general %3325, %3328, contracting_dims = [1] x [0] : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1752)
    %3330 = stablehlo.reshape %3329 : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc1753)
    %3331 = stablehlo.transpose %3330, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,273,64]{3,1,2,0}"} : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc1754)
    %3332 = stablehlo.convert %3331 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,273,64]{3,1,2,0}"} : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc1755)
    %3333 = stablehlo.transpose %3332, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,64,273]{2,1,3,0}"} : (tensor<1x20x273x64xf32>) -> tensor<1x20x64x273xf32> loc(#loc1756)
    %3334 = stablehlo.multiply %3333, %cst_2 : tensor<1x20x64x273xf32> loc(#loc1757)
    %3335 = stablehlo.dot_general %3318, %3334, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x20x16x64xf32>, tensor<1x20x64x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc1758)
    %3336 = stablehlo.convert %3335 : (tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf64> loc(#loc1759)
    %3337 = stablehlo.compare  EQ, %3336, %cst_1 : (tensor<1x20x16x273xf64>, tensor<1x20x16x273xf64>) -> tensor<1x20x16x273xi1> loc(#loc1759)
    %3338 = stablehlo.not %3337 : tensor<1x20x16x273xi1> loc(#loc1760)
    %3339 = stablehlo.reduce(%3338 init: %c_9) across dimensions = [3] : (tensor<1x20x16x273xi1>, tensor<i1>) -> tensor<1x20x16xi1>
     reducer(%arg558: tensor<i1> loc("3010|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_34aten__any"), %arg559: tensor<i1> loc("3010|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_34aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1762)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1763)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc1761)
    %3340 = stablehlo.reshape %3339 : (tensor<1x20x16xi1>) -> tensor<1x20x16x1xi1> loc(#loc1761)
    %3341 = stablehlo.not %3340 : tensor<1x20x16x1xi1> loc(#loc1764)
    %3342 = stablehlo.reshape %3341 : (tensor<1x20x16x1xi1>) -> tensor<1x20x16xi1> loc(#loc1765)
    %3343 = stablehlo.broadcast_in_dim %3342, dims = [0, 1, 2] : (tensor<1x20x16xi1>) -> tensor<1x20x16x273xi1> loc(#loc1765)
    %3344 = stablehlo.reduce(%3335 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x20x16x273xf32>, tensor<f32>) -> tensor<1x20x16xf32> loc(#loc1766)
    %3345 = stablehlo.broadcast_in_dim %3344, dims = [0, 1, 2] : (tensor<1x20x16xf32>) -> tensor<1x20x16x273xf32> loc(#loc1766)
    %3346 = stablehlo.subtract %3335, %3345 : tensor<1x20x16x273xf32> loc(#loc1766)
    %3347 = stablehlo.exponential %3346 : tensor<1x20x16x273xf32> loc(#loc1766)
    %3348 = stablehlo.reduce(%3347 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x20x16x273xf32>, tensor<f32>) -> tensor<1x20x16xf32> loc(#loc1766)
    %3349 = stablehlo.broadcast_in_dim %3348, dims = [0, 1, 2] : (tensor<1x20x16xf32>) -> tensor<1x20x16x273xf32> loc(#loc1766)
    %3350 = stablehlo.divide %3347, %3349 : tensor<1x20x16x273xf32> loc(#loc1766)
    %3351 = stablehlo.select %3343, %cst_0, %3350 : tensor<1x20x16x273xi1>, tensor<1x20x16x273xf32> loc(#loc1767)
    %3352 = stablehlo.reshape %arg523 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3353 = stablehlo.reshape %3352 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %3354 = stablehlo.transpose %3353, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1768)
    %3355 = stablehlo.dot_general %3325, %3354, contracting_dims = [1] x [0] : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1769)
    %3356 = stablehlo.reshape %3355 : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc1770)
    %3357 = stablehlo.transpose %3356, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,273,64]{3,1,2,0}"} : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc1771)
    %3358 = stablehlo.convert %3357 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,273,64]{3,1,2,0}"} : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc1772)
    %3359 = stablehlo.dot_general %3351, %3358, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x20x16x273xf32>, tensor<1x20x273x64xf32>) -> tensor<1x20x16x64xf32> loc(#loc1773)
    %3360 = stablehlo.convert %3359 : (tensor<1x20x16x64xf32>) -> tensor<1x20x16x64xbf16> loc(#loc1774)
    %3361 = stablehlo.transpose %3360, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,20,64]{3,1,2,0}"} : (tensor<1x20x16x64xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc1775)
    %3362 = stablehlo.reshape %3361 : (tensor<1x16x20x64xbf16>) -> tensor<16x1280xbf16> loc(#loc1776)
    %3363 = stablehlo.reshape %arg522 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3364 = stablehlo.reshape %3363 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %3365 = stablehlo.transpose %3364, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1777)
    %3366 = stablehlo.dot_general %3362, %3365, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1778)
    %3367 = stablehlo.reshape %3366 : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1779)
    %3368 = stablehlo.divide %3367, %cst : tensor<1x16x1280xbf16> loc(#loc1780)
    %3369 = stablehlo.add %3368, %3304 : tensor<1x16x1280xbf16> loc(#loc1781)
    %3370 = stablehlo.reshape %arg533 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3371 = stablehlo.reshape %3370 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3372 = stablehlo.reshape %arg532 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3373 = stablehlo.reshape %3372 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3374 = stablehlo.composite "tenstorrent.layer_norm" %3369, %3371, %3373 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_17} : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1782)
    %3375 = stablehlo.reshape %3374 : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1783)
    %3376 = stablehlo.reshape %arg531 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %3377 = stablehlo.reshape %3376 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %3378 = stablehlo.transpose %3377, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1784)
    %3379 = stablehlo.dot_general %3375, %3378, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc1785)
    %3380 = stablehlo.reshape %3379 : (tensor<16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc1783)
    %3381 = stablehlo.composite "tenstorrent.gelu" %3380 {decomposition = @tenstorrent.gelu.impl} : (tensor<1x16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc1786)
    %3382 = stablehlo.reshape %3381 : (tensor<1x16x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc1787)
    %3383 = stablehlo.reshape %arg530 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %3384 = stablehlo.reshape %3383 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %3385 = stablehlo.transpose %3384, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1788)
    %3386 = stablehlo.dot_general %3382, %3385, contracting_dims = [1] x [0] : (tensor<16x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1789)
    %3387 = stablehlo.reshape %3386 : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1787)
    %3388 = stablehlo.add %3387, %3369 : tensor<1x16x1280xbf16> loc(#loc1790)
    %3389 = stablehlo.reshape %arg537 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3390 = stablehlo.reshape %3389 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3391 = stablehlo.reshape %arg536 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3392 = stablehlo.reshape %3391 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3393 = stablehlo.composite "tenstorrent.layer_norm" %3388, %3390, %3392 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_49} : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1791)
    %3394 = stablehlo.reshape %3393 : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1792)
    %3395 = stablehlo.reshape %arg541 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3396 = stablehlo.reshape %3395 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %3397 = stablehlo.transpose %3396, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1793)
    %3398 = stablehlo.dot_general %3394, %3397, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1794)
    %3399 = stablehlo.reshape %3398 : (tensor<16x1280xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc1795)
    %3400 = stablehlo.transpose %3399, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,16,64]{3,1,2,0}"} : (tensor<1x16x20x64xbf16>) -> tensor<1x20x16x64xbf16> loc(#loc1796)
    %3401 = stablehlo.convert %3400 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,16,64]{3,1,2,0}"} : (tensor<1x20x16x64xbf16>) -> tensor<1x20x16x64xf32> loc(#loc1797)
    %3402 = stablehlo.multiply %3401, %cst_7 : tensor<1x20x16x64xf32> loc(#loc1798)
    %3403 = stablehlo.reshape %arg539 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3404 = stablehlo.reshape %3403 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3405 = stablehlo.reshape %arg538 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3406 = stablehlo.reshape %3405 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3407 = stablehlo.composite "tenstorrent.layer_norm" %3234, %3404, %3406 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_2} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1799)
    %3408 = stablehlo.concatenate %3407, %3393, dim = 1 : (tensor<1x257x1280xbf16>, tensor<1x16x1280xbf16>) -> tensor<1x273x1280xbf16> loc(#loc1800)
    %3409 = stablehlo.reshape %3408 : (tensor<1x273x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1801)
    %3410 = stablehlo.reshape %arg540 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3411 = stablehlo.reshape %3410 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %3412 = stablehlo.transpose %3411, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1802)
    %3413 = stablehlo.dot_general %3409, %3412, contracting_dims = [1] x [0] : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1803)
    %3414 = stablehlo.reshape %3413 : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc1804)
    %3415 = stablehlo.transpose %3414, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,273,64]{3,1,2,0}"} : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc1805)
    %3416 = stablehlo.convert %3415 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,273,64]{3,1,2,0}"} : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc1806)
    %3417 = stablehlo.transpose %3416, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,64,273]{2,1,3,0}"} : (tensor<1x20x273x64xf32>) -> tensor<1x20x64x273xf32> loc(#loc1807)
    %3418 = stablehlo.multiply %3417, %cst_2 : tensor<1x20x64x273xf32> loc(#loc1808)
    %3419 = stablehlo.dot_general %3402, %3418, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x20x16x64xf32>, tensor<1x20x64x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc1809)
    %3420 = stablehlo.convert %3419 : (tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf64> loc(#loc1810)
    %3421 = stablehlo.compare  EQ, %3420, %cst_1 : (tensor<1x20x16x273xf64>, tensor<1x20x16x273xf64>) -> tensor<1x20x16x273xi1> loc(#loc1810)
    %3422 = stablehlo.not %3421 : tensor<1x20x16x273xi1> loc(#loc1811)
    %3423 = stablehlo.reduce(%3422 init: %c_9) across dimensions = [3] : (tensor<1x20x16x273xi1>, tensor<i1>) -> tensor<1x20x16xi1>
     reducer(%arg558: tensor<i1> loc("3095|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_35aten__any"), %arg559: tensor<i1> loc("3095|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_35aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1813)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1814)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc1812)
    %3424 = stablehlo.reshape %3423 : (tensor<1x20x16xi1>) -> tensor<1x20x16x1xi1> loc(#loc1812)
    %3425 = stablehlo.not %3424 : tensor<1x20x16x1xi1> loc(#loc1815)
    %3426 = stablehlo.reshape %3425 : (tensor<1x20x16x1xi1>) -> tensor<1x20x16xi1> loc(#loc1816)
    %3427 = stablehlo.broadcast_in_dim %3426, dims = [0, 1, 2] : (tensor<1x20x16xi1>) -> tensor<1x20x16x273xi1> loc(#loc1816)
    %3428 = stablehlo.reduce(%3419 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x20x16x273xf32>, tensor<f32>) -> tensor<1x20x16xf32> loc(#loc1817)
    %3429 = stablehlo.broadcast_in_dim %3428, dims = [0, 1, 2] : (tensor<1x20x16xf32>) -> tensor<1x20x16x273xf32> loc(#loc1817)
    %3430 = stablehlo.subtract %3419, %3429 : tensor<1x20x16x273xf32> loc(#loc1817)
    %3431 = stablehlo.exponential %3430 : tensor<1x20x16x273xf32> loc(#loc1817)
    %3432 = stablehlo.reduce(%3431 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x20x16x273xf32>, tensor<f32>) -> tensor<1x20x16xf32> loc(#loc1817)
    %3433 = stablehlo.broadcast_in_dim %3432, dims = [0, 1, 2] : (tensor<1x20x16xf32>) -> tensor<1x20x16x273xf32> loc(#loc1817)
    %3434 = stablehlo.divide %3431, %3433 : tensor<1x20x16x273xf32> loc(#loc1817)
    %3435 = stablehlo.select %3427, %cst_0, %3434 : tensor<1x20x16x273xi1>, tensor<1x20x16x273xf32> loc(#loc1818)
    %3436 = stablehlo.reshape %arg535 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3437 = stablehlo.reshape %3436 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %3438 = stablehlo.transpose %3437, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1819)
    %3439 = stablehlo.dot_general %3409, %3438, contracting_dims = [1] x [0] : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1820)
    %3440 = stablehlo.reshape %3439 : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc1821)
    %3441 = stablehlo.transpose %3440, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,273,64]{3,1,2,0}"} : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc1822)
    %3442 = stablehlo.convert %3441 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,273,64]{3,1,2,0}"} : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc1823)
    %3443 = stablehlo.dot_general %3435, %3442, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x20x16x273xf32>, tensor<1x20x273x64xf32>) -> tensor<1x20x16x64xf32> loc(#loc1824)
    %3444 = stablehlo.convert %3443 : (tensor<1x20x16x64xf32>) -> tensor<1x20x16x64xbf16> loc(#loc1825)
    %3445 = stablehlo.transpose %3444, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,20,64]{3,1,2,0}"} : (tensor<1x20x16x64xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc1826)
    %3446 = stablehlo.reshape %3445 : (tensor<1x16x20x64xbf16>) -> tensor<16x1280xbf16> loc(#loc1827)
    %3447 = stablehlo.reshape %arg534 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3448 = stablehlo.reshape %3447 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %3449 = stablehlo.transpose %3448, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1828)
    %3450 = stablehlo.dot_general %3446, %3449, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1829)
    %3451 = stablehlo.reshape %3450 : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1830)
    %3452 = stablehlo.divide %3451, %cst : tensor<1x16x1280xbf16> loc(#loc1831)
    %3453 = stablehlo.add %3452, %3388 : tensor<1x16x1280xbf16> loc(#loc1832)
    %3454 = stablehlo.reshape %arg545 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3455 = stablehlo.reshape %3454 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3456 = stablehlo.reshape %arg544 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3457 = stablehlo.reshape %3456 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3458 = stablehlo.composite "tenstorrent.layer_norm" %3453, %3455, %3457 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_26} : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1833)
    %3459 = stablehlo.reshape %3458 : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1834)
    %3460 = stablehlo.reshape %arg543 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %3461 = stablehlo.reshape %3460 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %3462 = stablehlo.transpose %3461, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1835)
    %3463 = stablehlo.dot_general %3459, %3462, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc1836)
    %3464 = stablehlo.reshape %3463 : (tensor<16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc1834)
    %3465 = stablehlo.composite "tenstorrent.gelu" %3464 {decomposition = @tenstorrent.gelu.impl_3} : (tensor<1x16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc1837)
    %3466 = stablehlo.reshape %3465 : (tensor<1x16x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc1838)
    %3467 = stablehlo.reshape %arg542 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %3468 = stablehlo.reshape %3467 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %3469 = stablehlo.transpose %3468, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1839)
    %3470 = stablehlo.dot_general %3466, %3469, contracting_dims = [1] x [0] : (tensor<16x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1840)
    %3471 = stablehlo.reshape %3470 : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1838)
    %3472 = stablehlo.add %3471, %3453 : tensor<1x16x1280xbf16> loc(#loc1841)
    %3473 = stablehlo.reshape %arg549 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3474 = stablehlo.reshape %3473 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3475 = stablehlo.reshape %arg548 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3476 = stablehlo.reshape %3475 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3477 = stablehlo.composite "tenstorrent.layer_norm" %3472, %3474, %3476 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_71} : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1842)
    %3478 = stablehlo.reshape %3477 : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1843)
    %3479 = stablehlo.reshape %arg553 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3480 = stablehlo.reshape %3479 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %3481 = stablehlo.transpose %3480, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1844)
    %3482 = stablehlo.dot_general %3478, %3481, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1845)
    %3483 = stablehlo.reshape %3482 : (tensor<16x1280xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc1846)
    %3484 = stablehlo.transpose %3483, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,16,64]{3,1,2,0}"} : (tensor<1x16x20x64xbf16>) -> tensor<1x20x16x64xbf16> loc(#loc1847)
    %3485 = stablehlo.convert %3484 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,16,64]{3,1,2,0}"} : (tensor<1x20x16x64xbf16>) -> tensor<1x20x16x64xf32> loc(#loc1848)
    %3486 = stablehlo.multiply %3485, %cst_7 : tensor<1x20x16x64xf32> loc(#loc1849)
    %3487 = stablehlo.reshape %arg551 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3488 = stablehlo.reshape %3487 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3489 = stablehlo.reshape %arg550 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3490 = stablehlo.reshape %3489 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3491 = stablehlo.composite "tenstorrent.layer_norm" %3234, %3488, %3490 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_1} : (tensor<1x257x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x257x1280xbf16> loc(#loc1850)
    %3492 = stablehlo.concatenate %3491, %3477, dim = 1 : (tensor<1x257x1280xbf16>, tensor<1x16x1280xbf16>) -> tensor<1x273x1280xbf16> loc(#loc1851)
    %3493 = stablehlo.reshape %3492 : (tensor<1x273x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1852)
    %3494 = stablehlo.reshape %arg552 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3495 = stablehlo.reshape %3494 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %3496 = stablehlo.transpose %3495, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1853)
    %3497 = stablehlo.dot_general %3493, %3496, contracting_dims = [1] x [0] : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1854)
    %3498 = stablehlo.reshape %3497 : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc1855)
    %3499 = stablehlo.transpose %3498, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,273,64]{3,1,2,0}"} : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc1856)
    %3500 = stablehlo.convert %3499 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,273,64]{3,1,2,0}"} : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc1857)
    %3501 = stablehlo.transpose %3500, dims = [0, 1, 3, 2] {result_layout = dense<[2, 1, 3, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,64,273]{2,1,3,0}"} : (tensor<1x20x273x64xf32>) -> tensor<1x20x64x273xf32> loc(#loc1858)
    %3502 = stablehlo.multiply %3501, %cst_2 : tensor<1x20x64x273xf32> loc(#loc1859)
    %3503 = stablehlo.dot_general %3486, %3502, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x20x16x64xf32>, tensor<1x20x64x273xf32>) -> tensor<1x20x16x273xf32> loc(#loc1860)
    %3504 = stablehlo.convert %3503 : (tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf64> loc(#loc1861)
    %3505 = stablehlo.compare  EQ, %3504, %cst_1 : (tensor<1x20x16x273xf64>, tensor<1x20x16x273xf64>) -> tensor<1x20x16x273xi1> loc(#loc1861)
    %3506 = stablehlo.not %3505 : tensor<1x20x16x273xi1> loc(#loc1862)
    %3507 = stablehlo.reduce(%3506 init: %c_9) across dimensions = [3] : (tensor<1x20x16x273xi1>, tensor<i1>) -> tensor<1x20x16xi1>
     reducer(%arg558: tensor<i1> loc("3180|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_36aten__any"), %arg559: tensor<i1> loc("3180|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|any_36aten__any"))  {
      %3572 = stablehlo.or %arg558, %arg559 : tensor<i1> loc(#loc1864)
      %3573 = stablehlo.select %3572, %c, %c_9 : tensor<i1>, tensor<i1> loc(#loc1865)
      stablehlo.return %3573 : tensor<i1> loc(#loc)
    } loc(#loc1863)
    %3508 = stablehlo.reshape %3507 : (tensor<1x20x16xi1>) -> tensor<1x20x16x1xi1> loc(#loc1863)
    %3509 = stablehlo.not %3508 : tensor<1x20x16x1xi1> loc(#loc1866)
    %3510 = stablehlo.reshape %3509 : (tensor<1x20x16x1xi1>) -> tensor<1x20x16xi1> loc(#loc1867)
    %3511 = stablehlo.broadcast_in_dim %3510, dims = [0, 1, 2] : (tensor<1x20x16xi1>) -> tensor<1x20x16x273xi1> loc(#loc1867)
    %3512 = stablehlo.reduce(%3503 init: %cst_8) applies stablehlo.maximum across dimensions = [3] : (tensor<1x20x16x273xf32>, tensor<f32>) -> tensor<1x20x16xf32> loc(#loc1868)
    %3513 = stablehlo.broadcast_in_dim %3512, dims = [0, 1, 2] : (tensor<1x20x16xf32>) -> tensor<1x20x16x273xf32> loc(#loc1868)
    %3514 = stablehlo.subtract %3503, %3513 : tensor<1x20x16x273xf32> loc(#loc1868)
    %3515 = stablehlo.exponential %3514 : tensor<1x20x16x273xf32> loc(#loc1868)
    %3516 = stablehlo.reduce(%3515 init: %cst_10) applies stablehlo.add across dimensions = [3] : (tensor<1x20x16x273xf32>, tensor<f32>) -> tensor<1x20x16xf32> loc(#loc1868)
    %3517 = stablehlo.broadcast_in_dim %3516, dims = [0, 1, 2] : (tensor<1x20x16xf32>) -> tensor<1x20x16x273xf32> loc(#loc1868)
    %3518 = stablehlo.divide %3515, %3517 : tensor<1x20x16x273xf32> loc(#loc1868)
    %3519 = stablehlo.select %3511, %cst_0, %3518 : tensor<1x20x16x273xi1>, tensor<1x20x16x273xf32> loc(#loc1869)
    %3520 = stablehlo.reshape %arg547 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3521 = stablehlo.reshape %3520 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %3522 = stablehlo.transpose %3521, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1870)
    %3523 = stablehlo.dot_general %3493, %3522, contracting_dims = [1] x [0] : (tensor<273x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<273x1280xbf16> loc(#loc1871)
    %3524 = stablehlo.reshape %3523 : (tensor<273x1280xbf16>) -> tensor<1x273x20x64xbf16> loc(#loc1872)
    %3525 = stablehlo.transpose %3524, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,20,273,64]{3,1,2,0}"} : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16> loc(#loc1873)
    %3526 = stablehlo.convert %3525 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,20,273,64]{3,1,2,0}"} : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32> loc(#loc1874)
    %3527 = stablehlo.dot_general %3519, %3526, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x20x16x273xf32>, tensor<1x20x273x64xf32>) -> tensor<1x20x16x64xf32> loc(#loc1875)
    %3528 = stablehlo.convert %3527 : (tensor<1x20x16x64xf32>) -> tensor<1x20x16x64xbf16> loc(#loc1876)
    %3529 = stablehlo.transpose %3528, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,16,20,64]{3,1,2,0}"} : (tensor<1x20x16x64xbf16>) -> tensor<1x16x20x64xbf16> loc(#loc1877)
    %3530 = stablehlo.reshape %3529 : (tensor<1x16x20x64xbf16>) -> tensor<16x1280xbf16> loc(#loc1878)
    %3531 = stablehlo.reshape %arg546 : (tensor<1280x1280xbf16>) -> tensor<1x1280x1280xbf16> loc(#loc2)
    %3532 = stablehlo.reshape %3531 : (tensor<1x1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc2)
    %3533 = stablehlo.transpose %3532, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,1280]{0,1}"} : (tensor<1280x1280xbf16>) -> tensor<1280x1280xbf16> loc(#loc1879)
    %3534 = stablehlo.dot_general %3530, %3533, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1880)
    %3535 = stablehlo.reshape %3534 : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1881)
    %3536 = stablehlo.divide %3535, %cst : tensor<1x16x1280xbf16> loc(#loc1882)
    %3537 = stablehlo.add %3536, %3472 : tensor<1x16x1280xbf16> loc(#loc1883)
    %3538 = stablehlo.reshape %arg557 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3539 = stablehlo.reshape %3538 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3540 = stablehlo.reshape %arg556 : (tensor<1280xbf16>) -> tensor<1x1x1280xbf16> loc(#loc2)
    %3541 = stablehlo.reshape %3540 : (tensor<1x1x1280xbf16>) -> tensor<1280xbf16> loc(#loc2)
    %3542 = stablehlo.composite "tenstorrent.layer_norm" %3537, %3539, %3541 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<1280> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl_0} : (tensor<1x16x1280xbf16>, tensor<1280xbf16>, tensor<1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1884)
    %3543 = stablehlo.reshape %3542 : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1885)
    %3544 = stablehlo.reshape %arg555 : (tensor<5120x1280xbf16>) -> tensor<1x5120x1280xbf16> loc(#loc2)
    %3545 = stablehlo.reshape %3544 : (tensor<1x5120x1280xbf16>) -> tensor<5120x1280xbf16> loc(#loc2)
    %3546 = stablehlo.transpose %3545, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,5120]{0,1}"} : (tensor<5120x1280xbf16>) -> tensor<1280x5120xbf16> loc(#loc1886)
    %3547 = stablehlo.dot_general %3543, %3546, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc1887)
    %3548 = stablehlo.reshape %3547 : (tensor<16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc1885)
    %3549 = stablehlo.composite "tenstorrent.gelu" %3548 {decomposition = @tenstorrent.gelu.impl_24} : (tensor<1x16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc1888)
    %3550 = stablehlo.reshape %3549 : (tensor<1x16x5120xbf16>) -> tensor<16x5120xbf16> loc(#loc1889)
    %3551 = stablehlo.reshape %arg554 : (tensor<1280x5120xbf16>) -> tensor<1x1280x5120xbf16> loc(#loc2)
    %3552 = stablehlo.reshape %3551 : (tensor<1x1280x5120xbf16>) -> tensor<1280x5120xbf16> loc(#loc2)
    %3553 = stablehlo.transpose %3552, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1280]{0,1}"} : (tensor<1280x5120xbf16>) -> tensor<5120x1280xbf16> loc(#loc1890)
    %3554 = stablehlo.dot_general %3550, %3553, contracting_dims = [1] x [0] : (tensor<16x5120xbf16>, tensor<5120x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1891)
    %3555 = stablehlo.reshape %3554 : (tensor<16x1280xbf16>) -> tensor<1x16x1280xbf16> loc(#loc1889)
    %3556 = stablehlo.add %3555, %3537 : tensor<1x16x1280xbf16> loc(#loc1892)
    %3557 = stablehlo.reshape %3556 : (tensor<1x16x1280xbf16>) -> tensor<16x1280xbf16> loc(#loc1893)
    %3558 = stablehlo.reshape %arg3 : (tensor<2048x1280xbf16>) -> tensor<1x2048x1280xbf16> loc(#loc2)
    %3559 = stablehlo.reshape %3558 : (tensor<1x2048x1280xbf16>) -> tensor<2048x1280xbf16> loc(#loc2)
    %3560 = stablehlo.transpose %3559, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1280,2048]{0,1}"} : (tensor<2048x1280xbf16>) -> tensor<1280x2048xbf16> loc(#loc1894)
    %3561 = stablehlo.dot_general %3557, %3560, contracting_dims = [1] x [0] : (tensor<16x1280xbf16>, tensor<1280x2048xbf16>) -> tensor<16x2048xbf16> loc(#loc1895)
    %3562 = stablehlo.reshape %3561 : (tensor<16x2048xbf16>) -> tensor<1x16x2048xbf16> loc(#loc1893)
    %3563 = stablehlo.reshape %arg2 : (tensor<2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc2)
    %3564 = stablehlo.reshape %3563 : (tensor<1x1x2048xbf16>) -> tensor<2048xbf16> loc(#loc2)
    %3565 = stablehlo.broadcast_in_dim %3564, dims = [2] : (tensor<2048xbf16>) -> tensor<1x16x2048xbf16> loc(#loc1896)
    %3566 = stablehlo.add %3562, %3565 : tensor<1x16x2048xbf16> loc(#loc1896)
    %3567 = stablehlo.reshape %arg1 : (tensor<2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc2)
    %3568 = stablehlo.reshape %3567 : (tensor<1x1x2048xbf16>) -> tensor<2048xbf16> loc(#loc2)
    %3569 = stablehlo.reshape %arg0 : (tensor<2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc2)
    %3570 = stablehlo.reshape %3569 : (tensor<1x1x2048xbf16>) -> tensor<2048xbf16> loc(#loc2)
    %3571 = stablehlo.composite "tenstorrent.layer_norm" %3566, %3568, %3570 {composite_attributes = {epsilon = 9.99999974E-6 : f32, normalized_shape = dense<2048> : tensor<1xi64>}, decomposition = @tenstorrent.layer_norm.impl} : (tensor<1x16x2048xbf16>, tensor<2048xbf16>, tensor<2048xbf16>) -> tensor<1x16x2048xbf16> loc(#loc1897)
    return %3571 : tensor<1x16x2048xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl(%arg0: tensor<1x16x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("3219|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|mark_tensor_384xla__mark_tensor"), %arg1: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("3220|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|mark_tensor_385xla__mark_tensor"), %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("3221|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|mark_tensor_386xla__mark_tensor")) -> tensor<1x16x2048xbf16> {
    %cst = stablehlo.constant dense<4.8828125E-4> : tensor<1x16xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<9.99999974E-6> : tensor<1x16x1xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x16x2048xbf16>) -> tensor<1x16x2048xf32> loc(#loc1901)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x2048xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc1902)
    %2 = stablehlo.multiply %1, %cst : tensor<1x16xf32> loc(#loc1902)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x2048xf32> loc(#loc1903)
    %4 = stablehlo.subtract %0, %3 : tensor<1x16x2048xf32> loc(#loc1903)
    %5 = stablehlo.multiply %4, %4 : tensor<1x16x2048xf32> loc(#loc1902)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x2048xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc1902)
    %7 = stablehlo.multiply %6, %cst : tensor<1x16xf32> loc(#loc1902)
    %8 = stablehlo.reshape %7 : (tensor<1x16xf32>) -> tensor<1x16x1xf32> loc(#loc1902)
    %9 = stablehlo.add %8, %cst_0 : tensor<1x16x1xf32> loc(#loc1904)
    %10 = stablehlo.rsqrt %9 : tensor<1x16x1xf32> loc(#loc1905)
    %11 = stablehlo.reshape %10 : (tensor<1x16x1xf32>) -> tensor<1x16xf32> loc(#loc1906)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x2048xf32> loc(#loc1906)
    %13 = stablehlo.multiply %4, %12 : tensor<1x16x2048xf32> loc(#loc1906)
    %14 = stablehlo.convert %arg1 : (tensor<2048xbf16>) -> tensor<2048xf32> loc(#loc1907)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<2048xf32>) -> tensor<1x16x2048xf32> loc(#loc1908)
    %16 = stablehlo.multiply %13, %15 : tensor<1x16x2048xf32> loc(#loc1908)
    %17 = stablehlo.convert %arg2 : (tensor<2048xbf16>) -> tensor<2048xf32> loc(#loc1909)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<2048xf32>) -> tensor<1x16x2048xf32> loc(#loc1909)
    %19 = stablehlo.add %16, %18 : tensor<1x16x2048xf32> loc(#loc1909)
    %20 = stablehlo.convert %19 : (tensor<1x16x2048xf32>) -> tensor<1x16x2048xbf16> loc(#loc1910)
    return %20 : tensor<1x16x2048xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_0(%arg0: tensor<1x16x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("3194|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_378xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("3195|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_379xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("3196|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_380xla__mark_tensor")) -> tensor<1x16x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x16x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x16xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x16x1280xbf16>) -> tensor<1x16x1280xf32> loc(#loc1914)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc1915)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x16xf32> loc(#loc1915)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc1916)
    %4 = stablehlo.subtract %0, %3 : tensor<1x16x1280xf32> loc(#loc1916)
    %5 = stablehlo.multiply %4, %4 : tensor<1x16x1280xf32> loc(#loc1915)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc1915)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x16xf32> loc(#loc1915)
    %8 = stablehlo.reshape %7 : (tensor<1x16xf32>) -> tensor<1x16x1xf32> loc(#loc1915)
    %9 = stablehlo.add %8, %cst : tensor<1x16x1xf32> loc(#loc1917)
    %10 = stablehlo.rsqrt %9 : tensor<1x16x1xf32> loc(#loc1918)
    %11 = stablehlo.reshape %10 : (tensor<1x16x1xf32>) -> tensor<1x16xf32> loc(#loc1919)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc1919)
    %13 = stablehlo.multiply %4, %12 : tensor<1x16x1280xf32> loc(#loc1919)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc1920)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc1921)
    %16 = stablehlo.multiply %13, %15 : tensor<1x16x1280xf32> loc(#loc1921)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc1922)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc1922)
    %19 = stablehlo.add %16, %18 : tensor<1x16x1280xf32> loc(#loc1922)
    %20 = stablehlo.convert %19 : (tensor<1x16x1280xf32>) -> tensor<1x16x1280xbf16> loc(#loc1923)
    return %20 : tensor<1x16x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_1(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("3131|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_370xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("3132|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_371xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("3133|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_372xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc1927)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc1928)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc1928)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc1929)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc1929)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc1928)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc1928)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc1928)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc1928)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc1930)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc1931)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc1932)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc1932)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc1932)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc1933)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc1934)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc1934)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc1935)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc1935)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc1935)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc1936)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_2(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("3046|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_356xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("3047|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_357xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("3048|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_358xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc1940)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc1941)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc1941)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc1942)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc1942)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc1941)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc1941)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc1941)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc1941)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc1943)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc1944)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc1945)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc1945)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc1945)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc1946)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc1947)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc1947)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc1948)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc1948)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc1948)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc1949)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl(%arg0: tensor<1x16x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("3039|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|GELU[getattr(resampler.layers[1].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|mark_tensor_354xla__mark_tensor")) -> tensor<1x16x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x16x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x16x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x16x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x16x5120xbf16> loc(#loc1951)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x16x5120xbf16> loc(#loc1951)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc1951)
    %3 = stablehlo.add %2, %cst : tensor<1x16x5120xbf16> loc(#loc1951)
    %4 = stablehlo.multiply %0, %3 : tensor<1x16x5120xbf16> loc(#loc1951)
    return %4 : tensor<1x16x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_0(%arg0: tensor<1x16x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2954|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|GELU[getattr(resampler.layers[0].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|mark_tensor_340xla__mark_tensor")) -> tensor<1x16x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x16x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x16x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x16x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x16x5120xbf16> loc(#loc1953)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x16x5120xbf16> loc(#loc1953)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc1953)
    %3 = stablehlo.add %2, %cst : tensor<1x16x5120xbf16> loc(#loc1953)
    %4 = stablehlo.multiply %0, %3 : tensor<1x16x5120xbf16> loc(#loc1953)
    return %4 : tensor<1x16x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_3(%arg0: tensor<1x16x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2939|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_336xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2940|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_337xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2941|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_338xla__mark_tensor")) -> tensor<1x16x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x16x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x16xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x16x1280xbf16>) -> tensor<1x16x1280xf32> loc(#loc1957)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc1958)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x16xf32> loc(#loc1958)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc1959)
    %4 = stablehlo.subtract %0, %3 : tensor<1x16x1280xf32> loc(#loc1959)
    %5 = stablehlo.multiply %4, %4 : tensor<1x16x1280xf32> loc(#loc1958)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc1958)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x16xf32> loc(#loc1958)
    %8 = stablehlo.reshape %7 : (tensor<1x16xf32>) -> tensor<1x16x1xf32> loc(#loc1958)
    %9 = stablehlo.add %8, %cst : tensor<1x16x1xf32> loc(#loc1960)
    %10 = stablehlo.rsqrt %9 : tensor<1x16x1xf32> loc(#loc1961)
    %11 = stablehlo.reshape %10 : (tensor<1x16x1xf32>) -> tensor<1x16xf32> loc(#loc1962)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc1962)
    %13 = stablehlo.multiply %4, %12 : tensor<1x16x1280xf32> loc(#loc1962)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc1963)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc1964)
    %16 = stablehlo.multiply %13, %15 : tensor<1x16x1280xf32> loc(#loc1964)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc1965)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc1965)
    %19 = stablehlo.add %16, %18 : tensor<1x16x1280xf32> loc(#loc1965)
    %20 = stablehlo.convert %19 : (tensor<1x16x1280xf32>) -> tensor<1x16x1280xbf16> loc(#loc1966)
    return %20 : tensor<1x16x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_1(%arg0: tensor<1x257x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2791|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[29].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_302xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc1968)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc1968)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1968)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc1968)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc1968)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_4(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2724|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_294xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2725|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_295xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2726|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_296xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc1972)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc1973)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc1973)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc1974)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc1974)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc1973)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc1973)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc1973)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc1973)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc1975)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc1976)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc1977)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc1977)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc1977)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc1978)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc1979)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc1979)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc1980)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc1980)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc1980)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc1981)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_2(%arg0: tensor<1x257x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2717|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[28].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_292xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc1983)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc1983)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc1983)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc1983)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc1983)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_5(%arg0: tensor<1x16x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2974|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_346xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2975|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_347xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2976|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_348xla__mark_tensor")) -> tensor<1x16x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x16x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x16xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x16x1280xbf16>) -> tensor<1x16x1280xf32> loc(#loc1987)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc1988)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x16xf32> loc(#loc1988)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc1989)
    %4 = stablehlo.subtract %0, %3 : tensor<1x16x1280xf32> loc(#loc1989)
    %5 = stablehlo.multiply %4, %4 : tensor<1x16x1280xf32> loc(#loc1988)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc1988)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x16xf32> loc(#loc1988)
    %8 = stablehlo.reshape %7 : (tensor<1x16xf32>) -> tensor<1x16x1xf32> loc(#loc1988)
    %9 = stablehlo.add %8, %cst : tensor<1x16x1xf32> loc(#loc1990)
    %10 = stablehlo.rsqrt %9 : tensor<1x16x1xf32> loc(#loc1991)
    %11 = stablehlo.reshape %10 : (tensor<1x16x1xf32>) -> tensor<1x16xf32> loc(#loc1992)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc1992)
    %13 = stablehlo.multiply %4, %12 : tensor<1x16x1280xf32> loc(#loc1992)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc1993)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc1994)
    %16 = stablehlo.multiply %13, %15 : tensor<1x16x1280xf32> loc(#loc1994)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc1995)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc1995)
    %19 = stablehlo.add %16, %18 : tensor<1x16x1280xf32> loc(#loc1995)
    %20 = stablehlo.convert %19 : (tensor<1x16x1280xf32>) -> tensor<1x16x1280xbf16> loc(#loc1996)
    return %20 : tensor<1x16x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_6(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2701|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_288xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2702|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_289xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2703|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_290xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2000)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2001)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2001)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2002)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2002)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2001)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2001)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2001)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2001)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2003)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2004)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2005)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2005)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2005)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2006)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2007)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2007)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2008)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2008)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2008)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2009)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_7(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2650|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_284xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2651|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_285xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2652|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_286xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2013)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2014)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2014)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2015)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2015)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2014)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2014)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2014)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2014)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2016)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2017)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2018)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2018)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2018)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2019)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2020)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2020)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2021)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2021)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2021)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2022)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_3(%arg0: tensor<1x16x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("3124|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|GELU[getattr(resampler.layers[2].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|mark_tensor_368xla__mark_tensor")) -> tensor<1x16x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x16x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x16x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x16x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x16x5120xbf16> loc(#loc2024)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x16x5120xbf16> loc(#loc2024)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc2024)
    %3 = stablehlo.add %2, %cst : tensor<1x16x5120xbf16> loc(#loc2024)
    %4 = stablehlo.multiply %0, %3 : tensor<1x16x5120xbf16> loc(#loc2024)
    return %4 : tensor<1x16x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_8(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2576|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_274xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2577|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_275xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2578|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_276xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2028)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2029)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2029)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2030)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2030)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2029)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2029)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2029)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2029)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2031)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2032)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2033)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2033)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2033)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2034)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2035)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2035)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2036)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2036)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2036)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2037)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_9(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2553|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_268xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2554|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_269xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2555|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_270xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2041)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2042)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2042)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2043)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2043)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2042)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2042)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2042)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2042)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2044)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2045)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2046)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2046)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2046)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2047)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2048)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2048)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2049)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2049)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2049)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2050)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_10(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2502|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_264xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2503|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_265xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2504|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_266xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2054)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2055)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2055)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2056)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2056)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2055)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2055)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2055)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2055)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2057)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2058)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2059)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2059)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2059)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2060)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2061)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2061)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2062)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2062)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2062)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2063)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_4(%arg0: tensor<1x257x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2495|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[25].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_262xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2065)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2065)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2065)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2065)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2065)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_11(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2479|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_258xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2480|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_259xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2481|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_260xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2069)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2070)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2070)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2071)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2071)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2070)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2070)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2070)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2070)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2072)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2073)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2074)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2074)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2074)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2075)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2076)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2076)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2077)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2077)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2077)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2078)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_12(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2428|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_254xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2429|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_255xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2430|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_256xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2082)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2083)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2083)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2084)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2084)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2083)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2083)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2083)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2083)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2085)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2086)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2087)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2087)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2087)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2088)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2089)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2089)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2090)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2090)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2090)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2091)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_5(%arg0: tensor<1x257x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2421|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[24].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_252xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2093)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2093)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2093)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2093)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2093)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_13(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2405|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_248xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2406|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_249xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2407|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_250xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2097)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2098)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2098)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2099)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2099)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2098)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2098)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2098)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2098)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2100)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2101)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2102)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2102)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2102)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2103)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2104)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2104)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2105)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2105)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2105)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2106)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_14(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2354|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_244xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2355|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_245xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2356|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_246xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2110)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2111)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2111)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2112)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2112)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2111)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2111)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2111)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2111)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2113)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2114)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2115)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2115)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2115)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2116)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2117)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2117)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2118)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2118)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2118)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2119)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_6(%arg0: tensor<1x257x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2347|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[23].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_242xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2121)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2121)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2121)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2121)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2121)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_15(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2331|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_238xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2332|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_239xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2333|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_240xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2125)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2126)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2126)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2127)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2127)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2126)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2126)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2126)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2126)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2128)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2129)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2130)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2130)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2130)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2131)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2132)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2132)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2133)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2133)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2133)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2134)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_16(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2206|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_224xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2207|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_225xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2208|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_226xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2138)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2139)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2139)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2140)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2140)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2139)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2139)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2139)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2139)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2141)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2142)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2143)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2143)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2143)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2144)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2145)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2145)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2146)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2146)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2146)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2147)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_7(%arg0: tensor<1x257x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2199|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[21].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_222xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2149)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2149)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2149)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2149)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2149)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_17(%arg0: tensor<1x16x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("3024|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_350xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("3025|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_351xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("3026|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_352xla__mark_tensor")) -> tensor<1x16x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x16x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x16xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x16x1280xbf16>) -> tensor<1x16x1280xf32> loc(#loc2153)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc2154)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x16xf32> loc(#loc2154)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc2155)
    %4 = stablehlo.subtract %0, %3 : tensor<1x16x1280xf32> loc(#loc2155)
    %5 = stablehlo.multiply %4, %4 : tensor<1x16x1280xf32> loc(#loc2154)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc2154)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x16xf32> loc(#loc2154)
    %8 = stablehlo.reshape %7 : (tensor<1x16xf32>) -> tensor<1x16x1xf32> loc(#loc2154)
    %9 = stablehlo.add %8, %cst : tensor<1x16x1xf32> loc(#loc2156)
    %10 = stablehlo.rsqrt %9 : tensor<1x16x1xf32> loc(#loc2157)
    %11 = stablehlo.reshape %10 : (tensor<1x16x1xf32>) -> tensor<1x16xf32> loc(#loc2158)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc2158)
    %13 = stablehlo.multiply %4, %12 : tensor<1x16x1280xf32> loc(#loc2158)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2159)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc2160)
    %16 = stablehlo.multiply %13, %15 : tensor<1x16x1280xf32> loc(#loc2160)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2161)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc2161)
    %19 = stablehlo.add %16, %18 : tensor<1x16x1280xf32> loc(#loc2161)
    %20 = stablehlo.convert %19 : (tensor<1x16x1280xf32>) -> tensor<1x16x1280xbf16> loc(#loc2162)
    return %20 : tensor<1x16x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_18(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2183|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_218xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2184|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_219xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2185|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_220xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2166)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2167)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2167)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2168)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2168)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2167)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2167)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2167)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2167)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2169)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2170)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2171)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2171)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2171)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2172)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2173)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2173)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2174)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2174)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2174)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2175)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_8(%arg0: tensor<1x257x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2125|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[20].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_212xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2177)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2177)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2177)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2177)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2177)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_9(%arg0: tensor<1x257x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2569|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[26].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_272xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2179)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2179)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2179)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2179)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2179)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_19(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2058|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_204xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2059|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_205xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2060|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_206xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2183)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2184)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2184)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2185)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2185)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2184)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2184)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2184)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2184)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2186)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2187)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2188)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2188)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2188)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2189)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2190)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2190)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2191)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2191)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2191)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2192)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_10(%arg0: tensor<1x257x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2051|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[19].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_202xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2194)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2194)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2194)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2194)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2194)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_20(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1984|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_194xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1985|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_195xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1986|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_196xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2198)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2199)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2199)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2200)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2200)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2199)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2199)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2199)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2199)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2201)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2202)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2203)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2203)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2203)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2204)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2205)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2205)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2206)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2206)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2206)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2207)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_11(%arg0: tensor<1x257x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1237|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[8].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_92xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2209)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2209)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2209)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2209)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2209)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_21(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1318|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_104xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1319|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_105xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1320|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_106xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2213)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2214)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2214)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2215)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2215)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2214)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2214)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2214)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2214)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2216)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2217)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2218)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2218)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2218)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2219)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2220)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2220)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2221)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2221)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2221)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2222)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_22(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1170|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_84xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1171|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_85xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1172|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_86xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2226)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2227)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2227)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2228)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2228)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2227)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2227)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2227)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2227)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2229)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2230)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2231)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2231)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2231)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2232)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2233)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2233)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2234)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2234)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2234)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2235)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_12(%arg0: tensor<1x257x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2865|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[30].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_312xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2237)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2237)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2237)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2237)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2237)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_13(%arg0: tensor<1x257x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1163|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[7].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_82xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2239)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2239)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2239)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2239)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2239)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_23(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2849|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_308xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2850|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_309xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2851|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_310xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2243)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2244)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2244)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2245)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2245)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2244)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2244)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2244)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2244)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2246)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2247)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2248)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2248)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2248)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2249)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2250)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2250)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2251)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2251)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2251)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2252)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_14(%arg0: tensor<1x257x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2273|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[22].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_232xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2254)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2254)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2254)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2254)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2254)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_24(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2257|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_228xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2258|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_229xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2259|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_230xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2258)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2259)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2259)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2260)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2260)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2259)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2259)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2259)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2259)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2261)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2262)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2263)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2263)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2263)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2264)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2265)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2265)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2266)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2266)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2266)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2267)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_15(%arg0: tensor<1x257x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1089|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[6].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_72xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2269)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2269)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2269)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2269)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2269)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_25(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1073|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_68xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1074|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_69xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1075|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_70xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2273)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2274)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2274)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2275)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2275)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2274)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2274)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2274)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2274)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2276)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2277)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2278)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2278)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2278)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2279)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2280)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2280)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2281)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2281)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2281)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2282)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_16(%arg0: tensor<1x257x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("645|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[0].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_12xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2284)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2284)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2284)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2284)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2284)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_26(%arg0: tensor<1x16x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("3109|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_364xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("3110|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_365xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("3111|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_366xla__mark_tensor")) -> tensor<1x16x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x16x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x16xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x16x1280xbf16>) -> tensor<1x16x1280xf32> loc(#loc2288)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc2289)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x16xf32> loc(#loc2289)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc2290)
    %4 = stablehlo.subtract %0, %3 : tensor<1x16x1280xf32> loc(#loc2290)
    %5 = stablehlo.multiply %4, %4 : tensor<1x16x1280xf32> loc(#loc2289)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc2289)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x16xf32> loc(#loc2289)
    %8 = stablehlo.reshape %7 : (tensor<1x16xf32>) -> tensor<1x16x1xf32> loc(#loc2289)
    %9 = stablehlo.add %8, %cst : tensor<1x16x1xf32> loc(#loc2291)
    %10 = stablehlo.rsqrt %9 : tensor<1x16x1xf32> loc(#loc2292)
    %11 = stablehlo.reshape %10 : (tensor<1x16x1xf32>) -> tensor<1x16xf32> loc(#loc2293)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc2293)
    %13 = stablehlo.multiply %4, %12 : tensor<1x16x1280xf32> loc(#loc2293)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2294)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc2295)
    %16 = stablehlo.multiply %13, %15 : tensor<1x16x1280xf32> loc(#loc2295)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2296)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc2296)
    %19 = stablehlo.add %16, %18 : tensor<1x16x1280xf32> loc(#loc2296)
    %20 = stablehlo.convert %19 : (tensor<1x16x1280xf32>) -> tensor<1x16x1280xbf16> loc(#loc2297)
    return %20 : tensor<1x16x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_27(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1221|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_88xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1222|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_89xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1223|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_90xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2301)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2302)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2302)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2303)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2303)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2302)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2302)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2302)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2302)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2304)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2305)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2306)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2306)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2306)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2307)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2308)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2308)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2309)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2309)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2309)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2310)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_28(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("726|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_24xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("727|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_25xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("728|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_26xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2314)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2315)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2315)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2316)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2316)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2315)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2315)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2315)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2315)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2317)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2318)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2319)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2319)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2319)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2320)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2321)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2321)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2322)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2322)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2322)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2323)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_29(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1147|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_78xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1148|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_79xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1149|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_80xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2327)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2328)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2328)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2329)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2329)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2328)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2328)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2328)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2328)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2330)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2331)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2332)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2332)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2332)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2333)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2334)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2334)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2335)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2335)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2335)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2336)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_30(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1836|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_174xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1837|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_175xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1838|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_176xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2340)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2341)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2341)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2342)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2342)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2341)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2341)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2341)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2341)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2343)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2344)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2345)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2345)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2345)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2346)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2347)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2347)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2348)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2348)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2348)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2349)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_17(%arg0: tensor<1x257x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("941|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[4].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_52xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2351)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2351)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2351)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2351)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2351)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_31(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2798|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_304xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2799|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_305xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2800|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_306xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2355)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2356)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2356)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2357)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2357)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2356)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2356)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2356)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2356)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2358)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2359)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2360)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2360)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2360)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2361)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2362)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2362)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2363)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2363)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2363)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2364)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_32(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("777|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_28xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("778|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_29xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("779|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_30xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2368)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2369)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2369)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2370)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2370)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2369)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2369)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2369)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2369)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2371)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2372)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2373)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2373)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2373)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2374)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2375)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2375)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2376)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2376)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2376)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2377)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_33(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1665|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_148xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1666|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_149xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1667|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_150xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2381)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2382)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2382)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2383)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2383)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2382)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2382)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2382)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2382)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2384)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2385)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2386)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2386)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2386)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2387)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2388)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2388)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2389)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2389)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2389)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2390)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_34(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("578|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_4xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("579|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_5xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("580|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_6xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2394)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2395)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2395)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2396)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2396)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2395)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2395)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2395)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2395)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2397)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2398)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2399)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2399)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2399)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2400)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2401)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2401)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2402)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2402)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2402)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2403)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_18(%arg0: tensor<1x257x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1533|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[12].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_132xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2405)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2405)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2405)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2405)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2405)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_35(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("629|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_8xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("630|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_9xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("631|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_10xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2409)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2410)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2410)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2411)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2411)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2410)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2410)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2410)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2410)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2412)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2413)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2414)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2414)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2414)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2415)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2416)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2416)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2417)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2417)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2417)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2418)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_36(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("703|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_18xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("704|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_19xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("705|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_20xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2422)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2423)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2423)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2424)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2424)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2423)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2423)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2423)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2423)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2425)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2426)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2427)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2427)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2427)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2428)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2429)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2429)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2430)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2430)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2430)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2431)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_19(%arg0: tensor<1x257x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("719|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[1].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_22xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2433)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2433)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2433)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2433)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2433)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_37(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("565|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|mark_tensorxla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("566|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|mark_tensor_1xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("567|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|mark_tensor_2xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2437)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2438)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2438)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2439)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2439)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2438)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2438)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2438)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2438)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2440)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2441)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2442)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2442)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2442)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2443)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2444)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2444)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2445)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2445)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2445)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2446)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_38(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("925|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_48xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("926|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_49xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("927|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_50xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2450)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2451)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2451)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2452)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2452)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2451)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2451)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2451)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2451)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2453)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2454)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2455)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2455)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2455)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2456)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2457)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2457)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2458)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2458)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2458)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2459)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_39(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("652|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_14xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("653|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_15xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("654|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_16xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2463)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2464)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2464)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2465)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2465)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2464)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2464)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2464)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2464)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2466)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2467)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2468)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2468)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2468)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2469)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2470)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2470)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2471)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2471)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2471)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2472)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_40(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1688|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_154xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1689|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_155xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1690|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_156xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2476)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2477)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2477)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2478)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2478)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2477)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2477)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2477)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2477)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2479)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2480)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2481)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2481)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2481)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2482)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2483)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2483)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2484)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2484)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2484)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2485)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_20(%arg0: tensor<1x257x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("867|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[3].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_42xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2487)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2487)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2487)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2487)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2487)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_41(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("999|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_58xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1000|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_59xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1001|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_60xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2491)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2492)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2492)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2493)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2493)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2492)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2492)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2492)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2492)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2494)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2495)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2496)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2496)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2496)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2497)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2498)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2498)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2499)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2499)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2499)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2500)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_21(%arg0: tensor<1x257x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("793|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[2].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_32xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2502)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2502)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2502)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2502)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2502)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_42(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("948|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_54xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("949|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_55xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("950|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_56xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2506)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2507)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2507)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2508)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2508)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2507)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2507)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2507)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2507)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2509)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2510)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2511)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2511)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2511)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2512)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2513)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2513)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2514)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2514)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2514)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2515)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_43(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("800|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_34xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("801|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_35xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("802|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_36xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2519)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2520)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2520)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2521)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2521)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2520)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2520)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2520)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2520)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2522)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2523)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2524)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2524)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2524)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2525)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2526)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2526)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2527)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2527)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2527)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2528)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_44(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1762|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_164xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1763|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_165xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1764|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_166xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2532)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2533)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2533)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2534)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2534)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2533)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2533)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2533)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2533)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2535)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2536)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2537)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2537)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2537)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2538)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2539)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2539)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2540)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2540)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2540)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2541)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_45(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1295|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_98xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1296|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_99xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1297|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_100xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2545)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2546)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2546)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2547)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2547)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2546)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2546)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2546)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2546)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2548)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2549)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2550)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2550)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2550)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2551)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2552)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2552)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2553)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2553)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2553)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2554)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_46(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2627|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_278xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2628|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_279xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2629|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_280xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2558)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2559)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2559)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2560)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2560)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2559)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2559)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2559)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2559)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2561)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2562)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2563)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2563)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2563)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2564)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2565)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2565)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2566)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2566)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2566)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2567)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_47(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2109|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_208xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2110|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_209xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2111|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_210xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2571)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2572)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2572)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2573)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2573)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2572)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2572)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2572)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2572)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2574)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2575)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2576)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2576)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2576)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2577)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2578)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2578)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2579)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2579)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2579)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2580)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_48(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1739|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_158xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1740|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_159xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1741|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_160xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2584)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2585)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2585)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2586)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2586)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2585)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2585)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2585)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2585)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2587)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2588)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2589)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2589)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2589)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2590)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2591)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2591)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2592)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2592)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2592)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2593)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_22(%arg0: tensor<1x257x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1311|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[9].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_102xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2595)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2595)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2595)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2595)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2595)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_49(%arg0: tensor<1x16x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("3059|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_360xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("3060|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_361xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("3061|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_362xla__mark_tensor")) -> tensor<1x16x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x16x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x16xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x16x1280xbf16>) -> tensor<1x16x1280xf32> loc(#loc2599)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc2600)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x16xf32> loc(#loc2600)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc2601)
    %4 = stablehlo.subtract %0, %3 : tensor<1x16x1280xf32> loc(#loc2601)
    %5 = stablehlo.multiply %4, %4 : tensor<1x16x1280xf32> loc(#loc2600)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc2600)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x16xf32> loc(#loc2600)
    %8 = stablehlo.reshape %7 : (tensor<1x16xf32>) -> tensor<1x16x1xf32> loc(#loc2600)
    %9 = stablehlo.add %8, %cst : tensor<1x16x1xf32> loc(#loc2602)
    %10 = stablehlo.rsqrt %9 : tensor<1x16x1xf32> loc(#loc2603)
    %11 = stablehlo.reshape %10 : (tensor<1x16x1xf32>) -> tensor<1x16xf32> loc(#loc2604)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc2604)
    %13 = stablehlo.multiply %4, %12 : tensor<1x16x1280xf32> loc(#loc2604)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2605)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc2606)
    %16 = stablehlo.multiply %13, %15 : tensor<1x16x1280xf32> loc(#loc2606)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2607)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc2607)
    %19 = stablehlo.add %16, %18 : tensor<1x16x1280xf32> loc(#loc2607)
    %20 = stablehlo.convert %19 : (tensor<1x16x1280xf32>) -> tensor<1x16x1280xbf16> loc(#loc2608)
    return %20 : tensor<1x16x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_50(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1813|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_168xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1814|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_169xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1815|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_170xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2612)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2613)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2613)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2614)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2614)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2613)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2613)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2613)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2613)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2615)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2616)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2617)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2617)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2617)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2618)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2619)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2619)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2620)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2620)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2620)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2621)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_51(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2035|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_198xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2036|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_199xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2037|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_200xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2625)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2626)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2626)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2627)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2627)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2626)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2626)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2626)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2626)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2628)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2629)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2630)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2630)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2630)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2631)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2632)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2632)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2633)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2633)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2633)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2634)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_52(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("851|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_38xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("852|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_39xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("853|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_40xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2638)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2639)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2639)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2640)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2640)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2639)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2639)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2639)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2639)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2641)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2642)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2643)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2643)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2643)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2644)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2645)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2645)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2646)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2646)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2646)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2647)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_23(%arg0: tensor<1x257x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1977|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[18].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_192xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2649)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2649)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2649)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2649)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2649)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_24(%arg0: tensor<1x16x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("3209|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|GELU[getattr(resampler.layers[3].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|mark_tensor_382xla__mark_tensor")) -> tensor<1x16x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x16x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x16x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x16x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x16x5120xbf16> loc(#loc2651)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x16x5120xbf16> loc(#loc2651)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x16x5120xbf16>) -> tensor<1x16x5120xbf16> loc(#loc2651)
    %3 = stablehlo.add %2, %cst : tensor<1x16x5120xbf16> loc(#loc2651)
    %4 = stablehlo.multiply %0, %3 : tensor<1x16x5120xbf16> loc(#loc2651)
    return %4 : tensor<1x16x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_53(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1369|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_108xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1370|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_109xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1371|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_110xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2655)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2656)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2656)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2657)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2657)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2656)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2656)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2656)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2656)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2658)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2659)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2660)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2660)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2660)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2661)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2662)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2662)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2663)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2663)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2663)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2664)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_54(%arg0: tensor<1x16x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2889|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_332xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2890|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_333xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2891|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_334xla__mark_tensor")) -> tensor<1x16x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x16x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x16xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x16x1280xbf16>) -> tensor<1x16x1280xf32> loc(#loc2668)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc2669)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x16xf32> loc(#loc2669)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc2670)
    %4 = stablehlo.subtract %0, %3 : tensor<1x16x1280xf32> loc(#loc2670)
    %5 = stablehlo.multiply %4, %4 : tensor<1x16x1280xf32> loc(#loc2669)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc2669)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x16xf32> loc(#loc2669)
    %8 = stablehlo.reshape %7 : (tensor<1x16xf32>) -> tensor<1x16x1xf32> loc(#loc2669)
    %9 = stablehlo.add %8, %cst : tensor<1x16x1xf32> loc(#loc2671)
    %10 = stablehlo.rsqrt %9 : tensor<1x16x1xf32> loc(#loc2672)
    %11 = stablehlo.reshape %10 : (tensor<1x16x1xf32>) -> tensor<1x16xf32> loc(#loc2673)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc2673)
    %13 = stablehlo.multiply %4, %12 : tensor<1x16x1280xf32> loc(#loc2673)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2674)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc2675)
    %16 = stablehlo.multiply %13, %15 : tensor<1x16x1280xf32> loc(#loc2675)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2676)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc2676)
    %19 = stablehlo.add %16, %18 : tensor<1x16x1280xf32> loc(#loc2676)
    %20 = stablehlo.convert %19 : (tensor<1x16x1280xf32>) -> tensor<1x16x1280xbf16> loc(#loc2677)
    return %20 : tensor<1x16x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_25(%arg0: tensor<1x257x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1385|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[10].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_112xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2679)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2679)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2679)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2679)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2679)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_55(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2132|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_214xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2133|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_215xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2134|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_216xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2683)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2684)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2684)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2685)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2685)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2684)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2684)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2684)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2684)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2686)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2687)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2688)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2688)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2688)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2689)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2690)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2690)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2691)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2691)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2691)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2692)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_56(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1392|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_114xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1393|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_115xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1394|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_116xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2696)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2697)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2697)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2698)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2698)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2697)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2697)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2697)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2697)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2699)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2700)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2701)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2701)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2701)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2702)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2703)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2703)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2704)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2704)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2704)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2705)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_57(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1443|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_118xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1444|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_119xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1445|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_120xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2709)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2710)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2710)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2711)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2711)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2710)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2710)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2710)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2710)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2712)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2713)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2714)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2714)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2714)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2715)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2716)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2716)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2717)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2717)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2717)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2718)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_58(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2280|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_234xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2281|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_235xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2282|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_236xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2722)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2723)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2723)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2724)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2724)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2723)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2723)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2723)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2723)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2725)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2726)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2727)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2727)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2727)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2728)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2729)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2729)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2730)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2730)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2730)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2731)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_59(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1244|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_94xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1245|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_95xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1246|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_96xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2735)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2736)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2736)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2737)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2737)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2736)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2736)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2736)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2736)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2738)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2739)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2740)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2740)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2740)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2741)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2742)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2742)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2743)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2743)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2743)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2744)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_60(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1517|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_128xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1518|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_129xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1519|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_130xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2748)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2749)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2749)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2750)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2750)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2749)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2749)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2749)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2749)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2751)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2752)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2753)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2753)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2753)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2754)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2755)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2755)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2756)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2756)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2756)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2757)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_26(%arg0: tensor<1x257x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1015|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[5].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_62xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2759)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2759)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2759)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2759)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2759)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_61(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1961|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_188xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1962|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_189xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1963|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_190xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2763)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2764)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2764)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2765)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2765)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2764)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2764)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2764)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2764)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2766)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2767)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2768)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2768)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2768)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2769)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2770)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2770)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2771)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2771)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2771)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2772)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_62(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1591|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_138xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1592|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_139xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1593|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_140xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2776)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2777)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2777)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2778)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2778)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2777)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2777)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2777)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2777)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2779)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2780)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2781)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2781)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2781)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2782)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2783)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2783)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2784)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2784)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2784)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2785)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_63(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2876|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_328xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2877|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_329xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2878|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_330xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2789)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2790)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2790)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2791)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2791)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2790)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2790)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2790)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2790)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2792)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2793)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2794)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2794)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2794)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2795)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2796)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2796)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2797)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2797)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2797)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2798)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_64(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1022|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_64xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1023|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_65xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1024|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_66xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2802)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2803)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2803)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2804)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2804)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2803)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2803)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2803)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2803)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2805)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2806)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2807)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2807)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2807)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2808)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2809)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2809)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2810)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2810)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2810)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2811)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_27(%arg0: tensor<1x257x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1755|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[15].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_162xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2813)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2813)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2813)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2813)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2813)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_28(%arg0: tensor<1x257x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1607|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[13].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_142xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2815)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2815)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2815)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2815)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2815)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_29(%arg0: tensor<1x257x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2643|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[27].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_282xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2817)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2817)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2817)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2817)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2817)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_65(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1096|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_74xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1097|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_75xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1098|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_76xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2821)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2822)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2822)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2823)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2823)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2822)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2822)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2822)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2822)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2824)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2825)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2826)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2826)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2826)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2827)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2828)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2828)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2829)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2829)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2829)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2830)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_66(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1614|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_144xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1615|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_145xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1616|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_146xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2834)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2835)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2835)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2836)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2836)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2835)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2835)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2835)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2835)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2837)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2838)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2839)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2839)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2839)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2840)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2841)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2841)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2842)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2842)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2842)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2843)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_67(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2961|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_342xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2962|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_343xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2963|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_344xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2847)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2848)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2848)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2849)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2849)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2848)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2848)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2848)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2848)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2850)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2851)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2852)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2852)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2852)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2853)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2854)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2854)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2855)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2855)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2855)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2856)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_30(%arg0: tensor<1x257x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1459|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[11].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_122xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2858)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2858)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2858)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2858)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2858)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_31(%arg0: tensor<1x257x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1681|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[14].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_152xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2860)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2860)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2860)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2860)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2860)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_68(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("874|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_44xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("875|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_45xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("876|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_46xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2864)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2865)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2865)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2866)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2866)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2865)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2865)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2865)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2865)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2867)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2868)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2869)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2869)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2869)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2870)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2871)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2871)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2872)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2872)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2872)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2873)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_69(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1466|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_124xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1467|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_125xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1468|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_126xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2877)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2878)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2878)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2879)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2879)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2878)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2878)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2878)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2878)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2880)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2881)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2882)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2882)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2882)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2883)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2884)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2884)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2885)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2885)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2885)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2886)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_32(%arg0: tensor<1x257x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1829|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[16].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_172xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2888)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2888)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2888)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2888)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2888)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_70(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1887|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_178xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1888|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_179xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1889|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_180xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2892)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2893)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2893)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2894)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2894)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2893)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2893)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2893)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2893)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2895)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2896)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2897)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2897)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2897)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2898)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2899)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2899)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2900)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2900)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2900)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2901)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_71(%arg0: tensor<1x16x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("3144|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_374xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("3145|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_375xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("3146|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_376xla__mark_tensor")) -> tensor<1x16x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x16x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x16xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x16x1280xbf16>) -> tensor<1x16x1280xf32> loc(#loc2905)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc2906)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x16xf32> loc(#loc2906)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc2907)
    %4 = stablehlo.subtract %0, %3 : tensor<1x16x1280xf32> loc(#loc2907)
    %5 = stablehlo.multiply %4, %4 : tensor<1x16x1280xf32> loc(#loc2906)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x16x1280xf32>, tensor<f32>) -> tensor<1x16xf32> loc(#loc2906)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x16xf32> loc(#loc2906)
    %8 = stablehlo.reshape %7 : (tensor<1x16xf32>) -> tensor<1x16x1xf32> loc(#loc2906)
    %9 = stablehlo.add %8, %cst : tensor<1x16x1xf32> loc(#loc2908)
    %10 = stablehlo.rsqrt %9 : tensor<1x16x1xf32> loc(#loc2909)
    %11 = stablehlo.reshape %10 : (tensor<1x16x1xf32>) -> tensor<1x16xf32> loc(#loc2910)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<1x16x1280xf32> loc(#loc2910)
    %13 = stablehlo.multiply %4, %12 : tensor<1x16x1280xf32> loc(#loc2910)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2911)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc2912)
    %16 = stablehlo.multiply %13, %15 : tensor<1x16x1280xf32> loc(#loc2912)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2913)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x16x1280xf32> loc(#loc2913)
    %19 = stablehlo.add %16, %18 : tensor<1x16x1280xf32> loc(#loc2913)
    %20 = stablehlo.convert %19 : (tensor<1x16x1280xf32>) -> tensor<1x16x1280xbf16> loc(#loc2914)
    return %20 : tensor<1x16x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_72(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2775|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_298xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2776|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_299xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("2777|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_300xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2918)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2919)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2919)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2920)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2920)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2919)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2919)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2919)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2919)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2921)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2922)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2923)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2923)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2923)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2924)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2925)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2925)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2926)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2926)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2926)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2927)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_73(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1540|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_134xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1541|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_135xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1542|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_136xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2931)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2932)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2932)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2933)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2933)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2932)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2932)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2932)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2932)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2934)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2935)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2936)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2936)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2936)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2937)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2938)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2938)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2939)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2939)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2939)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2940)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.gelu.impl_33(%arg0: tensor<1x257x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1903|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[17].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_182xla__mark_tensor")) -> tensor<1x257x5120xbf16> {
    %cst = stablehlo.constant dense<1.000000e+00> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.070310e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<1x257x5120xbf16> loc(#loc)
    %0 = stablehlo.multiply %arg0, %cst_1 : tensor<1x257x5120xbf16> loc(#loc2942)
    %1 = stablehlo.multiply %arg0, %cst_0 : tensor<1x257x5120xbf16> loc(#loc2942)
    %2 = stablehlo.custom_call @mhlo.erf(%1) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1x257x5120xbf16>) -> tensor<1x257x5120xbf16> loc(#loc2942)
    %3 = stablehlo.add %2, %cst : tensor<1x257x5120xbf16> loc(#loc2942)
    %4 = stablehlo.multiply %0, %3 : tensor<1x257x5120xbf16> loc(#loc2942)
    return %4 : tensor<1x257x5120xbf16> loc(#loc)
  } loc(#loc)
  func.func private @tenstorrent.layer_norm.impl_74(%arg0: tensor<1x257x1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1910|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_184xla__mark_tensor"), %arg1: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1911|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_185xla__mark_tensor"), %arg2: tensor<1280xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc("1912|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_186xla__mark_tensor")) -> tensor<1x257x1280xbf16> {
    %cst = stablehlo.constant dense<9.99999974E-6> : tensor<1x257x1xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<7.812500e-04> : tensor<1x257xf32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.convert %arg0 : (tensor<1x257x1280xbf16>) -> tensor<1x257x1280xf32> loc(#loc2946)
    %1 = stablehlo.reduce(%0 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2947)
    %2 = stablehlo.multiply %1, %cst_0 : tensor<1x257xf32> loc(#loc2947)
    %3 = stablehlo.broadcast_in_dim %2, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2948)
    %4 = stablehlo.subtract %0, %3 : tensor<1x257x1280xf32> loc(#loc2948)
    %5 = stablehlo.multiply %4, %4 : tensor<1x257x1280xf32> loc(#loc2947)
    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.add across dimensions = [2] : (tensor<1x257x1280xf32>, tensor<f32>) -> tensor<1x257xf32> loc(#loc2947)
    %7 = stablehlo.multiply %6, %cst_0 : tensor<1x257xf32> loc(#loc2947)
    %8 = stablehlo.reshape %7 : (tensor<1x257xf32>) -> tensor<1x257x1xf32> loc(#loc2947)
    %9 = stablehlo.add %8, %cst : tensor<1x257x1xf32> loc(#loc2949)
    %10 = stablehlo.rsqrt %9 : tensor<1x257x1xf32> loc(#loc2950)
    %11 = stablehlo.reshape %10 : (tensor<1x257x1xf32>) -> tensor<1x257xf32> loc(#loc2951)
    %12 = stablehlo.broadcast_in_dim %11, dims = [0, 1] : (tensor<1x257xf32>) -> tensor<1x257x1280xf32> loc(#loc2951)
    %13 = stablehlo.multiply %4, %12 : tensor<1x257x1280xf32> loc(#loc2951)
    %14 = stablehlo.convert %arg1 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2952)
    %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2953)
    %16 = stablehlo.multiply %13, %15 : tensor<1x257x1280xf32> loc(#loc2953)
    %17 = stablehlo.convert %arg2 : (tensor<1280xbf16>) -> tensor<1280xf32> loc(#loc2954)
    %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<1280xf32>) -> tensor<1x257x1280xf32> loc(#loc2954)
    %19 = stablehlo.add %16, %18 : tensor<1x257x1280xf32> loc(#loc2954)
    %20 = stablehlo.convert %19 : (tensor<1x257x1280xf32>) -> tensor<1x257x1280xbf16> loc(#loc2955)
    return %20 : tensor<1x257x1280xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc2 = loc("-1|unknown|unknown|-1|unknownaten__view")
#loc3 = loc("2901|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_335xla__mark_tensor")
#loc4 = loc("2904|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_194aten__view")
#loc5 = loc("2903|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|permute_355aten__permute")
#loc6 = loc("2904|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_194aten__mm")
#loc7 = loc("2909|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|view_129aten__view")
#loc8 = loc("2910|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|permute_358aten__permute")
#loc9 = loc("2915|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_296xla__cast")
#loc10 = loc("2918|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_200aten__mul")
#loc11 = loc("558|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPVisionEmbeddings[image_encoder.vision_model.embeddings]|Conv2d[image_encoder.vision_model.embeddings.patch_embedding]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:195|forward|202|convolutionaten__convolution_overrideable")
#loc12 = loc("559|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPVisionEmbeddings[image_encoder.vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:195|forward|203|viewaten__view")
#loc13 = loc("560|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPVisionEmbeddings[image_encoder.vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:195|forward|203|permuteaten__permute")
#loc14 = loc("562|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPVisionEmbeddings[image_encoder.vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:195|forward|206|cataten__cat")
#loc15 = loc("563|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPVisionEmbeddings[image_encoder.vision_model.embeddings]|Embedding[image_encoder.vision_model.embeddings.position_embedding]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:195|forward|210|embeddingaten__view")
#loc16 = loc("563|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPVisionEmbeddings[image_encoder.vision_model.embeddings]|Embedding[image_encoder.vision_model.embeddings.position_embedding]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:195|forward|210|embeddingaten__index_select")
#loc17 = loc("564|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPVisionEmbeddings[image_encoder.vision_model.embeddings]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:195|forward|210|addaten__add")
#loc18 = loc("577|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|mark_tensor_3xla__mark_tensor")
#loc19 = loc("590|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_7xla__mark_tensor")
#loc20 = loc("592|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmulaten__view")
#loc21 = loc("591|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_1aten__permute")
#loc22 = loc("592|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmulaten__mm")
#loc23 = loc("593|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_5aten__add")
#loc24 = loc("600|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_1aten__view")
#loc25 = loc("601|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_4aten__permute")
#loc26 = loc("606|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_4xla__cast")
#loc27 = loc("609|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_4aten__mul")
#loc28 = loc("594|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_2aten__permute")
#loc29 = loc("595|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_1aten__mm")
#loc30 = loc("595|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_1aten__view")
#loc31 = loc("596|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_6aten__add")
#loc32 = loc("602|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_2aten__view")
#loc33 = loc("603|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_5aten__permute")
#loc34 = loc("607|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_5xla__cast")
#loc35 = loc("610|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_7aten__permute")
#loc36 = loc("611|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_5aten__mul")
#loc37 = loc("613|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmaxaten__einsum")
#loc38 = loc("614|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eqaten__eq")
#loc39 = loc("615|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_notaten__logical_not")
#loc41 = loc("or.2639")
#loc42 = loc("select.2640")
#loc43 = loc("617|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_1aten__logical_not")
#loc44 = loc("619|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|whereaten__expand")
#loc45 = loc("613|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmaxaten__softmax")
#loc46 = loc("619|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|whereaten__where")
#loc47 = loc("597|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_3aten__permute")
#loc48 = loc("598|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_2aten__mm")
#loc49 = loc("598|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_2aten__view")
#loc50 = loc("599|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_7aten__add")
#loc51 = loc("604|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_3aten__view")
#loc52 = loc("605|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_6aten__permute")
#loc53 = loc("608|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_6xla__cast")
#loc54 = loc("621|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_8aten__einsum")
#loc55 = loc("621|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_8xla__cast")
#loc56 = loc("623|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|cloneaten__permute")
#loc57 = loc("626|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_3aten__view")
#loc58 = loc("625|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_9aten__permute")
#loc59 = loc("626|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_3aten__mm")
#loc60 = loc("627|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPAttention[image_encoder.vision_model.encoder.layers[0].self_attn]|Linear[image_encoder.vision_model.encoder.layers[0].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_8aten__add")
#loc61 = loc("628|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_9aten__add")
#loc62 = loc("641|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_11xla__mark_tensor")
#loc63 = loc("643|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_4aten__view")
#loc64 = loc("642|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_10aten__permute")
#loc65 = loc("643|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_4aten__mm")
#loc66 = loc("644|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_12aten__add")
#loc67 = loc("647|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[0].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_13xla__mark_tensor")
#loc68 = loc("649|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_5aten__view")
#loc69 = loc("648|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_11aten__permute")
#loc70 = loc("649|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_5aten__mm")
#loc71 = loc("650|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|Linear[image_encoder.vision_model.encoder.layers[0].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_13aten__add")
#loc72 = loc("651|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_14aten__add")
#loc73 = loc("664|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_17xla__mark_tensor")
#loc74 = loc("666|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_6aten__view")
#loc75 = loc("665|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_12aten__permute")
#loc76 = loc("666|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_6aten__mm")
#loc77 = loc("667|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_17aten__add")
#loc78 = loc("674|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_5aten__view")
#loc79 = loc("675|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_15aten__permute")
#loc80 = loc("680|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_13xla__cast")
#loc81 = loc("683|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_10aten__mul")
#loc82 = loc("668|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_13aten__permute")
#loc83 = loc("669|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_7aten__mm")
#loc84 = loc("669|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_7aten__view")
#loc85 = loc("670|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_18aten__add")
#loc86 = loc("676|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_6aten__view")
#loc87 = loc("677|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_16aten__permute")
#loc88 = loc("681|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_14xla__cast")
#loc89 = loc("684|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_18aten__permute")
#loc90 = loc("685|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_11aten__mul")
#loc91 = loc("687|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_1aten__einsum")
#loc92 = loc("688|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_1aten__eq")
#loc93 = loc("689|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_2aten__logical_not")
#loc95 = loc("or.2955")
#loc96 = loc("select.2956")
#loc97 = loc("691|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_3aten__logical_not")
#loc98 = loc("693|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_1aten__expand")
#loc99 = loc("687|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_1aten__softmax")
#loc100 = loc("693|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_1aten__where")
#loc101 = loc("671|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_14aten__permute")
#loc102 = loc("672|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_8aten__mm")
#loc103 = loc("672|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_8aten__view")
#loc104 = loc("673|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_19aten__add")
#loc105 = loc("678|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_7aten__view")
#loc106 = loc("679|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_17aten__permute")
#loc107 = loc("682|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_15xla__cast")
#loc108 = loc("695|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_17aten__einsum")
#loc109 = loc("695|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_17xla__cast")
#loc110 = loc("697|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_1aten__permute")
#loc111 = loc("700|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_9aten__view")
#loc112 = loc("699|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_20aten__permute")
#loc113 = loc("700|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_9aten__mm")
#loc114 = loc("701|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPAttention[image_encoder.vision_model.encoder.layers[1].self_attn]|Linear[image_encoder.vision_model.encoder.layers[1].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_20aten__add")
#loc115 = loc("702|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_21aten__add")
#loc116 = loc("715|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_21xla__mark_tensor")
#loc117 = loc("717|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_10aten__view")
#loc118 = loc("716|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_21aten__permute")
#loc119 = loc("717|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_10aten__mm")
#loc120 = loc("718|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_24aten__add")
#loc121 = loc("721|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[1].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_23xla__mark_tensor")
#loc122 = loc("723|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_11aten__view")
#loc123 = loc("722|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_22aten__permute")
#loc124 = loc("723|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_11aten__mm")
#loc125 = loc("724|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|Linear[image_encoder.vision_model.encoder.layers[1].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_25aten__add")
#loc126 = loc("725|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_26aten__add")
#loc127 = loc("738|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_27xla__mark_tensor")
#loc128 = loc("740|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_12aten__view")
#loc129 = loc("739|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_23aten__permute")
#loc130 = loc("740|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_12aten__mm")
#loc131 = loc("741|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_29aten__add")
#loc132 = loc("748|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_9aten__view")
#loc133 = loc("749|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_26aten__permute")
#loc134 = loc("754|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_22xla__cast")
#loc135 = loc("757|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_16aten__mul")
#loc136 = loc("742|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_24aten__permute")
#loc137 = loc("743|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_13aten__mm")
#loc138 = loc("743|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_13aten__view")
#loc139 = loc("744|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_30aten__add")
#loc140 = loc("750|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_10aten__view")
#loc141 = loc("751|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_27aten__permute")
#loc142 = loc("755|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_23xla__cast")
#loc143 = loc("758|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_29aten__permute")
#loc144 = loc("759|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_17aten__mul")
#loc145 = loc("761|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_2aten__einsum")
#loc146 = loc("762|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_2aten__eq")
#loc147 = loc("763|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_4aten__logical_not")
#loc149 = loc("or.3271")
#loc150 = loc("select.3272")
#loc151 = loc("765|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_5aten__logical_not")
#loc152 = loc("767|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_2aten__expand")
#loc153 = loc("761|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_2aten__softmax")
#loc154 = loc("767|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_2aten__where")
#loc155 = loc("745|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_25aten__permute")
#loc156 = loc("746|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_14aten__mm")
#loc157 = loc("746|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_14aten__view")
#loc158 = loc("747|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_31aten__add")
#loc159 = loc("752|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_11aten__view")
#loc160 = loc("753|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_28aten__permute")
#loc161 = loc("756|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_24xla__cast")
#loc162 = loc("769|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_26aten__einsum")
#loc163 = loc("769|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_26xla__cast")
#loc164 = loc("771|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_2aten__permute")
#loc165 = loc("774|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_15aten__view")
#loc166 = loc("773|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_31aten__permute")
#loc167 = loc("774|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_15aten__mm")
#loc168 = loc("775|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPAttention[image_encoder.vision_model.encoder.layers[2].self_attn]|Linear[image_encoder.vision_model.encoder.layers[2].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_32aten__add")
#loc169 = loc("776|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_33aten__add")
#loc170 = loc("789|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_31xla__mark_tensor")
#loc171 = loc("791|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_16aten__view")
#loc172 = loc("790|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_32aten__permute")
#loc173 = loc("791|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_16aten__mm")
#loc174 = loc("792|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_36aten__add")
#loc175 = loc("795|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[2].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_33xla__mark_tensor")
#loc176 = loc("797|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_17aten__view")
#loc177 = loc("796|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_33aten__permute")
#loc178 = loc("797|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_17aten__mm")
#loc179 = loc("798|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|Linear[image_encoder.vision_model.encoder.layers[2].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_37aten__add")
#loc180 = loc("799|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_38aten__add")
#loc181 = loc("812|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_37xla__mark_tensor")
#loc182 = loc("814|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_18aten__view")
#loc183 = loc("813|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_34aten__permute")
#loc184 = loc("814|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_18aten__mm")
#loc185 = loc("815|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_41aten__add")
#loc186 = loc("822|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_13aten__view")
#loc187 = loc("823|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_37aten__permute")
#loc188 = loc("828|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_31xla__cast")
#loc189 = loc("831|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_22aten__mul")
#loc190 = loc("816|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_35aten__permute")
#loc191 = loc("817|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_19aten__mm")
#loc192 = loc("817|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_19aten__view")
#loc193 = loc("818|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_42aten__add")
#loc194 = loc("824|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_14aten__view")
#loc195 = loc("825|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_38aten__permute")
#loc196 = loc("829|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_32xla__cast")
#loc197 = loc("832|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_40aten__permute")
#loc198 = loc("833|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_23aten__mul")
#loc199 = loc("835|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_3aten__einsum")
#loc200 = loc("836|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_3aten__eq")
#loc201 = loc("837|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_6aten__logical_not")
#loc203 = loc("or.3587")
#loc204 = loc("select.3588")
#loc205 = loc("839|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_7aten__logical_not")
#loc206 = loc("841|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_3aten__expand")
#loc207 = loc("835|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_3aten__softmax")
#loc208 = loc("841|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_3aten__where")
#loc209 = loc("819|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_36aten__permute")
#loc210 = loc("820|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_20aten__mm")
#loc211 = loc("820|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_20aten__view")
#loc212 = loc("821|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_43aten__add")
#loc213 = loc("826|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_15aten__view")
#loc214 = loc("827|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_39aten__permute")
#loc215 = loc("830|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_33xla__cast")
#loc216 = loc("843|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_35aten__einsum")
#loc217 = loc("843|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_35xla__cast")
#loc218 = loc("845|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_3aten__permute")
#loc219 = loc("848|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_21aten__view")
#loc220 = loc("847|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_42aten__permute")
#loc221 = loc("848|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_21aten__mm")
#loc222 = loc("849|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPAttention[image_encoder.vision_model.encoder.layers[3].self_attn]|Linear[image_encoder.vision_model.encoder.layers[3].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_44aten__add")
#loc223 = loc("850|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_45aten__add")
#loc224 = loc("863|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_41xla__mark_tensor")
#loc225 = loc("865|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_22aten__view")
#loc226 = loc("864|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_43aten__permute")
#loc227 = loc("865|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_22aten__mm")
#loc228 = loc("866|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_48aten__add")
#loc229 = loc("869|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[3].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_43xla__mark_tensor")
#loc230 = loc("871|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_23aten__view")
#loc231 = loc("870|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_44aten__permute")
#loc232 = loc("871|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_23aten__mm")
#loc233 = loc("872|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|Linear[image_encoder.vision_model.encoder.layers[3].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_49aten__add")
#loc234 = loc("873|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_50aten__add")
#loc235 = loc("886|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_47xla__mark_tensor")
#loc236 = loc("888|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_24aten__view")
#loc237 = loc("887|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_45aten__permute")
#loc238 = loc("888|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_24aten__mm")
#loc239 = loc("889|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_53aten__add")
#loc240 = loc("896|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_17aten__view")
#loc241 = loc("897|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_48aten__permute")
#loc242 = loc("902|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_40xla__cast")
#loc243 = loc("905|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_28aten__mul")
#loc244 = loc("890|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_46aten__permute")
#loc245 = loc("891|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_25aten__mm")
#loc246 = loc("891|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_25aten__view")
#loc247 = loc("892|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_54aten__add")
#loc248 = loc("898|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_18aten__view")
#loc249 = loc("899|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_49aten__permute")
#loc250 = loc("903|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_41xla__cast")
#loc251 = loc("906|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_51aten__permute")
#loc252 = loc("907|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_29aten__mul")
#loc253 = loc("909|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_4aten__einsum")
#loc254 = loc("910|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_4aten__eq")
#loc255 = loc("911|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_8aten__logical_not")
#loc257 = loc("or.3903")
#loc258 = loc("select.3904")
#loc259 = loc("913|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_9aten__logical_not")
#loc260 = loc("915|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_4aten__expand")
#loc261 = loc("909|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_4aten__softmax")
#loc262 = loc("915|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_4aten__where")
#loc263 = loc("893|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_47aten__permute")
#loc264 = loc("894|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_26aten__mm")
#loc265 = loc("894|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_26aten__view")
#loc266 = loc("895|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_55aten__add")
#loc267 = loc("900|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_19aten__view")
#loc268 = loc("901|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_50aten__permute")
#loc269 = loc("904|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_42xla__cast")
#loc270 = loc("917|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_44aten__einsum")
#loc271 = loc("917|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_44xla__cast")
#loc272 = loc("919|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_4aten__permute")
#loc273 = loc("922|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_27aten__view")
#loc274 = loc("921|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_53aten__permute")
#loc275 = loc("922|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_27aten__mm")
#loc276 = loc("923|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPAttention[image_encoder.vision_model.encoder.layers[4].self_attn]|Linear[image_encoder.vision_model.encoder.layers[4].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_56aten__add")
#loc277 = loc("924|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_57aten__add")
#loc278 = loc("937|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_51xla__mark_tensor")
#loc279 = loc("939|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_28aten__view")
#loc280 = loc("938|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_54aten__permute")
#loc281 = loc("939|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_28aten__mm")
#loc282 = loc("940|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_60aten__add")
#loc283 = loc("943|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[4].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_53xla__mark_tensor")
#loc284 = loc("945|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_29aten__view")
#loc285 = loc("944|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_55aten__permute")
#loc286 = loc("945|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_29aten__mm")
#loc287 = loc("946|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|Linear[image_encoder.vision_model.encoder.layers[4].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_61aten__add")
#loc288 = loc("947|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_62aten__add")
#loc289 = loc("960|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_57xla__mark_tensor")
#loc290 = loc("962|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_30aten__view")
#loc291 = loc("961|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_56aten__permute")
#loc292 = loc("962|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_30aten__mm")
#loc293 = loc("963|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_65aten__add")
#loc294 = loc("970|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_21aten__view")
#loc295 = loc("971|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_59aten__permute")
#loc296 = loc("976|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_49xla__cast")
#loc297 = loc("979|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_34aten__mul")
#loc298 = loc("964|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_57aten__permute")
#loc299 = loc("965|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_31aten__mm")
#loc300 = loc("965|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_31aten__view")
#loc301 = loc("966|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_66aten__add")
#loc302 = loc("972|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_22aten__view")
#loc303 = loc("973|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_60aten__permute")
#loc304 = loc("977|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_50xla__cast")
#loc305 = loc("980|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_62aten__permute")
#loc306 = loc("981|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_35aten__mul")
#loc307 = loc("983|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_5aten__einsum")
#loc308 = loc("984|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_5aten__eq")
#loc309 = loc("985|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_10aten__logical_not")
#loc311 = loc("or.4219")
#loc312 = loc("select.4220")
#loc313 = loc("987|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_11aten__logical_not")
#loc314 = loc("989|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_5aten__expand")
#loc315 = loc("983|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_5aten__softmax")
#loc316 = loc("989|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_5aten__where")
#loc317 = loc("967|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_58aten__permute")
#loc318 = loc("968|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_32aten__mm")
#loc319 = loc("968|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_32aten__view")
#loc320 = loc("969|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_67aten__add")
#loc321 = loc("974|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_23aten__view")
#loc322 = loc("975|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_61aten__permute")
#loc323 = loc("978|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_51xla__cast")
#loc324 = loc("991|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_53aten__einsum")
#loc325 = loc("991|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_53xla__cast")
#loc326 = loc("993|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_5aten__permute")
#loc327 = loc("996|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_33aten__view")
#loc328 = loc("995|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_64aten__permute")
#loc329 = loc("996|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_33aten__mm")
#loc330 = loc("997|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPAttention[image_encoder.vision_model.encoder.layers[5].self_attn]|Linear[image_encoder.vision_model.encoder.layers[5].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_68aten__add")
#loc331 = loc("998|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_69aten__add")
#loc332 = loc("1011|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_61xla__mark_tensor")
#loc333 = loc("1013|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_34aten__view")
#loc334 = loc("1012|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_65aten__permute")
#loc335 = loc("1013|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_34aten__mm")
#loc336 = loc("1014|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_72aten__add")
#loc337 = loc("1017|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[5].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_63xla__mark_tensor")
#loc338 = loc("1019|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_35aten__view")
#loc339 = loc("1018|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_66aten__permute")
#loc340 = loc("1019|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_35aten__mm")
#loc341 = loc("1020|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|Linear[image_encoder.vision_model.encoder.layers[5].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_73aten__add")
#loc342 = loc("1021|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_74aten__add")
#loc343 = loc("1034|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_67xla__mark_tensor")
#loc344 = loc("1036|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_36aten__view")
#loc345 = loc("1035|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_67aten__permute")
#loc346 = loc("1036|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_36aten__mm")
#loc347 = loc("1037|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_77aten__add")
#loc348 = loc("1044|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_25aten__view")
#loc349 = loc("1045|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_70aten__permute")
#loc350 = loc("1050|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_58xla__cast")
#loc351 = loc("1053|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_40aten__mul")
#loc352 = loc("1038|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_68aten__permute")
#loc353 = loc("1039|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_37aten__mm")
#loc354 = loc("1039|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_37aten__view")
#loc355 = loc("1040|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_78aten__add")
#loc356 = loc("1046|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_26aten__view")
#loc357 = loc("1047|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_71aten__permute")
#loc358 = loc("1051|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_59xla__cast")
#loc359 = loc("1054|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_73aten__permute")
#loc360 = loc("1055|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_41aten__mul")
#loc361 = loc("1057|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_6aten__einsum")
#loc362 = loc("1058|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_6aten__eq")
#loc363 = loc("1059|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_12aten__logical_not")
#loc365 = loc("or.4535")
#loc366 = loc("select.4536")
#loc367 = loc("1061|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_13aten__logical_not")
#loc368 = loc("1063|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_6aten__expand")
#loc369 = loc("1057|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_6aten__softmax")
#loc370 = loc("1063|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_6aten__where")
#loc371 = loc("1041|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_69aten__permute")
#loc372 = loc("1042|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_38aten__mm")
#loc373 = loc("1042|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_38aten__view")
#loc374 = loc("1043|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_79aten__add")
#loc375 = loc("1048|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_27aten__view")
#loc376 = loc("1049|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_72aten__permute")
#loc377 = loc("1052|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_60xla__cast")
#loc378 = loc("1065|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_62aten__einsum")
#loc379 = loc("1065|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_62xla__cast")
#loc380 = loc("1067|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_6aten__permute")
#loc381 = loc("1070|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_39aten__view")
#loc382 = loc("1069|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_75aten__permute")
#loc383 = loc("1070|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_39aten__mm")
#loc384 = loc("1071|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPAttention[image_encoder.vision_model.encoder.layers[6].self_attn]|Linear[image_encoder.vision_model.encoder.layers[6].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_80aten__add")
#loc385 = loc("1072|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_81aten__add")
#loc386 = loc("1085|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_71xla__mark_tensor")
#loc387 = loc("1087|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_40aten__view")
#loc388 = loc("1086|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_76aten__permute")
#loc389 = loc("1087|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_40aten__mm")
#loc390 = loc("1088|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_84aten__add")
#loc391 = loc("1091|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[6].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_73xla__mark_tensor")
#loc392 = loc("1093|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_41aten__view")
#loc393 = loc("1092|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_77aten__permute")
#loc394 = loc("1093|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_41aten__mm")
#loc395 = loc("1094|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|Linear[image_encoder.vision_model.encoder.layers[6].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_85aten__add")
#loc396 = loc("1095|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_86aten__add")
#loc397 = loc("1108|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_77xla__mark_tensor")
#loc398 = loc("1110|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_42aten__view")
#loc399 = loc("1109|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_78aten__permute")
#loc400 = loc("1110|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_42aten__mm")
#loc401 = loc("1111|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_89aten__add")
#loc402 = loc("1118|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_29aten__view")
#loc403 = loc("1119|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_81aten__permute")
#loc404 = loc("1124|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_67xla__cast")
#loc405 = loc("1127|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_46aten__mul")
#loc406 = loc("1112|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_79aten__permute")
#loc407 = loc("1113|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_43aten__mm")
#loc408 = loc("1113|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_43aten__view")
#loc409 = loc("1114|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_90aten__add")
#loc410 = loc("1120|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_30aten__view")
#loc411 = loc("1121|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_82aten__permute")
#loc412 = loc("1125|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_68xla__cast")
#loc413 = loc("1128|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_84aten__permute")
#loc414 = loc("1129|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_47aten__mul")
#loc415 = loc("1131|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_7aten__einsum")
#loc416 = loc("1132|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_7aten__eq")
#loc417 = loc("1133|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_14aten__logical_not")
#loc419 = loc("or.4851")
#loc420 = loc("select.4852")
#loc421 = loc("1135|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_15aten__logical_not")
#loc422 = loc("1137|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_7aten__expand")
#loc423 = loc("1131|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_7aten__softmax")
#loc424 = loc("1137|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_7aten__where")
#loc425 = loc("1115|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_80aten__permute")
#loc426 = loc("1116|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_44aten__mm")
#loc427 = loc("1116|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_44aten__view")
#loc428 = loc("1117|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_91aten__add")
#loc429 = loc("1122|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_31aten__view")
#loc430 = loc("1123|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_83aten__permute")
#loc431 = loc("1126|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_69xla__cast")
#loc432 = loc("1139|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_71aten__einsum")
#loc433 = loc("1139|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_71xla__cast")
#loc434 = loc("1141|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_7aten__permute")
#loc435 = loc("1144|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_45aten__view")
#loc436 = loc("1143|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_86aten__permute")
#loc437 = loc("1144|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_45aten__mm")
#loc438 = loc("1145|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPAttention[image_encoder.vision_model.encoder.layers[7].self_attn]|Linear[image_encoder.vision_model.encoder.layers[7].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_92aten__add")
#loc439 = loc("1146|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_93aten__add")
#loc440 = loc("1159|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_81xla__mark_tensor")
#loc441 = loc("1161|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_46aten__view")
#loc442 = loc("1160|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_87aten__permute")
#loc443 = loc("1161|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_46aten__mm")
#loc444 = loc("1162|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_96aten__add")
#loc445 = loc("1165|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[7].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_83xla__mark_tensor")
#loc446 = loc("1167|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_47aten__view")
#loc447 = loc("1166|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_88aten__permute")
#loc448 = loc("1167|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_47aten__mm")
#loc449 = loc("1168|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|Linear[image_encoder.vision_model.encoder.layers[7].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_97aten__add")
#loc450 = loc("1169|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_98aten__add")
#loc451 = loc("1182|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_87xla__mark_tensor")
#loc452 = loc("1184|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_48aten__view")
#loc453 = loc("1183|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_89aten__permute")
#loc454 = loc("1184|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_48aten__mm")
#loc455 = loc("1185|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_101aten__add")
#loc456 = loc("1192|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_33aten__view")
#loc457 = loc("1193|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_92aten__permute")
#loc458 = loc("1198|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_76xla__cast")
#loc459 = loc("1201|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_52aten__mul")
#loc460 = loc("1186|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_90aten__permute")
#loc461 = loc("1187|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_49aten__mm")
#loc462 = loc("1187|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_49aten__view")
#loc463 = loc("1188|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_102aten__add")
#loc464 = loc("1194|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_34aten__view")
#loc465 = loc("1195|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_93aten__permute")
#loc466 = loc("1199|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_77xla__cast")
#loc467 = loc("1202|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_95aten__permute")
#loc468 = loc("1203|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_53aten__mul")
#loc469 = loc("1205|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_8aten__einsum")
#loc470 = loc("1206|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_8aten__eq")
#loc471 = loc("1207|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_16aten__logical_not")
#loc473 = loc("or.5167")
#loc474 = loc("select.5168")
#loc475 = loc("1209|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_17aten__logical_not")
#loc476 = loc("1211|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_8aten__expand")
#loc477 = loc("1205|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_8aten__softmax")
#loc478 = loc("1211|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_8aten__where")
#loc479 = loc("1189|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_91aten__permute")
#loc480 = loc("1190|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_50aten__mm")
#loc481 = loc("1190|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_50aten__view")
#loc482 = loc("1191|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_103aten__add")
#loc483 = loc("1196|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_35aten__view")
#loc484 = loc("1197|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_94aten__permute")
#loc485 = loc("1200|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_78xla__cast")
#loc486 = loc("1213|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_80aten__einsum")
#loc487 = loc("1213|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_80xla__cast")
#loc488 = loc("1215|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_8aten__permute")
#loc489 = loc("1218|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_51aten__view")
#loc490 = loc("1217|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_97aten__permute")
#loc491 = loc("1218|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_51aten__mm")
#loc492 = loc("1219|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPAttention[image_encoder.vision_model.encoder.layers[8].self_attn]|Linear[image_encoder.vision_model.encoder.layers[8].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_104aten__add")
#loc493 = loc("1220|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_105aten__add")
#loc494 = loc("1233|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_91xla__mark_tensor")
#loc495 = loc("1235|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_52aten__view")
#loc496 = loc("1234|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_98aten__permute")
#loc497 = loc("1235|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_52aten__mm")
#loc498 = loc("1236|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_108aten__add")
#loc499 = loc("1239|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[8].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_93xla__mark_tensor")
#loc500 = loc("1241|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_53aten__view")
#loc501 = loc("1240|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_99aten__permute")
#loc502 = loc("1241|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_53aten__mm")
#loc503 = loc("1242|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|Linear[image_encoder.vision_model.encoder.layers[8].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_109aten__add")
#loc504 = loc("1243|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_110aten__add")
#loc505 = loc("1256|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_97xla__mark_tensor")
#loc506 = loc("1258|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_54aten__view")
#loc507 = loc("1257|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_100aten__permute")
#loc508 = loc("1258|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_54aten__mm")
#loc509 = loc("1259|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_113aten__add")
#loc510 = loc("1266|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_37aten__view")
#loc511 = loc("1267|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_103aten__permute")
#loc512 = loc("1272|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_85xla__cast")
#loc513 = loc("1275|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_58aten__mul")
#loc514 = loc("1260|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_101aten__permute")
#loc515 = loc("1261|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_55aten__mm")
#loc516 = loc("1261|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_55aten__view")
#loc517 = loc("1262|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_114aten__add")
#loc518 = loc("1268|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_38aten__view")
#loc519 = loc("1269|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_104aten__permute")
#loc520 = loc("1273|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_86xla__cast")
#loc521 = loc("1276|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_106aten__permute")
#loc522 = loc("1277|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_59aten__mul")
#loc523 = loc("1279|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_9aten__einsum")
#loc524 = loc("1280|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_9aten__eq")
#loc525 = loc("1281|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_18aten__logical_not")
#loc527 = loc("or.5483")
#loc528 = loc("select.5484")
#loc529 = loc("1283|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_19aten__logical_not")
#loc530 = loc("1285|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_9aten__expand")
#loc531 = loc("1279|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_9aten__softmax")
#loc532 = loc("1285|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_9aten__where")
#loc533 = loc("1263|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_102aten__permute")
#loc534 = loc("1264|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_56aten__mm")
#loc535 = loc("1264|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_56aten__view")
#loc536 = loc("1265|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_115aten__add")
#loc537 = loc("1270|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_39aten__view")
#loc538 = loc("1271|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_105aten__permute")
#loc539 = loc("1274|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_87xla__cast")
#loc540 = loc("1287|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_89aten__einsum")
#loc541 = loc("1287|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_89xla__cast")
#loc542 = loc("1289|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_9aten__permute")
#loc543 = loc("1292|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_57aten__view")
#loc544 = loc("1291|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_108aten__permute")
#loc545 = loc("1292|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_57aten__mm")
#loc546 = loc("1293|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPAttention[image_encoder.vision_model.encoder.layers[9].self_attn]|Linear[image_encoder.vision_model.encoder.layers[9].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_116aten__add")
#loc547 = loc("1294|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_117aten__add")
#loc548 = loc("1307|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_101xla__mark_tensor")
#loc549 = loc("1309|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_58aten__view")
#loc550 = loc("1308|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_109aten__permute")
#loc551 = loc("1309|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_58aten__mm")
#loc552 = loc("1310|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_120aten__add")
#loc553 = loc("1313|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[9].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_103xla__mark_tensor")
#loc554 = loc("1315|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_59aten__view")
#loc555 = loc("1314|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_110aten__permute")
#loc556 = loc("1315|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_59aten__mm")
#loc557 = loc("1316|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|Linear[image_encoder.vision_model.encoder.layers[9].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_121aten__add")
#loc558 = loc("1317|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_122aten__add")
#loc559 = loc("1330|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_107xla__mark_tensor")
#loc560 = loc("1332|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_60aten__view")
#loc561 = loc("1331|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_111aten__permute")
#loc562 = loc("1332|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_60aten__mm")
#loc563 = loc("1333|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_125aten__add")
#loc564 = loc("1340|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_41aten__view")
#loc565 = loc("1341|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_114aten__permute")
#loc566 = loc("1346|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_94xla__cast")
#loc567 = loc("1349|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_64aten__mul")
#loc568 = loc("1334|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_112aten__permute")
#loc569 = loc("1335|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_61aten__mm")
#loc570 = loc("1335|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_61aten__view")
#loc571 = loc("1336|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_126aten__add")
#loc572 = loc("1342|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_42aten__view")
#loc573 = loc("1343|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_115aten__permute")
#loc574 = loc("1347|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_95xla__cast")
#loc575 = loc("1350|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_117aten__permute")
#loc576 = loc("1351|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_65aten__mul")
#loc577 = loc("1353|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_10aten__einsum")
#loc578 = loc("1354|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_10aten__eq")
#loc579 = loc("1355|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_20aten__logical_not")
#loc581 = loc("or.5799")
#loc582 = loc("select.5800")
#loc583 = loc("1357|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_21aten__logical_not")
#loc584 = loc("1359|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_10aten__expand")
#loc585 = loc("1353|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_10aten__softmax")
#loc586 = loc("1359|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_10aten__where")
#loc587 = loc("1337|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_113aten__permute")
#loc588 = loc("1338|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_62aten__mm")
#loc589 = loc("1338|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_62aten__view")
#loc590 = loc("1339|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_127aten__add")
#loc591 = loc("1344|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_43aten__view")
#loc592 = loc("1345|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_116aten__permute")
#loc593 = loc("1348|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_96xla__cast")
#loc594 = loc("1361|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_98aten__einsum")
#loc595 = loc("1361|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_98xla__cast")
#loc596 = loc("1363|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_10aten__permute")
#loc597 = loc("1366|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_63aten__view")
#loc598 = loc("1365|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_119aten__permute")
#loc599 = loc("1366|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_63aten__mm")
#loc600 = loc("1367|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPAttention[image_encoder.vision_model.encoder.layers[10].self_attn]|Linear[image_encoder.vision_model.encoder.layers[10].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_128aten__add")
#loc601 = loc("1368|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_129aten__add")
#loc602 = loc("1381|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_111xla__mark_tensor")
#loc603 = loc("1383|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_64aten__view")
#loc604 = loc("1382|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_120aten__permute")
#loc605 = loc("1383|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_64aten__mm")
#loc606 = loc("1384|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_132aten__add")
#loc607 = loc("1387|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[10].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_113xla__mark_tensor")
#loc608 = loc("1389|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_65aten__view")
#loc609 = loc("1388|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_121aten__permute")
#loc610 = loc("1389|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_65aten__mm")
#loc611 = loc("1390|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|Linear[image_encoder.vision_model.encoder.layers[10].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_133aten__add")
#loc612 = loc("1391|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_134aten__add")
#loc613 = loc("1404|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_117xla__mark_tensor")
#loc614 = loc("1406|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_66aten__view")
#loc615 = loc("1405|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_122aten__permute")
#loc616 = loc("1406|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_66aten__mm")
#loc617 = loc("1407|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_137aten__add")
#loc618 = loc("1414|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_45aten__view")
#loc619 = loc("1415|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_125aten__permute")
#loc620 = loc("1420|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_103xla__cast")
#loc621 = loc("1423|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_70aten__mul")
#loc622 = loc("1408|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_123aten__permute")
#loc623 = loc("1409|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_67aten__mm")
#loc624 = loc("1409|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_67aten__view")
#loc625 = loc("1410|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_138aten__add")
#loc626 = loc("1416|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_46aten__view")
#loc627 = loc("1417|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_126aten__permute")
#loc628 = loc("1421|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_104xla__cast")
#loc629 = loc("1424|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_128aten__permute")
#loc630 = loc("1425|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_71aten__mul")
#loc631 = loc("1427|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_11aten__einsum")
#loc632 = loc("1428|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_11aten__eq")
#loc633 = loc("1429|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_22aten__logical_not")
#loc635 = loc("or.6115")
#loc636 = loc("select.6116")
#loc637 = loc("1431|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_23aten__logical_not")
#loc638 = loc("1433|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_11aten__expand")
#loc639 = loc("1427|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_11aten__softmax")
#loc640 = loc("1433|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_11aten__where")
#loc641 = loc("1411|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_124aten__permute")
#loc642 = loc("1412|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_68aten__mm")
#loc643 = loc("1412|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_68aten__view")
#loc644 = loc("1413|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_139aten__add")
#loc645 = loc("1418|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_47aten__view")
#loc646 = loc("1419|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_127aten__permute")
#loc647 = loc("1422|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_105xla__cast")
#loc648 = loc("1435|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_107aten__einsum")
#loc649 = loc("1435|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_107xla__cast")
#loc650 = loc("1437|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_11aten__permute")
#loc651 = loc("1440|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_69aten__view")
#loc652 = loc("1439|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_130aten__permute")
#loc653 = loc("1440|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_69aten__mm")
#loc654 = loc("1441|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPAttention[image_encoder.vision_model.encoder.layers[11].self_attn]|Linear[image_encoder.vision_model.encoder.layers[11].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_140aten__add")
#loc655 = loc("1442|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_141aten__add")
#loc656 = loc("1455|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_121xla__mark_tensor")
#loc657 = loc("1457|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_70aten__view")
#loc658 = loc("1456|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_131aten__permute")
#loc659 = loc("1457|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_70aten__mm")
#loc660 = loc("1458|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_144aten__add")
#loc661 = loc("1461|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[11].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_123xla__mark_tensor")
#loc662 = loc("1463|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_71aten__view")
#loc663 = loc("1462|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_132aten__permute")
#loc664 = loc("1463|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_71aten__mm")
#loc665 = loc("1464|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|Linear[image_encoder.vision_model.encoder.layers[11].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_145aten__add")
#loc666 = loc("1465|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_146aten__add")
#loc667 = loc("1478|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_127xla__mark_tensor")
#loc668 = loc("1480|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_72aten__view")
#loc669 = loc("1479|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_133aten__permute")
#loc670 = loc("1480|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_72aten__mm")
#loc671 = loc("1481|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_149aten__add")
#loc672 = loc("1488|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_49aten__view")
#loc673 = loc("1489|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_136aten__permute")
#loc674 = loc("1494|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_112xla__cast")
#loc675 = loc("1497|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_76aten__mul")
#loc676 = loc("1482|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_134aten__permute")
#loc677 = loc("1483|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_73aten__mm")
#loc678 = loc("1483|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_73aten__view")
#loc679 = loc("1484|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_150aten__add")
#loc680 = loc("1490|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_50aten__view")
#loc681 = loc("1491|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_137aten__permute")
#loc682 = loc("1495|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_113xla__cast")
#loc683 = loc("1498|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_139aten__permute")
#loc684 = loc("1499|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_77aten__mul")
#loc685 = loc("1501|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_12aten__einsum")
#loc686 = loc("1502|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_12aten__eq")
#loc687 = loc("1503|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_24aten__logical_not")
#loc689 = loc("or.6431")
#loc690 = loc("select.6432")
#loc691 = loc("1505|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_25aten__logical_not")
#loc692 = loc("1507|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_12aten__expand")
#loc693 = loc("1501|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_12aten__softmax")
#loc694 = loc("1507|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_12aten__where")
#loc695 = loc("1485|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_135aten__permute")
#loc696 = loc("1486|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_74aten__mm")
#loc697 = loc("1486|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_74aten__view")
#loc698 = loc("1487|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_151aten__add")
#loc699 = loc("1492|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_51aten__view")
#loc700 = loc("1493|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_138aten__permute")
#loc701 = loc("1496|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_114xla__cast")
#loc702 = loc("1509|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_116aten__einsum")
#loc703 = loc("1509|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_116xla__cast")
#loc704 = loc("1511|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_12aten__permute")
#loc705 = loc("1514|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_75aten__view")
#loc706 = loc("1513|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_141aten__permute")
#loc707 = loc("1514|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_75aten__mm")
#loc708 = loc("1515|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPAttention[image_encoder.vision_model.encoder.layers[12].self_attn]|Linear[image_encoder.vision_model.encoder.layers[12].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_152aten__add")
#loc709 = loc("1516|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_153aten__add")
#loc710 = loc("1529|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_131xla__mark_tensor")
#loc711 = loc("1531|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_76aten__view")
#loc712 = loc("1530|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_142aten__permute")
#loc713 = loc("1531|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_76aten__mm")
#loc714 = loc("1532|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_156aten__add")
#loc715 = loc("1535|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[12].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_133xla__mark_tensor")
#loc716 = loc("1537|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_77aten__view")
#loc717 = loc("1536|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_143aten__permute")
#loc718 = loc("1537|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_77aten__mm")
#loc719 = loc("1538|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|Linear[image_encoder.vision_model.encoder.layers[12].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_157aten__add")
#loc720 = loc("1539|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_158aten__add")
#loc721 = loc("1552|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_137xla__mark_tensor")
#loc722 = loc("1554|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_78aten__view")
#loc723 = loc("1553|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_144aten__permute")
#loc724 = loc("1554|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_78aten__mm")
#loc725 = loc("1555|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_161aten__add")
#loc726 = loc("1562|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_53aten__view")
#loc727 = loc("1563|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_147aten__permute")
#loc728 = loc("1568|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_121xla__cast")
#loc729 = loc("1571|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_82aten__mul")
#loc730 = loc("1556|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_145aten__permute")
#loc731 = loc("1557|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_79aten__mm")
#loc732 = loc("1557|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_79aten__view")
#loc733 = loc("1558|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_162aten__add")
#loc734 = loc("1564|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_54aten__view")
#loc735 = loc("1565|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_148aten__permute")
#loc736 = loc("1569|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_122xla__cast")
#loc737 = loc("1572|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_150aten__permute")
#loc738 = loc("1573|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_83aten__mul")
#loc739 = loc("1575|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_13aten__einsum")
#loc740 = loc("1576|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_13aten__eq")
#loc741 = loc("1577|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_26aten__logical_not")
#loc743 = loc("or.6747")
#loc744 = loc("select.6748")
#loc745 = loc("1579|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_27aten__logical_not")
#loc746 = loc("1581|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_13aten__expand")
#loc747 = loc("1575|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_13aten__softmax")
#loc748 = loc("1581|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_13aten__where")
#loc749 = loc("1559|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_146aten__permute")
#loc750 = loc("1560|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_80aten__mm")
#loc751 = loc("1560|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_80aten__view")
#loc752 = loc("1561|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_163aten__add")
#loc753 = loc("1566|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_55aten__view")
#loc754 = loc("1567|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_149aten__permute")
#loc755 = loc("1570|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_123xla__cast")
#loc756 = loc("1583|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_125aten__einsum")
#loc757 = loc("1583|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_125xla__cast")
#loc758 = loc("1585|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_13aten__permute")
#loc759 = loc("1588|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_81aten__view")
#loc760 = loc("1587|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_152aten__permute")
#loc761 = loc("1588|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_81aten__mm")
#loc762 = loc("1589|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPAttention[image_encoder.vision_model.encoder.layers[13].self_attn]|Linear[image_encoder.vision_model.encoder.layers[13].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_164aten__add")
#loc763 = loc("1590|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_165aten__add")
#loc764 = loc("1603|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_141xla__mark_tensor")
#loc765 = loc("1605|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_82aten__view")
#loc766 = loc("1604|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_153aten__permute")
#loc767 = loc("1605|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_82aten__mm")
#loc768 = loc("1606|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_168aten__add")
#loc769 = loc("1609|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[13].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_143xla__mark_tensor")
#loc770 = loc("1611|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_83aten__view")
#loc771 = loc("1610|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_154aten__permute")
#loc772 = loc("1611|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_83aten__mm")
#loc773 = loc("1612|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|Linear[image_encoder.vision_model.encoder.layers[13].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_169aten__add")
#loc774 = loc("1613|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_170aten__add")
#loc775 = loc("1626|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_147xla__mark_tensor")
#loc776 = loc("1628|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_84aten__view")
#loc777 = loc("1627|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_155aten__permute")
#loc778 = loc("1628|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_84aten__mm")
#loc779 = loc("1629|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_173aten__add")
#loc780 = loc("1636|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_57aten__view")
#loc781 = loc("1637|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_158aten__permute")
#loc782 = loc("1642|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_130xla__cast")
#loc783 = loc("1645|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_88aten__mul")
#loc784 = loc("1630|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_156aten__permute")
#loc785 = loc("1631|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_85aten__mm")
#loc786 = loc("1631|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_85aten__view")
#loc787 = loc("1632|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_174aten__add")
#loc788 = loc("1638|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_58aten__view")
#loc789 = loc("1639|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_159aten__permute")
#loc790 = loc("1643|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_131xla__cast")
#loc791 = loc("1646|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_161aten__permute")
#loc792 = loc("1647|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_89aten__mul")
#loc793 = loc("1649|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_14aten__einsum")
#loc794 = loc("1650|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_14aten__eq")
#loc795 = loc("1651|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_28aten__logical_not")
#loc797 = loc("or.7063")
#loc798 = loc("select.7064")
#loc799 = loc("1653|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_29aten__logical_not")
#loc800 = loc("1655|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_14aten__expand")
#loc801 = loc("1649|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_14aten__softmax")
#loc802 = loc("1655|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_14aten__where")
#loc803 = loc("1633|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_157aten__permute")
#loc804 = loc("1634|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_86aten__mm")
#loc805 = loc("1634|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_86aten__view")
#loc806 = loc("1635|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_175aten__add")
#loc807 = loc("1640|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_59aten__view")
#loc808 = loc("1641|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_160aten__permute")
#loc809 = loc("1644|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_132xla__cast")
#loc810 = loc("1657|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_134aten__einsum")
#loc811 = loc("1657|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_134xla__cast")
#loc812 = loc("1659|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_14aten__permute")
#loc813 = loc("1662|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_87aten__view")
#loc814 = loc("1661|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_163aten__permute")
#loc815 = loc("1662|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_87aten__mm")
#loc816 = loc("1663|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPAttention[image_encoder.vision_model.encoder.layers[14].self_attn]|Linear[image_encoder.vision_model.encoder.layers[14].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_176aten__add")
#loc817 = loc("1664|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_177aten__add")
#loc818 = loc("1677|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_151xla__mark_tensor")
#loc819 = loc("1679|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_88aten__view")
#loc820 = loc("1678|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_164aten__permute")
#loc821 = loc("1679|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_88aten__mm")
#loc822 = loc("1680|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_180aten__add")
#loc823 = loc("1683|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[14].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_153xla__mark_tensor")
#loc824 = loc("1685|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_89aten__view")
#loc825 = loc("1684|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_165aten__permute")
#loc826 = loc("1685|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_89aten__mm")
#loc827 = loc("1686|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|Linear[image_encoder.vision_model.encoder.layers[14].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_181aten__add")
#loc828 = loc("1687|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_182aten__add")
#loc829 = loc("1700|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_157xla__mark_tensor")
#loc830 = loc("1702|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_90aten__view")
#loc831 = loc("1701|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_166aten__permute")
#loc832 = loc("1702|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_90aten__mm")
#loc833 = loc("1703|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_185aten__add")
#loc834 = loc("1710|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_61aten__view")
#loc835 = loc("1711|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_169aten__permute")
#loc836 = loc("1716|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_139xla__cast")
#loc837 = loc("1719|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_94aten__mul")
#loc838 = loc("1704|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_167aten__permute")
#loc839 = loc("1705|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_91aten__mm")
#loc840 = loc("1705|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_91aten__view")
#loc841 = loc("1706|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_186aten__add")
#loc842 = loc("1712|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_62aten__view")
#loc843 = loc("1713|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_170aten__permute")
#loc844 = loc("1717|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_140xla__cast")
#loc845 = loc("1720|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_172aten__permute")
#loc846 = loc("1721|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_95aten__mul")
#loc847 = loc("1723|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_15aten__einsum")
#loc848 = loc("1724|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_15aten__eq")
#loc849 = loc("1725|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_30aten__logical_not")
#loc851 = loc("or.7379")
#loc852 = loc("select.7380")
#loc853 = loc("1727|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_31aten__logical_not")
#loc854 = loc("1729|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_15aten__expand")
#loc855 = loc("1723|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_15aten__softmax")
#loc856 = loc("1729|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_15aten__where")
#loc857 = loc("1707|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_168aten__permute")
#loc858 = loc("1708|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_92aten__mm")
#loc859 = loc("1708|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_92aten__view")
#loc860 = loc("1709|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_187aten__add")
#loc861 = loc("1714|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_63aten__view")
#loc862 = loc("1715|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_171aten__permute")
#loc863 = loc("1718|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_141xla__cast")
#loc864 = loc("1731|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_143aten__einsum")
#loc865 = loc("1731|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_143xla__cast")
#loc866 = loc("1733|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_15aten__permute")
#loc867 = loc("1736|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_93aten__view")
#loc868 = loc("1735|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_174aten__permute")
#loc869 = loc("1736|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_93aten__mm")
#loc870 = loc("1737|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPAttention[image_encoder.vision_model.encoder.layers[15].self_attn]|Linear[image_encoder.vision_model.encoder.layers[15].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_188aten__add")
#loc871 = loc("1738|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_189aten__add")
#loc872 = loc("1751|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_161xla__mark_tensor")
#loc873 = loc("1753|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_94aten__view")
#loc874 = loc("1752|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_175aten__permute")
#loc875 = loc("1753|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_94aten__mm")
#loc876 = loc("1754|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_192aten__add")
#loc877 = loc("1757|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[15].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_163xla__mark_tensor")
#loc878 = loc("1759|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_95aten__view")
#loc879 = loc("1758|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_176aten__permute")
#loc880 = loc("1759|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_95aten__mm")
#loc881 = loc("1760|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|Linear[image_encoder.vision_model.encoder.layers[15].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_193aten__add")
#loc882 = loc("1761|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_194aten__add")
#loc883 = loc("1774|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_167xla__mark_tensor")
#loc884 = loc("1776|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_96aten__view")
#loc885 = loc("1775|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_177aten__permute")
#loc886 = loc("1776|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_96aten__mm")
#loc887 = loc("1777|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_197aten__add")
#loc888 = loc("1784|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_65aten__view")
#loc889 = loc("1785|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_180aten__permute")
#loc890 = loc("1790|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_148xla__cast")
#loc891 = loc("1793|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_100aten__mul")
#loc892 = loc("1778|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_178aten__permute")
#loc893 = loc("1779|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_97aten__mm")
#loc894 = loc("1779|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_97aten__view")
#loc895 = loc("1780|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_198aten__add")
#loc896 = loc("1786|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_66aten__view")
#loc897 = loc("1787|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_181aten__permute")
#loc898 = loc("1791|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_149xla__cast")
#loc899 = loc("1794|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_183aten__permute")
#loc900 = loc("1795|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_101aten__mul")
#loc901 = loc("1797|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_16aten__einsum")
#loc902 = loc("1798|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_16aten__eq")
#loc903 = loc("1799|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_32aten__logical_not")
#loc905 = loc("or.7695")
#loc906 = loc("select.7696")
#loc907 = loc("1801|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_33aten__logical_not")
#loc908 = loc("1803|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_16aten__expand")
#loc909 = loc("1797|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_16aten__softmax")
#loc910 = loc("1803|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_16aten__where")
#loc911 = loc("1781|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_179aten__permute")
#loc912 = loc("1782|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_98aten__mm")
#loc913 = loc("1782|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_98aten__view")
#loc914 = loc("1783|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_199aten__add")
#loc915 = loc("1788|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_67aten__view")
#loc916 = loc("1789|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_182aten__permute")
#loc917 = loc("1792|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_150xla__cast")
#loc918 = loc("1805|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_152aten__einsum")
#loc919 = loc("1805|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_152xla__cast")
#loc920 = loc("1807|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_16aten__permute")
#loc921 = loc("1810|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_99aten__view")
#loc922 = loc("1809|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_185aten__permute")
#loc923 = loc("1810|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_99aten__mm")
#loc924 = loc("1811|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPAttention[image_encoder.vision_model.encoder.layers[16].self_attn]|Linear[image_encoder.vision_model.encoder.layers[16].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_200aten__add")
#loc925 = loc("1812|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_201aten__add")
#loc926 = loc("1825|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_171xla__mark_tensor")
#loc927 = loc("1827|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_100aten__view")
#loc928 = loc("1826|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_186aten__permute")
#loc929 = loc("1827|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_100aten__mm")
#loc930 = loc("1828|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_204aten__add")
#loc931 = loc("1831|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[16].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_173xla__mark_tensor")
#loc932 = loc("1833|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_101aten__view")
#loc933 = loc("1832|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_187aten__permute")
#loc934 = loc("1833|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_101aten__mm")
#loc935 = loc("1834|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|Linear[image_encoder.vision_model.encoder.layers[16].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_205aten__add")
#loc936 = loc("1835|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_206aten__add")
#loc937 = loc("1848|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_177xla__mark_tensor")
#loc938 = loc("1850|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_102aten__view")
#loc939 = loc("1849|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_188aten__permute")
#loc940 = loc("1850|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_102aten__mm")
#loc941 = loc("1851|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_209aten__add")
#loc942 = loc("1858|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_69aten__view")
#loc943 = loc("1859|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_191aten__permute")
#loc944 = loc("1864|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_157xla__cast")
#loc945 = loc("1867|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_106aten__mul")
#loc946 = loc("1852|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_189aten__permute")
#loc947 = loc("1853|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_103aten__mm")
#loc948 = loc("1853|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_103aten__view")
#loc949 = loc("1854|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_210aten__add")
#loc950 = loc("1860|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_70aten__view")
#loc951 = loc("1861|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_192aten__permute")
#loc952 = loc("1865|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_158xla__cast")
#loc953 = loc("1868|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_194aten__permute")
#loc954 = loc("1869|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_107aten__mul")
#loc955 = loc("1871|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_17aten__einsum")
#loc956 = loc("1872|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_17aten__eq")
#loc957 = loc("1873|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_34aten__logical_not")
#loc959 = loc("or.8011")
#loc960 = loc("select.8012")
#loc961 = loc("1875|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_35aten__logical_not")
#loc962 = loc("1877|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_17aten__expand")
#loc963 = loc("1871|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_17aten__softmax")
#loc964 = loc("1877|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_17aten__where")
#loc965 = loc("1855|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_190aten__permute")
#loc966 = loc("1856|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_104aten__mm")
#loc967 = loc("1856|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_104aten__view")
#loc968 = loc("1857|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_211aten__add")
#loc969 = loc("1862|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_71aten__view")
#loc970 = loc("1863|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_193aten__permute")
#loc971 = loc("1866|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_159xla__cast")
#loc972 = loc("1879|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_161aten__einsum")
#loc973 = loc("1879|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_161xla__cast")
#loc974 = loc("1881|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_17aten__permute")
#loc975 = loc("1884|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_105aten__view")
#loc976 = loc("1883|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_196aten__permute")
#loc977 = loc("1884|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_105aten__mm")
#loc978 = loc("1885|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPAttention[image_encoder.vision_model.encoder.layers[17].self_attn]|Linear[image_encoder.vision_model.encoder.layers[17].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_212aten__add")
#loc979 = loc("1886|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_213aten__add")
#loc980 = loc("1899|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_181xla__mark_tensor")
#loc981 = loc("1901|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_106aten__view")
#loc982 = loc("1900|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_197aten__permute")
#loc983 = loc("1901|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_106aten__mm")
#loc984 = loc("1902|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_216aten__add")
#loc985 = loc("1905|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[17].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_183xla__mark_tensor")
#loc986 = loc("1907|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_107aten__view")
#loc987 = loc("1906|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_198aten__permute")
#loc988 = loc("1907|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_107aten__mm")
#loc989 = loc("1908|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|Linear[image_encoder.vision_model.encoder.layers[17].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_217aten__add")
#loc990 = loc("1909|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_218aten__add")
#loc991 = loc("1922|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_187xla__mark_tensor")
#loc992 = loc("1924|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_108aten__view")
#loc993 = loc("1923|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_199aten__permute")
#loc994 = loc("1924|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_108aten__mm")
#loc995 = loc("1925|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_221aten__add")
#loc996 = loc("1932|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_73aten__view")
#loc997 = loc("1933|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_202aten__permute")
#loc998 = loc("1938|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_166xla__cast")
#loc999 = loc("1941|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_112aten__mul")
#loc1000 = loc("1926|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_200aten__permute")
#loc1001 = loc("1927|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_109aten__mm")
#loc1002 = loc("1927|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_109aten__view")
#loc1003 = loc("1928|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_222aten__add")
#loc1004 = loc("1934|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_74aten__view")
#loc1005 = loc("1935|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_203aten__permute")
#loc1006 = loc("1939|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_167xla__cast")
#loc1007 = loc("1942|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_205aten__permute")
#loc1008 = loc("1943|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_113aten__mul")
#loc1009 = loc("1945|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_18aten__einsum")
#loc1010 = loc("1946|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_18aten__eq")
#loc1011 = loc("1947|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_36aten__logical_not")
#loc1013 = loc("or.8327")
#loc1014 = loc("select.8328")
#loc1015 = loc("1949|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_37aten__logical_not")
#loc1016 = loc("1951|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_18aten__expand")
#loc1017 = loc("1945|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_18aten__softmax")
#loc1018 = loc("1951|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_18aten__where")
#loc1019 = loc("1929|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_201aten__permute")
#loc1020 = loc("1930|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_110aten__mm")
#loc1021 = loc("1930|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_110aten__view")
#loc1022 = loc("1931|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_223aten__add")
#loc1023 = loc("1936|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_75aten__view")
#loc1024 = loc("1937|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_204aten__permute")
#loc1025 = loc("1940|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_168xla__cast")
#loc1026 = loc("1953|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_170aten__einsum")
#loc1027 = loc("1953|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_170xla__cast")
#loc1028 = loc("1955|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_18aten__permute")
#loc1029 = loc("1958|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_111aten__view")
#loc1030 = loc("1957|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_207aten__permute")
#loc1031 = loc("1958|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_111aten__mm")
#loc1032 = loc("1959|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPAttention[image_encoder.vision_model.encoder.layers[18].self_attn]|Linear[image_encoder.vision_model.encoder.layers[18].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_224aten__add")
#loc1033 = loc("1960|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_225aten__add")
#loc1034 = loc("1973|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_191xla__mark_tensor")
#loc1035 = loc("1975|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_112aten__view")
#loc1036 = loc("1974|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_208aten__permute")
#loc1037 = loc("1975|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_112aten__mm")
#loc1038 = loc("1976|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_228aten__add")
#loc1039 = loc("1979|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[18].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_193xla__mark_tensor")
#loc1040 = loc("1981|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_113aten__view")
#loc1041 = loc("1980|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_209aten__permute")
#loc1042 = loc("1981|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_113aten__mm")
#loc1043 = loc("1982|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|Linear[image_encoder.vision_model.encoder.layers[18].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_229aten__add")
#loc1044 = loc("1983|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_230aten__add")
#loc1045 = loc("1996|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_197xla__mark_tensor")
#loc1046 = loc("1998|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_114aten__view")
#loc1047 = loc("1997|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_210aten__permute")
#loc1048 = loc("1998|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_114aten__mm")
#loc1049 = loc("1999|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_233aten__add")
#loc1050 = loc("2006|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_77aten__view")
#loc1051 = loc("2007|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_213aten__permute")
#loc1052 = loc("2012|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_175xla__cast")
#loc1053 = loc("2015|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_118aten__mul")
#loc1054 = loc("2000|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_211aten__permute")
#loc1055 = loc("2001|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_115aten__mm")
#loc1056 = loc("2001|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_115aten__view")
#loc1057 = loc("2002|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_234aten__add")
#loc1058 = loc("2008|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_78aten__view")
#loc1059 = loc("2009|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_214aten__permute")
#loc1060 = loc("2013|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_176xla__cast")
#loc1061 = loc("2016|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_216aten__permute")
#loc1062 = loc("2017|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_119aten__mul")
#loc1063 = loc("2019|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_19aten__einsum")
#loc1064 = loc("2020|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_19aten__eq")
#loc1065 = loc("2021|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_38aten__logical_not")
#loc1067 = loc("or.8643")
#loc1068 = loc("select.8644")
#loc1069 = loc("2023|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_39aten__logical_not")
#loc1070 = loc("2025|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_19aten__expand")
#loc1071 = loc("2019|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_19aten__softmax")
#loc1072 = loc("2025|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_19aten__where")
#loc1073 = loc("2003|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_212aten__permute")
#loc1074 = loc("2004|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_116aten__mm")
#loc1075 = loc("2004|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_116aten__view")
#loc1076 = loc("2005|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_235aten__add")
#loc1077 = loc("2010|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_79aten__view")
#loc1078 = loc("2011|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_215aten__permute")
#loc1079 = loc("2014|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_177xla__cast")
#loc1080 = loc("2027|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_179aten__einsum")
#loc1081 = loc("2027|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_179xla__cast")
#loc1082 = loc("2029|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_19aten__permute")
#loc1083 = loc("2032|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_117aten__view")
#loc1084 = loc("2031|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_218aten__permute")
#loc1085 = loc("2032|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_117aten__mm")
#loc1086 = loc("2033|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPAttention[image_encoder.vision_model.encoder.layers[19].self_attn]|Linear[image_encoder.vision_model.encoder.layers[19].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_236aten__add")
#loc1087 = loc("2034|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_237aten__add")
#loc1088 = loc("2047|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_201xla__mark_tensor")
#loc1089 = loc("2049|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_118aten__view")
#loc1090 = loc("2048|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_219aten__permute")
#loc1091 = loc("2049|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_118aten__mm")
#loc1092 = loc("2050|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_240aten__add")
#loc1093 = loc("2053|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[19].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_203xla__mark_tensor")
#loc1094 = loc("2055|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_119aten__view")
#loc1095 = loc("2054|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_220aten__permute")
#loc1096 = loc("2055|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_119aten__mm")
#loc1097 = loc("2056|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|Linear[image_encoder.vision_model.encoder.layers[19].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_241aten__add")
#loc1098 = loc("2057|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_242aten__add")
#loc1099 = loc("2070|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_207xla__mark_tensor")
#loc1100 = loc("2072|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_120aten__view")
#loc1101 = loc("2071|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_221aten__permute")
#loc1102 = loc("2072|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_120aten__mm")
#loc1103 = loc("2073|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_245aten__add")
#loc1104 = loc("2080|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_81aten__view")
#loc1105 = loc("2081|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_224aten__permute")
#loc1106 = loc("2086|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_184xla__cast")
#loc1107 = loc("2089|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_124aten__mul")
#loc1108 = loc("2074|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_222aten__permute")
#loc1109 = loc("2075|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_121aten__mm")
#loc1110 = loc("2075|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_121aten__view")
#loc1111 = loc("2076|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_246aten__add")
#loc1112 = loc("2082|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_82aten__view")
#loc1113 = loc("2083|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_225aten__permute")
#loc1114 = loc("2087|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_185xla__cast")
#loc1115 = loc("2090|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_227aten__permute")
#loc1116 = loc("2091|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_125aten__mul")
#loc1117 = loc("2093|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_20aten__einsum")
#loc1118 = loc("2094|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_20aten__eq")
#loc1119 = loc("2095|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_40aten__logical_not")
#loc1121 = loc("or.8959")
#loc1122 = loc("select.8960")
#loc1123 = loc("2097|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_41aten__logical_not")
#loc1124 = loc("2099|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_20aten__expand")
#loc1125 = loc("2093|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_20aten__softmax")
#loc1126 = loc("2099|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_20aten__where")
#loc1127 = loc("2077|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_223aten__permute")
#loc1128 = loc("2078|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_122aten__mm")
#loc1129 = loc("2078|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_122aten__view")
#loc1130 = loc("2079|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_247aten__add")
#loc1131 = loc("2084|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_83aten__view")
#loc1132 = loc("2085|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_226aten__permute")
#loc1133 = loc("2088|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_186xla__cast")
#loc1134 = loc("2101|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_188aten__einsum")
#loc1135 = loc("2101|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_188xla__cast")
#loc1136 = loc("2103|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_20aten__permute")
#loc1137 = loc("2106|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_123aten__view")
#loc1138 = loc("2105|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_229aten__permute")
#loc1139 = loc("2106|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_123aten__mm")
#loc1140 = loc("2107|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPAttention[image_encoder.vision_model.encoder.layers[20].self_attn]|Linear[image_encoder.vision_model.encoder.layers[20].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_248aten__add")
#loc1141 = loc("2108|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_249aten__add")
#loc1142 = loc("2121|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_211xla__mark_tensor")
#loc1143 = loc("2123|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_124aten__view")
#loc1144 = loc("2122|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_230aten__permute")
#loc1145 = loc("2123|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_124aten__mm")
#loc1146 = loc("2124|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_252aten__add")
#loc1147 = loc("2127|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[20].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_213xla__mark_tensor")
#loc1148 = loc("2129|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_125aten__view")
#loc1149 = loc("2128|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_231aten__permute")
#loc1150 = loc("2129|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_125aten__mm")
#loc1151 = loc("2130|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|Linear[image_encoder.vision_model.encoder.layers[20].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_253aten__add")
#loc1152 = loc("2131|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_254aten__add")
#loc1153 = loc("2144|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_217xla__mark_tensor")
#loc1154 = loc("2146|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_126aten__view")
#loc1155 = loc("2145|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_232aten__permute")
#loc1156 = loc("2146|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_126aten__mm")
#loc1157 = loc("2147|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_257aten__add")
#loc1158 = loc("2154|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_85aten__view")
#loc1159 = loc("2155|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_235aten__permute")
#loc1160 = loc("2160|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_193xla__cast")
#loc1161 = loc("2163|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_130aten__mul")
#loc1162 = loc("2148|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_233aten__permute")
#loc1163 = loc("2149|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_127aten__mm")
#loc1164 = loc("2149|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_127aten__view")
#loc1165 = loc("2150|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_258aten__add")
#loc1166 = loc("2156|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_86aten__view")
#loc1167 = loc("2157|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_236aten__permute")
#loc1168 = loc("2161|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_194xla__cast")
#loc1169 = loc("2164|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_238aten__permute")
#loc1170 = loc("2165|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_131aten__mul")
#loc1171 = loc("2167|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_21aten__einsum")
#loc1172 = loc("2168|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_21aten__eq")
#loc1173 = loc("2169|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_42aten__logical_not")
#loc1175 = loc("or.9275")
#loc1176 = loc("select.9276")
#loc1177 = loc("2171|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_43aten__logical_not")
#loc1178 = loc("2173|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_21aten__expand")
#loc1179 = loc("2167|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_21aten__softmax")
#loc1180 = loc("2173|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_21aten__where")
#loc1181 = loc("2151|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_234aten__permute")
#loc1182 = loc("2152|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_128aten__mm")
#loc1183 = loc("2152|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_128aten__view")
#loc1184 = loc("2153|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_259aten__add")
#loc1185 = loc("2158|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_87aten__view")
#loc1186 = loc("2159|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_237aten__permute")
#loc1187 = loc("2162|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_195xla__cast")
#loc1188 = loc("2175|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_197aten__einsum")
#loc1189 = loc("2175|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_197xla__cast")
#loc1190 = loc("2177|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_21aten__permute")
#loc1191 = loc("2180|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_129aten__view")
#loc1192 = loc("2179|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_240aten__permute")
#loc1193 = loc("2180|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_129aten__mm")
#loc1194 = loc("2181|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPAttention[image_encoder.vision_model.encoder.layers[21].self_attn]|Linear[image_encoder.vision_model.encoder.layers[21].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_260aten__add")
#loc1195 = loc("2182|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_261aten__add")
#loc1196 = loc("2195|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_221xla__mark_tensor")
#loc1197 = loc("2197|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_130aten__view")
#loc1198 = loc("2196|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_241aten__permute")
#loc1199 = loc("2197|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_130aten__mm")
#loc1200 = loc("2198|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_264aten__add")
#loc1201 = loc("2201|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[21].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_223xla__mark_tensor")
#loc1202 = loc("2203|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_131aten__view")
#loc1203 = loc("2202|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_242aten__permute")
#loc1204 = loc("2203|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_131aten__mm")
#loc1205 = loc("2204|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|Linear[image_encoder.vision_model.encoder.layers[21].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_265aten__add")
#loc1206 = loc("2205|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_266aten__add")
#loc1207 = loc("2218|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_227xla__mark_tensor")
#loc1208 = loc("2220|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_132aten__view")
#loc1209 = loc("2219|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_243aten__permute")
#loc1210 = loc("2220|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_132aten__mm")
#loc1211 = loc("2221|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_269aten__add")
#loc1212 = loc("2228|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_89aten__view")
#loc1213 = loc("2229|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_246aten__permute")
#loc1214 = loc("2234|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_202xla__cast")
#loc1215 = loc("2237|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_136aten__mul")
#loc1216 = loc("2222|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_244aten__permute")
#loc1217 = loc("2223|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_133aten__mm")
#loc1218 = loc("2223|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_133aten__view")
#loc1219 = loc("2224|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_270aten__add")
#loc1220 = loc("2230|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_90aten__view")
#loc1221 = loc("2231|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_247aten__permute")
#loc1222 = loc("2235|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_203xla__cast")
#loc1223 = loc("2238|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_249aten__permute")
#loc1224 = loc("2239|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_137aten__mul")
#loc1225 = loc("2241|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_22aten__einsum")
#loc1226 = loc("2242|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_22aten__eq")
#loc1227 = loc("2243|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_44aten__logical_not")
#loc1229 = loc("or.9591")
#loc1230 = loc("select.9592")
#loc1231 = loc("2245|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_45aten__logical_not")
#loc1232 = loc("2247|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_22aten__expand")
#loc1233 = loc("2241|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_22aten__softmax")
#loc1234 = loc("2247|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_22aten__where")
#loc1235 = loc("2225|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_245aten__permute")
#loc1236 = loc("2226|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_134aten__mm")
#loc1237 = loc("2226|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_134aten__view")
#loc1238 = loc("2227|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_271aten__add")
#loc1239 = loc("2232|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_91aten__view")
#loc1240 = loc("2233|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_248aten__permute")
#loc1241 = loc("2236|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_204xla__cast")
#loc1242 = loc("2249|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_206aten__einsum")
#loc1243 = loc("2249|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_206xla__cast")
#loc1244 = loc("2251|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_22aten__permute")
#loc1245 = loc("2254|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_135aten__view")
#loc1246 = loc("2253|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_251aten__permute")
#loc1247 = loc("2254|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_135aten__mm")
#loc1248 = loc("2255|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPAttention[image_encoder.vision_model.encoder.layers[22].self_attn]|Linear[image_encoder.vision_model.encoder.layers[22].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_272aten__add")
#loc1249 = loc("2256|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_273aten__add")
#loc1250 = loc("2269|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_231xla__mark_tensor")
#loc1251 = loc("2271|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_136aten__view")
#loc1252 = loc("2270|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_252aten__permute")
#loc1253 = loc("2271|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_136aten__mm")
#loc1254 = loc("2272|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_276aten__add")
#loc1255 = loc("2275|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[22].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_233xla__mark_tensor")
#loc1256 = loc("2277|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_137aten__view")
#loc1257 = loc("2276|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_253aten__permute")
#loc1258 = loc("2277|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_137aten__mm")
#loc1259 = loc("2278|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|Linear[image_encoder.vision_model.encoder.layers[22].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_277aten__add")
#loc1260 = loc("2279|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_278aten__add")
#loc1261 = loc("2292|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_237xla__mark_tensor")
#loc1262 = loc("2294|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_138aten__view")
#loc1263 = loc("2293|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_254aten__permute")
#loc1264 = loc("2294|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_138aten__mm")
#loc1265 = loc("2295|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_281aten__add")
#loc1266 = loc("2302|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_93aten__view")
#loc1267 = loc("2303|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_257aten__permute")
#loc1268 = loc("2308|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_211xla__cast")
#loc1269 = loc("2311|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_142aten__mul")
#loc1270 = loc("2296|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_255aten__permute")
#loc1271 = loc("2297|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_139aten__mm")
#loc1272 = loc("2297|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_139aten__view")
#loc1273 = loc("2298|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_282aten__add")
#loc1274 = loc("2304|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_94aten__view")
#loc1275 = loc("2305|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_258aten__permute")
#loc1276 = loc("2309|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_212xla__cast")
#loc1277 = loc("2312|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_260aten__permute")
#loc1278 = loc("2313|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_143aten__mul")
#loc1279 = loc("2315|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_23aten__einsum")
#loc1280 = loc("2316|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_23aten__eq")
#loc1281 = loc("2317|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_46aten__logical_not")
#loc1283 = loc("or.9907")
#loc1284 = loc("select.9908")
#loc1285 = loc("2319|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_47aten__logical_not")
#loc1286 = loc("2321|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_23aten__expand")
#loc1287 = loc("2315|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_23aten__softmax")
#loc1288 = loc("2321|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_23aten__where")
#loc1289 = loc("2299|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_256aten__permute")
#loc1290 = loc("2300|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_140aten__mm")
#loc1291 = loc("2300|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_140aten__view")
#loc1292 = loc("2301|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_283aten__add")
#loc1293 = loc("2306|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_95aten__view")
#loc1294 = loc("2307|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_259aten__permute")
#loc1295 = loc("2310|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_213xla__cast")
#loc1296 = loc("2323|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_215aten__einsum")
#loc1297 = loc("2323|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_215xla__cast")
#loc1298 = loc("2325|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_23aten__permute")
#loc1299 = loc("2328|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_141aten__view")
#loc1300 = loc("2327|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_262aten__permute")
#loc1301 = loc("2328|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_141aten__mm")
#loc1302 = loc("2329|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPAttention[image_encoder.vision_model.encoder.layers[23].self_attn]|Linear[image_encoder.vision_model.encoder.layers[23].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_284aten__add")
#loc1303 = loc("2330|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_285aten__add")
#loc1304 = loc("2343|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_241xla__mark_tensor")
#loc1305 = loc("2345|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_142aten__view")
#loc1306 = loc("2344|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_263aten__permute")
#loc1307 = loc("2345|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_142aten__mm")
#loc1308 = loc("2346|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_288aten__add")
#loc1309 = loc("2349|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[23].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_243xla__mark_tensor")
#loc1310 = loc("2351|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_143aten__view")
#loc1311 = loc("2350|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_264aten__permute")
#loc1312 = loc("2351|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_143aten__mm")
#loc1313 = loc("2352|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|Linear[image_encoder.vision_model.encoder.layers[23].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_289aten__add")
#loc1314 = loc("2353|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_290aten__add")
#loc1315 = loc("2366|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_247xla__mark_tensor")
#loc1316 = loc("2368|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_144aten__view")
#loc1317 = loc("2367|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_265aten__permute")
#loc1318 = loc("2368|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_144aten__mm")
#loc1319 = loc("2369|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_293aten__add")
#loc1320 = loc("2376|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_97aten__view")
#loc1321 = loc("2377|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_268aten__permute")
#loc1322 = loc("2382|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_220xla__cast")
#loc1323 = loc("2385|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_148aten__mul")
#loc1324 = loc("2370|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_266aten__permute")
#loc1325 = loc("2371|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_145aten__mm")
#loc1326 = loc("2371|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_145aten__view")
#loc1327 = loc("2372|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_294aten__add")
#loc1328 = loc("2378|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_98aten__view")
#loc1329 = loc("2379|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_269aten__permute")
#loc1330 = loc("2383|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_221xla__cast")
#loc1331 = loc("2386|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_271aten__permute")
#loc1332 = loc("2387|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_149aten__mul")
#loc1333 = loc("2389|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_24aten__einsum")
#loc1334 = loc("2390|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_24aten__eq")
#loc1335 = loc("2391|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_48aten__logical_not")
#loc1337 = loc("or.10223")
#loc1338 = loc("select.10224")
#loc1339 = loc("2393|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_49aten__logical_not")
#loc1340 = loc("2395|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_24aten__expand")
#loc1341 = loc("2389|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_24aten__softmax")
#loc1342 = loc("2395|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_24aten__where")
#loc1343 = loc("2373|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_267aten__permute")
#loc1344 = loc("2374|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_146aten__mm")
#loc1345 = loc("2374|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_146aten__view")
#loc1346 = loc("2375|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_295aten__add")
#loc1347 = loc("2380|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_99aten__view")
#loc1348 = loc("2381|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_270aten__permute")
#loc1349 = loc("2384|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_222xla__cast")
#loc1350 = loc("2397|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_224aten__einsum")
#loc1351 = loc("2397|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_224xla__cast")
#loc1352 = loc("2399|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_24aten__permute")
#loc1353 = loc("2402|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_147aten__view")
#loc1354 = loc("2401|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_273aten__permute")
#loc1355 = loc("2402|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_147aten__mm")
#loc1356 = loc("2403|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPAttention[image_encoder.vision_model.encoder.layers[24].self_attn]|Linear[image_encoder.vision_model.encoder.layers[24].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_296aten__add")
#loc1357 = loc("2404|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_297aten__add")
#loc1358 = loc("2417|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_251xla__mark_tensor")
#loc1359 = loc("2419|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_148aten__view")
#loc1360 = loc("2418|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_274aten__permute")
#loc1361 = loc("2419|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_148aten__mm")
#loc1362 = loc("2420|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_300aten__add")
#loc1363 = loc("2423|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[24].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_253xla__mark_tensor")
#loc1364 = loc("2425|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_149aten__view")
#loc1365 = loc("2424|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_275aten__permute")
#loc1366 = loc("2425|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_149aten__mm")
#loc1367 = loc("2426|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|Linear[image_encoder.vision_model.encoder.layers[24].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_301aten__add")
#loc1368 = loc("2427|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_302aten__add")
#loc1369 = loc("2440|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_257xla__mark_tensor")
#loc1370 = loc("2442|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_150aten__view")
#loc1371 = loc("2441|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_276aten__permute")
#loc1372 = loc("2442|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_150aten__mm")
#loc1373 = loc("2443|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_305aten__add")
#loc1374 = loc("2450|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_101aten__view")
#loc1375 = loc("2451|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_279aten__permute")
#loc1376 = loc("2456|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_229xla__cast")
#loc1377 = loc("2459|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_154aten__mul")
#loc1378 = loc("2444|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_277aten__permute")
#loc1379 = loc("2445|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_151aten__mm")
#loc1380 = loc("2445|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_151aten__view")
#loc1381 = loc("2446|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_306aten__add")
#loc1382 = loc("2452|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_102aten__view")
#loc1383 = loc("2453|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_280aten__permute")
#loc1384 = loc("2457|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_230xla__cast")
#loc1385 = loc("2460|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_282aten__permute")
#loc1386 = loc("2461|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_155aten__mul")
#loc1387 = loc("2463|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_25aten__einsum")
#loc1388 = loc("2464|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_25aten__eq")
#loc1389 = loc("2465|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_50aten__logical_not")
#loc1391 = loc("or.10539")
#loc1392 = loc("select.10540")
#loc1393 = loc("2467|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_51aten__logical_not")
#loc1394 = loc("2469|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_25aten__expand")
#loc1395 = loc("2463|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_25aten__softmax")
#loc1396 = loc("2469|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_25aten__where")
#loc1397 = loc("2447|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_278aten__permute")
#loc1398 = loc("2448|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_152aten__mm")
#loc1399 = loc("2448|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_152aten__view")
#loc1400 = loc("2449|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_307aten__add")
#loc1401 = loc("2454|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_103aten__view")
#loc1402 = loc("2455|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_281aten__permute")
#loc1403 = loc("2458|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_231xla__cast")
#loc1404 = loc("2471|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_233aten__einsum")
#loc1405 = loc("2471|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_233xla__cast")
#loc1406 = loc("2473|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_25aten__permute")
#loc1407 = loc("2476|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_153aten__view")
#loc1408 = loc("2475|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_284aten__permute")
#loc1409 = loc("2476|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_153aten__mm")
#loc1410 = loc("2477|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPAttention[image_encoder.vision_model.encoder.layers[25].self_attn]|Linear[image_encoder.vision_model.encoder.layers[25].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_308aten__add")
#loc1411 = loc("2478|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_309aten__add")
#loc1412 = loc("2491|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_261xla__mark_tensor")
#loc1413 = loc("2493|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_154aten__view")
#loc1414 = loc("2492|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_285aten__permute")
#loc1415 = loc("2493|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_154aten__mm")
#loc1416 = loc("2494|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_312aten__add")
#loc1417 = loc("2497|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[25].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_263xla__mark_tensor")
#loc1418 = loc("2499|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_155aten__view")
#loc1419 = loc("2498|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_286aten__permute")
#loc1420 = loc("2499|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_155aten__mm")
#loc1421 = loc("2500|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|Linear[image_encoder.vision_model.encoder.layers[25].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_313aten__add")
#loc1422 = loc("2501|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_314aten__add")
#loc1423 = loc("2514|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_267xla__mark_tensor")
#loc1424 = loc("2516|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_156aten__view")
#loc1425 = loc("2515|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_287aten__permute")
#loc1426 = loc("2516|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_156aten__mm")
#loc1427 = loc("2517|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_317aten__add")
#loc1428 = loc("2524|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_105aten__view")
#loc1429 = loc("2525|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_290aten__permute")
#loc1430 = loc("2530|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_238xla__cast")
#loc1431 = loc("2533|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_160aten__mul")
#loc1432 = loc("2518|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_288aten__permute")
#loc1433 = loc("2519|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_157aten__mm")
#loc1434 = loc("2519|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_157aten__view")
#loc1435 = loc("2520|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_318aten__add")
#loc1436 = loc("2526|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_106aten__view")
#loc1437 = loc("2527|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_291aten__permute")
#loc1438 = loc("2531|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_239xla__cast")
#loc1439 = loc("2534|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_293aten__permute")
#loc1440 = loc("2535|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_161aten__mul")
#loc1441 = loc("2537|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_26aten__einsum")
#loc1442 = loc("2538|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_26aten__eq")
#loc1443 = loc("2539|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_52aten__logical_not")
#loc1445 = loc("or.10855")
#loc1446 = loc("select.10856")
#loc1447 = loc("2541|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_53aten__logical_not")
#loc1448 = loc("2543|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_26aten__expand")
#loc1449 = loc("2537|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_26aten__softmax")
#loc1450 = loc("2543|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_26aten__where")
#loc1451 = loc("2521|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_289aten__permute")
#loc1452 = loc("2522|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_158aten__mm")
#loc1453 = loc("2522|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_158aten__view")
#loc1454 = loc("2523|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_319aten__add")
#loc1455 = loc("2528|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_107aten__view")
#loc1456 = loc("2529|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_292aten__permute")
#loc1457 = loc("2532|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_240xla__cast")
#loc1458 = loc("2545|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_242aten__einsum")
#loc1459 = loc("2545|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_242xla__cast")
#loc1460 = loc("2547|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_26aten__permute")
#loc1461 = loc("2550|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_159aten__view")
#loc1462 = loc("2549|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_295aten__permute")
#loc1463 = loc("2550|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_159aten__mm")
#loc1464 = loc("2551|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPAttention[image_encoder.vision_model.encoder.layers[26].self_attn]|Linear[image_encoder.vision_model.encoder.layers[26].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_320aten__add")
#loc1465 = loc("2552|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_321aten__add")
#loc1466 = loc("2565|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_271xla__mark_tensor")
#loc1467 = loc("2567|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_160aten__view")
#loc1468 = loc("2566|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_296aten__permute")
#loc1469 = loc("2567|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_160aten__mm")
#loc1470 = loc("2568|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_324aten__add")
#loc1471 = loc("2571|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[26].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_273xla__mark_tensor")
#loc1472 = loc("2573|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_161aten__view")
#loc1473 = loc("2572|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_297aten__permute")
#loc1474 = loc("2573|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_161aten__mm")
#loc1475 = loc("2574|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|Linear[image_encoder.vision_model.encoder.layers[26].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_325aten__add")
#loc1476 = loc("2575|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_326aten__add")
#loc1477 = loc("2588|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_277xla__mark_tensor")
#loc1478 = loc("2590|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_162aten__view")
#loc1479 = loc("2589|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_298aten__permute")
#loc1480 = loc("2590|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_162aten__mm")
#loc1481 = loc("2591|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_329aten__add")
#loc1482 = loc("2598|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_109aten__view")
#loc1483 = loc("2599|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_301aten__permute")
#loc1484 = loc("2604|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_247xla__cast")
#loc1485 = loc("2607|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_166aten__mul")
#loc1486 = loc("2592|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_299aten__permute")
#loc1487 = loc("2593|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_163aten__mm")
#loc1488 = loc("2593|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_163aten__view")
#loc1489 = loc("2594|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_330aten__add")
#loc1490 = loc("2600|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_110aten__view")
#loc1491 = loc("2601|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_302aten__permute")
#loc1492 = loc("2605|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_248xla__cast")
#loc1493 = loc("2608|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_304aten__permute")
#loc1494 = loc("2609|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_167aten__mul")
#loc1495 = loc("2611|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_27aten__einsum")
#loc1496 = loc("2612|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_27aten__eq")
#loc1497 = loc("2613|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_54aten__logical_not")
#loc1499 = loc("or.11171")
#loc1500 = loc("select.11172")
#loc1501 = loc("2615|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_55aten__logical_not")
#loc1502 = loc("2617|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_27aten__expand")
#loc1503 = loc("2611|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_27aten__softmax")
#loc1504 = loc("2617|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_27aten__where")
#loc1505 = loc("2595|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_300aten__permute")
#loc1506 = loc("2596|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_164aten__mm")
#loc1507 = loc("2596|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_164aten__view")
#loc1508 = loc("2597|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_331aten__add")
#loc1509 = loc("2602|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_111aten__view")
#loc1510 = loc("2603|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_303aten__permute")
#loc1511 = loc("2606|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_249xla__cast")
#loc1512 = loc("2619|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_251aten__einsum")
#loc1513 = loc("2619|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_251xla__cast")
#loc1514 = loc("2621|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_27aten__permute")
#loc1515 = loc("2624|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_165aten__view")
#loc1516 = loc("2623|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_306aten__permute")
#loc1517 = loc("2624|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_165aten__mm")
#loc1518 = loc("2625|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPAttention[image_encoder.vision_model.encoder.layers[27].self_attn]|Linear[image_encoder.vision_model.encoder.layers[27].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_332aten__add")
#loc1519 = loc("2626|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_333aten__add")
#loc1520 = loc("2639|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_281xla__mark_tensor")
#loc1521 = loc("2641|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_166aten__view")
#loc1522 = loc("2640|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_307aten__permute")
#loc1523 = loc("2641|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_166aten__mm")
#loc1524 = loc("2642|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_336aten__add")
#loc1525 = loc("2645|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[27].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_283xla__mark_tensor")
#loc1526 = loc("2647|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_167aten__view")
#loc1527 = loc("2646|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_308aten__permute")
#loc1528 = loc("2647|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_167aten__mm")
#loc1529 = loc("2648|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|Linear[image_encoder.vision_model.encoder.layers[27].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_337aten__add")
#loc1530 = loc("2649|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_338aten__add")
#loc1531 = loc("2662|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_287xla__mark_tensor")
#loc1532 = loc("2664|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_168aten__view")
#loc1533 = loc("2663|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_309aten__permute")
#loc1534 = loc("2664|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_168aten__mm")
#loc1535 = loc("2665|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_341aten__add")
#loc1536 = loc("2672|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_113aten__view")
#loc1537 = loc("2673|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_312aten__permute")
#loc1538 = loc("2678|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_256xla__cast")
#loc1539 = loc("2681|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_172aten__mul")
#loc1540 = loc("2666|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_310aten__permute")
#loc1541 = loc("2667|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_169aten__mm")
#loc1542 = loc("2667|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_169aten__view")
#loc1543 = loc("2668|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_342aten__add")
#loc1544 = loc("2674|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_114aten__view")
#loc1545 = loc("2675|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_313aten__permute")
#loc1546 = loc("2679|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_257xla__cast")
#loc1547 = loc("2682|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_315aten__permute")
#loc1548 = loc("2683|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_173aten__mul")
#loc1549 = loc("2685|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_28aten__einsum")
#loc1550 = loc("2686|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_28aten__eq")
#loc1551 = loc("2687|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_56aten__logical_not")
#loc1553 = loc("or.11487")
#loc1554 = loc("select.11488")
#loc1555 = loc("2689|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_57aten__logical_not")
#loc1556 = loc("2691|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_28aten__expand")
#loc1557 = loc("2685|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_28aten__softmax")
#loc1558 = loc("2691|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_28aten__where")
#loc1559 = loc("2669|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_311aten__permute")
#loc1560 = loc("2670|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_170aten__mm")
#loc1561 = loc("2670|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_170aten__view")
#loc1562 = loc("2671|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_343aten__add")
#loc1563 = loc("2676|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_115aten__view")
#loc1564 = loc("2677|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_314aten__permute")
#loc1565 = loc("2680|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_258xla__cast")
#loc1566 = loc("2693|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_260aten__einsum")
#loc1567 = loc("2693|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_260xla__cast")
#loc1568 = loc("2695|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_28aten__permute")
#loc1569 = loc("2698|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_171aten__view")
#loc1570 = loc("2697|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_317aten__permute")
#loc1571 = loc("2698|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_171aten__mm")
#loc1572 = loc("2699|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPAttention[image_encoder.vision_model.encoder.layers[28].self_attn]|Linear[image_encoder.vision_model.encoder.layers[28].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_344aten__add")
#loc1573 = loc("2700|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_345aten__add")
#loc1574 = loc("2713|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_291xla__mark_tensor")
#loc1575 = loc("2715|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_172aten__view")
#loc1576 = loc("2714|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_318aten__permute")
#loc1577 = loc("2715|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_172aten__mm")
#loc1578 = loc("2716|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_348aten__add")
#loc1579 = loc("2719|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[28].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_293xla__mark_tensor")
#loc1580 = loc("2721|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_173aten__view")
#loc1581 = loc("2720|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_319aten__permute")
#loc1582 = loc("2721|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_173aten__mm")
#loc1583 = loc("2722|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|Linear[image_encoder.vision_model.encoder.layers[28].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_349aten__add")
#loc1584 = loc("2723|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_350aten__add")
#loc1585 = loc("2736|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_297xla__mark_tensor")
#loc1586 = loc("2738|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_174aten__view")
#loc1587 = loc("2737|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_320aten__permute")
#loc1588 = loc("2738|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_174aten__mm")
#loc1589 = loc("2739|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_353aten__add")
#loc1590 = loc("2746|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_117aten__view")
#loc1591 = loc("2747|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_323aten__permute")
#loc1592 = loc("2752|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_265xla__cast")
#loc1593 = loc("2755|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_178aten__mul")
#loc1594 = loc("2740|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_321aten__permute")
#loc1595 = loc("2741|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_175aten__mm")
#loc1596 = loc("2741|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_175aten__view")
#loc1597 = loc("2742|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_354aten__add")
#loc1598 = loc("2748|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_118aten__view")
#loc1599 = loc("2749|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_324aten__permute")
#loc1600 = loc("2753|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_266xla__cast")
#loc1601 = loc("2756|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_326aten__permute")
#loc1602 = loc("2757|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_179aten__mul")
#loc1603 = loc("2759|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_29aten__einsum")
#loc1604 = loc("2760|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_29aten__eq")
#loc1605 = loc("2761|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_58aten__logical_not")
#loc1607 = loc("or.11803")
#loc1608 = loc("select.11804")
#loc1609 = loc("2763|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_59aten__logical_not")
#loc1610 = loc("2765|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_29aten__expand")
#loc1611 = loc("2759|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_29aten__softmax")
#loc1612 = loc("2765|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_29aten__where")
#loc1613 = loc("2743|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_322aten__permute")
#loc1614 = loc("2744|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_176aten__mm")
#loc1615 = loc("2744|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_176aten__view")
#loc1616 = loc("2745|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_355aten__add")
#loc1617 = loc("2750|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_119aten__view")
#loc1618 = loc("2751|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_325aten__permute")
#loc1619 = loc("2754|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_267xla__cast")
#loc1620 = loc("2767|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_269aten__einsum")
#loc1621 = loc("2767|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_269xla__cast")
#loc1622 = loc("2769|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_29aten__permute")
#loc1623 = loc("2772|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_177aten__view")
#loc1624 = loc("2771|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_328aten__permute")
#loc1625 = loc("2772|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_177aten__mm")
#loc1626 = loc("2773|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPAttention[image_encoder.vision_model.encoder.layers[29].self_attn]|Linear[image_encoder.vision_model.encoder.layers[29].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_356aten__add")
#loc1627 = loc("2774|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_357aten__add")
#loc1628 = loc("2787|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_301xla__mark_tensor")
#loc1629 = loc("2789|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_178aten__view")
#loc1630 = loc("2788|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_329aten__permute")
#loc1631 = loc("2789|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_178aten__mm")
#loc1632 = loc("2790|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_360aten__add")
#loc1633 = loc("2793|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[29].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_303xla__mark_tensor")
#loc1634 = loc("2795|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_179aten__view")
#loc1635 = loc("2794|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_330aten__permute")
#loc1636 = loc("2795|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_179aten__mm")
#loc1637 = loc("2796|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|Linear[image_encoder.vision_model.encoder.layers[29].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_361aten__add")
#loc1638 = loc("2797|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_362aten__add")
#loc1639 = loc("2810|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mark_tensor_307xla__mark_tensor")
#loc1640 = loc("2812|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_180aten__view")
#loc1641 = loc("2811|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|permute_331aten__permute")
#loc1642 = loc("2812|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|matmul_180aten__mm")
#loc1643 = loc("2813|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.q_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|312|add_365aten__add")
#loc1644 = loc("2820|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|view_121aten__view")
#loc1645 = loc("2821|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|316|permute_334aten__permute")
#loc1646 = loc("2826|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_274xla__cast")
#loc1647 = loc("2829|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_184aten__mul")
#loc1648 = loc("2814|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|permute_332aten__permute")
#loc1649 = loc("2815|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_181aten__mm")
#loc1650 = loc("2815|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|matmul_181aten__view")
#loc1651 = loc("2816|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.k_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|313|add_366aten__add")
#loc1652 = loc("2822|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|view_122aten__view")
#loc1653 = loc("2823|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|317|permute_335aten__permute")
#loc1654 = loc("2827|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_275xla__cast")
#loc1655 = loc("2830|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|permute_337aten__permute")
#loc1656 = loc("2831|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|mul_185aten__mul")
#loc1657 = loc("2833|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_30aten__einsum")
#loc1658 = loc("2834|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|eq_30aten__eq")
#loc1659 = loc("2835|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_60aten__logical_not")
#loc1661 = loc("or.12119")
#loc1662 = loc("select.12120")
#loc1663 = loc("2837|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|logical_not_61aten__logical_not")
#loc1664 = loc("2839|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_30aten__expand")
#loc1665 = loc("2833|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_softmax_30aten__softmax")
#loc1666 = loc("2839|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|where_30aten__where")
#loc1667 = loc("2817|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|permute_333aten__permute")
#loc1668 = loc("2818|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_182aten__mm")
#loc1669 = loc("2818|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|matmul_182aten__view")
#loc1670 = loc("2819|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.v_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|314|add_367aten__add")
#loc1671 = loc("2824|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|view_123aten__view")
#loc1672 = loc("2825|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|318|permute_336aten__permute")
#loc1673 = loc("2828|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_276xla__cast")
#loc1674 = loc("2841|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_278aten__einsum")
#loc1675 = loc("2841|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|_to_copy_278xla__cast")
#loc1676 = loc("2843|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|333|clone_30aten__permute")
#loc1677 = loc("2846|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_183aten__view")
#loc1678 = loc("2845|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|permute_339aten__permute")
#loc1679 = loc("2846|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|matmul_183aten__mm")
#loc1680 = loc("2847|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPAttention[image_encoder.vision_model.encoder.layers[30].self_attn]|Linear[image_encoder.vision_model.encoder.layers[30].self_attn.out_proj]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:301|forward|346|add_368aten__add")
#loc1681 = loc("2848|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|403|add_369aten__add")
#loc1682 = loc("2861|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mark_tensor_311xla__mark_tensor")
#loc1683 = loc("2863|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_184aten__view")
#loc1684 = loc("2862|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|permute_340aten__permute")
#loc1685 = loc("2863|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|matmul_184aten__mm")
#loc1686 = loc("2864|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|362|add_372aten__add")
#loc1687 = loc("2867|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[30].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|mark_tensor_313xla__mark_tensor")
#loc1688 = loc("2869|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_185aten__view")
#loc1689 = loc("2868|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|permute_341aten__permute")
#loc1690 = loc("2869|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|matmul_185aten__mm")
#loc1691 = loc("2870|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|Linear[image_encoder.vision_model.encoder.layers[30].mlp.fc2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|364|add_373aten__add")
#loc1692 = loc("2871|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|408|add_374aten__add")
#loc1693 = loc("2874|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_in]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2303|matmul_193aten__view")
#loc1694 = loc("2873|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_in]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2303|permute_354aten__permute")
#loc1695 = loc("2874|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_in]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2303|matmul_193aten__mm")
#loc1696 = loc("2875|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_in]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2303|add_389aten__add")
#loc1697 = loc("2888|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_331xla__mark_tensor")
#loc1698 = loc("2902|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2248|cat_1aten__cat")
#loc1699 = loc("2906|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_195aten__view")
#loc1700 = loc("2905|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|permute_356aten__permute")
#loc1701 = loc("2906|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_195aten__mm")
#loc1702 = loc("2911|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|view_130aten__view")
#loc1703 = loc("2912|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|permute_359aten__permute")
#loc1704 = loc("2916|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_297xla__cast")
#loc1705 = loc("2919|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|permute_361aten__permute")
#loc1706 = loc("2920|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_201aten__mul")
#loc1707 = loc("2922|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_32aten__einsum")
#loc1708 = loc("2923|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|eq_32aten__eq")
#loc1709 = loc("2924|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_64aten__logical_not")
#loc1711 = loc("or.12421")
#loc1712 = loc("select.12422")
#loc1713 = loc("2926|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_65aten__logical_not")
#loc1714 = loc("2928|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_32aten__expand")
#loc1715 = loc("2922|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_32aten__softmax")
#loc1716 = loc("2928|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_32aten__where")
#loc1717 = loc("2907|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|permute_357aten__permute")
#loc1718 = loc("2908|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|matmul_196aten__mm")
#loc1719 = loc("2913|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|view_131aten__view")
#loc1720 = loc("2914|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|permute_360aten__permute")
#loc1721 = loc("2917|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_298xla__cast")
#loc1722 = loc("2930|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_300aten__einsum")
#loc1723 = loc("2930|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_300xla__cast")
#loc1724 = loc("2932|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2769|clone_32aten__permute")
#loc1725 = loc("2935|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_197aten__view")
#loc1726 = loc("2934|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|permute_363aten__permute")
#loc1727 = loc("2935|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Linear[resampler.layers[0].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_197aten__mm")
#loc1728 = loc("2936|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|Dropout[resampler.layers[0].attn.to_out[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2775|clone_33aten__view")
#loc1729 = loc("2937|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Attention[resampler.layers[0].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2783|divaten__div")
#loc1730 = loc("2938|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2249|add_394aten__add")
#loc1731 = loc("2951|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_339xla__mark_tensor")
#loc1732 = loc("2953|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|GELU[getattr(resampler.layers[0].ff, '1').net[0]]|Linear[getattr(resampler.layers[0].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_198aten__view")
#loc1733 = loc("2952|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|GELU[getattr(resampler.layers[0].ff, '1').net[0]]|Linear[getattr(resampler.layers[0].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|permute_364aten__permute")
#loc1734 = loc("2953|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|GELU[getattr(resampler.layers[0].ff, '1').net[0]]|Linear[getattr(resampler.layers[0].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_198aten__mm")
#loc1735 = loc("2957|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|Dropout[getattr(resampler.layers[0].ff, '1').net[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|clone_34xla__mark_tensor")
#loc1736 = loc("2959|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|Linear[getattr(resampler.layers[0].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_199aten__view")
#loc1737 = loc("2958|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|Linear[getattr(resampler.layers[0].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|permute_365aten__permute")
#loc1738 = loc("2959|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|Linear[getattr(resampler.layers[0].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_199aten__mm")
#loc1739 = loc("2960|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_397aten__add")
#loc1740 = loc("2986|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_349xla__mark_tensor")
#loc1741 = loc("2989|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_200aten__view")
#loc1742 = loc("2988|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|permute_366aten__permute")
#loc1743 = loc("2989|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_200aten__mm")
#loc1744 = loc("2994|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|view_133aten__view")
#loc1745 = loc("2995|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|permute_369aten__permute")
#loc1746 = loc("3000|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_307xla__cast")
#loc1747 = loc("3003|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_208aten__mul")
#loc1748 = loc("2973|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_345xla__mark_tensor")
#loc1749 = loc("2987|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2248|cat_2aten__cat")
#loc1750 = loc("2991|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_201aten__view")
#loc1751 = loc("2990|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|permute_367aten__permute")
#loc1752 = loc("2991|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_201aten__mm")
#loc1753 = loc("2996|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|view_134aten__view")
#loc1754 = loc("2997|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|permute_370aten__permute")
#loc1755 = loc("3001|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_308xla__cast")
#loc1756 = loc("3004|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|permute_372aten__permute")
#loc1757 = loc("3005|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_209aten__mul")
#loc1758 = loc("3007|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_33aten__einsum")
#loc1759 = loc("3008|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|eq_33aten__eq")
#loc1760 = loc("3009|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_66aten__logical_not")
#loc1762 = loc("or.12849")
#loc1763 = loc("select.12850")
#loc1764 = loc("3011|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_67aten__logical_not")
#loc1765 = loc("3013|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_33aten__expand")
#loc1766 = loc("3007|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_33aten__softmax")
#loc1767 = loc("3013|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_33aten__where")
#loc1768 = loc("2992|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|permute_368aten__permute")
#loc1769 = loc("2993|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|matmul_202aten__mm")
#loc1770 = loc("2998|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|view_135aten__view")
#loc1771 = loc("2999|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|permute_371aten__permute")
#loc1772 = loc("3002|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_309xla__cast")
#loc1773 = loc("3015|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_311aten__einsum")
#loc1774 = loc("3015|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_311xla__cast")
#loc1775 = loc("3017|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2769|clone_35aten__permute")
#loc1776 = loc("3020|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_203aten__view")
#loc1777 = loc("3019|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|permute_374aten__permute")
#loc1778 = loc("3020|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Linear[resampler.layers[1].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_203aten__mm")
#loc1779 = loc("3021|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|Dropout[resampler.layers[1].attn.to_out[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2775|clone_36aten__view")
#loc1780 = loc("3022|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Attention[resampler.layers[1].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2783|div_1aten__div")
#loc1781 = loc("3023|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2249|add_402aten__add")
#loc1782 = loc("3036|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_353xla__mark_tensor")
#loc1783 = loc("3038|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|GELU[getattr(resampler.layers[1].ff, '1').net[0]]|Linear[getattr(resampler.layers[1].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_204aten__view")
#loc1784 = loc("3037|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|GELU[getattr(resampler.layers[1].ff, '1').net[0]]|Linear[getattr(resampler.layers[1].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|permute_375aten__permute")
#loc1785 = loc("3038|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|GELU[getattr(resampler.layers[1].ff, '1').net[0]]|Linear[getattr(resampler.layers[1].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_204aten__mm")
#loc1786 = loc("3042|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|Dropout[getattr(resampler.layers[1].ff, '1').net[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|clone_37xla__mark_tensor")
#loc1787 = loc("3044|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|Linear[getattr(resampler.layers[1].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_205aten__view")
#loc1788 = loc("3043|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|Linear[getattr(resampler.layers[1].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|permute_376aten__permute")
#loc1789 = loc("3044|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|Linear[getattr(resampler.layers[1].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_205aten__mm")
#loc1790 = loc("3045|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_405aten__add")
#loc1791 = loc("3071|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_363xla__mark_tensor")
#loc1792 = loc("3074|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_206aten__view")
#loc1793 = loc("3073|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|permute_377aten__permute")
#loc1794 = loc("3074|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_206aten__mm")
#loc1795 = loc("3079|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|view_137aten__view")
#loc1796 = loc("3080|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|permute_380aten__permute")
#loc1797 = loc("3085|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_318xla__cast")
#loc1798 = loc("3088|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_216aten__mul")
#loc1799 = loc("3058|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_359xla__mark_tensor")
#loc1800 = loc("3072|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2248|cat_3aten__cat")
#loc1801 = loc("3076|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_207aten__view")
#loc1802 = loc("3075|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|permute_378aten__permute")
#loc1803 = loc("3076|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_207aten__mm")
#loc1804 = loc("3081|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|view_138aten__view")
#loc1805 = loc("3082|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|permute_381aten__permute")
#loc1806 = loc("3086|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_319xla__cast")
#loc1807 = loc("3089|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|permute_383aten__permute")
#loc1808 = loc("3090|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_217aten__mul")
#loc1809 = loc("3092|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_34aten__einsum")
#loc1810 = loc("3093|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|eq_34aten__eq")
#loc1811 = loc("3094|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_68aten__logical_not")
#loc1813 = loc("or.13277")
#loc1814 = loc("select.13278")
#loc1815 = loc("3096|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_69aten__logical_not")
#loc1816 = loc("3098|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_34aten__expand")
#loc1817 = loc("3092|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_34aten__softmax")
#loc1818 = loc("3098|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_34aten__where")
#loc1819 = loc("3077|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|permute_379aten__permute")
#loc1820 = loc("3078|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|matmul_208aten__mm")
#loc1821 = loc("3083|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|view_139aten__view")
#loc1822 = loc("3084|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|permute_382aten__permute")
#loc1823 = loc("3087|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_320xla__cast")
#loc1824 = loc("3100|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_322aten__einsum")
#loc1825 = loc("3100|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_322xla__cast")
#loc1826 = loc("3102|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2769|clone_38aten__permute")
#loc1827 = loc("3105|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_209aten__view")
#loc1828 = loc("3104|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|permute_385aten__permute")
#loc1829 = loc("3105|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Linear[resampler.layers[2].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_209aten__mm")
#loc1830 = loc("3106|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|Dropout[resampler.layers[2].attn.to_out[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2775|clone_39aten__view")
#loc1831 = loc("3107|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Attention[resampler.layers[2].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2783|div_2aten__div")
#loc1832 = loc("3108|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2249|add_410aten__add")
#loc1833 = loc("3121|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_367xla__mark_tensor")
#loc1834 = loc("3123|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|GELU[getattr(resampler.layers[2].ff, '1').net[0]]|Linear[getattr(resampler.layers[2].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_210aten__view")
#loc1835 = loc("3122|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|GELU[getattr(resampler.layers[2].ff, '1').net[0]]|Linear[getattr(resampler.layers[2].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|permute_386aten__permute")
#loc1836 = loc("3123|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|GELU[getattr(resampler.layers[2].ff, '1').net[0]]|Linear[getattr(resampler.layers[2].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_210aten__mm")
#loc1837 = loc("3127|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|Dropout[getattr(resampler.layers[2].ff, '1').net[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|clone_40xla__mark_tensor")
#loc1838 = loc("3129|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|Linear[getattr(resampler.layers[2].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_211aten__view")
#loc1839 = loc("3128|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|Linear[getattr(resampler.layers[2].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|permute_387aten__permute")
#loc1840 = loc("3129|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|Linear[getattr(resampler.layers[2].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_211aten__mm")
#loc1841 = loc("3130|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_413aten__add")
#loc1842 = loc("3156|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mark_tensor_377xla__mark_tensor")
#loc1843 = loc("3159|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_212aten__view")
#loc1844 = loc("3158|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|permute_388aten__permute")
#loc1845 = loc("3159|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_q]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2740|matmul_212aten__mm")
#loc1846 = loc("3164|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|view_141aten__view")
#loc1847 = loc("3165|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2753|permute_391aten__permute")
#loc1848 = loc("3170|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_329xla__cast")
#loc1849 = loc("3173|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_224aten__mul")
#loc1850 = loc("3143|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mark_tensor_373xla__mark_tensor")
#loc1851 = loc("3157|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2248|cat_4aten__cat")
#loc1852 = loc("3161|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_213aten__view")
#loc1853 = loc("3160|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|permute_389aten__permute")
#loc1854 = loc("3161|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_k]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2747|matmul_213aten__mm")
#loc1855 = loc("3166|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|view_142aten__view")
#loc1856 = loc("3167|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2755|permute_392aten__permute")
#loc1857 = loc("3171|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_330xla__cast")
#loc1858 = loc("3174|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|permute_394aten__permute")
#loc1859 = loc("3175|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|mul_225aten__mul")
#loc1860 = loc("3177|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_35aten__einsum")
#loc1861 = loc("3178|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|eq_35aten__eq")
#loc1862 = loc("3179|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_70aten__logical_not")
#loc1864 = loc("or.13705")
#loc1865 = loc("select.13706")
#loc1866 = loc("3181|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|logical_not_71aten__logical_not")
#loc1867 = loc("3183|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_35aten__expand")
#loc1868 = loc("3177|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_softmax_35aten__softmax")
#loc1869 = loc("3183|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|where_35aten__where")
#loc1870 = loc("3162|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|permute_390aten__permute")
#loc1871 = loc("3163|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_v]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2748|matmul_214aten__mm")
#loc1872 = loc("3168|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|view_143aten__view")
#loc1873 = loc("3169|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2756|permute_393aten__permute")
#loc1874 = loc("3172|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_331xla__cast")
#loc1875 = loc("3185|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_333aten__einsum")
#loc1876 = loc("3185|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2765|_to_copy_333xla__cast")
#loc1877 = loc("3187|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2769|clone_41aten__permute")
#loc1878 = loc("3190|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_215aten__view")
#loc1879 = loc("3189|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|permute_396aten__permute")
#loc1880 = loc("3190|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Linear[resampler.layers[3].attn.to_out[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2773|matmul_215aten__mm")
#loc1881 = loc("3191|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|Dropout[resampler.layers[3].attn.to_out[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2775|clone_42aten__view")
#loc1882 = loc("3192|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Attention[resampler.layers[3].attn]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention_processor.py:2703|__call__|2783|div_3aten__div")
#loc1883 = loc("3193|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2249|add_418aten__add")
#loc1884 = loc("3206|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mark_tensor_381xla__mark_tensor")
#loc1885 = loc("3208|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|GELU[getattr(resampler.layers[3].ff, '1').net[0]]|Linear[getattr(resampler.layers[3].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_216aten__view")
#loc1886 = loc("3207|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|GELU[getattr(resampler.layers[3].ff, '1').net[0]]|Linear[getattr(resampler.layers[3].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|permute_397aten__permute")
#loc1887 = loc("3208|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|GELU[getattr(resampler.layers[3].ff, '1').net[0]]|Linear[getattr(resampler.layers[3].ff, '1').net[0].proj]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:87|forward|88|matmul_216aten__mm")
#loc1888 = loc("3212|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|Dropout[getattr(resampler.layers[3].ff, '1').net[1]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|clone_43xla__mark_tensor")
#loc1889 = loc("3214|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|Linear[getattr(resampler.layers[3].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_217aten__view")
#loc1890 = loc("3213|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|Linear[getattr(resampler.layers[3].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|permute_398aten__permute")
#loc1891 = loc("3214|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|Linear[getattr(resampler.layers[3].ff, '1').net[2]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/attention.py:1736|forward|1741|matmul_217aten__mm")
#loc1892 = loc("3215|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_421aten__add")
#loc1893 = loc("3217|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2309|matmul_218aten__view")
#loc1894 = loc("3216|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2309|permute_399aten__permute")
#loc1895 = loc("3217|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2309|matmul_218aten__mm")
#loc1896 = loc("3218|IPAdapterPlusImageProjection[resampler]|Linear[resampler.proj_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2309|add_422aten__add")
#loc1897 = loc("3231|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|mark_tensor_387xla__mark_tensor")
#loc1901 = loc("3222|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|_to_copy_336xla__cast")
#loc1902 = loc("3223|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|var_mean_78aten__var_mean")
#loc1903 = loc("3226|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|sub_78aten__sub")
#loc1904 = loc("3224|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|add_423aten__add")
#loc1905 = loc("3225|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|rsqrt_78aten__rsqrt")
#loc1906 = loc("3227|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|mul_228aten__mul")
#loc1907 = loc("3228|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|mul_229xla__cast")
#loc1908 = loc("3228|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|mul_229aten__mul")
#loc1909 = loc("3229|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|add_424aten__add")
#loc1910 = loc("3230|IPAdapterPlusImageProjection[resampler]|LayerNorm[resampler.norm_out]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2293|forward|2310|_to_copy_337xla__cast")
#loc1914 = loc("3197|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|_to_copy_334xla__cast")
#loc1915 = loc("3198|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|var_mean_77aten__var_mean")
#loc1916 = loc("3201|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|sub_77aten__sub")
#loc1917 = loc("3199|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_419aten__add")
#loc1918 = loc("3200|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|rsqrt_77aten__rsqrt")
#loc1919 = loc("3202|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_226aten__mul")
#loc1920 = loc("3203|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_227xla__cast")
#loc1921 = loc("3203|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_227aten__mul")
#loc1922 = loc("3204|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_420aten__add")
#loc1923 = loc("3205|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|LayerNorm[getattr(resampler.layers[3].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|_to_copy_335xla__cast")
#loc1927 = loc("3134|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|_to_copy_325xla__cast")
#loc1928 = loc("3135|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|var_mean_75aten__var_mean")
#loc1929 = loc("3138|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|sub_75aten__sub")
#loc1930 = loc("3136|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|add_414aten__add")
#loc1931 = loc("3137|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|rsqrt_75aten__rsqrt")
#loc1932 = loc("3139|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_220aten__mul")
#loc1933 = loc("3140|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_221xla__cast")
#loc1934 = loc("3140|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_221aten__mul")
#loc1935 = loc("3141|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|add_415aten__add")
#loc1936 = loc("3142|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|_to_copy_326xla__cast")
#loc1940 = loc("3049|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|_to_copy_314xla__cast")
#loc1941 = loc("3050|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|var_mean_72aten__var_mean")
#loc1942 = loc("3053|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|sub_72aten__sub")
#loc1943 = loc("3051|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|add_406aten__add")
#loc1944 = loc("3052|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|rsqrt_72aten__rsqrt")
#loc1945 = loc("3054|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_212aten__mul")
#loc1946 = loc("3055|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_213xla__cast")
#loc1947 = loc("3055|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_213aten__mul")
#loc1948 = loc("3056|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|add_407aten__add")
#loc1949 = loc("3057|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|_to_copy_315xla__cast")
#loc1951 = loc("3040|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|FeedForward[getattr(resampler.layers[1].ff, '1')]|GELU[getattr(resampler.layers[1].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|gelu_33aten__gelu")
#loc1953 = loc("2955|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|FeedForward[getattr(resampler.layers[0].ff, '1')]|GELU[getattr(resampler.layers[0].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|gelu_32aten__gelu")
#loc1957 = loc("2942|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|_to_copy_301xla__cast")
#loc1958 = loc("2943|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|var_mean_68aten__var_mean")
#loc1959 = loc("2946|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|sub_68aten__sub")
#loc1960 = loc("2944|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_395aten__add")
#loc1961 = loc("2945|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|rsqrt_68aten__rsqrt")
#loc1962 = loc("2947|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_202aten__mul")
#loc1963 = loc("2948|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_203xla__cast")
#loc1964 = loc("2948|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_203aten__mul")
#loc1965 = loc("2949|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_396aten__add")
#loc1966 = loc("2950|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|Sequential[resampler.layers[0].ff]|LayerNorm[getattr(resampler.layers[0].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|_to_copy_302xla__cast")
#loc1968 = loc("2792|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|CLIPMLP[image_encoder.vision_model.encoder.layers[29].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[29].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_29aten__gelu")
#loc1972 = loc("2727|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_263xla__cast")
#loc1973 = loc("2728|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_59aten__var_mean")
#loc1974 = loc("2731|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_59aten__sub")
#loc1975 = loc("2729|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_351aten__add")
#loc1976 = loc("2730|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_59aten__rsqrt")
#loc1977 = loc("2732|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_176aten__mul")
#loc1978 = loc("2733|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_177xla__cast")
#loc1979 = loc("2733|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_177aten__mul")
#loc1980 = loc("2734|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_352aten__add")
#loc1981 = loc("2735|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_264xla__cast")
#loc1983 = loc("2718|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|CLIPMLP[image_encoder.vision_model.encoder.layers[28].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[28].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_28aten__gelu")
#loc1987 = loc("2977|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|_to_copy_305xla__cast")
#loc1988 = loc("2978|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|var_mean_70aten__var_mean")
#loc1989 = loc("2981|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|sub_70aten__sub")
#loc1990 = loc("2979|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|add_400aten__add")
#loc1991 = loc("2980|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|rsqrt_70aten__rsqrt")
#loc1992 = loc("2982|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_206aten__mul")
#loc1993 = loc("2983|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_207xla__cast")
#loc1994 = loc("2983|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_207aten__mul")
#loc1995 = loc("2984|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|add_401aten__add")
#loc1996 = loc("2985|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|_to_copy_306xla__cast")
#loc2000 = loc("2704|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_261xla__cast")
#loc2001 = loc("2705|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_58aten__var_mean")
#loc2002 = loc("2708|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_58aten__sub")
#loc2003 = loc("2706|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_346aten__add")
#loc2004 = loc("2707|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_58aten__rsqrt")
#loc2005 = loc("2709|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_174aten__mul")
#loc2006 = loc("2710|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_175xla__cast")
#loc2007 = loc("2710|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_175aten__mul")
#loc2008 = loc("2711|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_347aten__add")
#loc2009 = loc("2712|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_262xla__cast")
#loc2013 = loc("2653|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_254xla__cast")
#loc2014 = loc("2654|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_57aten__var_mean")
#loc2015 = loc("2657|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_57aten__sub")
#loc2016 = loc("2655|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_339aten__add")
#loc2017 = loc("2656|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_57aten__rsqrt")
#loc2018 = loc("2658|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_170aten__mul")
#loc2019 = loc("2659|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_171xla__cast")
#loc2020 = loc("2659|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_171aten__mul")
#loc2021 = loc("2660|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_340aten__add")
#loc2022 = loc("2661|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[28]]|LayerNorm[image_encoder.vision_model.encoder.layers[28].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_255xla__cast")
#loc2024 = loc("3125|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|FeedForward[getattr(resampler.layers[2].ff, '1')]|GELU[getattr(resampler.layers[2].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|gelu_34aten__gelu")
#loc2028 = loc("2579|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_245xla__cast")
#loc2029 = loc("2580|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_55aten__var_mean")
#loc2030 = loc("2583|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_55aten__sub")
#loc2031 = loc("2581|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_327aten__add")
#loc2032 = loc("2582|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_55aten__rsqrt")
#loc2033 = loc("2584|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_164aten__mul")
#loc2034 = loc("2585|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_165xla__cast")
#loc2035 = loc("2585|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_165aten__mul")
#loc2036 = loc("2586|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_328aten__add")
#loc2037 = loc("2587|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_246xla__cast")
#loc2041 = loc("2556|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_243xla__cast")
#loc2042 = loc("2557|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_54aten__var_mean")
#loc2043 = loc("2560|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_54aten__sub")
#loc2044 = loc("2558|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_322aten__add")
#loc2045 = loc("2559|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_54aten__rsqrt")
#loc2046 = loc("2561|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_162aten__mul")
#loc2047 = loc("2562|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_163xla__cast")
#loc2048 = loc("2562|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_163aten__mul")
#loc2049 = loc("2563|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_323aten__add")
#loc2050 = loc("2564|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_244xla__cast")
#loc2054 = loc("2505|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_236xla__cast")
#loc2055 = loc("2506|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_53aten__var_mean")
#loc2056 = loc("2509|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_53aten__sub")
#loc2057 = loc("2507|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_315aten__add")
#loc2058 = loc("2508|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_53aten__rsqrt")
#loc2059 = loc("2510|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_158aten__mul")
#loc2060 = loc("2511|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_159xla__cast")
#loc2061 = loc("2511|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_159aten__mul")
#loc2062 = loc("2512|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_316aten__add")
#loc2063 = loc("2513|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|LayerNorm[image_encoder.vision_model.encoder.layers[26].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_237xla__cast")
#loc2065 = loc("2496|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|CLIPMLP[image_encoder.vision_model.encoder.layers[25].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[25].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_25aten__gelu")
#loc2069 = loc("2482|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_234xla__cast")
#loc2070 = loc("2483|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_52aten__var_mean")
#loc2071 = loc("2486|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_52aten__sub")
#loc2072 = loc("2484|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_310aten__add")
#loc2073 = loc("2485|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_52aten__rsqrt")
#loc2074 = loc("2487|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_156aten__mul")
#loc2075 = loc("2488|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_157xla__cast")
#loc2076 = loc("2488|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_157aten__mul")
#loc2077 = loc("2489|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_311aten__add")
#loc2078 = loc("2490|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_235xla__cast")
#loc2082 = loc("2431|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_227xla__cast")
#loc2083 = loc("2432|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_51aten__var_mean")
#loc2084 = loc("2435|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_51aten__sub")
#loc2085 = loc("2433|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_303aten__add")
#loc2086 = loc("2434|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_51aten__rsqrt")
#loc2087 = loc("2436|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_152aten__mul")
#loc2088 = loc("2437|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_153xla__cast")
#loc2089 = loc("2437|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_153aten__mul")
#loc2090 = loc("2438|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_304aten__add")
#loc2091 = loc("2439|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[25]]|LayerNorm[image_encoder.vision_model.encoder.layers[25].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_228xla__cast")
#loc2093 = loc("2422|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|CLIPMLP[image_encoder.vision_model.encoder.layers[24].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[24].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_24aten__gelu")
#loc2097 = loc("2408|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_225xla__cast")
#loc2098 = loc("2409|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_50aten__var_mean")
#loc2099 = loc("2412|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_50aten__sub")
#loc2100 = loc("2410|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_298aten__add")
#loc2101 = loc("2411|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_50aten__rsqrt")
#loc2102 = loc("2413|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_150aten__mul")
#loc2103 = loc("2414|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_151xla__cast")
#loc2104 = loc("2414|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_151aten__mul")
#loc2105 = loc("2415|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_299aten__add")
#loc2106 = loc("2416|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_226xla__cast")
#loc2110 = loc("2357|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_218xla__cast")
#loc2111 = loc("2358|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_49aten__var_mean")
#loc2112 = loc("2361|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_49aten__sub")
#loc2113 = loc("2359|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_291aten__add")
#loc2114 = loc("2360|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_49aten__rsqrt")
#loc2115 = loc("2362|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_146aten__mul")
#loc2116 = loc("2363|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_147xla__cast")
#loc2117 = loc("2363|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_147aten__mul")
#loc2118 = loc("2364|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_292aten__add")
#loc2119 = loc("2365|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[24]]|LayerNorm[image_encoder.vision_model.encoder.layers[24].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_219xla__cast")
#loc2121 = loc("2348|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|CLIPMLP[image_encoder.vision_model.encoder.layers[23].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[23].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_23aten__gelu")
#loc2125 = loc("2334|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_216xla__cast")
#loc2126 = loc("2335|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_48aten__var_mean")
#loc2127 = loc("2338|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_48aten__sub")
#loc2128 = loc("2336|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_286aten__add")
#loc2129 = loc("2337|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_48aten__rsqrt")
#loc2130 = loc("2339|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_144aten__mul")
#loc2131 = loc("2340|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_145xla__cast")
#loc2132 = loc("2340|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_145aten__mul")
#loc2133 = loc("2341|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_287aten__add")
#loc2134 = loc("2342|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_217xla__cast")
#loc2138 = loc("2209|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_200xla__cast")
#loc2139 = loc("2210|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_45aten__var_mean")
#loc2140 = loc("2213|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_45aten__sub")
#loc2141 = loc("2211|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_267aten__add")
#loc2142 = loc("2212|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_45aten__rsqrt")
#loc2143 = loc("2214|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_134aten__mul")
#loc2144 = loc("2215|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_135xla__cast")
#loc2145 = loc("2215|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_135aten__mul")
#loc2146 = loc("2216|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_268aten__add")
#loc2147 = loc("2217|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_201xla__cast")
#loc2149 = loc("2200|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|CLIPMLP[image_encoder.vision_model.encoder.layers[21].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[21].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_21aten__gelu")
#loc2153 = loc("3027|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|_to_copy_312xla__cast")
#loc2154 = loc("3028|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|var_mean_71aten__var_mean")
#loc2155 = loc("3031|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|sub_71aten__sub")
#loc2156 = loc("3029|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_403aten__add")
#loc2157 = loc("3030|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|rsqrt_71aten__rsqrt")
#loc2158 = loc("3032|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_210aten__mul")
#loc2159 = loc("3033|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_211xla__cast")
#loc2160 = loc("3033|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_211aten__mul")
#loc2161 = loc("3034|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_404aten__add")
#loc2162 = loc("3035|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|Sequential[resampler.layers[1].ff]|LayerNorm[getattr(resampler.layers[1].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|_to_copy_313xla__cast")
#loc2166 = loc("2186|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_198xla__cast")
#loc2167 = loc("2187|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_44aten__var_mean")
#loc2168 = loc("2190|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_44aten__sub")
#loc2169 = loc("2188|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_262aten__add")
#loc2170 = loc("2189|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_44aten__rsqrt")
#loc2171 = loc("2191|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_132aten__mul")
#loc2172 = loc("2192|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_133xla__cast")
#loc2173 = loc("2192|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_133aten__mul")
#loc2174 = loc("2193|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_263aten__add")
#loc2175 = loc("2194|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_199xla__cast")
#loc2177 = loc("2126|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|CLIPMLP[image_encoder.vision_model.encoder.layers[20].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[20].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_20aten__gelu")
#loc2179 = loc("2570|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[26]]|CLIPMLP[image_encoder.vision_model.encoder.layers[26].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[26].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_26aten__gelu")
#loc2183 = loc("2061|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_182xla__cast")
#loc2184 = loc("2062|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_41aten__var_mean")
#loc2185 = loc("2065|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_41aten__sub")
#loc2186 = loc("2063|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_243aten__add")
#loc2187 = loc("2064|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_41aten__rsqrt")
#loc2188 = loc("2066|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_122aten__mul")
#loc2189 = loc("2067|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_123xla__cast")
#loc2190 = loc("2067|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_123aten__mul")
#loc2191 = loc("2068|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_244aten__add")
#loc2192 = loc("2069|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_183xla__cast")
#loc2194 = loc("2052|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|CLIPMLP[image_encoder.vision_model.encoder.layers[19].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[19].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_19aten__gelu")
#loc2198 = loc("1987|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_173xla__cast")
#loc2199 = loc("1988|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_39aten__var_mean")
#loc2200 = loc("1991|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_39aten__sub")
#loc2201 = loc("1989|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_231aten__add")
#loc2202 = loc("1990|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_39aten__rsqrt")
#loc2203 = loc("1992|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_116aten__mul")
#loc2204 = loc("1993|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_117xla__cast")
#loc2205 = loc("1993|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_117aten__mul")
#loc2206 = loc("1994|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_232aten__add")
#loc2207 = loc("1995|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_174xla__cast")
#loc2209 = loc("1238|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|CLIPMLP[image_encoder.vision_model.encoder.layers[8].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[8].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_8aten__gelu")
#loc2213 = loc("1321|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_92xla__cast")
#loc2214 = loc("1322|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_21aten__var_mean")
#loc2215 = loc("1325|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_21aten__sub")
#loc2216 = loc("1323|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_123aten__add")
#loc2217 = loc("1324|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_21aten__rsqrt")
#loc2218 = loc("1326|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_62aten__mul")
#loc2219 = loc("1327|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_63xla__cast")
#loc2220 = loc("1327|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_63aten__mul")
#loc2221 = loc("1328|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_124aten__add")
#loc2222 = loc("1329|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_93xla__cast")
#loc2226 = loc("1173|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_74xla__cast")
#loc2227 = loc("1174|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_17aten__var_mean")
#loc2228 = loc("1177|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_17aten__sub")
#loc2229 = loc("1175|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_99aten__add")
#loc2230 = loc("1176|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_17aten__rsqrt")
#loc2231 = loc("1178|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_50aten__mul")
#loc2232 = loc("1179|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_51xla__cast")
#loc2233 = loc("1179|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_51aten__mul")
#loc2234 = loc("1180|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_100aten__add")
#loc2235 = loc("1181|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_75xla__cast")
#loc2237 = loc("2866|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|CLIPMLP[image_encoder.vision_model.encoder.layers[30].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[30].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_30aten__gelu")
#loc2239 = loc("1164|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|CLIPMLP[image_encoder.vision_model.encoder.layers[7].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[7].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_7aten__gelu")
#loc2243 = loc("2852|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_279xla__cast")
#loc2244 = loc("2853|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_62aten__var_mean")
#loc2245 = loc("2856|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_62aten__sub")
#loc2246 = loc("2854|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_370aten__add")
#loc2247 = loc("2855|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_62aten__rsqrt")
#loc2248 = loc("2857|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_186aten__mul")
#loc2249 = loc("2858|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_187xla__cast")
#loc2250 = loc("2858|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_187aten__mul")
#loc2251 = loc("2859|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_371aten__add")
#loc2252 = loc("2860|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_280xla__cast")
#loc2254 = loc("2274|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|CLIPMLP[image_encoder.vision_model.encoder.layers[22].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[22].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_22aten__gelu")
#loc2258 = loc("2260|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_207xla__cast")
#loc2259 = loc("2261|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_46aten__var_mean")
#loc2260 = loc("2264|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_46aten__sub")
#loc2261 = loc("2262|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_274aten__add")
#loc2262 = loc("2263|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_46aten__rsqrt")
#loc2263 = loc("2265|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_138aten__mul")
#loc2264 = loc("2266|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_139xla__cast")
#loc2265 = loc("2266|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_139aten__mul")
#loc2266 = loc("2267|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_275aten__add")
#loc2267 = loc("2268|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[22]]|LayerNorm[image_encoder.vision_model.encoder.layers[22].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_208xla__cast")
#loc2269 = loc("1090|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|CLIPMLP[image_encoder.vision_model.encoder.layers[6].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[6].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_6aten__gelu")
#loc2273 = loc("1076|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_63xla__cast")
#loc2274 = loc("1077|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_14aten__var_mean")
#loc2275 = loc("1080|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_14aten__sub")
#loc2276 = loc("1078|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_82aten__add")
#loc2277 = loc("1079|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_14aten__rsqrt")
#loc2278 = loc("1081|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_42aten__mul")
#loc2279 = loc("1082|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_43xla__cast")
#loc2280 = loc("1082|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_43aten__mul")
#loc2281 = loc("1083|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_83aten__add")
#loc2282 = loc("1084|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_64xla__cast")
#loc2284 = loc("646|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|CLIPMLP[image_encoder.vision_model.encoder.layers[0].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[0].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|geluaten__gelu")
#loc2288 = loc("3112|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|_to_copy_323xla__cast")
#loc2289 = loc("3113|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|var_mean_74aten__var_mean")
#loc2290 = loc("3116|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|sub_74aten__sub")
#loc2291 = loc("3114|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_411aten__add")
#loc2292 = loc("3115|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|rsqrt_74aten__rsqrt")
#loc2293 = loc("3117|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_218aten__mul")
#loc2294 = loc("3118|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_219xla__cast")
#loc2295 = loc("3118|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|mul_219aten__mul")
#loc2296 = loc("3119|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|add_412aten__add")
#loc2297 = loc("3120|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|Sequential[resampler.layers[2].ff]|LayerNorm[getattr(resampler.layers[2].ff, '0')]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2250|_to_copy_324xla__cast")
#loc2301 = loc("1224|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_81xla__cast")
#loc2302 = loc("1225|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_18aten__var_mean")
#loc2303 = loc("1228|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_18aten__sub")
#loc2304 = loc("1226|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_106aten__add")
#loc2305 = loc("1227|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_18aten__rsqrt")
#loc2306 = loc("1229|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_54aten__mul")
#loc2307 = loc("1230|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_55xla__cast")
#loc2308 = loc("1230|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_55aten__mul")
#loc2309 = loc("1231|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_107aten__add")
#loc2310 = loc("1232|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[8]]|LayerNorm[image_encoder.vision_model.encoder.layers[8].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_82xla__cast")
#loc2314 = loc("729|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_20xla__cast")
#loc2315 = loc("730|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_5aten__var_mean")
#loc2316 = loc("733|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_5aten__sub")
#loc2317 = loc("731|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_27aten__add")
#loc2318 = loc("732|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_5aten__rsqrt")
#loc2319 = loc("734|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_14aten__mul")
#loc2320 = loc("735|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_15xla__cast")
#loc2321 = loc("735|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_15aten__mul")
#loc2322 = loc("736|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_28aten__add")
#loc2323 = loc("737|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_21xla__cast")
#loc2327 = loc("1150|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_72xla__cast")
#loc2328 = loc("1151|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_16aten__var_mean")
#loc2329 = loc("1154|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_16aten__sub")
#loc2330 = loc("1152|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_94aten__add")
#loc2331 = loc("1153|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_16aten__rsqrt")
#loc2332 = loc("1155|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_48aten__mul")
#loc2333 = loc("1156|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_49xla__cast")
#loc2334 = loc("1156|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_49aten__mul")
#loc2335 = loc("1157|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_95aten__add")
#loc2336 = loc("1158|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_73xla__cast")
#loc2340 = loc("1839|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_155xla__cast")
#loc2341 = loc("1840|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_35aten__var_mean")
#loc2342 = loc("1843|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_35aten__sub")
#loc2343 = loc("1841|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_207aten__add")
#loc2344 = loc("1842|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_35aten__rsqrt")
#loc2345 = loc("1844|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_104aten__mul")
#loc2346 = loc("1845|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_105xla__cast")
#loc2347 = loc("1845|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_105aten__mul")
#loc2348 = loc("1846|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_208aten__add")
#loc2349 = loc("1847|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_156xla__cast")
#loc2351 = loc("942|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|CLIPMLP[image_encoder.vision_model.encoder.layers[4].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[4].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_4aten__gelu")
#loc2355 = loc("2801|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_272xla__cast")
#loc2356 = loc("2802|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_61aten__var_mean")
#loc2357 = loc("2805|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_61aten__sub")
#loc2358 = loc("2803|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_363aten__add")
#loc2359 = loc("2804|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_61aten__rsqrt")
#loc2360 = loc("2806|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_182aten__mul")
#loc2361 = loc("2807|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_183xla__cast")
#loc2362 = loc("2807|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_183aten__mul")
#loc2363 = loc("2808|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_364aten__add")
#loc2364 = loc("2809|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[30]]|LayerNorm[image_encoder.vision_model.encoder.layers[30].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_273xla__cast")
#loc2368 = loc("780|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_27xla__cast")
#loc2369 = loc("781|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_6aten__var_mean")
#loc2370 = loc("784|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_6aten__sub")
#loc2371 = loc("782|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_34aten__add")
#loc2372 = loc("783|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_6aten__rsqrt")
#loc2373 = loc("785|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_18aten__mul")
#loc2374 = loc("786|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_19xla__cast")
#loc2375 = loc("786|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_19aten__mul")
#loc2376 = loc("787|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_35aten__add")
#loc2377 = loc("788|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|LayerNorm[image_encoder.vision_model.encoder.layers[2].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_28xla__cast")
#loc2381 = loc("1668|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_135xla__cast")
#loc2382 = loc("1669|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_30aten__var_mean")
#loc2383 = loc("1672|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_30aten__sub")
#loc2384 = loc("1670|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_178aten__add")
#loc2385 = loc("1671|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_30aten__rsqrt")
#loc2386 = loc("1673|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_90aten__mul")
#loc2387 = loc("1674|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_91xla__cast")
#loc2388 = loc("1674|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_91aten__mul")
#loc2389 = loc("1675|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_179aten__add")
#loc2390 = loc("1676|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_136xla__cast")
#loc2394 = loc("581|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_2xla__cast")
#loc2395 = loc("582|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_1aten__var_mean")
#loc2396 = loc("585|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_1aten__sub")
#loc2397 = loc("583|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_3aten__add")
#loc2398 = loc("584|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_1aten__rsqrt")
#loc2399 = loc("586|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_2aten__mul")
#loc2400 = loc("587|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_3xla__cast")
#loc2401 = loc("587|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_3aten__mul")
#loc2402 = loc("588|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_4aten__add")
#loc2403 = loc("589|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_3xla__cast")
#loc2405 = loc("1534|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|CLIPMLP[image_encoder.vision_model.encoder.layers[12].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[12].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_12aten__gelu")
#loc2409 = loc("632|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_9xla__cast")
#loc2410 = loc("633|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_2aten__var_mean")
#loc2411 = loc("636|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_2aten__sub")
#loc2412 = loc("634|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_10aten__add")
#loc2413 = loc("635|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_2aten__rsqrt")
#loc2414 = loc("637|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_6aten__mul")
#loc2415 = loc("638|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_7xla__cast")
#loc2416 = loc("638|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_7aten__mul")
#loc2417 = loc("639|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_11aten__add")
#loc2418 = loc("640|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[0]]|LayerNorm[image_encoder.vision_model.encoder.layers[0].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_10xla__cast")
#loc2422 = loc("706|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_18xla__cast")
#loc2423 = loc("707|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_4aten__var_mean")
#loc2424 = loc("710|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_4aten__sub")
#loc2425 = loc("708|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_22aten__add")
#loc2426 = loc("709|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_4aten__rsqrt")
#loc2427 = loc("711|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_12aten__mul")
#loc2428 = loc("712|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_13xla__cast")
#loc2429 = loc("712|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_13aten__mul")
#loc2430 = loc("713|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_23aten__add")
#loc2431 = loc("714|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_19xla__cast")
#loc2433 = loc("720|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|CLIPMLP[image_encoder.vision_model.encoder.layers[1].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[1].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_1aten__gelu")
#loc2437 = loc("568|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|_to_copyxla__cast")
#loc2438 = loc("569|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|var_meanaten__var_mean")
#loc2439 = loc("572|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|subaten__sub")
#loc2440 = loc("570|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|add_1aten__add")
#loc2441 = loc("571|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|rsqrtaten__rsqrt")
#loc2442 = loc("573|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|mulaten__mul")
#loc2443 = loc("574|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|mul_1xla__cast")
#loc2444 = loc("574|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|mul_1aten__mul")
#loc2445 = loc("575|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|add_2aten__add")
#loc2446 = loc("576|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|LayerNorm[image_encoder.vision_model.pre_layrnorm]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:727|forward|743|_to_copy_1xla__cast")
#loc2450 = loc("928|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_45xla__cast")
#loc2451 = loc("929|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_10aten__var_mean")
#loc2452 = loc("932|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_10aten__sub")
#loc2453 = loc("930|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_58aten__add")
#loc2454 = loc("931|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_10aten__rsqrt")
#loc2455 = loc("933|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_30aten__mul")
#loc2456 = loc("934|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_31xla__cast")
#loc2457 = loc("934|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_31aten__mul")
#loc2458 = loc("935|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_59aten__add")
#loc2459 = loc("936|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_46xla__cast")
#loc2463 = loc("655|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_11xla__cast")
#loc2464 = loc("656|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_3aten__var_mean")
#loc2465 = loc("659|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_3aten__sub")
#loc2466 = loc("657|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_15aten__add")
#loc2467 = loc("658|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_3aten__rsqrt")
#loc2468 = loc("660|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_8aten__mul")
#loc2469 = loc("661|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_9xla__cast")
#loc2470 = loc("661|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_9aten__mul")
#loc2471 = loc("662|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_16aten__add")
#loc2472 = loc("663|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[1]]|LayerNorm[image_encoder.vision_model.encoder.layers[1].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_12xla__cast")
#loc2476 = loc("1691|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_137xla__cast")
#loc2477 = loc("1692|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_31aten__var_mean")
#loc2478 = loc("1695|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_31aten__sub")
#loc2479 = loc("1693|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_183aten__add")
#loc2480 = loc("1694|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_31aten__rsqrt")
#loc2481 = loc("1696|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_92aten__mul")
#loc2482 = loc("1697|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_93xla__cast")
#loc2483 = loc("1697|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_93aten__mul")
#loc2484 = loc("1698|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_184aten__add")
#loc2485 = loc("1699|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_138xla__cast")
#loc2487 = loc("868|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|CLIPMLP[image_encoder.vision_model.encoder.layers[3].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[3].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_3aten__gelu")
#loc2491 = loc("1002|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_54xla__cast")
#loc2492 = loc("1003|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_12aten__var_mean")
#loc2493 = loc("1006|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_12aten__sub")
#loc2494 = loc("1004|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_70aten__add")
#loc2495 = loc("1005|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_12aten__rsqrt")
#loc2496 = loc("1007|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_36aten__mul")
#loc2497 = loc("1008|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_37xla__cast")
#loc2498 = loc("1008|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_37aten__mul")
#loc2499 = loc("1009|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_71aten__add")
#loc2500 = loc("1010|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_55xla__cast")
#loc2502 = loc("794|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[2]]|CLIPMLP[image_encoder.vision_model.encoder.layers[2].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[2].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_2aten__gelu")
#loc2506 = loc("951|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_47xla__cast")
#loc2507 = loc("952|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_11aten__var_mean")
#loc2508 = loc("955|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_11aten__sub")
#loc2509 = loc("953|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_63aten__add")
#loc2510 = loc("954|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_11aten__rsqrt")
#loc2511 = loc("956|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_32aten__mul")
#loc2512 = loc("957|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_33xla__cast")
#loc2513 = loc("957|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_33aten__mul")
#loc2514 = loc("958|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_64aten__add")
#loc2515 = loc("959|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|LayerNorm[image_encoder.vision_model.encoder.layers[5].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_48xla__cast")
#loc2519 = loc("803|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_29xla__cast")
#loc2520 = loc("804|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_7aten__var_mean")
#loc2521 = loc("807|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_7aten__sub")
#loc2522 = loc("805|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_39aten__add")
#loc2523 = loc("806|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_7aten__rsqrt")
#loc2524 = loc("808|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_20aten__mul")
#loc2525 = loc("809|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_21xla__cast")
#loc2526 = loc("809|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_21aten__mul")
#loc2527 = loc("810|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_40aten__add")
#loc2528 = loc("811|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_30xla__cast")
#loc2532 = loc("1765|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_146xla__cast")
#loc2533 = loc("1766|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_33aten__var_mean")
#loc2534 = loc("1769|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_33aten__sub")
#loc2535 = loc("1767|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_195aten__add")
#loc2536 = loc("1768|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_33aten__rsqrt")
#loc2537 = loc("1770|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_98aten__mul")
#loc2538 = loc("1771|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_99xla__cast")
#loc2539 = loc("1771|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_99aten__mul")
#loc2540 = loc("1772|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_196aten__add")
#loc2541 = loc("1773|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_147xla__cast")
#loc2545 = loc("1298|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_90xla__cast")
#loc2546 = loc("1299|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_20aten__var_mean")
#loc2547 = loc("1302|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_20aten__sub")
#loc2548 = loc("1300|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_118aten__add")
#loc2549 = loc("1301|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_20aten__rsqrt")
#loc2550 = loc("1303|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_60aten__mul")
#loc2551 = loc("1304|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_61xla__cast")
#loc2552 = loc("1304|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_61aten__mul")
#loc2553 = loc("1305|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_119aten__add")
#loc2554 = loc("1306|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_91xla__cast")
#loc2558 = loc("2630|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_252xla__cast")
#loc2559 = loc("2631|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_56aten__var_mean")
#loc2560 = loc("2634|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_56aten__sub")
#loc2561 = loc("2632|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_334aten__add")
#loc2562 = loc("2633|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_56aten__rsqrt")
#loc2563 = loc("2635|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_168aten__mul")
#loc2564 = loc("2636|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_169xla__cast")
#loc2565 = loc("2636|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_169aten__mul")
#loc2566 = loc("2637|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_335aten__add")
#loc2567 = loc("2638|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|LayerNorm[image_encoder.vision_model.encoder.layers[27].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_253xla__cast")
#loc2571 = loc("2112|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_189xla__cast")
#loc2572 = loc("2113|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_42aten__var_mean")
#loc2573 = loc("2116|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_42aten__sub")
#loc2574 = loc("2114|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_250aten__add")
#loc2575 = loc("2115|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_42aten__rsqrt")
#loc2576 = loc("2117|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_126aten__mul")
#loc2577 = loc("2118|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_127xla__cast")
#loc2578 = loc("2118|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_127aten__mul")
#loc2579 = loc("2119|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_251aten__add")
#loc2580 = loc("2120|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[20]]|LayerNorm[image_encoder.vision_model.encoder.layers[20].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_190xla__cast")
#loc2584 = loc("1742|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_144xla__cast")
#loc2585 = loc("1743|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_32aten__var_mean")
#loc2586 = loc("1746|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_32aten__sub")
#loc2587 = loc("1744|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_190aten__add")
#loc2588 = loc("1745|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_32aten__rsqrt")
#loc2589 = loc("1747|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_96aten__mul")
#loc2590 = loc("1748|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_97xla__cast")
#loc2591 = loc("1748|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_97aten__mul")
#loc2592 = loc("1749|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_191aten__add")
#loc2593 = loc("1750|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|LayerNorm[image_encoder.vision_model.encoder.layers[15].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_145xla__cast")
#loc2595 = loc("1312|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|CLIPMLP[image_encoder.vision_model.encoder.layers[9].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[9].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_9aten__gelu")
#loc2599 = loc("3062|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|_to_copy_316xla__cast")
#loc2600 = loc("3063|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|var_mean_73aten__var_mean")
#loc2601 = loc("3066|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|sub_73aten__sub")
#loc2602 = loc("3064|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|add_408aten__add")
#loc2603 = loc("3065|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|rsqrt_73aten__rsqrt")
#loc2604 = loc("3067|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_214aten__mul")
#loc2605 = loc("3068|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_215xla__cast")
#loc2606 = loc("3068|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_215aten__mul")
#loc2607 = loc("3069|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|add_409aten__add")
#loc2608 = loc("3070|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[2]]|LayerNorm[resampler.layers[2].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|_to_copy_317xla__cast")
#loc2612 = loc("1816|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_153xla__cast")
#loc2613 = loc("1817|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_34aten__var_mean")
#loc2614 = loc("1820|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_34aten__sub")
#loc2615 = loc("1818|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_202aten__add")
#loc2616 = loc("1819|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_34aten__rsqrt")
#loc2617 = loc("1821|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_102aten__mul")
#loc2618 = loc("1822|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_103xla__cast")
#loc2619 = loc("1822|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_103aten__mul")
#loc2620 = loc("1823|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_203aten__add")
#loc2621 = loc("1824|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|LayerNorm[image_encoder.vision_model.encoder.layers[16].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_154xla__cast")
#loc2625 = loc("2038|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_180xla__cast")
#loc2626 = loc("2039|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_40aten__var_mean")
#loc2627 = loc("2042|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_40aten__sub")
#loc2628 = loc("2040|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_238aten__add")
#loc2629 = loc("2041|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_40aten__rsqrt")
#loc2630 = loc("2043|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_120aten__mul")
#loc2631 = loc("2044|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_121xla__cast")
#loc2632 = loc("2044|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_121aten__mul")
#loc2633 = loc("2045|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_239aten__add")
#loc2634 = loc("2046|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[19]]|LayerNorm[image_encoder.vision_model.encoder.layers[19].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_181xla__cast")
#loc2638 = loc("854|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_36xla__cast")
#loc2639 = loc("855|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_8aten__var_mean")
#loc2640 = loc("858|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_8aten__sub")
#loc2641 = loc("856|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_46aten__add")
#loc2642 = loc("857|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_8aten__rsqrt")
#loc2643 = loc("859|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_24aten__mul")
#loc2644 = loc("860|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_25xla__cast")
#loc2645 = loc("860|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_25aten__mul")
#loc2646 = loc("861|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_47aten__add")
#loc2647 = loc("862|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[3]]|LayerNorm[image_encoder.vision_model.encoder.layers[3].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_37xla__cast")
#loc2649 = loc("1978|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|CLIPMLP[image_encoder.vision_model.encoder.layers[18].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[18].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_18aten__gelu")
#loc2651 = loc("3210|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|Sequential[resampler.layers[3].ff]|FeedForward[getattr(resampler.layers[3].ff, '1')]|GELU[getattr(resampler.layers[3].ff, '1').net[0]]|/usr/local/lib/python3.11/dist-packages/diffusers/models/activations.py:81|gelu|85|gelu_35aten__gelu")
#loc2655 = loc("1372|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_99xla__cast")
#loc2656 = loc("1373|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_22aten__var_mean")
#loc2657 = loc("1376|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_22aten__sub")
#loc2658 = loc("1374|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_130aten__add")
#loc2659 = loc("1375|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_22aten__rsqrt")
#loc2660 = loc("1377|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_66aten__mul")
#loc2661 = loc("1378|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_67xla__cast")
#loc2662 = loc("1378|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_67aten__mul")
#loc2663 = loc("1379|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_131aten__add")
#loc2664 = loc("1380|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|LayerNorm[image_encoder.vision_model.encoder.layers[10].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_100xla__cast")
#loc2668 = loc("2892|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|_to_copy_294xla__cast")
#loc2669 = loc("2893|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|var_mean_67aten__var_mean")
#loc2670 = loc("2896|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|sub_67aten__sub")
#loc2671 = loc("2894|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|add_392aten__add")
#loc2672 = loc("2895|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|rsqrt_67aten__rsqrt")
#loc2673 = loc("2897|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_198aten__mul")
#loc2674 = loc("2898|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_199xla__cast")
#loc2675 = loc("2898|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_199aten__mul")
#loc2676 = loc("2899|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|add_393aten__add")
#loc2677 = loc("2900|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|_to_copy_295xla__cast")
#loc2679 = loc("1386|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[10]]|CLIPMLP[image_encoder.vision_model.encoder.layers[10].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[10].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_10aten__gelu")
#loc2683 = loc("2135|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_191xla__cast")
#loc2684 = loc("2136|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_43aten__var_mean")
#loc2685 = loc("2139|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_43aten__sub")
#loc2686 = loc("2137|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_255aten__add")
#loc2687 = loc("2138|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_43aten__rsqrt")
#loc2688 = loc("2140|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_128aten__mul")
#loc2689 = loc("2141|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_129xla__cast")
#loc2690 = loc("2141|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_129aten__mul")
#loc2691 = loc("2142|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_256aten__add")
#loc2692 = loc("2143|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[21]]|LayerNorm[image_encoder.vision_model.encoder.layers[21].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_192xla__cast")
#loc2696 = loc("1395|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_101xla__cast")
#loc2697 = loc("1396|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_23aten__var_mean")
#loc2698 = loc("1399|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_23aten__sub")
#loc2699 = loc("1397|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_135aten__add")
#loc2700 = loc("1398|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_23aten__rsqrt")
#loc2701 = loc("1400|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_68aten__mul")
#loc2702 = loc("1401|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_69xla__cast")
#loc2703 = loc("1401|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_69aten__mul")
#loc2704 = loc("1402|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_136aten__add")
#loc2705 = loc("1403|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_102xla__cast")
#loc2709 = loc("1446|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_108xla__cast")
#loc2710 = loc("1447|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_24aten__var_mean")
#loc2711 = loc("1450|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_24aten__sub")
#loc2712 = loc("1448|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_142aten__add")
#loc2713 = loc("1449|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_24aten__rsqrt")
#loc2714 = loc("1451|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_72aten__mul")
#loc2715 = loc("1452|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_73xla__cast")
#loc2716 = loc("1452|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_73aten__mul")
#loc2717 = loc("1453|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_143aten__add")
#loc2718 = loc("1454|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|LayerNorm[image_encoder.vision_model.encoder.layers[11].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_109xla__cast")
#loc2722 = loc("2283|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_209xla__cast")
#loc2723 = loc("2284|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_47aten__var_mean")
#loc2724 = loc("2287|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_47aten__sub")
#loc2725 = loc("2285|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_279aten__add")
#loc2726 = loc("2286|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_47aten__rsqrt")
#loc2727 = loc("2288|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_140aten__mul")
#loc2728 = loc("2289|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_141xla__cast")
#loc2729 = loc("2289|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_141aten__mul")
#loc2730 = loc("2290|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_280aten__add")
#loc2731 = loc("2291|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[23]]|LayerNorm[image_encoder.vision_model.encoder.layers[23].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_210xla__cast")
#loc2735 = loc("1247|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_83xla__cast")
#loc2736 = loc("1248|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_19aten__var_mean")
#loc2737 = loc("1251|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_19aten__sub")
#loc2738 = loc("1249|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_111aten__add")
#loc2739 = loc("1250|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_19aten__rsqrt")
#loc2740 = loc("1252|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_56aten__mul")
#loc2741 = loc("1253|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_57xla__cast")
#loc2742 = loc("1253|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_57aten__mul")
#loc2743 = loc("1254|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_112aten__add")
#loc2744 = loc("1255|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[9]]|LayerNorm[image_encoder.vision_model.encoder.layers[9].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_84xla__cast")
#loc2748 = loc("1520|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_117xla__cast")
#loc2749 = loc("1521|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_26aten__var_mean")
#loc2750 = loc("1524|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_26aten__sub")
#loc2751 = loc("1522|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_154aten__add")
#loc2752 = loc("1523|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_26aten__rsqrt")
#loc2753 = loc("1525|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_78aten__mul")
#loc2754 = loc("1526|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_79xla__cast")
#loc2755 = loc("1526|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_79aten__mul")
#loc2756 = loc("1527|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_155aten__add")
#loc2757 = loc("1528|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_118xla__cast")
#loc2759 = loc("1016|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[5]]|CLIPMLP[image_encoder.vision_model.encoder.layers[5].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[5].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_5aten__gelu")
#loc2763 = loc("1964|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_171xla__cast")
#loc2764 = loc("1965|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_38aten__var_mean")
#loc2765 = loc("1968|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_38aten__sub")
#loc2766 = loc("1966|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_226aten__add")
#loc2767 = loc("1967|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_38aten__rsqrt")
#loc2768 = loc("1969|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_114aten__mul")
#loc2769 = loc("1970|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_115xla__cast")
#loc2770 = loc("1970|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_115aten__mul")
#loc2771 = loc("1971|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_227aten__add")
#loc2772 = loc("1972|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_172xla__cast")
#loc2776 = loc("1594|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_126xla__cast")
#loc2777 = loc("1595|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_28aten__var_mean")
#loc2778 = loc("1598|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_28aten__sub")
#loc2779 = loc("1596|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_166aten__add")
#loc2780 = loc("1597|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_28aten__rsqrt")
#loc2781 = loc("1599|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_84aten__mul")
#loc2782 = loc("1600|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_85xla__cast")
#loc2783 = loc("1600|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_85aten__mul")
#loc2784 = loc("1601|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_167aten__add")
#loc2785 = loc("1602|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_127xla__cast")
#loc2789 = loc("2879|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|_to_copy_292xla__cast")
#loc2790 = loc("2880|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|var_mean_66aten__var_mean")
#loc2791 = loc("2883|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|sub_66aten__sub")
#loc2792 = loc("2881|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|add_390aten__add")
#loc2793 = loc("2882|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|rsqrt_66aten__rsqrt")
#loc2794 = loc("2884|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_196aten__mul")
#loc2795 = loc("2885|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_197xla__cast")
#loc2796 = loc("2885|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_197aten__mul")
#loc2797 = loc("2886|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|add_391aten__add")
#loc2798 = loc("2887|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[0]]|LayerNorm[resampler.layers[0].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|_to_copy_293xla__cast")
#loc2802 = loc("1025|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_56xla__cast")
#loc2803 = loc("1026|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_13aten__var_mean")
#loc2804 = loc("1029|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_13aten__sub")
#loc2805 = loc("1027|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_75aten__add")
#loc2806 = loc("1028|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_13aten__rsqrt")
#loc2807 = loc("1030|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_38aten__mul")
#loc2808 = loc("1031|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_39xla__cast")
#loc2809 = loc("1031|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_39aten__mul")
#loc2810 = loc("1032|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_76aten__add")
#loc2811 = loc("1033|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[6]]|LayerNorm[image_encoder.vision_model.encoder.layers[6].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_57xla__cast")
#loc2813 = loc("1756|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[15]]|CLIPMLP[image_encoder.vision_model.encoder.layers[15].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[15].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_15aten__gelu")
#loc2815 = loc("1608|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|CLIPMLP[image_encoder.vision_model.encoder.layers[13].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[13].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_13aten__gelu")
#loc2817 = loc("2644|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[27]]|CLIPMLP[image_encoder.vision_model.encoder.layers[27].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[27].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_27aten__gelu")
#loc2821 = loc("1099|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_65xla__cast")
#loc2822 = loc("1100|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_15aten__var_mean")
#loc2823 = loc("1103|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_15aten__sub")
#loc2824 = loc("1101|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_87aten__add")
#loc2825 = loc("1102|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_15aten__rsqrt")
#loc2826 = loc("1104|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_44aten__mul")
#loc2827 = loc("1105|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_45xla__cast")
#loc2828 = loc("1105|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_45aten__mul")
#loc2829 = loc("1106|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_88aten__add")
#loc2830 = loc("1107|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[7]]|LayerNorm[image_encoder.vision_model.encoder.layers[7].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_66xla__cast")
#loc2834 = loc("1617|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_128xla__cast")
#loc2835 = loc("1618|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_29aten__var_mean")
#loc2836 = loc("1621|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_29aten__sub")
#loc2837 = loc("1619|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_171aten__add")
#loc2838 = loc("1620|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_29aten__rsqrt")
#loc2839 = loc("1622|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_86aten__mul")
#loc2840 = loc("1623|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_87xla__cast")
#loc2841 = loc("1623|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_87aten__mul")
#loc2842 = loc("1624|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_172aten__add")
#loc2843 = loc("1625|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|LayerNorm[image_encoder.vision_model.encoder.layers[14].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_129xla__cast")
#loc2847 = loc("2964|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|_to_copy_303xla__cast")
#loc2848 = loc("2965|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|var_mean_69aten__var_mean")
#loc2849 = loc("2968|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|sub_69aten__sub")
#loc2850 = loc("2966|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|add_398aten__add")
#loc2851 = loc("2967|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|rsqrt_69aten__rsqrt")
#loc2852 = loc("2969|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_204aten__mul")
#loc2853 = loc("2970|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_205xla__cast")
#loc2854 = loc("2970|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|mul_205aten__mul")
#loc2855 = loc("2971|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|add_399aten__add")
#loc2856 = loc("2972|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[1]]|LayerNorm[resampler.layers[1].ln0]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2246|_to_copy_304xla__cast")
#loc2858 = loc("1460|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[11]]|CLIPMLP[image_encoder.vision_model.encoder.layers[11].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[11].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_11aten__gelu")
#loc2860 = loc("1682|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[14]]|CLIPMLP[image_encoder.vision_model.encoder.layers[14].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[14].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_14aten__gelu")
#loc2864 = loc("877|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_38xla__cast")
#loc2865 = loc("878|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_9aten__var_mean")
#loc2866 = loc("881|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_9aten__sub")
#loc2867 = loc("879|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_51aten__add")
#loc2868 = loc("880|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_9aten__rsqrt")
#loc2869 = loc("882|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_26aten__mul")
#loc2870 = loc("883|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_27xla__cast")
#loc2871 = loc("883|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_27aten__mul")
#loc2872 = loc("884|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_52aten__add")
#loc2873 = loc("885|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[4]]|LayerNorm[image_encoder.vision_model.encoder.layers[4].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_39xla__cast")
#loc2877 = loc("1469|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_110xla__cast")
#loc2878 = loc("1470|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_25aten__var_mean")
#loc2879 = loc("1473|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_25aten__sub")
#loc2880 = loc("1471|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_147aten__add")
#loc2881 = loc("1472|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_25aten__rsqrt")
#loc2882 = loc("1474|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_74aten__mul")
#loc2883 = loc("1475|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_75xla__cast")
#loc2884 = loc("1475|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_75aten__mul")
#loc2885 = loc("1476|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_148aten__add")
#loc2886 = loc("1477|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[12]]|LayerNorm[image_encoder.vision_model.encoder.layers[12].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_111xla__cast")
#loc2888 = loc("1830|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[16]]|CLIPMLP[image_encoder.vision_model.encoder.layers[16].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[16].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_16aten__gelu")
#loc2892 = loc("1890|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_162xla__cast")
#loc2893 = loc("1891|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_36aten__var_mean")
#loc2894 = loc("1894|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_36aten__sub")
#loc2895 = loc("1892|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_214aten__add")
#loc2896 = loc("1893|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_36aten__rsqrt")
#loc2897 = loc("1895|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_108aten__mul")
#loc2898 = loc("1896|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_109xla__cast")
#loc2899 = loc("1896|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_109aten__mul")
#loc2900 = loc("1897|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_215aten__add")
#loc2901 = loc("1898|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|LayerNorm[image_encoder.vision_model.encoder.layers[17].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_163xla__cast")
#loc2905 = loc("3147|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|_to_copy_327xla__cast")
#loc2906 = loc("3148|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|var_mean_76aten__var_mean")
#loc2907 = loc("3151|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|sub_76aten__sub")
#loc2908 = loc("3149|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|add_416aten__add")
#loc2909 = loc("3150|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|rsqrt_76aten__rsqrt")
#loc2910 = loc("3152|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_222aten__mul")
#loc2911 = loc("3153|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_223xla__cast")
#loc2912 = loc("3153|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|mul_223aten__mul")
#loc2913 = loc("3154|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|add_417aten__add")
#loc2914 = loc("3155|IPAdapterPlusImageProjection[resampler]|IPAdapterPlusImageProjectionBlock[resampler.layers[3]]|LayerNorm[resampler.layers[3].ln1]|/usr/local/lib/python3.11/dist-packages/diffusers/models/embeddings.py:2245|forward|2247|_to_copy_328xla__cast")
#loc2918 = loc("2778|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_270xla__cast")
#loc2919 = loc("2779|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|var_mean_60aten__var_mean")
#loc2920 = loc("2782|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|sub_60aten__sub")
#loc2921 = loc("2780|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_358aten__add")
#loc2922 = loc("2781|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|rsqrt_60aten__rsqrt")
#loc2923 = loc("2783|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_180aten__mul")
#loc2924 = loc("2784|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_181xla__cast")
#loc2925 = loc("2784|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|mul_181aten__mul")
#loc2926 = loc("2785|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|add_359aten__add")
#loc2927 = loc("2786|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[29]]|LayerNorm[image_encoder.vision_model.encoder.layers[29].layer_norm2]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|406|_to_copy_271xla__cast")
#loc2931 = loc("1543|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_119xla__cast")
#loc2932 = loc("1544|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_27aten__var_mean")
#loc2933 = loc("1547|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_27aten__sub")
#loc2934 = loc("1545|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_159aten__add")
#loc2935 = loc("1546|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_27aten__rsqrt")
#loc2936 = loc("1548|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_80aten__mul")
#loc2937 = loc("1549|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_81xla__cast")
#loc2938 = loc("1549|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_81aten__mul")
#loc2939 = loc("1550|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_160aten__add")
#loc2940 = loc("1551|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[13]]|LayerNorm[image_encoder.vision_model.encoder.layers[13].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_120xla__cast")
#loc2942 = loc("1904|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[17]]|CLIPMLP[image_encoder.vision_model.encoder.layers[17].mlp]|GELUActivation[image_encoder.vision_model.encoder.layers[17].mlp.activation_fn]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:361|forward|363|gelu_17aten__gelu")
#loc2946 = loc("1913|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_164xla__cast")
#loc2947 = loc("1914|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|var_mean_37aten__var_mean")
#loc2948 = loc("1917|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|sub_37aten__sub")
#loc2949 = loc("1915|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_219aten__add")
#loc2950 = loc("1916|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|rsqrt_37aten__rsqrt")
#loc2951 = loc("1918|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_110aten__mul")
#loc2952 = loc("1919|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_111xla__cast")
#loc2953 = loc("1919|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|mul_111aten__mul")
#loc2954 = loc("1920|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|add_220aten__add")
#loc2955 = loc("1921|CLIPVisionModelWithProjection[image_encoder]|CLIPVisionTransformer[image_encoder.vision_model]|CLIPEncoder[image_encoder.vision_model.encoder]|CLIPEncoderLayer[image_encoder.vision_model.encoder.layers[18]]|LayerNorm[image_encoder.vision_model.encoder.layers[18].layer_norm1]|/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py:377|forward|396|_to_copy_165xla__cast")
