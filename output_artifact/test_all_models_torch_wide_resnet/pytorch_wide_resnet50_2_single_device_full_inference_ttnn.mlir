#dram = #ttnn.buffer_type<dram>
#loc = loc(unknown)
#loc55 = loc("p0.3")
#loc56 = loc("p1.10")
#loc57 = loc("p2.18")
#loc58 = loc("p3.22")
#loc59 = loc("p4.26")
#loc60 = loc("p5.30")
#loc61 = loc("p6.34")
#loc62 = loc("p7.42")
#loc63 = loc("p8.46")
#loc64 = loc("p9.50")
#loc65 = loc("p10.54")
#loc66 = loc("p11.58")
#loc67 = loc("p12.64")
#loc68 = loc("p13.68")
#loc69 = loc("p14.72")
#loc70 = loc("p15.76")
#loc71 = loc("p16.80")
#loc72 = loc("p17.85")
#loc73 = loc("p18.89")
#loc74 = loc("p19.93")
#loc75 = loc("p20.97")
#loc76 = loc("p21.101")
#loc77 = loc("p22.103")
#loc78 = loc("p23.107")
#loc79 = loc("p24.111")
#loc80 = loc("p25.115")
#loc81 = loc("p26.119")
#loc82 = loc("p27.121")
#loc83 = loc("p28.310")
#loc84 = loc("p29.314")
#loc85 = loc("p30.318")
#loc86 = loc("p31.322")
#loc87 = loc("p32.326")
#loc88 = loc("p33.328")
#loc89 = loc("p34.332")
#loc90 = loc("p35.336")
#loc91 = loc("p36.340")
#loc92 = loc("p37.344")
#loc93 = loc("p38.346")
#loc94 = loc("p39.350")
#loc95 = loc("p40.354")
#loc96 = loc("p41.358")
#loc97 = loc("p42.362")
#loc98 = loc("p43.394")
#loc99 = loc("p44.398")
#loc100 = loc("p45.402")
#loc101 = loc("p46.406")
#loc102 = loc("p47.410")
#loc103 = loc("p48.412")
#loc104 = loc("p49.416")
#loc105 = loc("p50.420")
#loc106 = loc("p51.424")
#loc107 = loc("p52.428")
#loc108 = loc("p53.430")
#loc109 = loc("p54.434")
#loc110 = loc("p55.438")
#loc111 = loc("p56.442")
#loc112 = loc("p57.446")
#loc113 = loc("p58.478")
#loc114 = loc("p59.482")
#loc115 = loc("p60.486")
#loc116 = loc("p61.490")
#loc117 = loc("p62.494")
#loc118 = loc("p63.496")
#loc119 = loc("p64.500")
#loc120 = loc("p65.504")
#loc121 = loc("p66.508")
#loc122 = loc("p67.512")
#loc123 = loc("p68.514")
#loc124 = loc("p69.518")
#loc125 = loc("p70.522")
#loc126 = loc("p71.526")
#loc127 = loc("p72.530")
#loc128 = loc("p73.568")
#loc129 = loc("p74.572")
#loc130 = loc("p75.576")
#loc131 = loc("p76.580")
#loc132 = loc("p77.584")
#loc133 = loc("p78.586")
#loc134 = loc("p79.590")
#loc135 = loc("p80.594")
#loc136 = loc("p81.598")
#loc137 = loc("p82.602")
#loc138 = loc("p83.604")
#loc139 = loc("p84.608")
#loc140 = loc("p85.612")
#loc141 = loc("p86.616")
#loc142 = loc("p87.620")
#loc143 = loc("p88.652")
#loc144 = loc("p89.656")
#loc145 = loc("p90.660")
#loc146 = loc("p91.664")
#loc147 = loc("p92.668")
#loc148 = loc("p93.670")
#loc149 = loc("p94.674")
#loc150 = loc("p95.678")
#loc151 = loc("p96.682")
#loc152 = loc("p97.686")
#loc153 = loc("p98.688")
#loc154 = loc("p99.692")
#loc155 = loc("p100.696")
#loc156 = loc("p101.700")
#loc157 = loc("p102.704")
#loc158 = loc("p103.736")
#loc159 = loc("p104.740")
#loc160 = loc("p105.744")
#loc161 = loc("p106.748")
#loc162 = loc("p107.752")
#loc163 = loc("p108.754")
#loc164 = loc("p109.758")
#loc165 = loc("p110.762")
#loc166 = loc("p111.766")
#loc167 = loc("p112.770")
#loc168 = loc("p113.772")
#loc169 = loc("p114.776")
#loc170 = loc("p115.780")
#loc171 = loc("p116.784")
#loc172 = loc("p117.788")
#loc173 = loc("p118.820")
#loc174 = loc("p119.824")
#loc175 = loc("p120.828")
#loc176 = loc("p121.832")
#loc177 = loc("p122.836")
#loc178 = loc("p123.838")
#loc179 = loc("p124.842")
#loc180 = loc("p125.846")
#loc181 = loc("p126.850")
#loc182 = loc("p127.854")
#loc183 = loc("p128.856")
#loc184 = loc("p129.860")
#loc185 = loc("p130.864")
#loc186 = loc("p131.868")
#loc187 = loc("p132.872")
#loc188 = loc("p133.910")
#loc189 = loc("p134.914")
#loc190 = loc("p135.918")
#loc191 = loc("p136.922")
#loc192 = loc("p137.926")
#loc193 = loc("p138.928")
#loc194 = loc("p139.932")
#loc195 = loc("p140.936")
#loc196 = loc("p141.940")
#loc197 = loc("p142.944")
#loc198 = loc("p143.946")
#loc199 = loc("p144.950")
#loc200 = loc("p145.954")
#loc201 = loc("p146.958")
#loc202 = loc("p147.962")
#loc203 = loc("p148.994")
#loc204 = loc("p149.998")
#loc205 = loc("p150.1002")
#loc206 = loc("p151.1006")
#loc207 = loc("p152.1010")
#loc208 = loc("p153.1012")
#loc209 = loc("p154.1016")
#loc210 = loc("p155.1020")
#loc211 = loc("p156.1024")
#loc212 = loc("p157.1028")
#loc213 = loc("p158.1030")
#loc214 = loc("p159.1034")
#loc215 = loc("p160.1038")
#loc216 = loc("p161.1042")
#loc217 = loc("p162.1046")
#loc218 = loc("p163.1078")
#loc219 = loc("p164.1082")
#loc220 = loc("p165.1086")
#loc221 = loc("p166.1090")
#loc222 = loc("p167.1094")
#loc223 = loc("p168.1096")
#loc224 = loc("p169.1100")
#loc225 = loc("p170.1104")
#loc226 = loc("p171.1108")
#loc227 = loc("p172.1112")
#loc228 = loc("p173.1114")
#loc229 = loc("p174.1118")
#loc230 = loc("p175.1122")
#loc231 = loc("p176.1126")
#loc232 = loc("p177.1130")
#loc233 = loc("p178.1162")
#loc234 = loc("p179.1166")
#loc235 = loc("p180.1170")
#loc236 = loc("p181.1174")
#loc237 = loc("p182.1178")
#loc238 = loc("p183.1180")
#loc239 = loc("p184.1184")
#loc240 = loc("p185.1188")
#loc241 = loc("p186.1192")
#loc242 = loc("p187.1196")
#loc243 = loc("p188.1198")
#loc244 = loc("p189.1202")
#loc245 = loc("p190.1206")
#loc246 = loc("p191.1210")
#loc247 = loc("p192.1214")
#loc248 = loc("p193.1246")
#loc249 = loc("p194.1250")
#loc250 = loc("p195.1254")
#loc251 = loc("p196.1258")
#loc252 = loc("p197.1262")
#loc253 = loc("p198.1264")
#loc254 = loc("p199.1268")
#loc255 = loc("p200.1272")
#loc256 = loc("p201.1276")
#loc257 = loc("p202.1280")
#loc258 = loc("p203.1282")
#loc259 = loc("p204.1286")
#loc260 = loc("p205.1290")
#loc261 = loc("p206.1294")
#loc262 = loc("p207.1298")
#loc263 = loc("p208.1330")
#loc264 = loc("p209.1334")
#loc265 = loc("p210.1338")
#loc266 = loc("p211.1342")
#loc267 = loc("p212.1346")
#loc268 = loc("p213.1348")
#loc269 = loc("p214.1352")
#loc270 = loc("p215.1356")
#loc271 = loc("p216.1360")
#loc272 = loc("p217.1364")
#loc273 = loc("p218.1366")
#loc274 = loc("p219.1370")
#loc275 = loc("p220.1374")
#loc276 = loc("p221.1378")
#loc277 = loc("p222.1382")
#loc278 = loc("p223.1420")
#loc279 = loc("p224.1424")
#loc280 = loc("p225.1428")
#loc281 = loc("p226.1432")
#loc282 = loc("p227.1436")
#loc283 = loc("p228.1438")
#loc284 = loc("p229.1442")
#loc285 = loc("p230.1446")
#loc286 = loc("p231.1450")
#loc287 = loc("p232.1454")
#loc288 = loc("p233.1456")
#loc289 = loc("p234.1460")
#loc290 = loc("p235.1464")
#loc291 = loc("p236.1468")
#loc292 = loc("p237.1472")
#loc293 = loc("p238.1504")
#loc294 = loc("p239.1508")
#loc295 = loc("p240.1512")
#loc296 = loc("p241.1516")
#loc297 = loc("p242.1520")
#loc298 = loc("p243.1522")
#loc299 = loc("p244.1526")
#loc300 = loc("p245.1530")
#loc301 = loc("p246.1534")
#loc302 = loc("p247.1538")
#loc303 = loc("p248.1540")
#loc304 = loc("p249.1544")
#loc305 = loc("p250.1548")
#loc306 = loc("p251.1552")
#loc307 = loc("p252.1556")
#loc308 = loc("p253.1588")
#loc309 = loc("p254.1592")
#loc310 = loc("p255.1596")
#loc311 = loc("p256.1600")
#loc312 = loc("p257.1604")
#loc313 = loc("p258.1606")
#loc314 = loc("p259.1610")
#loc315 = loc("p260.1614")
#loc316 = loc("p261.1618")
#loc317 = loc("p262.1622")
#loc318 = loc("p263.1624")
#loc319 = loc("p264.1628")
#loc320 = loc("p265.1632")
#loc321 = loc("p266.1636")
#loc322 = loc("p267.1640")
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101664, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073106400, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101664, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073114880, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 32 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 32 + d2, d3), <1x1>, memref<256x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 65536 + d1 * 32 + d2, d3), <1x1>, memref<2048x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32768 + d1 * 32 + d2, d3), <1x1>, memref<1024x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 + d2, d3), <1x1>, memref<2097152x1xbf16, #system_memory>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 + d2, d3), <1x1>, memref<524288x1xbf16, #system_memory>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 + d2, d3), <1x1>, memref<131072x1xbf16, #system_memory>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 64 + d1 + d2, d3), <1x1>, memref<16384x1xbf16, #system_memory>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 21 + d1 * 7 + d2, d3), <1x1>, memref<1344x7xbf16, #system_memory>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 672 + d1 * 224 + d2, d3), <1x1>, memref<21x7x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 + d2, d3), <1x1>, memref<32768x1xbf16, #system_memory>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 3 + d2, d3), <1x1>, memref<49152x3xbf16, #system_memory>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 64 + d1 + d2, d3), <1x1>, memref<8192x1xbf16, #system_memory>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 + d2, d3), <1x1>, memref<32768x1xbf16, #system_memory>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 3 + d2, d3), <1x1>, memref<196608x3xbf16, #system_memory>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 + d2, d3), <1x1>, memref<65536x1xbf16, #system_memory>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 + d2, d3), <1x1>, memref<131072x1xbf16, #system_memory>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 3 + d2, d3), <1x1>, memref<786432x3xbf16, #system_memory>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 + d2, d3), <1x1>, memref<262144x1xbf16, #system_memory>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 + d2, d3), <1x1>, memref<524288x1xbf16, #system_memory>>
#ttnn_layout30 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 3 + d2, d3), <1x1>, memref<3145728x3xbf16, #system_memory>>
#ttnn_layout31 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 + d2, d3), <1x1>, memref<1048576x1xbf16, #system_memory>>
#ttnn_layout32 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 + d2, d3), <1x1>, memref<2097152x1xbf16, #system_memory>>
#ttnn_layout33 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 50176 + d1 * 224 + d2, d3), <1x1>, memref<1568x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout34 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 50176 + d1 * 50176 + d2, d3), <1x1>, memref<1568x1x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout35 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 50176 + d1 * 50176 + d2, d3), <1x1>, memref<50176x3xbf16, #dram>, <interleaved>>
#ttnn_layout36 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12544 + d1 * 12544 + d2, d3), <1x1>, memref<392x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout37 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 14336 + d1 * 128 + d2, d3), <1x1>, memref<448x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout38 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 128 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout39 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12544 + d1 * 12544 + d2, d3), <1x1>, memref<12544x64xbf16, #dram>, <interleaved>>
#ttnn_layout40 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3136 + d1 * 3136 + d2, d3), <1x1>, memref<3136x64xbf16, #dram>, <interleaved>>
#ttnn_layout41 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3136 + d1 * 3136 + d2, d3), <1x1>, memref<98x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout42 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3584 + d1 * 64 + d2, d3), <1x1>, memref<112x4x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout43 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 64 + d2, d3), <1x1>, memref<256x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout44 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3136 + d1 * 3136 + d2, d3), <1x1>, memref<3136x128xbf16, #dram>, <interleaved>>
#ttnn_layout45 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3136 + d1 * 3136 + d2, d3), <1x1>, memref<98x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout46 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3584 + d1 * 64 + d2, d3), <1x1>, memref<112x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout47 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 64 + d2, d3), <1x1>, memref<512x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout48 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3136 + d1 * 3136 + d2, d3), <1x1>, memref<3136x256xbf16, #dram>, <interleaved>>
#ttnn_layout49 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout50 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 896 + d1 * 32 + d2, d3), <1x1>, memref<28x8x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout51 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 784 + d1 * 784 + d2, d3), <1x1>, memref<784x256xbf16, #dram>, <interleaved>>
#ttnn_layout52 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout53 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 896 + d1 * 32 + d2, d3), <1x1>, memref<28x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout54 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 784 + d1 * 784 + d2, d3), <1x1>, memref<784x512xbf16, #dram>, <interleaved>>
#ttnn_layout55 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 224 + d2, d3), <1x1>, memref<7x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout56 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x16x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout57 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 196 + d1 * 196 + d2, d3), <1x1>, memref<196x512xbf16, #dram>, <interleaved>>
#ttnn_layout58 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 224 + d2, d3), <1x1>, memref<7x32x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout59 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x32x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout60 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 196 + d1 * 196 + d2, d3), <1x1>, memref<196x1024xbf16, #dram>, <interleaved>>
#ttnn_layout61 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 64 + d1 * 64 + d2, d3), <1x1>, memref<2x32x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout62 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x32x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout63 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 49 + d1 * 49 + d2, d3), <1x1>, memref<49x1024xbf16, #dram>, <interleaved>>
#ttnn_layout64 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 64 + d1 * 64 + d2, d3), <1x1>, memref<2x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout65 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout66 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 49 + d1 * 49 + d2, d3), <1x1>, memref<49x2048xbf16, #dram>, <interleaved>>
#ttnn_layout67 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x64x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
module @SyncTensorsGraph.1698 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.1698 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x1, chipIds = [0]> loc(#loc)
      func.func @main_const_eval_0(%arg0: tensor<64xbf16, #ttnn_layout> loc(unknown)) -> tensor<1x64x1x1xbf16, #ttnn_layout1> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout>) -> tensor<1x64x1x1xbf16, #ttnn_layout1> loc(#loc1)
        return %0 : tensor<1x64x1x1xbf16, #ttnn_layout1> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_1(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc2)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_2(%arg0: tensor<128xbf16, #ttnn_layout4> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout5> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc3)
        return %0 : tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_3(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc4)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_4(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc5)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_5(%arg0: tensor<2048xbf16, #ttnn_layout8> loc(unknown)) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32, 1 : i32, 1 : i32]}> : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc6)
        return %0 : tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_6(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc7)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_7(%arg0: tensor<128xbf16, #ttnn_layout4> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout5> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc8)
        return %0 : tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_8(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc9)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_9(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc10)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_10(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc11)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_11(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc12)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_12(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc13)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_13(%arg0: tensor<128xbf16, #ttnn_layout4> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout5> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc14)
        return %0 : tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_14(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc15)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_15(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc16)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_16(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc5)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_17(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc17)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_18(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc18)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_19(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc19)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_20(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc20)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_21(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc11)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_22(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc13)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_23(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc21)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_24(%arg0: tensor<128xbf16, #ttnn_layout4> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout5> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc8)
        return %0 : tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_25(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc2)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_26(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc22)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_27(%arg0: tensor<128xbf16, #ttnn_layout4> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout5> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc23)
        return %0 : tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_28(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc24)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_29(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc25)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_30(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc26)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_31(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc27)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_32(%arg0: tensor<128xbf16, #ttnn_layout4> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout5> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc14)
        return %0 : tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_33(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc18)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_34(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc28)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_35(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc29)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_36(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc9)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_37(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc22)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_38(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc30)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_39(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc18)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_40(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc31)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_41(%arg0: tensor<2048xbf16, #ttnn_layout8> loc(unknown)) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32, 1 : i32, 1 : i32]}> : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc32)
        return %0 : tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_42(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc33)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_43(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc34)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_44(%arg0: tensor<1000xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1000xbf16, #ttnn_layout12> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1000 : i32]}> : (tensor<1000xbf16, #ttnn_layout10>) -> tensor<1x1000xbf16, #ttnn_layout12> loc(#loc35)
        return %0 : tensor<1x1000xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_45(%arg0: tensor<2048xbf16, #ttnn_layout8> loc(unknown)) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32, 1 : i32, 1 : i32]}> : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc32)
        return %0 : tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_46(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc16)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_47(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc36)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_48(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc37)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_49(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc10)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_50(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc38)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_51(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc39)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_52(%arg0: tensor<2048xbf16, #ttnn_layout8> loc(unknown)) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32, 1 : i32, 1 : i32]}> : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc6)
        return %0 : tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_53(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc31)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_54(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc9)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_55(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc20)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_56(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc39)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_57(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc2)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_58(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc29)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_59(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc19)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_60(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc40)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_61(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc4)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_62(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc39)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_63(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc25)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_64(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc17)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_65(%arg0: tensor<64xbf16, #ttnn_layout> loc(unknown)) -> tensor<1x64x1x1xbf16, #ttnn_layout1> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout>) -> tensor<1x64x1x1xbf16, #ttnn_layout1> loc(#loc1)
        return %0 : tensor<1x64x1x1xbf16, #ttnn_layout1> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_66(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc41)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_67(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc10)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_68(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc24)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_69(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc42)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_70(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc9)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_71(%arg0: tensor<2048xbf16, #ttnn_layout8> loc(unknown)) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32, 1 : i32, 1 : i32]}> : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc43)
        return %0 : tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_72(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc33)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_73(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc42)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_74(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc44)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_75(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc45)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_76(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc16)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_77(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc12)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_78(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc46)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_79(%arg0: tensor<128xbf16, #ttnn_layout4> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout5> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc14)
        return %0 : tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_80(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc24)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_81(%arg0: tensor<128xbf16, #ttnn_layout4> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout5> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc8)
        return %0 : tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_82(%arg0: tensor<128xbf16, #ttnn_layout4> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout5> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc23)
        return %0 : tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_83(%arg0: tensor<2048xbf16, #ttnn_layout8> loc(unknown)) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32, 1 : i32, 1 : i32]}> : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc43)
        return %0 : tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_84(%arg0: tensor<2048xbf16, #ttnn_layout8> loc(unknown)) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32, 1 : i32, 1 : i32]}> : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc47)
        return %0 : tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_85(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc36)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_86(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc48)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_87(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc45)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_88(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc28)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_89(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc41)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_90(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc49)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_91(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc28)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_92(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc2)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_93(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc27)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_94(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc12)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_95(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc37)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_96(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc37)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_97(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc33)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_98(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc16)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_99(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc48)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_100(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc46)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_101(%arg0: tensor<128xbf16, #ttnn_layout4> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout5> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc50)
        return %0 : tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_102(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc51)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_103(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc29)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_104(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc5)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_105(%arg0: tensor<128xbf16, #ttnn_layout4> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout5> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc3)
        return %0 : tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_106(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc49)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_107(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc26)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_108(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc19)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_109(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc52)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_110(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc15)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_111(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc31)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_112(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc52)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_113(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc39)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_114(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc48)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_115(%arg0: tensor<128xbf16, #ttnn_layout4> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout5> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc3)
        return %0 : tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_116(%arg0: tensor<2048xbf16, #ttnn_layout8> loc(unknown)) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32, 1 : i32, 1 : i32]}> : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc47)
        return %0 : tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_117(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc41)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_118(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc53)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_119(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc49)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_120(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc45)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_121(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc31)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_122(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc26)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_123(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc30)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_124(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc26)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_125(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc46)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_126(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc5)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_127(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc13)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_128(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc7)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_129(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc19)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_130(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc53)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_131(%arg0: tensor<2048xbf16, #ttnn_layout8> loc(unknown)) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32, 1 : i32, 1 : i32]}> : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc43)
        return %0 : tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_132(%arg0: tensor<2048xbf16, #ttnn_layout8> loc(unknown)) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32, 1 : i32, 1 : i32]}> : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc32)
        return %0 : tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_133(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc7)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_134(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc49)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_135(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc27)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_136(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc51)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_137(%arg0: tensor<128xbf16, #ttnn_layout4> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout5> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc23)
        return %0 : tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_138(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc44)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_139(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc17)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_140(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc42)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_141(%arg0: tensor<128xbf16, #ttnn_layout4> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout5> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc23)
        return %0 : tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_142(%arg0: tensor<128xbf16, #ttnn_layout4> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout5> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc50)
        return %0 : tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_143(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc18)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_144(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc38)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_145(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc52)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_146(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc20)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_147(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc10)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_148(%arg0: tensor<2048xbf16, #ttnn_layout8> loc(unknown)) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32, 1 : i32, 1 : i32]}> : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc47)
        return %0 : tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_149(%arg0: tensor<128xbf16, #ttnn_layout4> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout5> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc3)
        return %0 : tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_150(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc4)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_151(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc24)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_152(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc28)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_153(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc12)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_154(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc52)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_155(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc41)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_156(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc21)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_157(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc34)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_158(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc25)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_159(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc51)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_160(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc25)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_161(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc34)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_162(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc30)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_163(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc11)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_164(%arg0: tensor<128xbf16, #ttnn_layout4> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout5> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc50)
        return %0 : tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_165(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc20)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_166(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc21)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_167(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc30)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_168(%arg0: tensor<128xbf16, #ttnn_layout4> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout5> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc50)
        return %0 : tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_169(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc37)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_170(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc40)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_171(%arg0: tensor<128xbf16, #ttnn_layout4> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout5> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc54)
        return %0 : tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_172(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc17)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_173(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc48)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_174(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc44)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_175(%arg0: tensor<128xbf16, #ttnn_layout4> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout5> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc8)
        return %0 : tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_176(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc33)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_177(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc36)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_178(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc42)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_179(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc34)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_180(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc36)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_181(%arg0: tensor<2048xbf16, #ttnn_layout8> loc(unknown)) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32, 1 : i32, 1 : i32]}> : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc47)
        return %0 : tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_182(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc53)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_183(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc53)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_184(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc38)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_185(%arg0: tensor<128xbf16, #ttnn_layout4> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout5> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc54)
        return %0 : tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_186(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc29)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_187(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc40)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_188(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc38)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_189(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc11)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_190(%arg0: tensor<128xbf16, #ttnn_layout4> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout5> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc54)
        return %0 : tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_191(%arg0: tensor<2048xbf16, #ttnn_layout8> loc(unknown)) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32, 1 : i32, 1 : i32]}> : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc6)
        return %0 : tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_192(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc44)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_193(%arg0: tensor<128xbf16, #ttnn_layout4> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout5> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc54)
        return %0 : tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_194(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc40)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_195(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc15)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_196(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc21)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_197(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc46)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_198(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc13)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_199(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc51)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_200(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc27)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_201(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc22)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_202(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc4)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_203(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc7)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_204(%arg0: tensor<64xbf16, #ttnn_layout> loc(unknown)) -> tensor<1x64x1x1xbf16, #ttnn_layout1> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout>) -> tensor<1x64x1x1xbf16, #ttnn_layout1> loc(#loc1)
        return %0 : tensor<1x64x1x1xbf16, #ttnn_layout1> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_205(%arg0: tensor<2048xbf16, #ttnn_layout8> loc(unknown)) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32, 1 : i32, 1 : i32]}> : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc32)
        return %0 : tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_206(%arg0: tensor<2048xbf16, #ttnn_layout8> loc(unknown)) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32, 1 : i32, 1 : i32]}> : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc6)
        return %0 : tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_207(%arg0: tensor<128xbf16, #ttnn_layout4> loc(unknown)) -> tensor<1x128x1x1xbf16, #ttnn_layout5> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc14)
        return %0 : tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_208(%arg0: tensor<1024xbf16, #ttnn_layout10> loc(unknown)) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1024 : i32, 1 : i32, 1 : i32]}> : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc15)
        return %0 : tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_209(%arg0: tensor<64xbf16, #ttnn_layout> loc(unknown)) -> tensor<1x64x1x1xbf16, #ttnn_layout1> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xbf16, #ttnn_layout>) -> tensor<1x64x1x1xbf16, #ttnn_layout1> loc(#loc1)
        return %0 : tensor<1x64x1x1xbf16, #ttnn_layout1> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_210(%arg0: tensor<512xbf16, #ttnn_layout2> loc(unknown)) -> tensor<1x512x1x1xbf16, #ttnn_layout3> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc45)
        return %0 : tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_211(%arg0: tensor<256xbf16, #ttnn_layout6> loc(unknown)) -> tensor<1x256x1x1xbf16, #ttnn_layout7> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc22)
        return %0 : tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_212(%arg0: tensor<2048xbf16, #ttnn_layout8> loc(unknown)) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 2048 : i32, 1 : i32, 1 : i32]}> : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc43)
        return %0 : tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
      } loc(#loc)
      func.func @main(%arg0: tensor<1000xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___fc_bias"} loc("p0.3"), %arg1: tensor<1000x2048xbf16, #ttnn_layout13> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___fc_weight"} loc("p1.10"), %arg2: tensor<2048xbf16, #ttnn_layout8> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___downsample_1_running_var"} loc("p2.18"), %arg3: tensor<2048xbf16, #ttnn_layout8> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___downsample_1_running_mean"} loc("p3.22"), %arg4: tensor<2048xbf16, #ttnn_layout8> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___downsample_1_bias"} loc("p4.26"), %arg5: tensor<2048xbf16, #ttnn_layout8> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___downsample_1_weight"} loc("p5.30"), %arg6: tensor<2048x1024x1x1xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer4___0___downsample_0_weight"} loc("p6.34"), %arg7: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___downsample_1_running_var"} loc("p7.42"), %arg8: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___downsample_1_running_mean"} loc("p8.46"), %arg9: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___downsample_1_bias"} loc("p9.50"), %arg10: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___downsample_1_weight"} loc("p10.54"), %arg11: tensor<1024x512x1x1xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer3___0___downsample_0_weight"} loc("p11.58"), %arg12: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___downsample_1_running_var"} loc("p12.64"), %arg13: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___downsample_1_running_mean"} loc("p13.68"), %arg14: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___downsample_1_bias"} loc("p14.72"), %arg15: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___downsample_1_weight"} loc("p15.76"), %arg16: tensor<512x256x1x1xbf16, #ttnn_layout16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer2___0___downsample_0_weight"} loc("p16.80"), %arg17: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___downsample_1_running_var"} loc("p17.85"), %arg18: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___downsample_1_running_mean"} loc("p18.89"), %arg19: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___downsample_1_bias"} loc("p19.93"), %arg20: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___downsample_1_weight"} loc("p20.97"), %arg21: tensor<256x64x1x1xbf16, #ttnn_layout17> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer1___0___downsample_0_weight"} loc("p21.101"), %arg22: tensor<64xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___bn1_running_var"} loc("p22.103"), %arg23: tensor<64xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___bn1_running_mean"} loc("p23.107"), %arg24: tensor<64xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___bn1_bias"} loc("p24.111"), %arg25: tensor<64xbf16, #ttnn_layout> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___bn1_weight"} loc("p25.115"), %arg26: tensor<64x3x7x7xbf16, #ttnn_layout18> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___conv1_weight"} loc("p26.119"), %arg27: tensor<1x3x224x224xbf16, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_0"} loc("p27.121"), %arg28: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___bn3_running_var"} loc("p28.310"), %arg29: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___bn3_running_mean"} loc("p29.314"), %arg30: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___bn3_bias"} loc("p30.318"), %arg31: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___bn3_weight"} loc("p31.322"), %arg32: tensor<256x128x1x1xbf16, #ttnn_layout20> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer1___0___conv3_weight"} loc("p32.326"), %arg33: tensor<128xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___bn2_running_var"} loc("p33.328"), %arg34: tensor<128xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___bn2_running_mean"} loc("p34.332"), %arg35: tensor<128xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___bn2_bias"} loc("p35.336"), %arg36: tensor<128xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___bn2_weight"} loc("p36.340"), %arg37: tensor<128x128x3x3xbf16, #ttnn_layout21> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer1___0___conv2_weight"} loc("p37.344"), %arg38: tensor<128xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___bn1_running_var"} loc("p38.346"), %arg39: tensor<128xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___bn1_running_mean"} loc("p39.350"), %arg40: tensor<128xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___bn1_bias"} loc("p40.354"), %arg41: tensor<128xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___bn1_weight"} loc("p41.358"), %arg42: tensor<128x64x1x1xbf16, #ttnn_layout22> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer1___0___conv1_weight"} loc("p42.362"), %arg43: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___1___bn3_running_var"} loc("p43.394"), %arg44: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___1___bn3_running_mean"} loc("p44.398"), %arg45: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___1___bn3_bias"} loc("p45.402"), %arg46: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___1___bn3_weight"} loc("p46.406"), %arg47: tensor<256x128x1x1xbf16, #ttnn_layout20> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer1___1___conv3_weight"} loc("p47.410"), %arg48: tensor<128xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___1___bn2_running_var"} loc("p48.412"), %arg49: tensor<128xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___1___bn2_running_mean"} loc("p49.416"), %arg50: tensor<128xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___1___bn2_bias"} loc("p50.420"), %arg51: tensor<128xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___1___bn2_weight"} loc("p51.424"), %arg52: tensor<128x128x3x3xbf16, #ttnn_layout21> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer1___1___conv2_weight"} loc("p52.428"), %arg53: tensor<128xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___1___bn1_running_var"} loc("p53.430"), %arg54: tensor<128xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___1___bn1_running_mean"} loc("p54.434"), %arg55: tensor<128xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___1___bn1_bias"} loc("p55.438"), %arg56: tensor<128xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___1___bn1_weight"} loc("p56.442"), %arg57: tensor<128x256x1x1xbf16, #ttnn_layout23> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer1___1___conv1_weight"} loc("p57.446"), %arg58: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___2___bn3_running_var"} loc("p58.478"), %arg59: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___2___bn3_running_mean"} loc("p59.482"), %arg60: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___2___bn3_bias"} loc("p60.486"), %arg61: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___2___bn3_weight"} loc("p61.490"), %arg62: tensor<256x128x1x1xbf16, #ttnn_layout20> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer1___2___conv3_weight"} loc("p62.494"), %arg63: tensor<128xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___2___bn2_running_var"} loc("p63.496"), %arg64: tensor<128xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___2___bn2_running_mean"} loc("p64.500"), %arg65: tensor<128xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___2___bn2_bias"} loc("p65.504"), %arg66: tensor<128xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___2___bn2_weight"} loc("p66.508"), %arg67: tensor<128x128x3x3xbf16, #ttnn_layout21> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer1___2___conv2_weight"} loc("p67.512"), %arg68: tensor<128xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___2___bn1_running_var"} loc("p68.514"), %arg69: tensor<128xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___2___bn1_running_mean"} loc("p69.518"), %arg70: tensor<128xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___2___bn1_bias"} loc("p70.522"), %arg71: tensor<128xbf16, #ttnn_layout4> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___2___bn1_weight"} loc("p71.526"), %arg72: tensor<128x256x1x1xbf16, #ttnn_layout23> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer1___2___conv1_weight"} loc("p72.530"), %arg73: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___bn3_running_var"} loc("p73.568"), %arg74: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___bn3_running_mean"} loc("p74.572"), %arg75: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___bn3_bias"} loc("p75.576"), %arg76: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___bn3_weight"} loc("p76.580"), %arg77: tensor<512x256x1x1xbf16, #ttnn_layout16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer2___0___conv3_weight"} loc("p77.584"), %arg78: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___bn2_running_var"} loc("p78.586"), %arg79: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___bn2_running_mean"} loc("p79.590"), %arg80: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___bn2_bias"} loc("p80.594"), %arg81: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___bn2_weight"} loc("p81.598"), %arg82: tensor<256x256x3x3xbf16, #ttnn_layout24> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer2___0___conv2_weight"} loc("p82.602"), %arg83: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___bn1_running_var"} loc("p83.604"), %arg84: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___bn1_running_mean"} loc("p84.608"), %arg85: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___bn1_bias"} loc("p85.612"), %arg86: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___bn1_weight"} loc("p86.616"), %arg87: tensor<256x256x1x1xbf16, #ttnn_layout25> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer2___0___conv1_weight"} loc("p87.620"), %arg88: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___1___bn3_running_var"} loc("p88.652"), %arg89: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___1___bn3_running_mean"} loc("p89.656"), %arg90: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___1___bn3_bias"} loc("p90.660"), %arg91: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___1___bn3_weight"} loc("p91.664"), %arg92: tensor<512x256x1x1xbf16, #ttnn_layout16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer2___1___conv3_weight"} loc("p92.668"), %arg93: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___1___bn2_running_var"} loc("p93.670"), %arg94: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___1___bn2_running_mean"} loc("p94.674"), %arg95: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___1___bn2_bias"} loc("p95.678"), %arg96: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___1___bn2_weight"} loc("p96.682"), %arg97: tensor<256x256x3x3xbf16, #ttnn_layout24> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer2___1___conv2_weight"} loc("p97.686"), %arg98: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___1___bn1_running_var"} loc("p98.688"), %arg99: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___1___bn1_running_mean"} loc("p99.692"), %arg100: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___1___bn1_bias"} loc("p100.696"), %arg101: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___1___bn1_weight"} loc("p101.700"), %arg102: tensor<256x512x1x1xbf16, #ttnn_layout26> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer2___1___conv1_weight"} loc("p102.704"), %arg103: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___2___bn3_running_var"} loc("p103.736"), %arg104: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___2___bn3_running_mean"} loc("p104.740"), %arg105: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___2___bn3_bias"} loc("p105.744"), %arg106: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___2___bn3_weight"} loc("p106.748"), %arg107: tensor<512x256x1x1xbf16, #ttnn_layout16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer2___2___conv3_weight"} loc("p107.752"), %arg108: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___2___bn2_running_var"} loc("p108.754"), %arg109: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___2___bn2_running_mean"} loc("p109.758"), %arg110: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___2___bn2_bias"} loc("p110.762"), %arg111: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___2___bn2_weight"} loc("p111.766"), %arg112: tensor<256x256x3x3xbf16, #ttnn_layout24> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer2___2___conv2_weight"} loc("p112.770"), %arg113: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___2___bn1_running_var"} loc("p113.772"), %arg114: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___2___bn1_running_mean"} loc("p114.776"), %arg115: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___2___bn1_bias"} loc("p115.780"), %arg116: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___2___bn1_weight"} loc("p116.784"), %arg117: tensor<256x512x1x1xbf16, #ttnn_layout26> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer2___2___conv1_weight"} loc("p117.788"), %arg118: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___3___bn3_running_var"} loc("p118.820"), %arg119: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___3___bn3_running_mean"} loc("p119.824"), %arg120: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___3___bn3_bias"} loc("p120.828"), %arg121: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___3___bn3_weight"} loc("p121.832"), %arg122: tensor<512x256x1x1xbf16, #ttnn_layout16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer2___3___conv3_weight"} loc("p122.836"), %arg123: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___3___bn2_running_var"} loc("p123.838"), %arg124: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___3___bn2_running_mean"} loc("p124.842"), %arg125: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___3___bn2_bias"} loc("p125.846"), %arg126: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___3___bn2_weight"} loc("p126.850"), %arg127: tensor<256x256x3x3xbf16, #ttnn_layout24> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer2___3___conv2_weight"} loc("p127.854"), %arg128: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___3___bn1_running_var"} loc("p128.856"), %arg129: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___3___bn1_running_mean"} loc("p129.860"), %arg130: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___3___bn1_bias"} loc("p130.864"), %arg131: tensor<256xbf16, #ttnn_layout6> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___3___bn1_weight"} loc("p131.868"), %arg132: tensor<256x512x1x1xbf16, #ttnn_layout26> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer2___3___conv1_weight"} loc("p132.872"), %arg133: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___bn3_running_var"} loc("p133.910"), %arg134: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___bn3_running_mean"} loc("p134.914"), %arg135: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___bn3_bias"} loc("p135.918"), %arg136: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___bn3_weight"} loc("p136.922"), %arg137: tensor<1024x512x1x1xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer3___0___conv3_weight"} loc("p137.926"), %arg138: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___bn2_running_var"} loc("p138.928"), %arg139: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___bn2_running_mean"} loc("p139.932"), %arg140: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___bn2_bias"} loc("p140.936"), %arg141: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___bn2_weight"} loc("p141.940"), %arg142: tensor<512x512x3x3xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer3___0___conv2_weight"} loc("p142.944"), %arg143: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___bn1_running_var"} loc("p143.946"), %arg144: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___bn1_running_mean"} loc("p144.950"), %arg145: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___bn1_bias"} loc("p145.954"), %arg146: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___bn1_weight"} loc("p146.958"), %arg147: tensor<512x512x1x1xbf16, #ttnn_layout28> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer3___0___conv1_weight"} loc("p147.962"), %arg148: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___1___bn3_running_var"} loc("p148.994"), %arg149: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___1___bn3_running_mean"} loc("p149.998"), %arg150: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___1___bn3_bias"} loc("p150.1002"), %arg151: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___1___bn3_weight"} loc("p151.1006"), %arg152: tensor<1024x512x1x1xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer3___1___conv3_weight"} loc("p152.1010"), %arg153: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___1___bn2_running_var"} loc("p153.1012"), %arg154: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___1___bn2_running_mean"} loc("p154.1016"), %arg155: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___1___bn2_bias"} loc("p155.1020"), %arg156: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___1___bn2_weight"} loc("p156.1024"), %arg157: tensor<512x512x3x3xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer3___1___conv2_weight"} loc("p157.1028"), %arg158: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___1___bn1_running_var"} loc("p158.1030"), %arg159: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___1___bn1_running_mean"} loc("p159.1034"), %arg160: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___1___bn1_bias"} loc("p160.1038"), %arg161: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___1___bn1_weight"} loc("p161.1042"), %arg162: tensor<512x1024x1x1xbf16, #ttnn_layout29> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer3___1___conv1_weight"} loc("p162.1046"), %arg163: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___2___bn3_running_var"} loc("p163.1078"), %arg164: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___2___bn3_running_mean"} loc("p164.1082"), %arg165: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___2___bn3_bias"} loc("p165.1086"), %arg166: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___2___bn3_weight"} loc("p166.1090"), %arg167: tensor<1024x512x1x1xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer3___2___conv3_weight"} loc("p167.1094"), %arg168: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___2___bn2_running_var"} loc("p168.1096"), %arg169: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___2___bn2_running_mean"} loc("p169.1100"), %arg170: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___2___bn2_bias"} loc("p170.1104"), %arg171: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___2___bn2_weight"} loc("p171.1108"), %arg172: tensor<512x512x3x3xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer3___2___conv2_weight"} loc("p172.1112"), %arg173: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___2___bn1_running_var"} loc("p173.1114"), %arg174: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___2___bn1_running_mean"} loc("p174.1118"), %arg175: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___2___bn1_bias"} loc("p175.1122"), %arg176: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___2___bn1_weight"} loc("p176.1126"), %arg177: tensor<512x1024x1x1xbf16, #ttnn_layout29> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer3___2___conv1_weight"} loc("p177.1130"), %arg178: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___3___bn3_running_var"} loc("p178.1162"), %arg179: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___3___bn3_running_mean"} loc("p179.1166"), %arg180: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___3___bn3_bias"} loc("p180.1170"), %arg181: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___3___bn3_weight"} loc("p181.1174"), %arg182: tensor<1024x512x1x1xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer3___3___conv3_weight"} loc("p182.1178"), %arg183: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___3___bn2_running_var"} loc("p183.1180"), %arg184: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___3___bn2_running_mean"} loc("p184.1184"), %arg185: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___3___bn2_bias"} loc("p185.1188"), %arg186: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___3___bn2_weight"} loc("p186.1192"), %arg187: tensor<512x512x3x3xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer3___3___conv2_weight"} loc("p187.1196"), %arg188: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___3___bn1_running_var"} loc("p188.1198"), %arg189: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___3___bn1_running_mean"} loc("p189.1202"), %arg190: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___3___bn1_bias"} loc("p190.1206"), %arg191: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___3___bn1_weight"} loc("p191.1210"), %arg192: tensor<512x1024x1x1xbf16, #ttnn_layout29> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer3___3___conv1_weight"} loc("p192.1214"), %arg193: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___4___bn3_running_var"} loc("p193.1246"), %arg194: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___4___bn3_running_mean"} loc("p194.1250"), %arg195: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___4___bn3_bias"} loc("p195.1254"), %arg196: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___4___bn3_weight"} loc("p196.1258"), %arg197: tensor<1024x512x1x1xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer3___4___conv3_weight"} loc("p197.1262"), %arg198: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___4___bn2_running_var"} loc("p198.1264"), %arg199: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___4___bn2_running_mean"} loc("p199.1268"), %arg200: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___4___bn2_bias"} loc("p200.1272"), %arg201: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___4___bn2_weight"} loc("p201.1276"), %arg202: tensor<512x512x3x3xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer3___4___conv2_weight"} loc("p202.1280"), %arg203: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___4___bn1_running_var"} loc("p203.1282"), %arg204: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___4___bn1_running_mean"} loc("p204.1286"), %arg205: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___4___bn1_bias"} loc("p205.1290"), %arg206: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___4___bn1_weight"} loc("p206.1294"), %arg207: tensor<512x1024x1x1xbf16, #ttnn_layout29> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer3___4___conv1_weight"} loc("p207.1298"), %arg208: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___5___bn3_running_var"} loc("p208.1330"), %arg209: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___5___bn3_running_mean"} loc("p209.1334"), %arg210: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___5___bn3_bias"} loc("p210.1338"), %arg211: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___5___bn3_weight"} loc("p211.1342"), %arg212: tensor<1024x512x1x1xbf16, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer3___5___conv3_weight"} loc("p212.1346"), %arg213: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___5___bn2_running_var"} loc("p213.1348"), %arg214: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___5___bn2_running_mean"} loc("p214.1352"), %arg215: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___5___bn2_bias"} loc("p215.1356"), %arg216: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___5___bn2_weight"} loc("p216.1360"), %arg217: tensor<512x512x3x3xbf16, #ttnn_layout27> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer3___5___conv2_weight"} loc("p217.1364"), %arg218: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___5___bn1_running_var"} loc("p218.1366"), %arg219: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___5___bn1_running_mean"} loc("p219.1370"), %arg220: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___5___bn1_bias"} loc("p220.1374"), %arg221: tensor<512xbf16, #ttnn_layout2> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___5___bn1_weight"} loc("p221.1378"), %arg222: tensor<512x1024x1x1xbf16, #ttnn_layout29> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer3___5___conv1_weight"} loc("p222.1382"), %arg223: tensor<2048xbf16, #ttnn_layout8> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___bn3_running_var"} loc("p223.1420"), %arg224: tensor<2048xbf16, #ttnn_layout8> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___bn3_running_mean"} loc("p224.1424"), %arg225: tensor<2048xbf16, #ttnn_layout8> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___bn3_bias"} loc("p225.1428"), %arg226: tensor<2048xbf16, #ttnn_layout8> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___bn3_weight"} loc("p226.1432"), %arg227: tensor<2048x1024x1x1xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer4___0___conv3_weight"} loc("p227.1436"), %arg228: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___bn2_running_var"} loc("p228.1438"), %arg229: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___bn2_running_mean"} loc("p229.1442"), %arg230: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___bn2_bias"} loc("p230.1446"), %arg231: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___bn2_weight"} loc("p231.1450"), %arg232: tensor<1024x1024x3x3xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer4___0___conv2_weight"} loc("p232.1454"), %arg233: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___bn1_running_var"} loc("p233.1456"), %arg234: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___bn1_running_mean"} loc("p234.1460"), %arg235: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___bn1_bias"} loc("p235.1464"), %arg236: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___bn1_weight"} loc("p236.1468"), %arg237: tensor<1024x1024x1x1xbf16, #ttnn_layout31> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer4___0___conv1_weight"} loc("p237.1472"), %arg238: tensor<2048xbf16, #ttnn_layout8> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___1___bn3_running_var"} loc("p238.1504"), %arg239: tensor<2048xbf16, #ttnn_layout8> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___1___bn3_running_mean"} loc("p239.1508"), %arg240: tensor<2048xbf16, #ttnn_layout8> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___1___bn3_bias"} loc("p240.1512"), %arg241: tensor<2048xbf16, #ttnn_layout8> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___1___bn3_weight"} loc("p241.1516"), %arg242: tensor<2048x1024x1x1xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer4___1___conv3_weight"} loc("p242.1520"), %arg243: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___1___bn2_running_var"} loc("p243.1522"), %arg244: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___1___bn2_running_mean"} loc("p244.1526"), %arg245: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___1___bn2_bias"} loc("p245.1530"), %arg246: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___1___bn2_weight"} loc("p246.1534"), %arg247: tensor<1024x1024x3x3xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer4___1___conv2_weight"} loc("p247.1538"), %arg248: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___1___bn1_running_var"} loc("p248.1540"), %arg249: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___1___bn1_running_mean"} loc("p249.1544"), %arg250: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___1___bn1_bias"} loc("p250.1548"), %arg251: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___1___bn1_weight"} loc("p251.1552"), %arg252: tensor<1024x2048x1x1xbf16, #ttnn_layout32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer4___1___conv1_weight"} loc("p252.1556"), %arg253: tensor<2048xbf16, #ttnn_layout8> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___2___bn3_running_var"} loc("p253.1588"), %arg254: tensor<2048xbf16, #ttnn_layout8> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___2___bn3_running_mean"} loc("p254.1592"), %arg255: tensor<2048xbf16, #ttnn_layout8> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___2___bn3_bias"} loc("p255.1596"), %arg256: tensor<2048xbf16, #ttnn_layout8> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___2___bn3_weight"} loc("p256.1600"), %arg257: tensor<2048x1024x1x1xbf16, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer4___2___conv3_weight"} loc("p257.1604"), %arg258: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___2___bn2_running_var"} loc("p258.1606"), %arg259: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___2___bn2_running_mean"} loc("p259.1610"), %arg260: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___2___bn2_bias"} loc("p260.1614"), %arg261: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___2___bn2_weight"} loc("p261.1618"), %arg262: tensor<1024x1024x3x3xbf16, #ttnn_layout30> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer4___2___conv2_weight"} loc("p262.1622"), %arg263: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___2___bn1_running_var"} loc("p263.1624"), %arg264: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___2___bn1_running_mean"} loc("p264.1628"), %arg265: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___2___bn1_bias"} loc("p265.1632"), %arg266: tensor<1024xbf16, #ttnn_layout10> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___2___bn1_weight"} loc("p266.1636"), %arg267: tensor<1024x2048x1x1xbf16, #ttnn_layout32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___layer4___2___conv1_weight"} loc("p267.1640")) -> (tensor<1x1000xbf16, #ttnn_layout12> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, [%arg23]) : (tensor<64xbf16, #ttnn_layout>) -> tensor<1x64x1x1xbf16, #ttnn_layout1> loc(#loc)
        "ttnn.deallocate"(%arg23) <{force = false}> : (tensor<64xbf16, #ttnn_layout>) -> () loc(#loc)
        %1 = ttcore.load_cached(@main_const_eval_1, [%arg91]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg91) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %2 = ttcore.load_cached(@main_const_eval_2, [%arg63]) : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
        "ttnn.deallocate"(%arg63) <{force = false}> : (tensor<128xbf16, #ttnn_layout4>) -> () loc(#loc)
        %3 = ttcore.load_cached(@main_const_eval_3, [%arg95]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg95) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %4 = ttcore.load_cached(@main_const_eval_4, [%arg78]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg78) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %5 = ttcore.load_cached(@main_const_eval_5, [%arg255]) : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg255) <{force = false}> : (tensor<2048xbf16, #ttnn_layout8>) -> () loc(#loc)
        %6 = ttcore.load_cached(@main_const_eval_6, [%arg28]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg28) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %7 = ttcore.load_cached(@main_const_eval_7, [%arg38]) : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
        "ttnn.deallocate"(%arg38) <{force = false}> : (tensor<128xbf16, #ttnn_layout4>) -> () loc(#loc)
        %8 = ttcore.load_cached(@main_const_eval_8, [%arg176]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg176) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %9 = ttcore.load_cached(@main_const_eval_9, [%arg124]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg124) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %10 = ttcore.load_cached(@main_const_eval_10, [%arg230]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg230) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %11 = ttcore.load_cached(@main_const_eval_11, [%arg43]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg43) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %12 = ttcore.load_cached(@main_const_eval_12, [%arg108]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg108) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %13 = ttcore.load_cached(@main_const_eval_13, [%arg34]) : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
        "ttnn.deallocate"(%arg34) <{force = false}> : (tensor<128xbf16, #ttnn_layout4>) -> () loc(#loc)
        %14 = ttcore.load_cached(@main_const_eval_14, [%arg181]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg181) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %15 = ttcore.load_cached(@main_const_eval_15, [%arg13]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg13) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %16 = ttcore.load_cached(@main_const_eval_16, [%arg81]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg81) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %17 = ttcore.load_cached(@main_const_eval_17, [%arg265]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg265) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %18 = ttcore.load_cached(@main_const_eval_18, [%arg101]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg101) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %19 = ttcore.load_cached(@main_const_eval_19, [%arg7]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg7) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %20 = ttcore.load_cached(@main_const_eval_20, [%arg169]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg169) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %21 = ttcore.load_cached(@main_const_eval_21, [%arg231]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg231) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %22 = ttcore.load_cached(@main_const_eval_22, [%arg111]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg111) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %23 = ttcore.load_cached(@main_const_eval_23, [%arg209]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg209) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %24 = ttcore.load_cached(@main_const_eval_24, [%arg39]) : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
        "ttnn.deallocate"(%arg39) <{force = false}> : (tensor<128xbf16, #ttnn_layout4>) -> () loc(#loc)
        %25 = ttcore.load_cached(@main_const_eval_25, [%arg90]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg90) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %26 = ttcore.load_cached(@main_const_eval_26, [%arg58]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg58) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %27 = ttcore.load_cached(@main_const_eval_27, [%arg54]) : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
        "ttnn.deallocate"(%arg54) <{force = false}> : (tensor<128xbf16, #ttnn_layout4>) -> () loc(#loc)
        %28 = ttcore.load_cached(@main_const_eval_28, [%arg246]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg246) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %29 = ttcore.load_cached(@main_const_eval_29, [%arg195]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg195) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %30 = ttcore.load_cached(@main_const_eval_30, [%arg218]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg218) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %31 = ttcore.load_cached(@main_const_eval_31, [%arg133]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg133) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %32 = ttcore.load_cached(@main_const_eval_32, [%arg33]) : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
        "ttnn.deallocate"(%arg33) <{force = false}> : (tensor<128xbf16, #ttnn_layout4>) -> () loc(#loc)
        %33 = ttcore.load_cached(@main_const_eval_33, [%arg99]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg99) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %34 = ttcore.load_cached(@main_const_eval_34, [%arg248]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg248) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %35 = ttcore.load_cached(@main_const_eval_35, [%arg105]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg105) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %36 = ttcore.load_cached(@main_const_eval_36, [%arg175]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg175) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %37 = ttcore.load_cached(@main_const_eval_37, [%arg61]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg61) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %38 = ttcore.load_cached(@main_const_eval_38, [%arg189]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg189) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %39 = ttcore.load_cached(@main_const_eval_39, [%arg100]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg100) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %40 = ttcore.load_cached(@main_const_eval_40, [%arg20]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg20) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %41 = ttcore.load_cached(@main_const_eval_41, [%arg223]) : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg223) <{force = false}> : (tensor<2048xbf16, #ttnn_layout8>) -> () loc(#loc)
        %42 = ttcore.load_cached(@main_const_eval_42, [%arg75]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg75) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %43 = ttcore.load_cached(@main_const_eval_43, [%arg144]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg144) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %44 = ttcore.load_cached(@main_const_eval_44, [%arg0]) : (tensor<1000xbf16, #ttnn_layout10>) -> tensor<1x1000xbf16, #ttnn_layout12> loc(#loc)
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1000xbf16, #ttnn_layout10>) -> () loc(#loc)
        %45 = ttcore.load_cached(@main_const_eval_45, [%arg224]) : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg224) <{force = false}> : (tensor<2048xbf16, #ttnn_layout8>) -> () loc(#loc)
        %46 = ttcore.load_cached(@main_const_eval_46, [%arg12]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg12) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %47 = ttcore.load_cached(@main_const_eval_47, [%arg186]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg186) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %48 = ttcore.load_cached(@main_const_eval_48, [%arg213]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg213) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %49 = ttcore.load_cached(@main_const_eval_49, [%arg125]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg125) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %50 = ttcore.load_cached(@main_const_eval_50, [%arg261]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg261) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %51 = ttcore.load_cached(@main_const_eval_51, [%arg233]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg233) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %52 = ttcore.load_cached(@main_const_eval_52, [%arg256]) : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg256) <{force = false}> : (tensor<2048xbf16, #ttnn_layout8>) -> () loc(#loc)
        %53 = ttcore.load_cached(@main_const_eval_53, [%arg17]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg17) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %54 = ttcore.load_cached(@main_const_eval_54, [%arg174]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg174) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %55 = ttcore.load_cached(@main_const_eval_55, [%arg168]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg168) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %56 = ttcore.load_cached(@main_const_eval_56, [%arg236]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg236) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %57 = ttcore.load_cached(@main_const_eval_57, [%arg89]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg89) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %58 = ttcore.load_cached(@main_const_eval_58, [%arg106]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg106) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %59 = ttcore.load_cached(@main_const_eval_59, [%arg8]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg8) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %60 = ttcore.load_cached(@main_const_eval_60, [%arg155]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg155) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %61 = ttcore.load_cached(@main_const_eval_61, [%arg93]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg93) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %62 = ttcore.load_cached(@main_const_eval_62, [%arg234]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg234) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %63 = ttcore.load_cached(@main_const_eval_63, [%arg196]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg196) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %64 = ttcore.load_cached(@main_const_eval_64, [%arg264]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg264) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %65 = ttcore.load_cached(@main_const_eval_65, [%arg25]) : (tensor<64xbf16, #ttnn_layout>) -> tensor<1x64x1x1xbf16, #ttnn_layout1> loc(#loc)
        "ttnn.deallocate"(%arg25) <{force = false}> : (tensor<64xbf16, #ttnn_layout>) -> () loc(#loc)
        %66 = ttcore.load_cached(@main_const_eval_66, [%arg206]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg206) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %67 = ttcore.load_cached(@main_const_eval_67, [%arg123]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg123) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %68 = ttcore.load_cached(@main_const_eval_68, [%arg244]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg244) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %69 = ttcore.load_cached(@main_const_eval_69, [%arg113]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg113) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %70 = ttcore.load_cached(@main_const_eval_70, [%arg173]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg173) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %71 = ttcore.load_cached(@main_const_eval_71, [%arg241]) : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg241) <{force = false}> : (tensor<2048xbf16, #ttnn_layout8>) -> () loc(#loc)
        %72 = ttcore.load_cached(@main_const_eval_72, [%arg73]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg73) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %73 = ttcore.load_cached(@main_const_eval_73, [%arg116]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg116) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %74 = ttcore.load_cached(@main_const_eval_74, [%arg85]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg85) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %75 = ttcore.load_cached(@main_const_eval_75, [%arg121]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg121) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %76 = ttcore.load_cached(@main_const_eval_76, [%arg14]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg14) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %77 = ttcore.load_cached(@main_const_eval_77, [%arg45]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg45) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %78 = ttcore.load_cached(@main_const_eval_78, [%arg163]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg163) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %79 = ttcore.load_cached(@main_const_eval_79, [%arg36]) : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
        "ttnn.deallocate"(%arg36) <{force = false}> : (tensor<128xbf16, #ttnn_layout4>) -> () loc(#loc)
        %80 = ttcore.load_cached(@main_const_eval_80, [%arg243]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg243) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %81 = ttcore.load_cached(@main_const_eval_81, [%arg40]) : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
        "ttnn.deallocate"(%arg40) <{force = false}> : (tensor<128xbf16, #ttnn_layout4>) -> () loc(#loc)
        %82 = ttcore.load_cached(@main_const_eval_82, [%arg55]) : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
        "ttnn.deallocate"(%arg55) <{force = false}> : (tensor<128xbf16, #ttnn_layout4>) -> () loc(#loc)
        %83 = ttcore.load_cached(@main_const_eval_83, [%arg238]) : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg238) <{force = false}> : (tensor<2048xbf16, #ttnn_layout8>) -> () loc(#loc)
        %84 = ttcore.load_cached(@main_const_eval_84, [%arg4]) : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<2048xbf16, #ttnn_layout8>) -> () loc(#loc)
        %85 = ttcore.load_cached(@main_const_eval_85, [%arg185]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg185) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %86 = ttcore.load_cached(@main_const_eval_86, [%arg140]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg140) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %87 = ttcore.load_cached(@main_const_eval_87, [%arg120]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg120) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %88 = ttcore.load_cached(@main_const_eval_88, [%arg249]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg249) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %89 = ttcore.load_cached(@main_const_eval_89, [%arg205]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg205) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %90 = ttcore.load_cached(@main_const_eval_90, [%arg148]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg148) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %91 = ttcore.load_cached(@main_const_eval_91, [%arg251]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg251) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %92 = ttcore.load_cached(@main_const_eval_92, [%arg88]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg88) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %93 = ttcore.load_cached(@main_const_eval_93, [%arg135]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg135) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %94 = ttcore.load_cached(@main_const_eval_94, [%arg46]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg46) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %95 = ttcore.load_cached(@main_const_eval_95, [%arg215]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg215) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %96 = ttcore.load_cached(@main_const_eval_96, [%arg216]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg216) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %97 = ttcore.load_cached(@main_const_eval_97, [%arg74]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg74) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %98 = ttcore.load_cached(@main_const_eval_98, [%arg15]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg15) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %99 = ttcore.load_cached(@main_const_eval_99, [%arg141]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg141) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %100 = ttcore.load_cached(@main_const_eval_100, [%arg166]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg166) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %101 = ttcore.load_cached(@main_const_eval_101, [%arg69]) : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
        "ttnn.deallocate"(%arg69) <{force = false}> : (tensor<128xbf16, #ttnn_layout4>) -> () loc(#loc)
        %102 = ttcore.load_cached(@main_const_eval_102, [%arg200]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg200) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %103 = ttcore.load_cached(@main_const_eval_103, [%arg104]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg104) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %104 = ttcore.load_cached(@main_const_eval_104, [%arg79]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg79) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %105 = ttcore.load_cached(@main_const_eval_105, [%arg66]) : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
        "ttnn.deallocate"(%arg66) <{force = false}> : (tensor<128xbf16, #ttnn_layout4>) -> () loc(#loc)
        %106 = ttcore.load_cached(@main_const_eval_106, [%arg151]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg151) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %107 = ttcore.load_cached(@main_const_eval_107, [%arg219]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg219) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %108 = ttcore.load_cached(@main_const_eval_108, [%arg10]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg10) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %109 = ttcore.load_cached(@main_const_eval_109, [%arg159]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg159) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %110 = ttcore.load_cached(@main_const_eval_110, [%arg178]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg178) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %111 = ttcore.load_cached(@main_const_eval_111, [%arg19]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg19) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %112 = ttcore.load_cached(@main_const_eval_112, [%arg161]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg161) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %113 = ttcore.load_cached(@main_const_eval_113, [%arg235]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg235) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %114 = ttcore.load_cached(@main_const_eval_114, [%arg138]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg138) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %115 = ttcore.load_cached(@main_const_eval_115, [%arg65]) : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
        "ttnn.deallocate"(%arg65) <{force = false}> : (tensor<128xbf16, #ttnn_layout4>) -> () loc(#loc)
        %116 = ttcore.load_cached(@main_const_eval_116, [%arg3]) : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<2048xbf16, #ttnn_layout8>) -> () loc(#loc)
        %117 = ttcore.load_cached(@main_const_eval_117, [%arg203]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg203) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %118 = ttcore.load_cached(@main_const_eval_118, [%arg130]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg130) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %119 = ttcore.load_cached(@main_const_eval_119, [%arg150]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg150) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %120 = ttcore.load_cached(@main_const_eval_120, [%arg118]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg118) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %121 = ttcore.load_cached(@main_const_eval_121, [%arg18]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg18) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %122 = ttcore.load_cached(@main_const_eval_122, [%arg220]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg220) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %123 = ttcore.load_cached(@main_const_eval_123, [%arg188]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg188) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %124 = ttcore.load_cached(@main_const_eval_124, [%arg221]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg221) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %125 = ttcore.load_cached(@main_const_eval_125, [%arg165]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg165) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %126 = ttcore.load_cached(@main_const_eval_126, [%arg80]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg80) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %127 = ttcore.load_cached(@main_const_eval_127, [%arg109]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg109) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %128 = ttcore.load_cached(@main_const_eval_128, [%arg30]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg30) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %129 = ttcore.load_cached(@main_const_eval_129, [%arg9]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg9) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %130 = ttcore.load_cached(@main_const_eval_130, [%arg129]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg129) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %131 = ttcore.load_cached(@main_const_eval_131, [%arg239]) : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg239) <{force = false}> : (tensor<2048xbf16, #ttnn_layout8>) -> () loc(#loc)
        %132 = ttcore.load_cached(@main_const_eval_132, [%arg225]) : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg225) <{force = false}> : (tensor<2048xbf16, #ttnn_layout8>) -> () loc(#loc)
        %133 = ttcore.load_cached(@main_const_eval_133, [%arg31]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg31) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %134 = ttcore.load_cached(@main_const_eval_134, [%arg149]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg149) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %135 = ttcore.load_cached(@main_const_eval_135, [%arg134]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg134) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %136 = ttcore.load_cached(@main_const_eval_136, [%arg201]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg201) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %137 = ttcore.load_cached(@main_const_eval_137, [%arg53]) : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
        "ttnn.deallocate"(%arg53) <{force = false}> : (tensor<128xbf16, #ttnn_layout4>) -> () loc(#loc)
        %138 = ttcore.load_cached(@main_const_eval_138, [%arg84]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg84) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %139 = ttcore.load_cached(@main_const_eval_139, [%arg263]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg263) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %140 = ttcore.load_cached(@main_const_eval_140, [%arg115]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg115) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %141 = ttcore.load_cached(@main_const_eval_141, [%arg56]) : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
        "ttnn.deallocate"(%arg56) <{force = false}> : (tensor<128xbf16, #ttnn_layout4>) -> () loc(#loc)
        %142 = ttcore.load_cached(@main_const_eval_142, [%arg70]) : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
        "ttnn.deallocate"(%arg70) <{force = false}> : (tensor<128xbf16, #ttnn_layout4>) -> () loc(#loc)
        %143 = ttcore.load_cached(@main_const_eval_143, [%arg98]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg98) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %144 = ttcore.load_cached(@main_const_eval_144, [%arg260]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg260) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %145 = ttcore.load_cached(@main_const_eval_145, [%arg158]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg158) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %146 = ttcore.load_cached(@main_const_eval_146, [%arg170]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg170) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %147 = ttcore.load_cached(@main_const_eval_147, [%arg126]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg126) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %148 = ttcore.load_cached(@main_const_eval_148, [%arg2]) : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<2048xbf16, #ttnn_layout8>) -> () loc(#loc)
        %149 = ttcore.load_cached(@main_const_eval_149, [%arg64]) : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
        "ttnn.deallocate"(%arg64) <{force = false}> : (tensor<128xbf16, #ttnn_layout4>) -> () loc(#loc)
        %150 = ttcore.load_cached(@main_const_eval_150, [%arg94]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg94) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %151 = ttcore.load_cached(@main_const_eval_151, [%arg245]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg245) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %152 = ttcore.load_cached(@main_const_eval_152, [%arg250]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg250) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %153 = ttcore.load_cached(@main_const_eval_153, [%arg44]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg44) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %154 = ttcore.load_cached(@main_const_eval_154, [%arg160]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg160) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %155 = ttcore.load_cached(@main_const_eval_155, [%arg204]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg204) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %156 = ttcore.load_cached(@main_const_eval_156, [%arg211]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg211) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %157 = ttcore.load_cached(@main_const_eval_157, [%arg145]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg145) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %158 = ttcore.load_cached(@main_const_eval_158, [%arg193]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg193) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %159 = ttcore.load_cached(@main_const_eval_159, [%arg198]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg198) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %160 = ttcore.load_cached(@main_const_eval_160, [%arg194]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg194) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %161 = ttcore.load_cached(@main_const_eval_161, [%arg143]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg143) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %162 = ttcore.load_cached(@main_const_eval_162, [%arg190]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg190) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %163 = ttcore.load_cached(@main_const_eval_163, [%arg228]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg228) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %164 = ttcore.load_cached(@main_const_eval_164, [%arg68]) : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
        "ttnn.deallocate"(%arg68) <{force = false}> : (tensor<128xbf16, #ttnn_layout4>) -> () loc(#loc)
        %165 = ttcore.load_cached(@main_const_eval_165, [%arg171]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg171) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %166 = ttcore.load_cached(@main_const_eval_166, [%arg210]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg210) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %167 = ttcore.load_cached(@main_const_eval_167, [%arg191]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg191) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %168 = ttcore.load_cached(@main_const_eval_168, [%arg71]) : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
        "ttnn.deallocate"(%arg71) <{force = false}> : (tensor<128xbf16, #ttnn_layout4>) -> () loc(#loc)
        %169 = ttcore.load_cached(@main_const_eval_169, [%arg214]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg214) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %170 = ttcore.load_cached(@main_const_eval_170, [%arg154]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg154) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %171 = ttcore.load_cached(@main_const_eval_171, [%arg48]) : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
        "ttnn.deallocate"(%arg48) <{force = false}> : (tensor<128xbf16, #ttnn_layout4>) -> () loc(#loc)
        %172 = ttcore.load_cached(@main_const_eval_172, [%arg266]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg266) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %173 = ttcore.load_cached(@main_const_eval_173, [%arg139]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg139) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %174 = ttcore.load_cached(@main_const_eval_174, [%arg86]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg86) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %175 = ttcore.load_cached(@main_const_eval_175, [%arg41]) : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
        "ttnn.deallocate"(%arg41) <{force = false}> : (tensor<128xbf16, #ttnn_layout4>) -> () loc(#loc)
        %176 = ttcore.load_cached(@main_const_eval_176, [%arg76]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg76) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %177 = ttcore.load_cached(@main_const_eval_177, [%arg184]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg184) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %178 = ttcore.load_cached(@main_const_eval_178, [%arg114]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg114) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %179 = ttcore.load_cached(@main_const_eval_179, [%arg146]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg146) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %180 = ttcore.load_cached(@main_const_eval_180, [%arg183]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg183) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %181 = ttcore.load_cached(@main_const_eval_181, [%arg5]) : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg5) <{force = false}> : (tensor<2048xbf16, #ttnn_layout8>) -> () loc(#loc)
        %182 = ttcore.load_cached(@main_const_eval_182, [%arg131]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg131) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %183 = ttcore.load_cached(@main_const_eval_183, [%arg128]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg128) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %184 = ttcore.load_cached(@main_const_eval_184, [%arg259]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg259) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %185 = ttcore.load_cached(@main_const_eval_185, [%arg50]) : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
        "ttnn.deallocate"(%arg50) <{force = false}> : (tensor<128xbf16, #ttnn_layout4>) -> () loc(#loc)
        %186 = ttcore.load_cached(@main_const_eval_186, [%arg103]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg103) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %187 = ttcore.load_cached(@main_const_eval_187, [%arg153]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg153) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %188 = ttcore.load_cached(@main_const_eval_188, [%arg258]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg258) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %189 = ttcore.load_cached(@main_const_eval_189, [%arg229]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg229) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %190 = ttcore.load_cached(@main_const_eval_190, [%arg51]) : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
        "ttnn.deallocate"(%arg51) <{force = false}> : (tensor<128xbf16, #ttnn_layout4>) -> () loc(#loc)
        %191 = ttcore.load_cached(@main_const_eval_191, [%arg254]) : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg254) <{force = false}> : (tensor<2048xbf16, #ttnn_layout8>) -> () loc(#loc)
        %192 = ttcore.load_cached(@main_const_eval_192, [%arg83]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg83) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %193 = ttcore.load_cached(@main_const_eval_193, [%arg49]) : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
        "ttnn.deallocate"(%arg49) <{force = false}> : (tensor<128xbf16, #ttnn_layout4>) -> () loc(#loc)
        %194 = ttcore.load_cached(@main_const_eval_194, [%arg156]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg156) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %195 = ttcore.load_cached(@main_const_eval_195, [%arg180]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg180) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %196 = ttcore.load_cached(@main_const_eval_196, [%arg208]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg208) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %197 = ttcore.load_cached(@main_const_eval_197, [%arg164]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg164) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %198 = ttcore.load_cached(@main_const_eval_198, [%arg110]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg110) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %199 = ttcore.load_cached(@main_const_eval_199, [%arg199]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg199) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %200 = ttcore.load_cached(@main_const_eval_200, [%arg136]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg136) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %201 = ttcore.load_cached(@main_const_eval_201, [%arg59]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg59) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %202 = ttcore.load_cached(@main_const_eval_202, [%arg96]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg96) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %203 = ttcore.load_cached(@main_const_eval_203, [%arg29]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg29) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %204 = ttcore.load_cached(@main_const_eval_204, [%arg24]) : (tensor<64xbf16, #ttnn_layout>) -> tensor<1x64x1x1xbf16, #ttnn_layout1> loc(#loc)
        "ttnn.deallocate"(%arg24) <{force = false}> : (tensor<64xbf16, #ttnn_layout>) -> () loc(#loc)
        %205 = ttcore.load_cached(@main_const_eval_205, [%arg226]) : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg226) <{force = false}> : (tensor<2048xbf16, #ttnn_layout8>) -> () loc(#loc)
        %206 = ttcore.load_cached(@main_const_eval_206, [%arg253]) : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg253) <{force = false}> : (tensor<2048xbf16, #ttnn_layout8>) -> () loc(#loc)
        %207 = ttcore.load_cached(@main_const_eval_207, [%arg35]) : (tensor<128xbf16, #ttnn_layout4>) -> tensor<1x128x1x1xbf16, #ttnn_layout5> loc(#loc)
        "ttnn.deallocate"(%arg35) <{force = false}> : (tensor<128xbf16, #ttnn_layout4>) -> () loc(#loc)
        %208 = ttcore.load_cached(@main_const_eval_208, [%arg179]) : (tensor<1024xbf16, #ttnn_layout10>) -> tensor<1x1024x1x1xbf16, #ttnn_layout11> loc(#loc)
        "ttnn.deallocate"(%arg179) <{force = false}> : (tensor<1024xbf16, #ttnn_layout10>) -> () loc(#loc)
        %209 = ttcore.load_cached(@main_const_eval_209, [%arg22]) : (tensor<64xbf16, #ttnn_layout>) -> tensor<1x64x1x1xbf16, #ttnn_layout1> loc(#loc)
        "ttnn.deallocate"(%arg22) <{force = false}> : (tensor<64xbf16, #ttnn_layout>) -> () loc(#loc)
        %210 = ttcore.load_cached(@main_const_eval_210, [%arg119]) : (tensor<512xbf16, #ttnn_layout2>) -> tensor<1x512x1x1xbf16, #ttnn_layout3> loc(#loc)
        "ttnn.deallocate"(%arg119) <{force = false}> : (tensor<512xbf16, #ttnn_layout2>) -> () loc(#loc)
        %211 = ttcore.load_cached(@main_const_eval_211, [%arg60]) : (tensor<256xbf16, #ttnn_layout6>) -> tensor<1x256x1x1xbf16, #ttnn_layout7> loc(#loc)
        "ttnn.deallocate"(%arg60) <{force = false}> : (tensor<256xbf16, #ttnn_layout6>) -> () loc(#loc)
        %212 = ttcore.load_cached(@main_const_eval_212, [%arg240]) : (tensor<2048xbf16, #ttnn_layout8>) -> tensor<1x2048x1x1xbf16, #ttnn_layout9> loc(#loc)
        "ttnn.deallocate"(%arg240) <{force = false}> : (tensor<2048xbf16, #ttnn_layout8>) -> () loc(#loc)
        %213 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc323)
        %214 = "ttnn.permute"(%arg27) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x3x224x224xbf16, #ttnn_layout19>) -> tensor<1x224x224x3xbf16, #ttnn_layout33> loc(#loc444)
        "ttnn.deallocate"(%arg27) <{force = false}> : (tensor<1x3x224x224xbf16, #ttnn_layout19>) -> () loc(#loc444)
        %215 = "ttnn.reshape"(%214) <{shape = [1 : i32, 1 : i32, 50176 : i32, 3 : i32]}> : (tensor<1x224x224x3xbf16, #ttnn_layout33>) -> tensor<1x1x50176x3xbf16, #ttnn_layout34> loc(#loc555)
        "ttnn.deallocate"(%214) <{force = false}> : (tensor<1x224x224x3xbf16, #ttnn_layout33>) -> () loc(#loc555)
        %216 = "ttnn.to_layout"(%215) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x50176x3xbf16, #ttnn_layout34>) -> tensor<1x1x50176x3xbf16, #ttnn_layout35> loc(#loc445)
        "ttnn.deallocate"(%215) <{force = false}> : (tensor<1x1x50176x3xbf16, #ttnn_layout34>) -> () loc(#loc445)
        %217 = "ttnn.conv2d"(%216, %arg26, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 3 : i32, input_height = 224 : i32, input_width = 224 : i32, kernel_size = array<i32: 7, 7>, out_channels = 64 : i32, padding = array<i32: 3, 3, 3, 3>, stride = array<i32: 2, 2>}> : (tensor<1x1x50176x3xbf16, #ttnn_layout35>, tensor<64x3x7x7xbf16, #ttnn_layout18>, !ttnn.device) -> tensor<1x1x12544x64xbf16, #ttnn_layout36> loc(#loc323)
        "ttnn.deallocate"(%216) <{force = false}> : (tensor<1x1x50176x3xbf16, #ttnn_layout35>) -> () loc(#loc323)
        "ttnn.deallocate"(%arg26) <{force = false}> : (tensor<64x3x7x7xbf16, #ttnn_layout18>) -> () loc(#loc323)
        %218 = "ttnn.reshape"(%217) <{shape = [1 : i32, 112 : i32, 112 : i32, 64 : i32]}> : (tensor<1x1x12544x64xbf16, #ttnn_layout36>) -> tensor<1x112x112x64xbf16, #ttnn_layout37> loc(#loc446)
        "ttnn.deallocate"(%217) <{force = false}> : (tensor<1x1x12544x64xbf16, #ttnn_layout36>) -> () loc(#loc446)
        %219 = "ttnn.permute"(%218) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x112x112x64xbf16, #ttnn_layout37>) -> tensor<1x64x112x112xbf16, #ttnn_layout38> loc(#loc323)
        "ttnn.deallocate"(%218) <{force = false}> : (tensor<1x112x112x64xbf16, #ttnn_layout37>) -> () loc(#loc323)
        %220 = "ttnn.batch_norm_inference"(%219, %0, %209, %65, %204) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x64x112x112xbf16, #ttnn_layout38>, tensor<1x64x1x1xbf16, #ttnn_layout1>, tensor<1x64x1x1xbf16, #ttnn_layout1>, tensor<1x64x1x1xbf16, #ttnn_layout1>, tensor<1x64x1x1xbf16, #ttnn_layout1>) -> tensor<1x64x112x112xbf16, #ttnn_layout38> loc(#loc1)
        "ttnn.deallocate"(%219) <{force = false}> : (tensor<1x64x112x112xbf16, #ttnn_layout38>) -> () loc(#loc1)
        "ttnn.deallocate"(%209) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout1>) -> () loc(#loc1)
        "ttnn.deallocate"(%204) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout1>) -> () loc(#loc1)
        "ttnn.deallocate"(%65) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout1>) -> () loc(#loc1)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x64x1x1xbf16, #ttnn_layout1>) -> () loc(#loc1)
        %221 = "ttnn.relu"(%220) : (tensor<1x64x112x112xbf16, #ttnn_layout38>) -> tensor<1x64x112x112xbf16, #ttnn_layout38> loc(#loc324)
        "ttnn.deallocate"(%220) <{force = false}> : (tensor<1x64x112x112xbf16, #ttnn_layout38>) -> () loc(#loc324)
        %222 = "ttnn.permute"(%221) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x112x112xbf16, #ttnn_layout38>) -> tensor<1x112x112x64xbf16, #ttnn_layout37> loc(#loc324)
        "ttnn.deallocate"(%221) <{force = false}> : (tensor<1x64x112x112xbf16, #ttnn_layout38>) -> () loc(#loc324)
        %223 = "ttnn.reshape"(%222) <{shape = [1 : i32, 1 : i32, 12544 : i32, 64 : i32]}> : (tensor<1x112x112x64xbf16, #ttnn_layout37>) -> tensor<1x1x12544x64xbf16, #ttnn_layout36> loc(#loc324)
        "ttnn.deallocate"(%222) <{force = false}> : (tensor<1x112x112x64xbf16, #ttnn_layout37>) -> () loc(#loc324)
        %224 = "ttnn.to_layout"(%223) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x12544x64xbf16, #ttnn_layout36>) -> tensor<1x1x12544x64xbf16, #ttnn_layout39> loc(#loc447)
        "ttnn.deallocate"(%223) <{force = false}> : (tensor<1x1x12544x64xbf16, #ttnn_layout36>) -> () loc(#loc447)
        %225 = "ttnn.max_pool2d"(%224) <{batch_size = 1 : si32, ceil_mode = false, channels = 64 : si32, dilation = array<i32: 1, 1>, in_place_halo = false, input_height = 112 : si32, input_width = 112 : si32, kernel_size = array<i32: 3, 3>, padding = array<i32: 1, 1>, stride = array<i32: 2, 2>}> : (tensor<1x1x12544x64xbf16, #ttnn_layout39>) -> tensor<1x1x3136x64xbf16, #ttnn_layout40> loc(#loc325)
        "ttnn.deallocate"(%224) <{force = false}> : (tensor<1x1x12544x64xbf16, #ttnn_layout39>) -> () loc(#loc325)
        %226 = "ttnn.conv2d"(%225, %arg42, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 64 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 128 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x3136x64xbf16, #ttnn_layout40>, tensor<128x64x1x1xbf16, #ttnn_layout22>, !ttnn.device) -> tensor<1x1x3136x128xbf16, #ttnn_layout41> loc(#loc326)
        "ttnn.deallocate"(%arg42) <{force = false}> : (tensor<128x64x1x1xbf16, #ttnn_layout22>) -> () loc(#loc326)
        %227 = "ttnn.reshape"(%226) <{shape = [1 : i32, 56 : i32, 56 : i32, 128 : i32]}> : (tensor<1x1x3136x128xbf16, #ttnn_layout41>) -> tensor<1x56x56x128xbf16, #ttnn_layout42> loc(#loc448)
        "ttnn.deallocate"(%226) <{force = false}> : (tensor<1x1x3136x128xbf16, #ttnn_layout41>) -> () loc(#loc448)
        %228 = "ttnn.permute"(%227) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x56x56x128xbf16, #ttnn_layout42>) -> tensor<1x128x56x56xbf16, #ttnn_layout43> loc(#loc326)
        "ttnn.deallocate"(%227) <{force = false}> : (tensor<1x56x56x128xbf16, #ttnn_layout42>) -> () loc(#loc326)
        %229 = "ttnn.batch_norm_inference"(%228, %24, %7, %175, %81) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x128x56x56xbf16, #ttnn_layout43>, tensor<1x128x1x1xbf16, #ttnn_layout5>, tensor<1x128x1x1xbf16, #ttnn_layout5>, tensor<1x128x1x1xbf16, #ttnn_layout5>, tensor<1x128x1x1xbf16, #ttnn_layout5>) -> tensor<1x128x56x56xbf16, #ttnn_layout43> loc(#loc8)
        "ttnn.deallocate"(%228) <{force = false}> : (tensor<1x128x56x56xbf16, #ttnn_layout43>) -> () loc(#loc8)
        "ttnn.deallocate"(%175) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout5>) -> () loc(#loc8)
        "ttnn.deallocate"(%81) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout5>) -> () loc(#loc8)
        "ttnn.deallocate"(%24) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout5>) -> () loc(#loc8)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout5>) -> () loc(#loc8)
        %230 = "ttnn.relu"(%229) : (tensor<1x128x56x56xbf16, #ttnn_layout43>) -> tensor<1x128x56x56xbf16, #ttnn_layout43> loc(#loc327)
        "ttnn.deallocate"(%229) <{force = false}> : (tensor<1x128x56x56xbf16, #ttnn_layout43>) -> () loc(#loc327)
        %231 = "ttnn.permute"(%230) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x128x56x56xbf16, #ttnn_layout43>) -> tensor<1x56x56x128xbf16, #ttnn_layout42> loc(#loc327)
        "ttnn.deallocate"(%230) <{force = false}> : (tensor<1x128x56x56xbf16, #ttnn_layout43>) -> () loc(#loc327)
        %232 = "ttnn.reshape"(%231) <{shape = [1 : i32, 1 : i32, 3136 : i32, 128 : i32]}> : (tensor<1x56x56x128xbf16, #ttnn_layout42>) -> tensor<1x1x3136x128xbf16, #ttnn_layout41> loc(#loc327)
        "ttnn.deallocate"(%231) <{force = false}> : (tensor<1x56x56x128xbf16, #ttnn_layout42>) -> () loc(#loc327)
        %233 = "ttnn.to_layout"(%232) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x3136x128xbf16, #ttnn_layout41>) -> tensor<1x1x3136x128xbf16, #ttnn_layout44> loc(#loc449)
        "ttnn.deallocate"(%232) <{force = false}> : (tensor<1x1x3136x128xbf16, #ttnn_layout41>) -> () loc(#loc449)
        %234 = "ttnn.conv2d"(%233, %arg37, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 128 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 3, 3>, out_channels = 128 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x3136x128xbf16, #ttnn_layout44>, tensor<128x128x3x3xbf16, #ttnn_layout21>, !ttnn.device) -> tensor<1x1x3136x128xbf16, #ttnn_layout41> loc(#loc328)
        "ttnn.deallocate"(%233) <{force = false}> : (tensor<1x1x3136x128xbf16, #ttnn_layout44>) -> () loc(#loc328)
        "ttnn.deallocate"(%arg37) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout21>) -> () loc(#loc328)
        %235 = "ttnn.reshape"(%234) <{shape = [1 : i32, 56 : i32, 56 : i32, 128 : i32]}> : (tensor<1x1x3136x128xbf16, #ttnn_layout41>) -> tensor<1x56x56x128xbf16, #ttnn_layout42> loc(#loc450)
        "ttnn.deallocate"(%234) <{force = false}> : (tensor<1x1x3136x128xbf16, #ttnn_layout41>) -> () loc(#loc450)
        %236 = "ttnn.permute"(%235) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x56x56x128xbf16, #ttnn_layout42>) -> tensor<1x128x56x56xbf16, #ttnn_layout43> loc(#loc328)
        "ttnn.deallocate"(%235) <{force = false}> : (tensor<1x56x56x128xbf16, #ttnn_layout42>) -> () loc(#loc328)
        %237 = "ttnn.batch_norm_inference"(%236, %13, %32, %79, %207) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x128x56x56xbf16, #ttnn_layout43>, tensor<1x128x1x1xbf16, #ttnn_layout5>, tensor<1x128x1x1xbf16, #ttnn_layout5>, tensor<1x128x1x1xbf16, #ttnn_layout5>, tensor<1x128x1x1xbf16, #ttnn_layout5>) -> tensor<1x128x56x56xbf16, #ttnn_layout43> loc(#loc14)
        "ttnn.deallocate"(%236) <{force = false}> : (tensor<1x128x56x56xbf16, #ttnn_layout43>) -> () loc(#loc14)
        "ttnn.deallocate"(%207) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout5>) -> () loc(#loc14)
        "ttnn.deallocate"(%79) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout5>) -> () loc(#loc14)
        "ttnn.deallocate"(%32) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout5>) -> () loc(#loc14)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout5>) -> () loc(#loc14)
        %238 = "ttnn.relu"(%237) : (tensor<1x128x56x56xbf16, #ttnn_layout43>) -> tensor<1x128x56x56xbf16, #ttnn_layout43> loc(#loc329)
        "ttnn.deallocate"(%237) <{force = false}> : (tensor<1x128x56x56xbf16, #ttnn_layout43>) -> () loc(#loc329)
        %239 = "ttnn.permute"(%238) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x128x56x56xbf16, #ttnn_layout43>) -> tensor<1x56x56x128xbf16, #ttnn_layout42> loc(#loc329)
        "ttnn.deallocate"(%238) <{force = false}> : (tensor<1x128x56x56xbf16, #ttnn_layout43>) -> () loc(#loc329)
        %240 = "ttnn.reshape"(%239) <{shape = [1 : i32, 1 : i32, 3136 : i32, 128 : i32]}> : (tensor<1x56x56x128xbf16, #ttnn_layout42>) -> tensor<1x1x3136x128xbf16, #ttnn_layout41> loc(#loc329)
        "ttnn.deallocate"(%239) <{force = false}> : (tensor<1x56x56x128xbf16, #ttnn_layout42>) -> () loc(#loc329)
        %241 = "ttnn.to_layout"(%240) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x3136x128xbf16, #ttnn_layout41>) -> tensor<1x1x3136x128xbf16, #ttnn_layout44> loc(#loc451)
        "ttnn.deallocate"(%240) <{force = false}> : (tensor<1x1x3136x128xbf16, #ttnn_layout41>) -> () loc(#loc451)
        %242 = "ttnn.conv2d"(%241, %arg32, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 128 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x3136x128xbf16, #ttnn_layout44>, tensor<256x128x1x1xbf16, #ttnn_layout20>, !ttnn.device) -> tensor<1x1x3136x256xbf16, #ttnn_layout45> loc(#loc330)
        "ttnn.deallocate"(%241) <{force = false}> : (tensor<1x1x3136x128xbf16, #ttnn_layout44>) -> () loc(#loc330)
        "ttnn.deallocate"(%arg32) <{force = false}> : (tensor<256x128x1x1xbf16, #ttnn_layout20>) -> () loc(#loc330)
        %243 = "ttnn.reshape"(%242) <{shape = [1 : i32, 56 : i32, 56 : i32, 256 : i32]}> : (tensor<1x1x3136x256xbf16, #ttnn_layout45>) -> tensor<1x56x56x256xbf16, #ttnn_layout46> loc(#loc452)
        "ttnn.deallocate"(%242) <{force = false}> : (tensor<1x1x3136x256xbf16, #ttnn_layout45>) -> () loc(#loc452)
        %244 = "ttnn.permute"(%243) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x56x56x256xbf16, #ttnn_layout46>) -> tensor<1x256x56x56xbf16, #ttnn_layout47> loc(#loc330)
        "ttnn.deallocate"(%243) <{force = false}> : (tensor<1x56x56x256xbf16, #ttnn_layout46>) -> () loc(#loc330)
        %245 = "ttnn.batch_norm_inference"(%244, %203, %6, %133, %128) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x256x56x56xbf16, #ttnn_layout47>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>) -> tensor<1x256x56x56xbf16, #ttnn_layout47> loc(#loc7)
        "ttnn.deallocate"(%244) <{force = false}> : (tensor<1x256x56x56xbf16, #ttnn_layout47>) -> () loc(#loc7)
        "ttnn.deallocate"(%203) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc7)
        "ttnn.deallocate"(%133) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc7)
        "ttnn.deallocate"(%128) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc7)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc7)
        %246 = "ttnn.conv2d"(%225, %arg21, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 64 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x3136x64xbf16, #ttnn_layout40>, tensor<256x64x1x1xbf16, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x3136x256xbf16, #ttnn_layout45> loc(#loc331)
        "ttnn.deallocate"(%225) <{force = false}> : (tensor<1x1x3136x64xbf16, #ttnn_layout40>) -> () loc(#loc331)
        "ttnn.deallocate"(%arg21) <{force = false}> : (tensor<256x64x1x1xbf16, #ttnn_layout17>) -> () loc(#loc331)
        %247 = "ttnn.reshape"(%246) <{shape = [1 : i32, 56 : i32, 56 : i32, 256 : i32]}> : (tensor<1x1x3136x256xbf16, #ttnn_layout45>) -> tensor<1x56x56x256xbf16, #ttnn_layout46> loc(#loc453)
        "ttnn.deallocate"(%246) <{force = false}> : (tensor<1x1x3136x256xbf16, #ttnn_layout45>) -> () loc(#loc453)
        %248 = "ttnn.permute"(%247) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x56x56x256xbf16, #ttnn_layout46>) -> tensor<1x256x56x56xbf16, #ttnn_layout47> loc(#loc331)
        "ttnn.deallocate"(%247) <{force = false}> : (tensor<1x56x56x256xbf16, #ttnn_layout46>) -> () loc(#loc331)
        %249 = "ttnn.batch_norm_inference"(%248, %121, %53, %40, %111) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x256x56x56xbf16, #ttnn_layout47>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>) -> tensor<1x256x56x56xbf16, #ttnn_layout47> loc(#loc31)
        "ttnn.deallocate"(%248) <{force = false}> : (tensor<1x256x56x56xbf16, #ttnn_layout47>) -> () loc(#loc31)
        "ttnn.deallocate"(%121) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc31)
        "ttnn.deallocate"(%111) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc31)
        "ttnn.deallocate"(%53) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc31)
        "ttnn.deallocate"(%40) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc31)
        %250 = "ttnn.add"(%245, %249) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x56x56xbf16, #ttnn_layout47>, tensor<1x256x56x56xbf16, #ttnn_layout47>) -> tensor<1x256x56x56xbf16, #ttnn_layout47> loc(#loc332)
        "ttnn.deallocate"(%249) <{force = false}> : (tensor<1x256x56x56xbf16, #ttnn_layout47>) -> () loc(#loc332)
        "ttnn.deallocate"(%245) <{force = false}> : (tensor<1x256x56x56xbf16, #ttnn_layout47>) -> () loc(#loc332)
        %251 = "ttnn.relu"(%250) : (tensor<1x256x56x56xbf16, #ttnn_layout47>) -> tensor<1x256x56x56xbf16, #ttnn_layout47> loc(#loc333)
        "ttnn.deallocate"(%250) <{force = false}> : (tensor<1x256x56x56xbf16, #ttnn_layout47>) -> () loc(#loc333)
        %252 = "ttnn.permute"(%251) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x256x56x56xbf16, #ttnn_layout47>) -> tensor<1x56x56x256xbf16, #ttnn_layout46> loc(#loc333)
        %253 = "ttnn.reshape"(%252) <{shape = [1 : i32, 1 : i32, 3136 : i32, 256 : i32]}> : (tensor<1x56x56x256xbf16, #ttnn_layout46>) -> tensor<1x1x3136x256xbf16, #ttnn_layout45> loc(#loc333)
        "ttnn.deallocate"(%252) <{force = false}> : (tensor<1x56x56x256xbf16, #ttnn_layout46>) -> () loc(#loc333)
        %254 = "ttnn.to_layout"(%253) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x3136x256xbf16, #ttnn_layout45>) -> tensor<1x1x3136x256xbf16, #ttnn_layout48> loc(#loc454)
        "ttnn.deallocate"(%253) <{force = false}> : (tensor<1x1x3136x256xbf16, #ttnn_layout45>) -> () loc(#loc454)
        %255 = "ttnn.conv2d"(%254, %arg57, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 128 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x3136x256xbf16, #ttnn_layout48>, tensor<128x256x1x1xbf16, #ttnn_layout23>, !ttnn.device) -> tensor<1x1x3136x128xbf16, #ttnn_layout41> loc(#loc334)
        "ttnn.deallocate"(%254) <{force = false}> : (tensor<1x1x3136x256xbf16, #ttnn_layout48>) -> () loc(#loc334)
        "ttnn.deallocate"(%arg57) <{force = false}> : (tensor<128x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc334)
        %256 = "ttnn.reshape"(%255) <{shape = [1 : i32, 56 : i32, 56 : i32, 128 : i32]}> : (tensor<1x1x3136x128xbf16, #ttnn_layout41>) -> tensor<1x56x56x128xbf16, #ttnn_layout42> loc(#loc455)
        "ttnn.deallocate"(%255) <{force = false}> : (tensor<1x1x3136x128xbf16, #ttnn_layout41>) -> () loc(#loc455)
        %257 = "ttnn.permute"(%256) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x56x56x128xbf16, #ttnn_layout42>) -> tensor<1x128x56x56xbf16, #ttnn_layout43> loc(#loc334)
        "ttnn.deallocate"(%256) <{force = false}> : (tensor<1x56x56x128xbf16, #ttnn_layout42>) -> () loc(#loc334)
        %258 = "ttnn.batch_norm_inference"(%257, %27, %137, %141, %82) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x128x56x56xbf16, #ttnn_layout43>, tensor<1x128x1x1xbf16, #ttnn_layout5>, tensor<1x128x1x1xbf16, #ttnn_layout5>, tensor<1x128x1x1xbf16, #ttnn_layout5>, tensor<1x128x1x1xbf16, #ttnn_layout5>) -> tensor<1x128x56x56xbf16, #ttnn_layout43> loc(#loc23)
        "ttnn.deallocate"(%257) <{force = false}> : (tensor<1x128x56x56xbf16, #ttnn_layout43>) -> () loc(#loc23)
        "ttnn.deallocate"(%141) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout5>) -> () loc(#loc23)
        "ttnn.deallocate"(%137) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout5>) -> () loc(#loc23)
        "ttnn.deallocate"(%82) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout5>) -> () loc(#loc23)
        "ttnn.deallocate"(%27) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout5>) -> () loc(#loc23)
        %259 = "ttnn.relu"(%258) : (tensor<1x128x56x56xbf16, #ttnn_layout43>) -> tensor<1x128x56x56xbf16, #ttnn_layout43> loc(#loc335)
        "ttnn.deallocate"(%258) <{force = false}> : (tensor<1x128x56x56xbf16, #ttnn_layout43>) -> () loc(#loc335)
        %260 = "ttnn.permute"(%259) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x128x56x56xbf16, #ttnn_layout43>) -> tensor<1x56x56x128xbf16, #ttnn_layout42> loc(#loc335)
        "ttnn.deallocate"(%259) <{force = false}> : (tensor<1x128x56x56xbf16, #ttnn_layout43>) -> () loc(#loc335)
        %261 = "ttnn.reshape"(%260) <{shape = [1 : i32, 1 : i32, 3136 : i32, 128 : i32]}> : (tensor<1x56x56x128xbf16, #ttnn_layout42>) -> tensor<1x1x3136x128xbf16, #ttnn_layout41> loc(#loc335)
        "ttnn.deallocate"(%260) <{force = false}> : (tensor<1x56x56x128xbf16, #ttnn_layout42>) -> () loc(#loc335)
        %262 = "ttnn.to_layout"(%261) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x3136x128xbf16, #ttnn_layout41>) -> tensor<1x1x3136x128xbf16, #ttnn_layout44> loc(#loc456)
        "ttnn.deallocate"(%261) <{force = false}> : (tensor<1x1x3136x128xbf16, #ttnn_layout41>) -> () loc(#loc456)
        %263 = "ttnn.conv2d"(%262, %arg52, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 128 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 3, 3>, out_channels = 128 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x3136x128xbf16, #ttnn_layout44>, tensor<128x128x3x3xbf16, #ttnn_layout21>, !ttnn.device) -> tensor<1x1x3136x128xbf16, #ttnn_layout41> loc(#loc336)
        "ttnn.deallocate"(%262) <{force = false}> : (tensor<1x1x3136x128xbf16, #ttnn_layout44>) -> () loc(#loc336)
        "ttnn.deallocate"(%arg52) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout21>) -> () loc(#loc336)
        %264 = "ttnn.reshape"(%263) <{shape = [1 : i32, 56 : i32, 56 : i32, 128 : i32]}> : (tensor<1x1x3136x128xbf16, #ttnn_layout41>) -> tensor<1x56x56x128xbf16, #ttnn_layout42> loc(#loc457)
        "ttnn.deallocate"(%263) <{force = false}> : (tensor<1x1x3136x128xbf16, #ttnn_layout41>) -> () loc(#loc457)
        %265 = "ttnn.permute"(%264) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x56x56x128xbf16, #ttnn_layout42>) -> tensor<1x128x56x56xbf16, #ttnn_layout43> loc(#loc336)
        "ttnn.deallocate"(%264) <{force = false}> : (tensor<1x56x56x128xbf16, #ttnn_layout42>) -> () loc(#loc336)
        %266 = "ttnn.batch_norm_inference"(%265, %193, %171, %190, %185) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x128x56x56xbf16, #ttnn_layout43>, tensor<1x128x1x1xbf16, #ttnn_layout5>, tensor<1x128x1x1xbf16, #ttnn_layout5>, tensor<1x128x1x1xbf16, #ttnn_layout5>, tensor<1x128x1x1xbf16, #ttnn_layout5>) -> tensor<1x128x56x56xbf16, #ttnn_layout43> loc(#loc54)
        "ttnn.deallocate"(%265) <{force = false}> : (tensor<1x128x56x56xbf16, #ttnn_layout43>) -> () loc(#loc54)
        "ttnn.deallocate"(%193) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout5>) -> () loc(#loc54)
        "ttnn.deallocate"(%190) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout5>) -> () loc(#loc54)
        "ttnn.deallocate"(%185) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout5>) -> () loc(#loc54)
        "ttnn.deallocate"(%171) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout5>) -> () loc(#loc54)
        %267 = "ttnn.relu"(%266) : (tensor<1x128x56x56xbf16, #ttnn_layout43>) -> tensor<1x128x56x56xbf16, #ttnn_layout43> loc(#loc337)
        "ttnn.deallocate"(%266) <{force = false}> : (tensor<1x128x56x56xbf16, #ttnn_layout43>) -> () loc(#loc337)
        %268 = "ttnn.permute"(%267) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x128x56x56xbf16, #ttnn_layout43>) -> tensor<1x56x56x128xbf16, #ttnn_layout42> loc(#loc337)
        "ttnn.deallocate"(%267) <{force = false}> : (tensor<1x128x56x56xbf16, #ttnn_layout43>) -> () loc(#loc337)
        %269 = "ttnn.reshape"(%268) <{shape = [1 : i32, 1 : i32, 3136 : i32, 128 : i32]}> : (tensor<1x56x56x128xbf16, #ttnn_layout42>) -> tensor<1x1x3136x128xbf16, #ttnn_layout41> loc(#loc337)
        "ttnn.deallocate"(%268) <{force = false}> : (tensor<1x56x56x128xbf16, #ttnn_layout42>) -> () loc(#loc337)
        %270 = "ttnn.to_layout"(%269) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x3136x128xbf16, #ttnn_layout41>) -> tensor<1x1x3136x128xbf16, #ttnn_layout44> loc(#loc458)
        "ttnn.deallocate"(%269) <{force = false}> : (tensor<1x1x3136x128xbf16, #ttnn_layout41>) -> () loc(#loc458)
        %271 = "ttnn.conv2d"(%270, %arg47, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 128 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x3136x128xbf16, #ttnn_layout44>, tensor<256x128x1x1xbf16, #ttnn_layout20>, !ttnn.device) -> tensor<1x1x3136x256xbf16, #ttnn_layout45> loc(#loc338)
        "ttnn.deallocate"(%270) <{force = false}> : (tensor<1x1x3136x128xbf16, #ttnn_layout44>) -> () loc(#loc338)
        "ttnn.deallocate"(%arg47) <{force = false}> : (tensor<256x128x1x1xbf16, #ttnn_layout20>) -> () loc(#loc338)
        %272 = "ttnn.reshape"(%271) <{shape = [1 : i32, 56 : i32, 56 : i32, 256 : i32]}> : (tensor<1x1x3136x256xbf16, #ttnn_layout45>) -> tensor<1x56x56x256xbf16, #ttnn_layout46> loc(#loc459)
        "ttnn.deallocate"(%271) <{force = false}> : (tensor<1x1x3136x256xbf16, #ttnn_layout45>) -> () loc(#loc459)
        %273 = "ttnn.permute"(%272) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x56x56x256xbf16, #ttnn_layout46>) -> tensor<1x256x56x56xbf16, #ttnn_layout47> loc(#loc338)
        "ttnn.deallocate"(%272) <{force = false}> : (tensor<1x56x56x256xbf16, #ttnn_layout46>) -> () loc(#loc338)
        %274 = "ttnn.batch_norm_inference"(%273, %153, %11, %94, %77) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x256x56x56xbf16, #ttnn_layout47>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>) -> tensor<1x256x56x56xbf16, #ttnn_layout47> loc(#loc12)
        "ttnn.deallocate"(%273) <{force = false}> : (tensor<1x256x56x56xbf16, #ttnn_layout47>) -> () loc(#loc12)
        "ttnn.deallocate"(%153) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc12)
        "ttnn.deallocate"(%94) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc12)
        "ttnn.deallocate"(%77) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc12)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc12)
        %275 = "ttnn.add"(%274, %251) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x56x56xbf16, #ttnn_layout47>, tensor<1x256x56x56xbf16, #ttnn_layout47>) -> tensor<1x256x56x56xbf16, #ttnn_layout47> loc(#loc339)
        "ttnn.deallocate"(%274) <{force = false}> : (tensor<1x256x56x56xbf16, #ttnn_layout47>) -> () loc(#loc339)
        "ttnn.deallocate"(%251) <{force = false}> : (tensor<1x256x56x56xbf16, #ttnn_layout47>) -> () loc(#loc339)
        %276 = "ttnn.relu"(%275) : (tensor<1x256x56x56xbf16, #ttnn_layout47>) -> tensor<1x256x56x56xbf16, #ttnn_layout47> loc(#loc340)
        "ttnn.deallocate"(%275) <{force = false}> : (tensor<1x256x56x56xbf16, #ttnn_layout47>) -> () loc(#loc340)
        %277 = "ttnn.permute"(%276) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x256x56x56xbf16, #ttnn_layout47>) -> tensor<1x56x56x256xbf16, #ttnn_layout46> loc(#loc340)
        %278 = "ttnn.reshape"(%277) <{shape = [1 : i32, 1 : i32, 3136 : i32, 256 : i32]}> : (tensor<1x56x56x256xbf16, #ttnn_layout46>) -> tensor<1x1x3136x256xbf16, #ttnn_layout45> loc(#loc340)
        "ttnn.deallocate"(%277) <{force = false}> : (tensor<1x56x56x256xbf16, #ttnn_layout46>) -> () loc(#loc340)
        %279 = "ttnn.to_layout"(%278) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x3136x256xbf16, #ttnn_layout45>) -> tensor<1x1x3136x256xbf16, #ttnn_layout48> loc(#loc460)
        "ttnn.deallocate"(%278) <{force = false}> : (tensor<1x1x3136x256xbf16, #ttnn_layout45>) -> () loc(#loc460)
        %280 = "ttnn.conv2d"(%279, %arg72, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 128 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x3136x256xbf16, #ttnn_layout48>, tensor<128x256x1x1xbf16, #ttnn_layout23>, !ttnn.device) -> tensor<1x1x3136x128xbf16, #ttnn_layout41> loc(#loc341)
        "ttnn.deallocate"(%279) <{force = false}> : (tensor<1x1x3136x256xbf16, #ttnn_layout48>) -> () loc(#loc341)
        "ttnn.deallocate"(%arg72) <{force = false}> : (tensor<128x256x1x1xbf16, #ttnn_layout23>) -> () loc(#loc341)
        %281 = "ttnn.reshape"(%280) <{shape = [1 : i32, 56 : i32, 56 : i32, 128 : i32]}> : (tensor<1x1x3136x128xbf16, #ttnn_layout41>) -> tensor<1x56x56x128xbf16, #ttnn_layout42> loc(#loc461)
        "ttnn.deallocate"(%280) <{force = false}> : (tensor<1x1x3136x128xbf16, #ttnn_layout41>) -> () loc(#loc461)
        %282 = "ttnn.permute"(%281) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x56x56x128xbf16, #ttnn_layout42>) -> tensor<1x128x56x56xbf16, #ttnn_layout43> loc(#loc341)
        "ttnn.deallocate"(%281) <{force = false}> : (tensor<1x56x56x128xbf16, #ttnn_layout42>) -> () loc(#loc341)
        %283 = "ttnn.batch_norm_inference"(%282, %101, %164, %168, %142) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x128x56x56xbf16, #ttnn_layout43>, tensor<1x128x1x1xbf16, #ttnn_layout5>, tensor<1x128x1x1xbf16, #ttnn_layout5>, tensor<1x128x1x1xbf16, #ttnn_layout5>, tensor<1x128x1x1xbf16, #ttnn_layout5>) -> tensor<1x128x56x56xbf16, #ttnn_layout43> loc(#loc50)
        "ttnn.deallocate"(%282) <{force = false}> : (tensor<1x128x56x56xbf16, #ttnn_layout43>) -> () loc(#loc50)
        "ttnn.deallocate"(%168) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout5>) -> () loc(#loc50)
        "ttnn.deallocate"(%164) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout5>) -> () loc(#loc50)
        "ttnn.deallocate"(%142) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout5>) -> () loc(#loc50)
        "ttnn.deallocate"(%101) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout5>) -> () loc(#loc50)
        %284 = "ttnn.relu"(%283) : (tensor<1x128x56x56xbf16, #ttnn_layout43>) -> tensor<1x128x56x56xbf16, #ttnn_layout43> loc(#loc342)
        "ttnn.deallocate"(%283) <{force = false}> : (tensor<1x128x56x56xbf16, #ttnn_layout43>) -> () loc(#loc342)
        %285 = "ttnn.permute"(%284) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x128x56x56xbf16, #ttnn_layout43>) -> tensor<1x56x56x128xbf16, #ttnn_layout42> loc(#loc342)
        "ttnn.deallocate"(%284) <{force = false}> : (tensor<1x128x56x56xbf16, #ttnn_layout43>) -> () loc(#loc342)
        %286 = "ttnn.reshape"(%285) <{shape = [1 : i32, 1 : i32, 3136 : i32, 128 : i32]}> : (tensor<1x56x56x128xbf16, #ttnn_layout42>) -> tensor<1x1x3136x128xbf16, #ttnn_layout41> loc(#loc342)
        "ttnn.deallocate"(%285) <{force = false}> : (tensor<1x56x56x128xbf16, #ttnn_layout42>) -> () loc(#loc342)
        %287 = "ttnn.to_layout"(%286) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x3136x128xbf16, #ttnn_layout41>) -> tensor<1x1x3136x128xbf16, #ttnn_layout44> loc(#loc462)
        "ttnn.deallocate"(%286) <{force = false}> : (tensor<1x1x3136x128xbf16, #ttnn_layout41>) -> () loc(#loc462)
        %288 = "ttnn.conv2d"(%287, %arg67, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 128 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 3, 3>, out_channels = 128 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x3136x128xbf16, #ttnn_layout44>, tensor<128x128x3x3xbf16, #ttnn_layout21>, !ttnn.device) -> tensor<1x1x3136x128xbf16, #ttnn_layout41> loc(#loc343)
        "ttnn.deallocate"(%287) <{force = false}> : (tensor<1x1x3136x128xbf16, #ttnn_layout44>) -> () loc(#loc343)
        "ttnn.deallocate"(%arg67) <{force = false}> : (tensor<128x128x3x3xbf16, #ttnn_layout21>) -> () loc(#loc343)
        %289 = "ttnn.reshape"(%288) <{shape = [1 : i32, 56 : i32, 56 : i32, 128 : i32]}> : (tensor<1x1x3136x128xbf16, #ttnn_layout41>) -> tensor<1x56x56x128xbf16, #ttnn_layout42> loc(#loc463)
        "ttnn.deallocate"(%288) <{force = false}> : (tensor<1x1x3136x128xbf16, #ttnn_layout41>) -> () loc(#loc463)
        %290 = "ttnn.permute"(%289) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x56x56x128xbf16, #ttnn_layout42>) -> tensor<1x128x56x56xbf16, #ttnn_layout43> loc(#loc343)
        "ttnn.deallocate"(%289) <{force = false}> : (tensor<1x56x56x128xbf16, #ttnn_layout42>) -> () loc(#loc343)
        %291 = "ttnn.batch_norm_inference"(%290, %149, %2, %105, %115) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x128x56x56xbf16, #ttnn_layout43>, tensor<1x128x1x1xbf16, #ttnn_layout5>, tensor<1x128x1x1xbf16, #ttnn_layout5>, tensor<1x128x1x1xbf16, #ttnn_layout5>, tensor<1x128x1x1xbf16, #ttnn_layout5>) -> tensor<1x128x56x56xbf16, #ttnn_layout43> loc(#loc3)
        "ttnn.deallocate"(%290) <{force = false}> : (tensor<1x128x56x56xbf16, #ttnn_layout43>) -> () loc(#loc3)
        "ttnn.deallocate"(%149) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout5>) -> () loc(#loc3)
        "ttnn.deallocate"(%115) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout5>) -> () loc(#loc3)
        "ttnn.deallocate"(%105) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout5>) -> () loc(#loc3)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x128x1x1xbf16, #ttnn_layout5>) -> () loc(#loc3)
        %292 = "ttnn.relu"(%291) : (tensor<1x128x56x56xbf16, #ttnn_layout43>) -> tensor<1x128x56x56xbf16, #ttnn_layout43> loc(#loc344)
        "ttnn.deallocate"(%291) <{force = false}> : (tensor<1x128x56x56xbf16, #ttnn_layout43>) -> () loc(#loc344)
        %293 = "ttnn.permute"(%292) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x128x56x56xbf16, #ttnn_layout43>) -> tensor<1x56x56x128xbf16, #ttnn_layout42> loc(#loc344)
        "ttnn.deallocate"(%292) <{force = false}> : (tensor<1x128x56x56xbf16, #ttnn_layout43>) -> () loc(#loc344)
        %294 = "ttnn.reshape"(%293) <{shape = [1 : i32, 1 : i32, 3136 : i32, 128 : i32]}> : (tensor<1x56x56x128xbf16, #ttnn_layout42>) -> tensor<1x1x3136x128xbf16, #ttnn_layout41> loc(#loc344)
        "ttnn.deallocate"(%293) <{force = false}> : (tensor<1x56x56x128xbf16, #ttnn_layout42>) -> () loc(#loc344)
        %295 = "ttnn.to_layout"(%294) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x3136x128xbf16, #ttnn_layout41>) -> tensor<1x1x3136x128xbf16, #ttnn_layout44> loc(#loc464)
        "ttnn.deallocate"(%294) <{force = false}> : (tensor<1x1x3136x128xbf16, #ttnn_layout41>) -> () loc(#loc464)
        %296 = "ttnn.conv2d"(%295, %arg62, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 128 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x3136x128xbf16, #ttnn_layout44>, tensor<256x128x1x1xbf16, #ttnn_layout20>, !ttnn.device) -> tensor<1x1x3136x256xbf16, #ttnn_layout45> loc(#loc345)
        "ttnn.deallocate"(%295) <{force = false}> : (tensor<1x1x3136x128xbf16, #ttnn_layout44>) -> () loc(#loc345)
        "ttnn.deallocate"(%arg62) <{force = false}> : (tensor<256x128x1x1xbf16, #ttnn_layout20>) -> () loc(#loc345)
        %297 = "ttnn.reshape"(%296) <{shape = [1 : i32, 56 : i32, 56 : i32, 256 : i32]}> : (tensor<1x1x3136x256xbf16, #ttnn_layout45>) -> tensor<1x56x56x256xbf16, #ttnn_layout46> loc(#loc465)
        "ttnn.deallocate"(%296) <{force = false}> : (tensor<1x1x3136x256xbf16, #ttnn_layout45>) -> () loc(#loc465)
        %298 = "ttnn.permute"(%297) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x56x56x256xbf16, #ttnn_layout46>) -> tensor<1x256x56x56xbf16, #ttnn_layout47> loc(#loc345)
        "ttnn.deallocate"(%297) <{force = false}> : (tensor<1x56x56x256xbf16, #ttnn_layout46>) -> () loc(#loc345)
        %299 = "ttnn.batch_norm_inference"(%298, %201, %26, %37, %211) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x256x56x56xbf16, #ttnn_layout47>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>) -> tensor<1x256x56x56xbf16, #ttnn_layout47> loc(#loc22)
        "ttnn.deallocate"(%298) <{force = false}> : (tensor<1x256x56x56xbf16, #ttnn_layout47>) -> () loc(#loc22)
        "ttnn.deallocate"(%211) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc22)
        "ttnn.deallocate"(%201) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc22)
        "ttnn.deallocate"(%37) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc22)
        "ttnn.deallocate"(%26) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc22)
        %300 = "ttnn.add"(%299, %276) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x256x56x56xbf16, #ttnn_layout47>, tensor<1x256x56x56xbf16, #ttnn_layout47>) -> tensor<1x256x56x56xbf16, #ttnn_layout47> loc(#loc346)
        "ttnn.deallocate"(%299) <{force = false}> : (tensor<1x256x56x56xbf16, #ttnn_layout47>) -> () loc(#loc346)
        "ttnn.deallocate"(%276) <{force = false}> : (tensor<1x256x56x56xbf16, #ttnn_layout47>) -> () loc(#loc346)
        %301 = "ttnn.relu"(%300) : (tensor<1x256x56x56xbf16, #ttnn_layout47>) -> tensor<1x256x56x56xbf16, #ttnn_layout47> loc(#loc347)
        "ttnn.deallocate"(%300) <{force = false}> : (tensor<1x256x56x56xbf16, #ttnn_layout47>) -> () loc(#loc347)
        %302 = "ttnn.permute"(%301) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x256x56x56xbf16, #ttnn_layout47>) -> tensor<1x56x56x256xbf16, #ttnn_layout46> loc(#loc347)
        "ttnn.deallocate"(%301) <{force = false}> : (tensor<1x256x56x56xbf16, #ttnn_layout47>) -> () loc(#loc347)
        %303 = "ttnn.reshape"(%302) <{shape = [1 : i32, 1 : i32, 3136 : i32, 256 : i32]}> : (tensor<1x56x56x256xbf16, #ttnn_layout46>) -> tensor<1x1x3136x256xbf16, #ttnn_layout45> loc(#loc347)
        "ttnn.deallocate"(%302) <{force = false}> : (tensor<1x56x56x256xbf16, #ttnn_layout46>) -> () loc(#loc347)
        %304 = "ttnn.to_layout"(%303) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x3136x256xbf16, #ttnn_layout45>) -> tensor<1x1x3136x256xbf16, #ttnn_layout48> loc(#loc466)
        %305 = "ttnn.conv2d"(%304, %arg87, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x3136x256xbf16, #ttnn_layout48>, tensor<256x256x1x1xbf16, #ttnn_layout25>, !ttnn.device) -> tensor<1x1x3136x256xbf16, #ttnn_layout45> loc(#loc348)
        "ttnn.deallocate"(%304) <{force = false}> : (tensor<1x1x3136x256xbf16, #ttnn_layout48>) -> () loc(#loc348)
        "ttnn.deallocate"(%arg87) <{force = false}> : (tensor<256x256x1x1xbf16, #ttnn_layout25>) -> () loc(#loc348)
        %306 = "ttnn.reshape"(%305) <{shape = [1 : i32, 56 : i32, 56 : i32, 256 : i32]}> : (tensor<1x1x3136x256xbf16, #ttnn_layout45>) -> tensor<1x56x56x256xbf16, #ttnn_layout46> loc(#loc467)
        "ttnn.deallocate"(%305) <{force = false}> : (tensor<1x1x3136x256xbf16, #ttnn_layout45>) -> () loc(#loc467)
        %307 = "ttnn.permute"(%306) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x56x56x256xbf16, #ttnn_layout46>) -> tensor<1x256x56x56xbf16, #ttnn_layout47> loc(#loc348)
        "ttnn.deallocate"(%306) <{force = false}> : (tensor<1x56x56x256xbf16, #ttnn_layout46>) -> () loc(#loc348)
        %308 = "ttnn.batch_norm_inference"(%307, %138, %192, %174, %74) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x256x56x56xbf16, #ttnn_layout47>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>) -> tensor<1x256x56x56xbf16, #ttnn_layout47> loc(#loc44)
        "ttnn.deallocate"(%307) <{force = false}> : (tensor<1x256x56x56xbf16, #ttnn_layout47>) -> () loc(#loc44)
        "ttnn.deallocate"(%192) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc44)
        "ttnn.deallocate"(%174) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc44)
        "ttnn.deallocate"(%138) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc44)
        "ttnn.deallocate"(%74) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc44)
        %309 = "ttnn.relu"(%308) : (tensor<1x256x56x56xbf16, #ttnn_layout47>) -> tensor<1x256x56x56xbf16, #ttnn_layout47> loc(#loc349)
        "ttnn.deallocate"(%308) <{force = false}> : (tensor<1x256x56x56xbf16, #ttnn_layout47>) -> () loc(#loc349)
        %310 = "ttnn.permute"(%309) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x256x56x56xbf16, #ttnn_layout47>) -> tensor<1x56x56x256xbf16, #ttnn_layout46> loc(#loc349)
        "ttnn.deallocate"(%309) <{force = false}> : (tensor<1x256x56x56xbf16, #ttnn_layout47>) -> () loc(#loc349)
        %311 = "ttnn.reshape"(%310) <{shape = [1 : i32, 1 : i32, 3136 : i32, 256 : i32]}> : (tensor<1x56x56x256xbf16, #ttnn_layout46>) -> tensor<1x1x3136x256xbf16, #ttnn_layout45> loc(#loc349)
        "ttnn.deallocate"(%310) <{force = false}> : (tensor<1x56x56x256xbf16, #ttnn_layout46>) -> () loc(#loc349)
        %312 = "ttnn.to_layout"(%311) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x3136x256xbf16, #ttnn_layout45>) -> tensor<1x1x3136x256xbf16, #ttnn_layout48> loc(#loc468)
        "ttnn.deallocate"(%311) <{force = false}> : (tensor<1x1x3136x256xbf16, #ttnn_layout45>) -> () loc(#loc468)
        %313 = "ttnn.conv2d"(%312, %arg82, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 3, 3>, out_channels = 256 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 2, 2>}> : (tensor<1x1x3136x256xbf16, #ttnn_layout48>, tensor<256x256x3x3xbf16, #ttnn_layout24>, !ttnn.device) -> tensor<1x1x784x256xbf16, #ttnn_layout49> loc(#loc350)
        "ttnn.deallocate"(%312) <{force = false}> : (tensor<1x1x3136x256xbf16, #ttnn_layout48>) -> () loc(#loc350)
        "ttnn.deallocate"(%arg82) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout24>) -> () loc(#loc350)
        %314 = "ttnn.reshape"(%313) <{shape = [1 : i32, 28 : i32, 28 : i32, 256 : i32]}> : (tensor<1x1x784x256xbf16, #ttnn_layout49>) -> tensor<1x28x28x256xbf16, #ttnn_layout50> loc(#loc469)
        "ttnn.deallocate"(%313) <{force = false}> : (tensor<1x1x784x256xbf16, #ttnn_layout49>) -> () loc(#loc469)
        %315 = "ttnn.permute"(%314) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x28x28x256xbf16, #ttnn_layout50>) -> tensor<1x256x28x28xbf16, #ttnn_layout7> loc(#loc350)
        "ttnn.deallocate"(%314) <{force = false}> : (tensor<1x28x28x256xbf16, #ttnn_layout50>) -> () loc(#loc350)
        %316 = "ttnn.batch_norm_inference"(%315, %104, %4, %16, %126) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>) -> tensor<1x256x28x28xbf16, #ttnn_layout7> loc(#loc5)
        "ttnn.deallocate"(%315) <{force = false}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> () loc(#loc5)
        "ttnn.deallocate"(%126) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc5)
        "ttnn.deallocate"(%104) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc5)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc5)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc5)
        %317 = "ttnn.relu"(%316) : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> tensor<1x256x28x28xbf16, #ttnn_layout7> loc(#loc351)
        "ttnn.deallocate"(%316) <{force = false}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> () loc(#loc351)
        %318 = "ttnn.permute"(%317) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> tensor<1x28x28x256xbf16, #ttnn_layout50> loc(#loc351)
        "ttnn.deallocate"(%317) <{force = false}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> () loc(#loc351)
        %319 = "ttnn.reshape"(%318) <{shape = [1 : i32, 1 : i32, 784 : i32, 256 : i32]}> : (tensor<1x28x28x256xbf16, #ttnn_layout50>) -> tensor<1x1x784x256xbf16, #ttnn_layout49> loc(#loc351)
        "ttnn.deallocate"(%318) <{force = false}> : (tensor<1x28x28x256xbf16, #ttnn_layout50>) -> () loc(#loc351)
        %320 = "ttnn.to_layout"(%319) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x784x256xbf16, #ttnn_layout49>) -> tensor<1x1x784x256xbf16, #ttnn_layout51> loc(#loc470)
        "ttnn.deallocate"(%319) <{force = false}> : (tensor<1x1x784x256xbf16, #ttnn_layout49>) -> () loc(#loc470)
        %321 = "ttnn.conv2d"(%320, %arg77, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x256xbf16, #ttnn_layout51>, tensor<512x256x1x1xbf16, #ttnn_layout16>, !ttnn.device) -> tensor<1x1x784x512xbf16, #ttnn_layout52> loc(#loc352)
        "ttnn.deallocate"(%320) <{force = false}> : (tensor<1x1x784x256xbf16, #ttnn_layout51>) -> () loc(#loc352)
        "ttnn.deallocate"(%arg77) <{force = false}> : (tensor<512x256x1x1xbf16, #ttnn_layout16>) -> () loc(#loc352)
        %322 = "ttnn.reshape"(%321) <{shape = [1 : i32, 28 : i32, 28 : i32, 512 : i32]}> : (tensor<1x1x784x512xbf16, #ttnn_layout52>) -> tensor<1x28x28x512xbf16, #ttnn_layout53> loc(#loc471)
        "ttnn.deallocate"(%321) <{force = false}> : (tensor<1x1x784x512xbf16, #ttnn_layout52>) -> () loc(#loc471)
        %323 = "ttnn.permute"(%322) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x28x28x512xbf16, #ttnn_layout53>) -> tensor<1x512x28x28xbf16, #ttnn_layout3> loc(#loc352)
        "ttnn.deallocate"(%322) <{force = false}> : (tensor<1x28x28x512xbf16, #ttnn_layout53>) -> () loc(#loc352)
        %324 = "ttnn.batch_norm_inference"(%323, %97, %72, %176, %42) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>) -> tensor<1x512x28x28xbf16, #ttnn_layout3> loc(#loc33)
        "ttnn.deallocate"(%323) <{force = false}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>) -> () loc(#loc33)
        "ttnn.deallocate"(%176) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc33)
        "ttnn.deallocate"(%97) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc33)
        "ttnn.deallocate"(%72) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc33)
        "ttnn.deallocate"(%42) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc33)
        %325 = "ttnn.to_layout"(%303) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x3136x256xbf16, #ttnn_layout45>) -> tensor<1x1x3136x256xbf16, #ttnn_layout48> loc(#loc472)
        "ttnn.deallocate"(%303) <{force = false}> : (tensor<1x1x3136x256xbf16, #ttnn_layout45>) -> () loc(#loc472)
        %326 = "ttnn.conv2d"(%325, %arg16, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x3136x256xbf16, #ttnn_layout48>, tensor<512x256x1x1xbf16, #ttnn_layout16>, !ttnn.device) -> tensor<1x1x784x512xbf16, #ttnn_layout52> loc(#loc353)
        "ttnn.deallocate"(%325) <{force = false}> : (tensor<1x1x3136x256xbf16, #ttnn_layout48>) -> () loc(#loc353)
        "ttnn.deallocate"(%arg16) <{force = false}> : (tensor<512x256x1x1xbf16, #ttnn_layout16>) -> () loc(#loc353)
        %327 = "ttnn.reshape"(%326) <{shape = [1 : i32, 28 : i32, 28 : i32, 512 : i32]}> : (tensor<1x1x784x512xbf16, #ttnn_layout52>) -> tensor<1x28x28x512xbf16, #ttnn_layout53> loc(#loc473)
        "ttnn.deallocate"(%326) <{force = false}> : (tensor<1x1x784x512xbf16, #ttnn_layout52>) -> () loc(#loc473)
        %328 = "ttnn.permute"(%327) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x28x28x512xbf16, #ttnn_layout53>) -> tensor<1x512x28x28xbf16, #ttnn_layout3> loc(#loc353)
        "ttnn.deallocate"(%327) <{force = false}> : (tensor<1x28x28x512xbf16, #ttnn_layout53>) -> () loc(#loc353)
        %329 = "ttnn.batch_norm_inference"(%328, %15, %46, %98, %76) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>) -> tensor<1x512x28x28xbf16, #ttnn_layout3> loc(#loc16)
        "ttnn.deallocate"(%328) <{force = false}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>) -> () loc(#loc16)
        "ttnn.deallocate"(%98) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc16)
        "ttnn.deallocate"(%76) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc16)
        "ttnn.deallocate"(%46) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc16)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc16)
        %330 = "ttnn.add"(%324, %329) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>, tensor<1x512x28x28xbf16, #ttnn_layout3>) -> tensor<1x512x28x28xbf16, #ttnn_layout3> loc(#loc354)
        "ttnn.deallocate"(%329) <{force = false}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>) -> () loc(#loc354)
        "ttnn.deallocate"(%324) <{force = false}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>) -> () loc(#loc354)
        %331 = "ttnn.relu"(%330) : (tensor<1x512x28x28xbf16, #ttnn_layout3>) -> tensor<1x512x28x28xbf16, #ttnn_layout3> loc(#loc355)
        "ttnn.deallocate"(%330) <{force = false}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>) -> () loc(#loc355)
        %332 = "ttnn.permute"(%331) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>) -> tensor<1x28x28x512xbf16, #ttnn_layout53> loc(#loc355)
        %333 = "ttnn.reshape"(%332) <{shape = [1 : i32, 1 : i32, 784 : i32, 512 : i32]}> : (tensor<1x28x28x512xbf16, #ttnn_layout53>) -> tensor<1x1x784x512xbf16, #ttnn_layout52> loc(#loc355)
        "ttnn.deallocate"(%332) <{force = false}> : (tensor<1x28x28x512xbf16, #ttnn_layout53>) -> () loc(#loc355)
        %334 = "ttnn.to_layout"(%333) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x784x512xbf16, #ttnn_layout52>) -> tensor<1x1x784x512xbf16, #ttnn_layout54> loc(#loc474)
        "ttnn.deallocate"(%333) <{force = false}> : (tensor<1x1x784x512xbf16, #ttnn_layout52>) -> () loc(#loc474)
        %335 = "ttnn.conv2d"(%334, %arg102, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 512 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x512xbf16, #ttnn_layout54>, tensor<256x512x1x1xbf16, #ttnn_layout26>, !ttnn.device) -> tensor<1x1x784x256xbf16, #ttnn_layout49> loc(#loc356)
        "ttnn.deallocate"(%334) <{force = false}> : (tensor<1x1x784x512xbf16, #ttnn_layout54>) -> () loc(#loc356)
        "ttnn.deallocate"(%arg102) <{force = false}> : (tensor<256x512x1x1xbf16, #ttnn_layout26>) -> () loc(#loc356)
        %336 = "ttnn.reshape"(%335) <{shape = [1 : i32, 28 : i32, 28 : i32, 256 : i32]}> : (tensor<1x1x784x256xbf16, #ttnn_layout49>) -> tensor<1x28x28x256xbf16, #ttnn_layout50> loc(#loc475)
        "ttnn.deallocate"(%335) <{force = false}> : (tensor<1x1x784x256xbf16, #ttnn_layout49>) -> () loc(#loc475)
        %337 = "ttnn.permute"(%336) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x28x28x256xbf16, #ttnn_layout50>) -> tensor<1x256x28x28xbf16, #ttnn_layout7> loc(#loc356)
        "ttnn.deallocate"(%336) <{force = false}> : (tensor<1x28x28x256xbf16, #ttnn_layout50>) -> () loc(#loc356)
        %338 = "ttnn.batch_norm_inference"(%337, %33, %143, %18, %39) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>) -> tensor<1x256x28x28xbf16, #ttnn_layout7> loc(#loc18)
        "ttnn.deallocate"(%337) <{force = false}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> () loc(#loc18)
        "ttnn.deallocate"(%143) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc18)
        "ttnn.deallocate"(%39) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc18)
        "ttnn.deallocate"(%33) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc18)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc18)
        %339 = "ttnn.relu"(%338) : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> tensor<1x256x28x28xbf16, #ttnn_layout7> loc(#loc357)
        "ttnn.deallocate"(%338) <{force = false}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> () loc(#loc357)
        %340 = "ttnn.permute"(%339) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> tensor<1x28x28x256xbf16, #ttnn_layout50> loc(#loc357)
        "ttnn.deallocate"(%339) <{force = false}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> () loc(#loc357)
        %341 = "ttnn.reshape"(%340) <{shape = [1 : i32, 1 : i32, 784 : i32, 256 : i32]}> : (tensor<1x28x28x256xbf16, #ttnn_layout50>) -> tensor<1x1x784x256xbf16, #ttnn_layout49> loc(#loc357)
        "ttnn.deallocate"(%340) <{force = false}> : (tensor<1x28x28x256xbf16, #ttnn_layout50>) -> () loc(#loc357)
        %342 = "ttnn.to_layout"(%341) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x784x256xbf16, #ttnn_layout49>) -> tensor<1x1x784x256xbf16, #ttnn_layout51> loc(#loc476)
        "ttnn.deallocate"(%341) <{force = false}> : (tensor<1x1x784x256xbf16, #ttnn_layout49>) -> () loc(#loc476)
        %343 = "ttnn.conv2d"(%342, %arg97, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 256 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x256xbf16, #ttnn_layout51>, tensor<256x256x3x3xbf16, #ttnn_layout24>, !ttnn.device) -> tensor<1x1x784x256xbf16, #ttnn_layout49> loc(#loc358)
        "ttnn.deallocate"(%342) <{force = false}> : (tensor<1x1x784x256xbf16, #ttnn_layout51>) -> () loc(#loc358)
        "ttnn.deallocate"(%arg97) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout24>) -> () loc(#loc358)
        %344 = "ttnn.reshape"(%343) <{shape = [1 : i32, 28 : i32, 28 : i32, 256 : i32]}> : (tensor<1x1x784x256xbf16, #ttnn_layout49>) -> tensor<1x28x28x256xbf16, #ttnn_layout50> loc(#loc477)
        "ttnn.deallocate"(%343) <{force = false}> : (tensor<1x1x784x256xbf16, #ttnn_layout49>) -> () loc(#loc477)
        %345 = "ttnn.permute"(%344) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x28x28x256xbf16, #ttnn_layout50>) -> tensor<1x256x28x28xbf16, #ttnn_layout7> loc(#loc358)
        "ttnn.deallocate"(%344) <{force = false}> : (tensor<1x28x28x256xbf16, #ttnn_layout50>) -> () loc(#loc358)
        %346 = "ttnn.batch_norm_inference"(%345, %150, %61, %202, %3) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>) -> tensor<1x256x28x28xbf16, #ttnn_layout7> loc(#loc4)
        "ttnn.deallocate"(%345) <{force = false}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> () loc(#loc4)
        "ttnn.deallocate"(%202) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc4)
        "ttnn.deallocate"(%150) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc4)
        "ttnn.deallocate"(%61) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc4)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc4)
        %347 = "ttnn.relu"(%346) : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> tensor<1x256x28x28xbf16, #ttnn_layout7> loc(#loc359)
        "ttnn.deallocate"(%346) <{force = false}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> () loc(#loc359)
        %348 = "ttnn.permute"(%347) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> tensor<1x28x28x256xbf16, #ttnn_layout50> loc(#loc359)
        "ttnn.deallocate"(%347) <{force = false}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> () loc(#loc359)
        %349 = "ttnn.reshape"(%348) <{shape = [1 : i32, 1 : i32, 784 : i32, 256 : i32]}> : (tensor<1x28x28x256xbf16, #ttnn_layout50>) -> tensor<1x1x784x256xbf16, #ttnn_layout49> loc(#loc359)
        "ttnn.deallocate"(%348) <{force = false}> : (tensor<1x28x28x256xbf16, #ttnn_layout50>) -> () loc(#loc359)
        %350 = "ttnn.to_layout"(%349) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x784x256xbf16, #ttnn_layout49>) -> tensor<1x1x784x256xbf16, #ttnn_layout51> loc(#loc478)
        "ttnn.deallocate"(%349) <{force = false}> : (tensor<1x1x784x256xbf16, #ttnn_layout49>) -> () loc(#loc478)
        %351 = "ttnn.conv2d"(%350, %arg92, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x256xbf16, #ttnn_layout51>, tensor<512x256x1x1xbf16, #ttnn_layout16>, !ttnn.device) -> tensor<1x1x784x512xbf16, #ttnn_layout52> loc(#loc360)
        "ttnn.deallocate"(%350) <{force = false}> : (tensor<1x1x784x256xbf16, #ttnn_layout51>) -> () loc(#loc360)
        "ttnn.deallocate"(%arg92) <{force = false}> : (tensor<512x256x1x1xbf16, #ttnn_layout16>) -> () loc(#loc360)
        %352 = "ttnn.reshape"(%351) <{shape = [1 : i32, 28 : i32, 28 : i32, 512 : i32]}> : (tensor<1x1x784x512xbf16, #ttnn_layout52>) -> tensor<1x28x28x512xbf16, #ttnn_layout53> loc(#loc479)
        "ttnn.deallocate"(%351) <{force = false}> : (tensor<1x1x784x512xbf16, #ttnn_layout52>) -> () loc(#loc479)
        %353 = "ttnn.permute"(%352) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x28x28x512xbf16, #ttnn_layout53>) -> tensor<1x512x28x28xbf16, #ttnn_layout3> loc(#loc360)
        "ttnn.deallocate"(%352) <{force = false}> : (tensor<1x28x28x512xbf16, #ttnn_layout53>) -> () loc(#loc360)
        %354 = "ttnn.batch_norm_inference"(%353, %57, %92, %1, %25) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>) -> tensor<1x512x28x28xbf16, #ttnn_layout3> loc(#loc2)
        "ttnn.deallocate"(%353) <{force = false}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>) -> () loc(#loc2)
        "ttnn.deallocate"(%92) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc2)
        "ttnn.deallocate"(%57) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc2)
        "ttnn.deallocate"(%25) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc2)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc2)
        %355 = "ttnn.add"(%354, %331) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>, tensor<1x512x28x28xbf16, #ttnn_layout3>) -> tensor<1x512x28x28xbf16, #ttnn_layout3> loc(#loc361)
        "ttnn.deallocate"(%354) <{force = false}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>) -> () loc(#loc361)
        "ttnn.deallocate"(%331) <{force = false}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>) -> () loc(#loc361)
        %356 = "ttnn.relu"(%355) : (tensor<1x512x28x28xbf16, #ttnn_layout3>) -> tensor<1x512x28x28xbf16, #ttnn_layout3> loc(#loc362)
        "ttnn.deallocate"(%355) <{force = false}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>) -> () loc(#loc362)
        %357 = "ttnn.permute"(%356) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>) -> tensor<1x28x28x512xbf16, #ttnn_layout53> loc(#loc362)
        %358 = "ttnn.reshape"(%357) <{shape = [1 : i32, 1 : i32, 784 : i32, 512 : i32]}> : (tensor<1x28x28x512xbf16, #ttnn_layout53>) -> tensor<1x1x784x512xbf16, #ttnn_layout52> loc(#loc362)
        "ttnn.deallocate"(%357) <{force = false}> : (tensor<1x28x28x512xbf16, #ttnn_layout53>) -> () loc(#loc362)
        %359 = "ttnn.to_layout"(%358) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x784x512xbf16, #ttnn_layout52>) -> tensor<1x1x784x512xbf16, #ttnn_layout54> loc(#loc480)
        "ttnn.deallocate"(%358) <{force = false}> : (tensor<1x1x784x512xbf16, #ttnn_layout52>) -> () loc(#loc480)
        %360 = "ttnn.conv2d"(%359, %arg117, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 512 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x512xbf16, #ttnn_layout54>, tensor<256x512x1x1xbf16, #ttnn_layout26>, !ttnn.device) -> tensor<1x1x784x256xbf16, #ttnn_layout49> loc(#loc363)
        "ttnn.deallocate"(%359) <{force = false}> : (tensor<1x1x784x512xbf16, #ttnn_layout54>) -> () loc(#loc363)
        "ttnn.deallocate"(%arg117) <{force = false}> : (tensor<256x512x1x1xbf16, #ttnn_layout26>) -> () loc(#loc363)
        %361 = "ttnn.reshape"(%360) <{shape = [1 : i32, 28 : i32, 28 : i32, 256 : i32]}> : (tensor<1x1x784x256xbf16, #ttnn_layout49>) -> tensor<1x28x28x256xbf16, #ttnn_layout50> loc(#loc481)
        "ttnn.deallocate"(%360) <{force = false}> : (tensor<1x1x784x256xbf16, #ttnn_layout49>) -> () loc(#loc481)
        %362 = "ttnn.permute"(%361) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x28x28x256xbf16, #ttnn_layout50>) -> tensor<1x256x28x28xbf16, #ttnn_layout7> loc(#loc363)
        "ttnn.deallocate"(%361) <{force = false}> : (tensor<1x28x28x256xbf16, #ttnn_layout50>) -> () loc(#loc363)
        %363 = "ttnn.batch_norm_inference"(%362, %178, %69, %73, %140) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>) -> tensor<1x256x28x28xbf16, #ttnn_layout7> loc(#loc42)
        "ttnn.deallocate"(%362) <{force = false}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> () loc(#loc42)
        "ttnn.deallocate"(%178) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc42)
        "ttnn.deallocate"(%140) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc42)
        "ttnn.deallocate"(%73) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc42)
        "ttnn.deallocate"(%69) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc42)
        %364 = "ttnn.relu"(%363) : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> tensor<1x256x28x28xbf16, #ttnn_layout7> loc(#loc364)
        "ttnn.deallocate"(%363) <{force = false}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> () loc(#loc364)
        %365 = "ttnn.permute"(%364) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> tensor<1x28x28x256xbf16, #ttnn_layout50> loc(#loc364)
        "ttnn.deallocate"(%364) <{force = false}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> () loc(#loc364)
        %366 = "ttnn.reshape"(%365) <{shape = [1 : i32, 1 : i32, 784 : i32, 256 : i32]}> : (tensor<1x28x28x256xbf16, #ttnn_layout50>) -> tensor<1x1x784x256xbf16, #ttnn_layout49> loc(#loc364)
        "ttnn.deallocate"(%365) <{force = false}> : (tensor<1x28x28x256xbf16, #ttnn_layout50>) -> () loc(#loc364)
        %367 = "ttnn.to_layout"(%366) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x784x256xbf16, #ttnn_layout49>) -> tensor<1x1x784x256xbf16, #ttnn_layout51> loc(#loc482)
        "ttnn.deallocate"(%366) <{force = false}> : (tensor<1x1x784x256xbf16, #ttnn_layout49>) -> () loc(#loc482)
        %368 = "ttnn.conv2d"(%367, %arg112, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 256 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x256xbf16, #ttnn_layout51>, tensor<256x256x3x3xbf16, #ttnn_layout24>, !ttnn.device) -> tensor<1x1x784x256xbf16, #ttnn_layout49> loc(#loc365)
        "ttnn.deallocate"(%367) <{force = false}> : (tensor<1x1x784x256xbf16, #ttnn_layout51>) -> () loc(#loc365)
        "ttnn.deallocate"(%arg112) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout24>) -> () loc(#loc365)
        %369 = "ttnn.reshape"(%368) <{shape = [1 : i32, 28 : i32, 28 : i32, 256 : i32]}> : (tensor<1x1x784x256xbf16, #ttnn_layout49>) -> tensor<1x28x28x256xbf16, #ttnn_layout50> loc(#loc483)
        "ttnn.deallocate"(%368) <{force = false}> : (tensor<1x1x784x256xbf16, #ttnn_layout49>) -> () loc(#loc483)
        %370 = "ttnn.permute"(%369) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x28x28x256xbf16, #ttnn_layout50>) -> tensor<1x256x28x28xbf16, #ttnn_layout7> loc(#loc365)
        "ttnn.deallocate"(%369) <{force = false}> : (tensor<1x28x28x256xbf16, #ttnn_layout50>) -> () loc(#loc365)
        %371 = "ttnn.batch_norm_inference"(%370, %127, %12, %22, %198) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>) -> tensor<1x256x28x28xbf16, #ttnn_layout7> loc(#loc13)
        "ttnn.deallocate"(%370) <{force = false}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> () loc(#loc13)
        "ttnn.deallocate"(%198) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc13)
        "ttnn.deallocate"(%127) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc13)
        "ttnn.deallocate"(%22) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc13)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc13)
        %372 = "ttnn.relu"(%371) : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> tensor<1x256x28x28xbf16, #ttnn_layout7> loc(#loc366)
        "ttnn.deallocate"(%371) <{force = false}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> () loc(#loc366)
        %373 = "ttnn.permute"(%372) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> tensor<1x28x28x256xbf16, #ttnn_layout50> loc(#loc366)
        "ttnn.deallocate"(%372) <{force = false}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> () loc(#loc366)
        %374 = "ttnn.reshape"(%373) <{shape = [1 : i32, 1 : i32, 784 : i32, 256 : i32]}> : (tensor<1x28x28x256xbf16, #ttnn_layout50>) -> tensor<1x1x784x256xbf16, #ttnn_layout49> loc(#loc366)
        "ttnn.deallocate"(%373) <{force = false}> : (tensor<1x28x28x256xbf16, #ttnn_layout50>) -> () loc(#loc366)
        %375 = "ttnn.to_layout"(%374) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x784x256xbf16, #ttnn_layout49>) -> tensor<1x1x784x256xbf16, #ttnn_layout51> loc(#loc484)
        "ttnn.deallocate"(%374) <{force = false}> : (tensor<1x1x784x256xbf16, #ttnn_layout49>) -> () loc(#loc484)
        %376 = "ttnn.conv2d"(%375, %arg107, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x256xbf16, #ttnn_layout51>, tensor<512x256x1x1xbf16, #ttnn_layout16>, !ttnn.device) -> tensor<1x1x784x512xbf16, #ttnn_layout52> loc(#loc367)
        "ttnn.deallocate"(%375) <{force = false}> : (tensor<1x1x784x256xbf16, #ttnn_layout51>) -> () loc(#loc367)
        "ttnn.deallocate"(%arg107) <{force = false}> : (tensor<512x256x1x1xbf16, #ttnn_layout16>) -> () loc(#loc367)
        %377 = "ttnn.reshape"(%376) <{shape = [1 : i32, 28 : i32, 28 : i32, 512 : i32]}> : (tensor<1x1x784x512xbf16, #ttnn_layout52>) -> tensor<1x28x28x512xbf16, #ttnn_layout53> loc(#loc485)
        "ttnn.deallocate"(%376) <{force = false}> : (tensor<1x1x784x512xbf16, #ttnn_layout52>) -> () loc(#loc485)
        %378 = "ttnn.permute"(%377) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x28x28x512xbf16, #ttnn_layout53>) -> tensor<1x512x28x28xbf16, #ttnn_layout3> loc(#loc367)
        "ttnn.deallocate"(%377) <{force = false}> : (tensor<1x28x28x512xbf16, #ttnn_layout53>) -> () loc(#loc367)
        %379 = "ttnn.batch_norm_inference"(%378, %103, %186, %58, %35) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>) -> tensor<1x512x28x28xbf16, #ttnn_layout3> loc(#loc29)
        "ttnn.deallocate"(%378) <{force = false}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>) -> () loc(#loc29)
        "ttnn.deallocate"(%186) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc29)
        "ttnn.deallocate"(%103) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc29)
        "ttnn.deallocate"(%58) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc29)
        "ttnn.deallocate"(%35) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc29)
        %380 = "ttnn.add"(%379, %356) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>, tensor<1x512x28x28xbf16, #ttnn_layout3>) -> tensor<1x512x28x28xbf16, #ttnn_layout3> loc(#loc368)
        "ttnn.deallocate"(%379) <{force = false}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>) -> () loc(#loc368)
        "ttnn.deallocate"(%356) <{force = false}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>) -> () loc(#loc368)
        %381 = "ttnn.relu"(%380) : (tensor<1x512x28x28xbf16, #ttnn_layout3>) -> tensor<1x512x28x28xbf16, #ttnn_layout3> loc(#loc369)
        "ttnn.deallocate"(%380) <{force = false}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>) -> () loc(#loc369)
        %382 = "ttnn.permute"(%381) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>) -> tensor<1x28x28x512xbf16, #ttnn_layout53> loc(#loc369)
        %383 = "ttnn.reshape"(%382) <{shape = [1 : i32, 1 : i32, 784 : i32, 512 : i32]}> : (tensor<1x28x28x512xbf16, #ttnn_layout53>) -> tensor<1x1x784x512xbf16, #ttnn_layout52> loc(#loc369)
        "ttnn.deallocate"(%382) <{force = false}> : (tensor<1x28x28x512xbf16, #ttnn_layout53>) -> () loc(#loc369)
        %384 = "ttnn.to_layout"(%383) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x784x512xbf16, #ttnn_layout52>) -> tensor<1x1x784x512xbf16, #ttnn_layout54> loc(#loc486)
        "ttnn.deallocate"(%383) <{force = false}> : (tensor<1x1x784x512xbf16, #ttnn_layout52>) -> () loc(#loc486)
        %385 = "ttnn.conv2d"(%384, %arg132, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 512 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x512xbf16, #ttnn_layout54>, tensor<256x512x1x1xbf16, #ttnn_layout26>, !ttnn.device) -> tensor<1x1x784x256xbf16, #ttnn_layout49> loc(#loc370)
        "ttnn.deallocate"(%384) <{force = false}> : (tensor<1x1x784x512xbf16, #ttnn_layout54>) -> () loc(#loc370)
        "ttnn.deallocate"(%arg132) <{force = false}> : (tensor<256x512x1x1xbf16, #ttnn_layout26>) -> () loc(#loc370)
        %386 = "ttnn.reshape"(%385) <{shape = [1 : i32, 28 : i32, 28 : i32, 256 : i32]}> : (tensor<1x1x784x256xbf16, #ttnn_layout49>) -> tensor<1x28x28x256xbf16, #ttnn_layout50> loc(#loc487)
        "ttnn.deallocate"(%385) <{force = false}> : (tensor<1x1x784x256xbf16, #ttnn_layout49>) -> () loc(#loc487)
        %387 = "ttnn.permute"(%386) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x28x28x256xbf16, #ttnn_layout50>) -> tensor<1x256x28x28xbf16, #ttnn_layout7> loc(#loc370)
        "ttnn.deallocate"(%386) <{force = false}> : (tensor<1x28x28x256xbf16, #ttnn_layout50>) -> () loc(#loc370)
        %388 = "ttnn.batch_norm_inference"(%387, %130, %183, %182, %118) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>) -> tensor<1x256x28x28xbf16, #ttnn_layout7> loc(#loc53)
        "ttnn.deallocate"(%387) <{force = false}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> () loc(#loc53)
        "ttnn.deallocate"(%183) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc53)
        "ttnn.deallocate"(%182) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc53)
        "ttnn.deallocate"(%130) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc53)
        "ttnn.deallocate"(%118) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc53)
        %389 = "ttnn.relu"(%388) : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> tensor<1x256x28x28xbf16, #ttnn_layout7> loc(#loc371)
        "ttnn.deallocate"(%388) <{force = false}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> () loc(#loc371)
        %390 = "ttnn.permute"(%389) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> tensor<1x28x28x256xbf16, #ttnn_layout50> loc(#loc371)
        "ttnn.deallocate"(%389) <{force = false}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> () loc(#loc371)
        %391 = "ttnn.reshape"(%390) <{shape = [1 : i32, 1 : i32, 784 : i32, 256 : i32]}> : (tensor<1x28x28x256xbf16, #ttnn_layout50>) -> tensor<1x1x784x256xbf16, #ttnn_layout49> loc(#loc371)
        "ttnn.deallocate"(%390) <{force = false}> : (tensor<1x28x28x256xbf16, #ttnn_layout50>) -> () loc(#loc371)
        %392 = "ttnn.to_layout"(%391) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x784x256xbf16, #ttnn_layout49>) -> tensor<1x1x784x256xbf16, #ttnn_layout51> loc(#loc488)
        "ttnn.deallocate"(%391) <{force = false}> : (tensor<1x1x784x256xbf16, #ttnn_layout49>) -> () loc(#loc488)
        %393 = "ttnn.conv2d"(%392, %arg127, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 256 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x256xbf16, #ttnn_layout51>, tensor<256x256x3x3xbf16, #ttnn_layout24>, !ttnn.device) -> tensor<1x1x784x256xbf16, #ttnn_layout49> loc(#loc372)
        "ttnn.deallocate"(%392) <{force = false}> : (tensor<1x1x784x256xbf16, #ttnn_layout51>) -> () loc(#loc372)
        "ttnn.deallocate"(%arg127) <{force = false}> : (tensor<256x256x3x3xbf16, #ttnn_layout24>) -> () loc(#loc372)
        %394 = "ttnn.reshape"(%393) <{shape = [1 : i32, 28 : i32, 28 : i32, 256 : i32]}> : (tensor<1x1x784x256xbf16, #ttnn_layout49>) -> tensor<1x28x28x256xbf16, #ttnn_layout50> loc(#loc489)
        "ttnn.deallocate"(%393) <{force = false}> : (tensor<1x1x784x256xbf16, #ttnn_layout49>) -> () loc(#loc489)
        %395 = "ttnn.permute"(%394) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x28x28x256xbf16, #ttnn_layout50>) -> tensor<1x256x28x28xbf16, #ttnn_layout7> loc(#loc372)
        "ttnn.deallocate"(%394) <{force = false}> : (tensor<1x28x28x256xbf16, #ttnn_layout50>) -> () loc(#loc372)
        %396 = "ttnn.batch_norm_inference"(%395, %9, %67, %147, %49) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>, tensor<1x256x1x1xbf16, #ttnn_layout7>) -> tensor<1x256x28x28xbf16, #ttnn_layout7> loc(#loc10)
        "ttnn.deallocate"(%395) <{force = false}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> () loc(#loc10)
        "ttnn.deallocate"(%147) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc10)
        "ttnn.deallocate"(%67) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc10)
        "ttnn.deallocate"(%49) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc10)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x256x1x1xbf16, #ttnn_layout7>) -> () loc(#loc10)
        %397 = "ttnn.relu"(%396) : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> tensor<1x256x28x28xbf16, #ttnn_layout7> loc(#loc373)
        "ttnn.deallocate"(%396) <{force = false}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> () loc(#loc373)
        %398 = "ttnn.permute"(%397) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> tensor<1x28x28x256xbf16, #ttnn_layout50> loc(#loc373)
        "ttnn.deallocate"(%397) <{force = false}> : (tensor<1x256x28x28xbf16, #ttnn_layout7>) -> () loc(#loc373)
        %399 = "ttnn.reshape"(%398) <{shape = [1 : i32, 1 : i32, 784 : i32, 256 : i32]}> : (tensor<1x28x28x256xbf16, #ttnn_layout50>) -> tensor<1x1x784x256xbf16, #ttnn_layout49> loc(#loc373)
        "ttnn.deallocate"(%398) <{force = false}> : (tensor<1x28x28x256xbf16, #ttnn_layout50>) -> () loc(#loc373)
        %400 = "ttnn.to_layout"(%399) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x784x256xbf16, #ttnn_layout49>) -> tensor<1x1x784x256xbf16, #ttnn_layout51> loc(#loc490)
        "ttnn.deallocate"(%399) <{force = false}> : (tensor<1x1x784x256xbf16, #ttnn_layout49>) -> () loc(#loc490)
        %401 = "ttnn.conv2d"(%400, %arg122, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 256 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x256xbf16, #ttnn_layout51>, tensor<512x256x1x1xbf16, #ttnn_layout16>, !ttnn.device) -> tensor<1x1x784x512xbf16, #ttnn_layout52> loc(#loc374)
        "ttnn.deallocate"(%400) <{force = false}> : (tensor<1x1x784x256xbf16, #ttnn_layout51>) -> () loc(#loc374)
        "ttnn.deallocate"(%arg122) <{force = false}> : (tensor<512x256x1x1xbf16, #ttnn_layout16>) -> () loc(#loc374)
        %402 = "ttnn.reshape"(%401) <{shape = [1 : i32, 28 : i32, 28 : i32, 512 : i32]}> : (tensor<1x1x784x512xbf16, #ttnn_layout52>) -> tensor<1x28x28x512xbf16, #ttnn_layout53> loc(#loc491)
        "ttnn.deallocate"(%401) <{force = false}> : (tensor<1x1x784x512xbf16, #ttnn_layout52>) -> () loc(#loc491)
        %403 = "ttnn.permute"(%402) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x28x28x512xbf16, #ttnn_layout53>) -> tensor<1x512x28x28xbf16, #ttnn_layout3> loc(#loc374)
        "ttnn.deallocate"(%402) <{force = false}> : (tensor<1x28x28x512xbf16, #ttnn_layout53>) -> () loc(#loc374)
        %404 = "ttnn.batch_norm_inference"(%403, %210, %120, %75, %87) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>) -> tensor<1x512x28x28xbf16, #ttnn_layout3> loc(#loc45)
        "ttnn.deallocate"(%403) <{force = false}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>) -> () loc(#loc45)
        "ttnn.deallocate"(%210) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc45)
        "ttnn.deallocate"(%120) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc45)
        "ttnn.deallocate"(%87) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc45)
        "ttnn.deallocate"(%75) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc45)
        %405 = "ttnn.add"(%404, %381) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>, tensor<1x512x28x28xbf16, #ttnn_layout3>) -> tensor<1x512x28x28xbf16, #ttnn_layout3> loc(#loc375)
        "ttnn.deallocate"(%404) <{force = false}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>) -> () loc(#loc375)
        "ttnn.deallocate"(%381) <{force = false}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>) -> () loc(#loc375)
        %406 = "ttnn.relu"(%405) : (tensor<1x512x28x28xbf16, #ttnn_layout3>) -> tensor<1x512x28x28xbf16, #ttnn_layout3> loc(#loc376)
        "ttnn.deallocate"(%405) <{force = false}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>) -> () loc(#loc376)
        %407 = "ttnn.permute"(%406) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>) -> tensor<1x28x28x512xbf16, #ttnn_layout53> loc(#loc376)
        "ttnn.deallocate"(%406) <{force = false}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>) -> () loc(#loc376)
        %408 = "ttnn.reshape"(%407) <{shape = [1 : i32, 1 : i32, 784 : i32, 512 : i32]}> : (tensor<1x28x28x512xbf16, #ttnn_layout53>) -> tensor<1x1x784x512xbf16, #ttnn_layout52> loc(#loc376)
        "ttnn.deallocate"(%407) <{force = false}> : (tensor<1x28x28x512xbf16, #ttnn_layout53>) -> () loc(#loc376)
        %409 = "ttnn.to_layout"(%408) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x784x512xbf16, #ttnn_layout52>) -> tensor<1x1x784x512xbf16, #ttnn_layout54> loc(#loc492)
        %410 = "ttnn.conv2d"(%409, %arg147, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 512 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x512xbf16, #ttnn_layout54>, tensor<512x512x1x1xbf16, #ttnn_layout28>, !ttnn.device) -> tensor<1x1x784x512xbf16, #ttnn_layout52> loc(#loc377)
        "ttnn.deallocate"(%409) <{force = false}> : (tensor<1x1x784x512xbf16, #ttnn_layout54>) -> () loc(#loc377)
        "ttnn.deallocate"(%arg147) <{force = false}> : (tensor<512x512x1x1xbf16, #ttnn_layout28>) -> () loc(#loc377)
        %411 = "ttnn.reshape"(%410) <{shape = [1 : i32, 28 : i32, 28 : i32, 512 : i32]}> : (tensor<1x1x784x512xbf16, #ttnn_layout52>) -> tensor<1x28x28x512xbf16, #ttnn_layout53> loc(#loc493)
        "ttnn.deallocate"(%410) <{force = false}> : (tensor<1x1x784x512xbf16, #ttnn_layout52>) -> () loc(#loc493)
        %412 = "ttnn.permute"(%411) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x28x28x512xbf16, #ttnn_layout53>) -> tensor<1x512x28x28xbf16, #ttnn_layout3> loc(#loc377)
        "ttnn.deallocate"(%411) <{force = false}> : (tensor<1x28x28x512xbf16, #ttnn_layout53>) -> () loc(#loc377)
        %413 = "ttnn.batch_norm_inference"(%412, %43, %161, %179, %157) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>) -> tensor<1x512x28x28xbf16, #ttnn_layout3> loc(#loc34)
        "ttnn.deallocate"(%412) <{force = false}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>) -> () loc(#loc34)
        "ttnn.deallocate"(%179) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc34)
        "ttnn.deallocate"(%161) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc34)
        "ttnn.deallocate"(%157) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc34)
        "ttnn.deallocate"(%43) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc34)
        %414 = "ttnn.relu"(%413) : (tensor<1x512x28x28xbf16, #ttnn_layout3>) -> tensor<1x512x28x28xbf16, #ttnn_layout3> loc(#loc378)
        "ttnn.deallocate"(%413) <{force = false}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>) -> () loc(#loc378)
        %415 = "ttnn.permute"(%414) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>) -> tensor<1x28x28x512xbf16, #ttnn_layout53> loc(#loc378)
        "ttnn.deallocate"(%414) <{force = false}> : (tensor<1x512x28x28xbf16, #ttnn_layout3>) -> () loc(#loc378)
        %416 = "ttnn.reshape"(%415) <{shape = [1 : i32, 1 : i32, 784 : i32, 512 : i32]}> : (tensor<1x28x28x512xbf16, #ttnn_layout53>) -> tensor<1x1x784x512xbf16, #ttnn_layout52> loc(#loc378)
        "ttnn.deallocate"(%415) <{force = false}> : (tensor<1x28x28x512xbf16, #ttnn_layout53>) -> () loc(#loc378)
        %417 = "ttnn.to_layout"(%416) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x784x512xbf16, #ttnn_layout52>) -> tensor<1x1x784x512xbf16, #ttnn_layout54> loc(#loc494)
        "ttnn.deallocate"(%416) <{force = false}> : (tensor<1x1x784x512xbf16, #ttnn_layout52>) -> () loc(#loc494)
        %418 = "ttnn.conv2d"(%417, %arg142, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 512 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 512 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 2, 2>}> : (tensor<1x1x784x512xbf16, #ttnn_layout54>, tensor<512x512x3x3xbf16, #ttnn_layout27>, !ttnn.device) -> tensor<1x1x196x512xbf16, #ttnn_layout55> loc(#loc379)
        "ttnn.deallocate"(%417) <{force = false}> : (tensor<1x1x784x512xbf16, #ttnn_layout54>) -> () loc(#loc379)
        "ttnn.deallocate"(%arg142) <{force = false}> : (tensor<512x512x3x3xbf16, #ttnn_layout27>) -> () loc(#loc379)
        %419 = "ttnn.reshape"(%418) <{shape = [1 : i32, 14 : i32, 14 : i32, 512 : i32]}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> tensor<1x14x14x512xbf16, #ttnn_layout56> loc(#loc495)
        "ttnn.deallocate"(%418) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> () loc(#loc495)
        %420 = "ttnn.permute"(%419) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc379)
        "ttnn.deallocate"(%419) <{force = false}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> () loc(#loc379)
        %421 = "ttnn.batch_norm_inference"(%420, %173, %114, %99, %86) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc48)
        "ttnn.deallocate"(%420) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc48)
        "ttnn.deallocate"(%173) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc48)
        "ttnn.deallocate"(%114) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc48)
        "ttnn.deallocate"(%99) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc48)
        "ttnn.deallocate"(%86) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc48)
        %422 = "ttnn.relu"(%421) : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc380)
        "ttnn.deallocate"(%421) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc380)
        %423 = "ttnn.permute"(%422) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> tensor<1x14x14x512xbf16, #ttnn_layout56> loc(#loc380)
        "ttnn.deallocate"(%422) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc380)
        %424 = "ttnn.reshape"(%423) <{shape = [1 : i32, 1 : i32, 196 : i32, 512 : i32]}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> tensor<1x1x196x512xbf16, #ttnn_layout55> loc(#loc380)
        "ttnn.deallocate"(%423) <{force = false}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> () loc(#loc380)
        %425 = "ttnn.to_layout"(%424) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> tensor<1x1x196x512xbf16, #ttnn_layout57> loc(#loc496)
        "ttnn.deallocate"(%424) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> () loc(#loc496)
        %426 = "ttnn.conv2d"(%425, %arg137, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 512 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 1024 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x196x512xbf16, #ttnn_layout57>, tensor<1024x512x1x1xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<1x1x196x1024xbf16, #ttnn_layout58> loc(#loc381)
        "ttnn.deallocate"(%425) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout57>) -> () loc(#loc381)
        "ttnn.deallocate"(%arg137) <{force = false}> : (tensor<1024x512x1x1xbf16, #ttnn_layout15>) -> () loc(#loc381)
        %427 = "ttnn.reshape"(%426) <{shape = [1 : i32, 14 : i32, 14 : i32, 1024 : i32]}> : (tensor<1x1x196x1024xbf16, #ttnn_layout58>) -> tensor<1x14x14x1024xbf16, #ttnn_layout59> loc(#loc497)
        "ttnn.deallocate"(%426) <{force = false}> : (tensor<1x1x196x1024xbf16, #ttnn_layout58>) -> () loc(#loc497)
        %428 = "ttnn.permute"(%427) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x14x14x1024xbf16, #ttnn_layout59>) -> tensor<1x1024x14x14xbf16, #ttnn_layout11> loc(#loc381)
        "ttnn.deallocate"(%427) <{force = false}> : (tensor<1x14x14x1024xbf16, #ttnn_layout59>) -> () loc(#loc381)
        %429 = "ttnn.batch_norm_inference"(%428, %135, %31, %200, %93) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> tensor<1x1024x14x14xbf16, #ttnn_layout11> loc(#loc27)
        "ttnn.deallocate"(%428) <{force = false}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> () loc(#loc27)
        "ttnn.deallocate"(%200) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc27)
        "ttnn.deallocate"(%135) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc27)
        "ttnn.deallocate"(%93) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc27)
        "ttnn.deallocate"(%31) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc27)
        %430 = "ttnn.to_layout"(%408) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x784x512xbf16, #ttnn_layout52>) -> tensor<1x1x784x512xbf16, #ttnn_layout54> loc(#loc498)
        "ttnn.deallocate"(%408) <{force = false}> : (tensor<1x1x784x512xbf16, #ttnn_layout52>) -> () loc(#loc498)
        %431 = "ttnn.conv2d"(%430, %arg11, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 512 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 1024 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x784x512xbf16, #ttnn_layout54>, tensor<1024x512x1x1xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<1x1x196x1024xbf16, #ttnn_layout58> loc(#loc382)
        "ttnn.deallocate"(%430) <{force = false}> : (tensor<1x1x784x512xbf16, #ttnn_layout54>) -> () loc(#loc382)
        "ttnn.deallocate"(%arg11) <{force = false}> : (tensor<1024x512x1x1xbf16, #ttnn_layout15>) -> () loc(#loc382)
        %432 = "ttnn.reshape"(%431) <{shape = [1 : i32, 14 : i32, 14 : i32, 1024 : i32]}> : (tensor<1x1x196x1024xbf16, #ttnn_layout58>) -> tensor<1x14x14x1024xbf16, #ttnn_layout59> loc(#loc499)
        "ttnn.deallocate"(%431) <{force = false}> : (tensor<1x1x196x1024xbf16, #ttnn_layout58>) -> () loc(#loc499)
        %433 = "ttnn.permute"(%432) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x14x14x1024xbf16, #ttnn_layout59>) -> tensor<1x1024x14x14xbf16, #ttnn_layout11> loc(#loc382)
        "ttnn.deallocate"(%432) <{force = false}> : (tensor<1x14x14x1024xbf16, #ttnn_layout59>) -> () loc(#loc382)
        %434 = "ttnn.batch_norm_inference"(%433, %59, %19, %108, %129) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> tensor<1x1024x14x14xbf16, #ttnn_layout11> loc(#loc19)
        "ttnn.deallocate"(%433) <{force = false}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> () loc(#loc19)
        "ttnn.deallocate"(%129) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc19)
        "ttnn.deallocate"(%108) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc19)
        "ttnn.deallocate"(%59) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc19)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc19)
        %435 = "ttnn.add"(%429, %434) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>, tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> tensor<1x1024x14x14xbf16, #ttnn_layout11> loc(#loc383)
        "ttnn.deallocate"(%434) <{force = false}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> () loc(#loc383)
        "ttnn.deallocate"(%429) <{force = false}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> () loc(#loc383)
        %436 = "ttnn.relu"(%435) : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> tensor<1x1024x14x14xbf16, #ttnn_layout11> loc(#loc384)
        "ttnn.deallocate"(%435) <{force = false}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> () loc(#loc384)
        %437 = "ttnn.permute"(%436) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> tensor<1x14x14x1024xbf16, #ttnn_layout59> loc(#loc384)
        %438 = "ttnn.reshape"(%437) <{shape = [1 : i32, 1 : i32, 196 : i32, 1024 : i32]}> : (tensor<1x14x14x1024xbf16, #ttnn_layout59>) -> tensor<1x1x196x1024xbf16, #ttnn_layout58> loc(#loc384)
        "ttnn.deallocate"(%437) <{force = false}> : (tensor<1x14x14x1024xbf16, #ttnn_layout59>) -> () loc(#loc384)
        %439 = "ttnn.to_layout"(%438) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x196x1024xbf16, #ttnn_layout58>) -> tensor<1x1x196x1024xbf16, #ttnn_layout60> loc(#loc500)
        "ttnn.deallocate"(%438) <{force = false}> : (tensor<1x1x196x1024xbf16, #ttnn_layout58>) -> () loc(#loc500)
        %440 = "ttnn.conv2d"(%439, %arg162, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 1024 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x196x1024xbf16, #ttnn_layout60>, tensor<512x1024x1x1xbf16, #ttnn_layout29>, !ttnn.device) -> tensor<1x1x196x512xbf16, #ttnn_layout55> loc(#loc385)
        "ttnn.deallocate"(%439) <{force = false}> : (tensor<1x1x196x1024xbf16, #ttnn_layout60>) -> () loc(#loc385)
        "ttnn.deallocate"(%arg162) <{force = false}> : (tensor<512x1024x1x1xbf16, #ttnn_layout29>) -> () loc(#loc385)
        %441 = "ttnn.reshape"(%440) <{shape = [1 : i32, 14 : i32, 14 : i32, 512 : i32]}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> tensor<1x14x14x512xbf16, #ttnn_layout56> loc(#loc501)
        "ttnn.deallocate"(%440) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> () loc(#loc501)
        %442 = "ttnn.permute"(%441) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc385)
        "ttnn.deallocate"(%441) <{force = false}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> () loc(#loc385)
        %443 = "ttnn.batch_norm_inference"(%442, %109, %145, %112, %154) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc52)
        "ttnn.deallocate"(%442) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc52)
        "ttnn.deallocate"(%154) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc52)
        "ttnn.deallocate"(%145) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc52)
        "ttnn.deallocate"(%112) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc52)
        "ttnn.deallocate"(%109) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc52)
        %444 = "ttnn.relu"(%443) : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc386)
        "ttnn.deallocate"(%443) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc386)
        %445 = "ttnn.permute"(%444) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> tensor<1x14x14x512xbf16, #ttnn_layout56> loc(#loc386)
        "ttnn.deallocate"(%444) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc386)
        %446 = "ttnn.reshape"(%445) <{shape = [1 : i32, 1 : i32, 196 : i32, 512 : i32]}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> tensor<1x1x196x512xbf16, #ttnn_layout55> loc(#loc386)
        "ttnn.deallocate"(%445) <{force = false}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> () loc(#loc386)
        %447 = "ttnn.to_layout"(%446) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> tensor<1x1x196x512xbf16, #ttnn_layout57> loc(#loc502)
        "ttnn.deallocate"(%446) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> () loc(#loc502)
        %448 = "ttnn.conv2d"(%447, %arg157, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 512 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 3, 3>, out_channels = 512 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x196x512xbf16, #ttnn_layout57>, tensor<512x512x3x3xbf16, #ttnn_layout27>, !ttnn.device) -> tensor<1x1x196x512xbf16, #ttnn_layout55> loc(#loc387)
        "ttnn.deallocate"(%447) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout57>) -> () loc(#loc387)
        "ttnn.deallocate"(%arg157) <{force = false}> : (tensor<512x512x3x3xbf16, #ttnn_layout27>) -> () loc(#loc387)
        %449 = "ttnn.reshape"(%448) <{shape = [1 : i32, 14 : i32, 14 : i32, 512 : i32]}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> tensor<1x14x14x512xbf16, #ttnn_layout56> loc(#loc503)
        "ttnn.deallocate"(%448) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> () loc(#loc503)
        %450 = "ttnn.permute"(%449) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc387)
        "ttnn.deallocate"(%449) <{force = false}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> () loc(#loc387)
        %451 = "ttnn.batch_norm_inference"(%450, %170, %187, %194, %60) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc40)
        "ttnn.deallocate"(%450) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc40)
        "ttnn.deallocate"(%194) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc40)
        "ttnn.deallocate"(%187) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc40)
        "ttnn.deallocate"(%170) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc40)
        "ttnn.deallocate"(%60) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc40)
        %452 = "ttnn.relu"(%451) : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc388)
        "ttnn.deallocate"(%451) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc388)
        %453 = "ttnn.permute"(%452) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> tensor<1x14x14x512xbf16, #ttnn_layout56> loc(#loc388)
        "ttnn.deallocate"(%452) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc388)
        %454 = "ttnn.reshape"(%453) <{shape = [1 : i32, 1 : i32, 196 : i32, 512 : i32]}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> tensor<1x1x196x512xbf16, #ttnn_layout55> loc(#loc388)
        "ttnn.deallocate"(%453) <{force = false}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> () loc(#loc388)
        %455 = "ttnn.to_layout"(%454) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> tensor<1x1x196x512xbf16, #ttnn_layout57> loc(#loc504)
        "ttnn.deallocate"(%454) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> () loc(#loc504)
        %456 = "ttnn.conv2d"(%455, %arg152, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 512 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 1024 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x196x512xbf16, #ttnn_layout57>, tensor<1024x512x1x1xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<1x1x196x1024xbf16, #ttnn_layout58> loc(#loc389)
        "ttnn.deallocate"(%455) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout57>) -> () loc(#loc389)
        "ttnn.deallocate"(%arg152) <{force = false}> : (tensor<1024x512x1x1xbf16, #ttnn_layout15>) -> () loc(#loc389)
        %457 = "ttnn.reshape"(%456) <{shape = [1 : i32, 14 : i32, 14 : i32, 1024 : i32]}> : (tensor<1x1x196x1024xbf16, #ttnn_layout58>) -> tensor<1x14x14x1024xbf16, #ttnn_layout59> loc(#loc505)
        "ttnn.deallocate"(%456) <{force = false}> : (tensor<1x1x196x1024xbf16, #ttnn_layout58>) -> () loc(#loc505)
        %458 = "ttnn.permute"(%457) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x14x14x1024xbf16, #ttnn_layout59>) -> tensor<1x1024x14x14xbf16, #ttnn_layout11> loc(#loc389)
        "ttnn.deallocate"(%457) <{force = false}> : (tensor<1x14x14x1024xbf16, #ttnn_layout59>) -> () loc(#loc389)
        %459 = "ttnn.batch_norm_inference"(%458, %134, %90, %106, %119) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> tensor<1x1024x14x14xbf16, #ttnn_layout11> loc(#loc49)
        "ttnn.deallocate"(%458) <{force = false}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> () loc(#loc49)
        "ttnn.deallocate"(%134) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc49)
        "ttnn.deallocate"(%119) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc49)
        "ttnn.deallocate"(%106) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc49)
        "ttnn.deallocate"(%90) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc49)
        %460 = "ttnn.add"(%459, %436) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>, tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> tensor<1x1024x14x14xbf16, #ttnn_layout11> loc(#loc390)
        "ttnn.deallocate"(%459) <{force = false}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> () loc(#loc390)
        "ttnn.deallocate"(%436) <{force = false}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> () loc(#loc390)
        %461 = "ttnn.relu"(%460) : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> tensor<1x1024x14x14xbf16, #ttnn_layout11> loc(#loc391)
        "ttnn.deallocate"(%460) <{force = false}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> () loc(#loc391)
        %462 = "ttnn.permute"(%461) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> tensor<1x14x14x1024xbf16, #ttnn_layout59> loc(#loc391)
        %463 = "ttnn.reshape"(%462) <{shape = [1 : i32, 1 : i32, 196 : i32, 1024 : i32]}> : (tensor<1x14x14x1024xbf16, #ttnn_layout59>) -> tensor<1x1x196x1024xbf16, #ttnn_layout58> loc(#loc391)
        "ttnn.deallocate"(%462) <{force = false}> : (tensor<1x14x14x1024xbf16, #ttnn_layout59>) -> () loc(#loc391)
        %464 = "ttnn.to_layout"(%463) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x196x1024xbf16, #ttnn_layout58>) -> tensor<1x1x196x1024xbf16, #ttnn_layout60> loc(#loc506)
        "ttnn.deallocate"(%463) <{force = false}> : (tensor<1x1x196x1024xbf16, #ttnn_layout58>) -> () loc(#loc506)
        %465 = "ttnn.conv2d"(%464, %arg177, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 1024 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x196x1024xbf16, #ttnn_layout60>, tensor<512x1024x1x1xbf16, #ttnn_layout29>, !ttnn.device) -> tensor<1x1x196x512xbf16, #ttnn_layout55> loc(#loc392)
        "ttnn.deallocate"(%464) <{force = false}> : (tensor<1x1x196x1024xbf16, #ttnn_layout60>) -> () loc(#loc392)
        "ttnn.deallocate"(%arg177) <{force = false}> : (tensor<512x1024x1x1xbf16, #ttnn_layout29>) -> () loc(#loc392)
        %466 = "ttnn.reshape"(%465) <{shape = [1 : i32, 14 : i32, 14 : i32, 512 : i32]}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> tensor<1x14x14x512xbf16, #ttnn_layout56> loc(#loc507)
        "ttnn.deallocate"(%465) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> () loc(#loc507)
        %467 = "ttnn.permute"(%466) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc392)
        "ttnn.deallocate"(%466) <{force = false}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> () loc(#loc392)
        %468 = "ttnn.batch_norm_inference"(%467, %54, %70, %8, %36) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc9)
        "ttnn.deallocate"(%467) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc9)
        "ttnn.deallocate"(%70) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc9)
        "ttnn.deallocate"(%54) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc9)
        "ttnn.deallocate"(%36) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc9)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc9)
        %469 = "ttnn.relu"(%468) : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc393)
        "ttnn.deallocate"(%468) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc393)
        %470 = "ttnn.permute"(%469) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> tensor<1x14x14x512xbf16, #ttnn_layout56> loc(#loc393)
        "ttnn.deallocate"(%469) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc393)
        %471 = "ttnn.reshape"(%470) <{shape = [1 : i32, 1 : i32, 196 : i32, 512 : i32]}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> tensor<1x1x196x512xbf16, #ttnn_layout55> loc(#loc393)
        "ttnn.deallocate"(%470) <{force = false}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> () loc(#loc393)
        %472 = "ttnn.to_layout"(%471) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> tensor<1x1x196x512xbf16, #ttnn_layout57> loc(#loc508)
        "ttnn.deallocate"(%471) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> () loc(#loc508)
        %473 = "ttnn.conv2d"(%472, %arg172, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 512 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 3, 3>, out_channels = 512 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x196x512xbf16, #ttnn_layout57>, tensor<512x512x3x3xbf16, #ttnn_layout27>, !ttnn.device) -> tensor<1x1x196x512xbf16, #ttnn_layout55> loc(#loc394)
        "ttnn.deallocate"(%472) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout57>) -> () loc(#loc394)
        "ttnn.deallocate"(%arg172) <{force = false}> : (tensor<512x512x3x3xbf16, #ttnn_layout27>) -> () loc(#loc394)
        %474 = "ttnn.reshape"(%473) <{shape = [1 : i32, 14 : i32, 14 : i32, 512 : i32]}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> tensor<1x14x14x512xbf16, #ttnn_layout56> loc(#loc509)
        "ttnn.deallocate"(%473) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> () loc(#loc509)
        %475 = "ttnn.permute"(%474) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc394)
        "ttnn.deallocate"(%474) <{force = false}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> () loc(#loc394)
        %476 = "ttnn.batch_norm_inference"(%475, %20, %55, %165, %146) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc20)
        "ttnn.deallocate"(%475) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc20)
        "ttnn.deallocate"(%165) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc20)
        "ttnn.deallocate"(%146) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc20)
        "ttnn.deallocate"(%55) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc20)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc20)
        %477 = "ttnn.relu"(%476) : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc395)
        "ttnn.deallocate"(%476) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc395)
        %478 = "ttnn.permute"(%477) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> tensor<1x14x14x512xbf16, #ttnn_layout56> loc(#loc395)
        "ttnn.deallocate"(%477) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc395)
        %479 = "ttnn.reshape"(%478) <{shape = [1 : i32, 1 : i32, 196 : i32, 512 : i32]}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> tensor<1x1x196x512xbf16, #ttnn_layout55> loc(#loc395)
        "ttnn.deallocate"(%478) <{force = false}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> () loc(#loc395)
        %480 = "ttnn.to_layout"(%479) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> tensor<1x1x196x512xbf16, #ttnn_layout57> loc(#loc510)
        "ttnn.deallocate"(%479) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> () loc(#loc510)
        %481 = "ttnn.conv2d"(%480, %arg167, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 512 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 1024 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x196x512xbf16, #ttnn_layout57>, tensor<1024x512x1x1xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<1x1x196x1024xbf16, #ttnn_layout58> loc(#loc396)
        "ttnn.deallocate"(%480) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout57>) -> () loc(#loc396)
        "ttnn.deallocate"(%arg167) <{force = false}> : (tensor<1024x512x1x1xbf16, #ttnn_layout15>) -> () loc(#loc396)
        %482 = "ttnn.reshape"(%481) <{shape = [1 : i32, 14 : i32, 14 : i32, 1024 : i32]}> : (tensor<1x1x196x1024xbf16, #ttnn_layout58>) -> tensor<1x14x14x1024xbf16, #ttnn_layout59> loc(#loc511)
        "ttnn.deallocate"(%481) <{force = false}> : (tensor<1x1x196x1024xbf16, #ttnn_layout58>) -> () loc(#loc511)
        %483 = "ttnn.permute"(%482) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x14x14x1024xbf16, #ttnn_layout59>) -> tensor<1x1024x14x14xbf16, #ttnn_layout11> loc(#loc396)
        "ttnn.deallocate"(%482) <{force = false}> : (tensor<1x14x14x1024xbf16, #ttnn_layout59>) -> () loc(#loc396)
        %484 = "ttnn.batch_norm_inference"(%483, %197, %78, %100, %125) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> tensor<1x1024x14x14xbf16, #ttnn_layout11> loc(#loc46)
        "ttnn.deallocate"(%483) <{force = false}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> () loc(#loc46)
        "ttnn.deallocate"(%197) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc46)
        "ttnn.deallocate"(%125) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc46)
        "ttnn.deallocate"(%100) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc46)
        "ttnn.deallocate"(%78) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc46)
        %485 = "ttnn.add"(%484, %461) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>, tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> tensor<1x1024x14x14xbf16, #ttnn_layout11> loc(#loc397)
        "ttnn.deallocate"(%484) <{force = false}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> () loc(#loc397)
        "ttnn.deallocate"(%461) <{force = false}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> () loc(#loc397)
        %486 = "ttnn.relu"(%485) : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> tensor<1x1024x14x14xbf16, #ttnn_layout11> loc(#loc398)
        "ttnn.deallocate"(%485) <{force = false}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> () loc(#loc398)
        %487 = "ttnn.permute"(%486) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> tensor<1x14x14x1024xbf16, #ttnn_layout59> loc(#loc398)
        %488 = "ttnn.reshape"(%487) <{shape = [1 : i32, 1 : i32, 196 : i32, 1024 : i32]}> : (tensor<1x14x14x1024xbf16, #ttnn_layout59>) -> tensor<1x1x196x1024xbf16, #ttnn_layout58> loc(#loc398)
        "ttnn.deallocate"(%487) <{force = false}> : (tensor<1x14x14x1024xbf16, #ttnn_layout59>) -> () loc(#loc398)
        %489 = "ttnn.to_layout"(%488) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x196x1024xbf16, #ttnn_layout58>) -> tensor<1x1x196x1024xbf16, #ttnn_layout60> loc(#loc512)
        "ttnn.deallocate"(%488) <{force = false}> : (tensor<1x1x196x1024xbf16, #ttnn_layout58>) -> () loc(#loc512)
        %490 = "ttnn.conv2d"(%489, %arg192, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 1024 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x196x1024xbf16, #ttnn_layout60>, tensor<512x1024x1x1xbf16, #ttnn_layout29>, !ttnn.device) -> tensor<1x1x196x512xbf16, #ttnn_layout55> loc(#loc399)
        "ttnn.deallocate"(%489) <{force = false}> : (tensor<1x1x196x1024xbf16, #ttnn_layout60>) -> () loc(#loc399)
        "ttnn.deallocate"(%arg192) <{force = false}> : (tensor<512x1024x1x1xbf16, #ttnn_layout29>) -> () loc(#loc399)
        %491 = "ttnn.reshape"(%490) <{shape = [1 : i32, 14 : i32, 14 : i32, 512 : i32]}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> tensor<1x14x14x512xbf16, #ttnn_layout56> loc(#loc513)
        "ttnn.deallocate"(%490) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> () loc(#loc513)
        %492 = "ttnn.permute"(%491) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc399)
        "ttnn.deallocate"(%491) <{force = false}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> () loc(#loc399)
        %493 = "ttnn.batch_norm_inference"(%492, %38, %123, %167, %162) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc30)
        "ttnn.deallocate"(%492) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc30)
        "ttnn.deallocate"(%167) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc30)
        "ttnn.deallocate"(%162) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc30)
        "ttnn.deallocate"(%123) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc30)
        "ttnn.deallocate"(%38) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc30)
        %494 = "ttnn.relu"(%493) : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc400)
        "ttnn.deallocate"(%493) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc400)
        %495 = "ttnn.permute"(%494) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> tensor<1x14x14x512xbf16, #ttnn_layout56> loc(#loc400)
        "ttnn.deallocate"(%494) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc400)
        %496 = "ttnn.reshape"(%495) <{shape = [1 : i32, 1 : i32, 196 : i32, 512 : i32]}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> tensor<1x1x196x512xbf16, #ttnn_layout55> loc(#loc400)
        "ttnn.deallocate"(%495) <{force = false}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> () loc(#loc400)
        %497 = "ttnn.to_layout"(%496) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> tensor<1x1x196x512xbf16, #ttnn_layout57> loc(#loc514)
        "ttnn.deallocate"(%496) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> () loc(#loc514)
        %498 = "ttnn.conv2d"(%497, %arg187, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 512 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 3, 3>, out_channels = 512 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x196x512xbf16, #ttnn_layout57>, tensor<512x512x3x3xbf16, #ttnn_layout27>, !ttnn.device) -> tensor<1x1x196x512xbf16, #ttnn_layout55> loc(#loc401)
        "ttnn.deallocate"(%497) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout57>) -> () loc(#loc401)
        "ttnn.deallocate"(%arg187) <{force = false}> : (tensor<512x512x3x3xbf16, #ttnn_layout27>) -> () loc(#loc401)
        %499 = "ttnn.reshape"(%498) <{shape = [1 : i32, 14 : i32, 14 : i32, 512 : i32]}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> tensor<1x14x14x512xbf16, #ttnn_layout56> loc(#loc515)
        "ttnn.deallocate"(%498) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> () loc(#loc515)
        %500 = "ttnn.permute"(%499) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc401)
        "ttnn.deallocate"(%499) <{force = false}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> () loc(#loc401)
        %501 = "ttnn.batch_norm_inference"(%500, %177, %180, %47, %85) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc36)
        "ttnn.deallocate"(%500) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc36)
        "ttnn.deallocate"(%180) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc36)
        "ttnn.deallocate"(%177) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc36)
        "ttnn.deallocate"(%85) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc36)
        "ttnn.deallocate"(%47) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc36)
        %502 = "ttnn.relu"(%501) : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc402)
        "ttnn.deallocate"(%501) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc402)
        %503 = "ttnn.permute"(%502) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> tensor<1x14x14x512xbf16, #ttnn_layout56> loc(#loc402)
        "ttnn.deallocate"(%502) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc402)
        %504 = "ttnn.reshape"(%503) <{shape = [1 : i32, 1 : i32, 196 : i32, 512 : i32]}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> tensor<1x1x196x512xbf16, #ttnn_layout55> loc(#loc402)
        "ttnn.deallocate"(%503) <{force = false}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> () loc(#loc402)
        %505 = "ttnn.to_layout"(%504) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> tensor<1x1x196x512xbf16, #ttnn_layout57> loc(#loc516)
        "ttnn.deallocate"(%504) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> () loc(#loc516)
        %506 = "ttnn.conv2d"(%505, %arg182, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 512 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 1024 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x196x512xbf16, #ttnn_layout57>, tensor<1024x512x1x1xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<1x1x196x1024xbf16, #ttnn_layout58> loc(#loc403)
        "ttnn.deallocate"(%505) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout57>) -> () loc(#loc403)
        "ttnn.deallocate"(%arg182) <{force = false}> : (tensor<1024x512x1x1xbf16, #ttnn_layout15>) -> () loc(#loc403)
        %507 = "ttnn.reshape"(%506) <{shape = [1 : i32, 14 : i32, 14 : i32, 1024 : i32]}> : (tensor<1x1x196x1024xbf16, #ttnn_layout58>) -> tensor<1x14x14x1024xbf16, #ttnn_layout59> loc(#loc517)
        "ttnn.deallocate"(%506) <{force = false}> : (tensor<1x1x196x1024xbf16, #ttnn_layout58>) -> () loc(#loc517)
        %508 = "ttnn.permute"(%507) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x14x14x1024xbf16, #ttnn_layout59>) -> tensor<1x1024x14x14xbf16, #ttnn_layout11> loc(#loc403)
        "ttnn.deallocate"(%507) <{force = false}> : (tensor<1x14x14x1024xbf16, #ttnn_layout59>) -> () loc(#loc403)
        %509 = "ttnn.batch_norm_inference"(%508, %208, %110, %14, %195) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> tensor<1x1024x14x14xbf16, #ttnn_layout11> loc(#loc15)
        "ttnn.deallocate"(%508) <{force = false}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> () loc(#loc15)
        "ttnn.deallocate"(%208) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc15)
        "ttnn.deallocate"(%195) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc15)
        "ttnn.deallocate"(%110) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc15)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc15)
        %510 = "ttnn.add"(%509, %486) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>, tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> tensor<1x1024x14x14xbf16, #ttnn_layout11> loc(#loc404)
        "ttnn.deallocate"(%509) <{force = false}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> () loc(#loc404)
        "ttnn.deallocate"(%486) <{force = false}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> () loc(#loc404)
        %511 = "ttnn.relu"(%510) : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> tensor<1x1024x14x14xbf16, #ttnn_layout11> loc(#loc405)
        "ttnn.deallocate"(%510) <{force = false}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> () loc(#loc405)
        %512 = "ttnn.permute"(%511) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> tensor<1x14x14x1024xbf16, #ttnn_layout59> loc(#loc405)
        %513 = "ttnn.reshape"(%512) <{shape = [1 : i32, 1 : i32, 196 : i32, 1024 : i32]}> : (tensor<1x14x14x1024xbf16, #ttnn_layout59>) -> tensor<1x1x196x1024xbf16, #ttnn_layout58> loc(#loc405)
        "ttnn.deallocate"(%512) <{force = false}> : (tensor<1x14x14x1024xbf16, #ttnn_layout59>) -> () loc(#loc405)
        %514 = "ttnn.to_layout"(%513) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x196x1024xbf16, #ttnn_layout58>) -> tensor<1x1x196x1024xbf16, #ttnn_layout60> loc(#loc518)
        "ttnn.deallocate"(%513) <{force = false}> : (tensor<1x1x196x1024xbf16, #ttnn_layout58>) -> () loc(#loc518)
        %515 = "ttnn.conv2d"(%514, %arg207, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 1024 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x196x1024xbf16, #ttnn_layout60>, tensor<512x1024x1x1xbf16, #ttnn_layout29>, !ttnn.device) -> tensor<1x1x196x512xbf16, #ttnn_layout55> loc(#loc406)
        "ttnn.deallocate"(%514) <{force = false}> : (tensor<1x1x196x1024xbf16, #ttnn_layout60>) -> () loc(#loc406)
        "ttnn.deallocate"(%arg207) <{force = false}> : (tensor<512x1024x1x1xbf16, #ttnn_layout29>) -> () loc(#loc406)
        %516 = "ttnn.reshape"(%515) <{shape = [1 : i32, 14 : i32, 14 : i32, 512 : i32]}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> tensor<1x14x14x512xbf16, #ttnn_layout56> loc(#loc519)
        "ttnn.deallocate"(%515) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> () loc(#loc519)
        %517 = "ttnn.permute"(%516) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc406)
        "ttnn.deallocate"(%516) <{force = false}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> () loc(#loc406)
        %518 = "ttnn.batch_norm_inference"(%517, %155, %117, %66, %89) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc41)
        "ttnn.deallocate"(%517) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc41)
        "ttnn.deallocate"(%155) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc41)
        "ttnn.deallocate"(%117) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc41)
        "ttnn.deallocate"(%89) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc41)
        "ttnn.deallocate"(%66) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc41)
        %519 = "ttnn.relu"(%518) : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc407)
        "ttnn.deallocate"(%518) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc407)
        %520 = "ttnn.permute"(%519) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> tensor<1x14x14x512xbf16, #ttnn_layout56> loc(#loc407)
        "ttnn.deallocate"(%519) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc407)
        %521 = "ttnn.reshape"(%520) <{shape = [1 : i32, 1 : i32, 196 : i32, 512 : i32]}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> tensor<1x1x196x512xbf16, #ttnn_layout55> loc(#loc407)
        "ttnn.deallocate"(%520) <{force = false}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> () loc(#loc407)
        %522 = "ttnn.to_layout"(%521) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> tensor<1x1x196x512xbf16, #ttnn_layout57> loc(#loc520)
        "ttnn.deallocate"(%521) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> () loc(#loc520)
        %523 = "ttnn.conv2d"(%522, %arg202, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 512 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 3, 3>, out_channels = 512 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x196x512xbf16, #ttnn_layout57>, tensor<512x512x3x3xbf16, #ttnn_layout27>, !ttnn.device) -> tensor<1x1x196x512xbf16, #ttnn_layout55> loc(#loc408)
        "ttnn.deallocate"(%522) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout57>) -> () loc(#loc408)
        "ttnn.deallocate"(%arg202) <{force = false}> : (tensor<512x512x3x3xbf16, #ttnn_layout27>) -> () loc(#loc408)
        %524 = "ttnn.reshape"(%523) <{shape = [1 : i32, 14 : i32, 14 : i32, 512 : i32]}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> tensor<1x14x14x512xbf16, #ttnn_layout56> loc(#loc521)
        "ttnn.deallocate"(%523) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> () loc(#loc521)
        %525 = "ttnn.permute"(%524) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc408)
        "ttnn.deallocate"(%524) <{force = false}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> () loc(#loc408)
        %526 = "ttnn.batch_norm_inference"(%525, %199, %159, %136, %102) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc51)
        "ttnn.deallocate"(%525) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc51)
        "ttnn.deallocate"(%199) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc51)
        "ttnn.deallocate"(%159) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc51)
        "ttnn.deallocate"(%136) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc51)
        "ttnn.deallocate"(%102) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc51)
        %527 = "ttnn.relu"(%526) : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc409)
        "ttnn.deallocate"(%526) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc409)
        %528 = "ttnn.permute"(%527) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> tensor<1x14x14x512xbf16, #ttnn_layout56> loc(#loc409)
        "ttnn.deallocate"(%527) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc409)
        %529 = "ttnn.reshape"(%528) <{shape = [1 : i32, 1 : i32, 196 : i32, 512 : i32]}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> tensor<1x1x196x512xbf16, #ttnn_layout55> loc(#loc409)
        "ttnn.deallocate"(%528) <{force = false}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> () loc(#loc409)
        %530 = "ttnn.to_layout"(%529) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> tensor<1x1x196x512xbf16, #ttnn_layout57> loc(#loc522)
        "ttnn.deallocate"(%529) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> () loc(#loc522)
        %531 = "ttnn.conv2d"(%530, %arg197, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 512 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 1024 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x196x512xbf16, #ttnn_layout57>, tensor<1024x512x1x1xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<1x1x196x1024xbf16, #ttnn_layout58> loc(#loc410)
        "ttnn.deallocate"(%530) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout57>) -> () loc(#loc410)
        "ttnn.deallocate"(%arg197) <{force = false}> : (tensor<1024x512x1x1xbf16, #ttnn_layout15>) -> () loc(#loc410)
        %532 = "ttnn.reshape"(%531) <{shape = [1 : i32, 14 : i32, 14 : i32, 1024 : i32]}> : (tensor<1x1x196x1024xbf16, #ttnn_layout58>) -> tensor<1x14x14x1024xbf16, #ttnn_layout59> loc(#loc523)
        "ttnn.deallocate"(%531) <{force = false}> : (tensor<1x1x196x1024xbf16, #ttnn_layout58>) -> () loc(#loc523)
        %533 = "ttnn.permute"(%532) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x14x14x1024xbf16, #ttnn_layout59>) -> tensor<1x1024x14x14xbf16, #ttnn_layout11> loc(#loc410)
        "ttnn.deallocate"(%532) <{force = false}> : (tensor<1x14x14x1024xbf16, #ttnn_layout59>) -> () loc(#loc410)
        %534 = "ttnn.batch_norm_inference"(%533, %160, %158, %63, %29) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> tensor<1x1024x14x14xbf16, #ttnn_layout11> loc(#loc25)
        "ttnn.deallocate"(%533) <{force = false}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> () loc(#loc25)
        "ttnn.deallocate"(%160) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc25)
        "ttnn.deallocate"(%158) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc25)
        "ttnn.deallocate"(%63) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc25)
        "ttnn.deallocate"(%29) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc25)
        %535 = "ttnn.add"(%534, %511) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>, tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> tensor<1x1024x14x14xbf16, #ttnn_layout11> loc(#loc411)
        "ttnn.deallocate"(%534) <{force = false}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> () loc(#loc411)
        "ttnn.deallocate"(%511) <{force = false}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> () loc(#loc411)
        %536 = "ttnn.relu"(%535) : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> tensor<1x1024x14x14xbf16, #ttnn_layout11> loc(#loc412)
        "ttnn.deallocate"(%535) <{force = false}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> () loc(#loc412)
        %537 = "ttnn.permute"(%536) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> tensor<1x14x14x1024xbf16, #ttnn_layout59> loc(#loc412)
        %538 = "ttnn.reshape"(%537) <{shape = [1 : i32, 1 : i32, 196 : i32, 1024 : i32]}> : (tensor<1x14x14x1024xbf16, #ttnn_layout59>) -> tensor<1x1x196x1024xbf16, #ttnn_layout58> loc(#loc412)
        "ttnn.deallocate"(%537) <{force = false}> : (tensor<1x14x14x1024xbf16, #ttnn_layout59>) -> () loc(#loc412)
        %539 = "ttnn.to_layout"(%538) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x196x1024xbf16, #ttnn_layout58>) -> tensor<1x1x196x1024xbf16, #ttnn_layout60> loc(#loc524)
        "ttnn.deallocate"(%538) <{force = false}> : (tensor<1x1x196x1024xbf16, #ttnn_layout58>) -> () loc(#loc524)
        %540 = "ttnn.conv2d"(%539, %arg222, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 1024 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x196x1024xbf16, #ttnn_layout60>, tensor<512x1024x1x1xbf16, #ttnn_layout29>, !ttnn.device) -> tensor<1x1x196x512xbf16, #ttnn_layout55> loc(#loc413)
        "ttnn.deallocate"(%539) <{force = false}> : (tensor<1x1x196x1024xbf16, #ttnn_layout60>) -> () loc(#loc413)
        "ttnn.deallocate"(%arg222) <{force = false}> : (tensor<512x1024x1x1xbf16, #ttnn_layout29>) -> () loc(#loc413)
        %541 = "ttnn.reshape"(%540) <{shape = [1 : i32, 14 : i32, 14 : i32, 512 : i32]}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> tensor<1x14x14x512xbf16, #ttnn_layout56> loc(#loc525)
        "ttnn.deallocate"(%540) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> () loc(#loc525)
        %542 = "ttnn.permute"(%541) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc413)
        "ttnn.deallocate"(%541) <{force = false}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> () loc(#loc413)
        %543 = "ttnn.batch_norm_inference"(%542, %107, %30, %124, %122) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc26)
        "ttnn.deallocate"(%542) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc26)
        "ttnn.deallocate"(%124) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc26)
        "ttnn.deallocate"(%122) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc26)
        "ttnn.deallocate"(%107) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc26)
        "ttnn.deallocate"(%30) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc26)
        %544 = "ttnn.relu"(%543) : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc414)
        "ttnn.deallocate"(%543) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc414)
        %545 = "ttnn.permute"(%544) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> tensor<1x14x14x512xbf16, #ttnn_layout56> loc(#loc414)
        "ttnn.deallocate"(%544) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc414)
        %546 = "ttnn.reshape"(%545) <{shape = [1 : i32, 1 : i32, 196 : i32, 512 : i32]}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> tensor<1x1x196x512xbf16, #ttnn_layout55> loc(#loc414)
        "ttnn.deallocate"(%545) <{force = false}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> () loc(#loc414)
        %547 = "ttnn.to_layout"(%546) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> tensor<1x1x196x512xbf16, #ttnn_layout57> loc(#loc526)
        "ttnn.deallocate"(%546) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> () loc(#loc526)
        %548 = "ttnn.conv2d"(%547, %arg217, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 512 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 3, 3>, out_channels = 512 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x196x512xbf16, #ttnn_layout57>, tensor<512x512x3x3xbf16, #ttnn_layout27>, !ttnn.device) -> tensor<1x1x196x512xbf16, #ttnn_layout55> loc(#loc415)
        "ttnn.deallocate"(%547) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout57>) -> () loc(#loc415)
        "ttnn.deallocate"(%arg217) <{force = false}> : (tensor<512x512x3x3xbf16, #ttnn_layout27>) -> () loc(#loc415)
        %549 = "ttnn.reshape"(%548) <{shape = [1 : i32, 14 : i32, 14 : i32, 512 : i32]}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> tensor<1x14x14x512xbf16, #ttnn_layout56> loc(#loc527)
        "ttnn.deallocate"(%548) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> () loc(#loc527)
        %550 = "ttnn.permute"(%549) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc415)
        "ttnn.deallocate"(%549) <{force = false}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> () loc(#loc415)
        %551 = "ttnn.batch_norm_inference"(%550, %169, %48, %96, %95) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>, tensor<1x512x1x1xbf16, #ttnn_layout3>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc37)
        "ttnn.deallocate"(%550) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc37)
        "ttnn.deallocate"(%169) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc37)
        "ttnn.deallocate"(%96) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc37)
        "ttnn.deallocate"(%95) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc37)
        "ttnn.deallocate"(%48) <{force = false}> : (tensor<1x512x1x1xbf16, #ttnn_layout3>) -> () loc(#loc37)
        %552 = "ttnn.relu"(%551) : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> tensor<1x512x14x14xbf16, #ttnn_layout3> loc(#loc416)
        "ttnn.deallocate"(%551) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc416)
        %553 = "ttnn.permute"(%552) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> tensor<1x14x14x512xbf16, #ttnn_layout56> loc(#loc416)
        "ttnn.deallocate"(%552) <{force = false}> : (tensor<1x512x14x14xbf16, #ttnn_layout3>) -> () loc(#loc416)
        %554 = "ttnn.reshape"(%553) <{shape = [1 : i32, 1 : i32, 196 : i32, 512 : i32]}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> tensor<1x1x196x512xbf16, #ttnn_layout55> loc(#loc416)
        "ttnn.deallocate"(%553) <{force = false}> : (tensor<1x14x14x512xbf16, #ttnn_layout56>) -> () loc(#loc416)
        %555 = "ttnn.to_layout"(%554) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> tensor<1x1x196x512xbf16, #ttnn_layout57> loc(#loc528)
        "ttnn.deallocate"(%554) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout55>) -> () loc(#loc528)
        %556 = "ttnn.conv2d"(%555, %arg212, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 512 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 1024 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x196x512xbf16, #ttnn_layout57>, tensor<1024x512x1x1xbf16, #ttnn_layout15>, !ttnn.device) -> tensor<1x1x196x1024xbf16, #ttnn_layout58> loc(#loc417)
        "ttnn.deallocate"(%555) <{force = false}> : (tensor<1x1x196x512xbf16, #ttnn_layout57>) -> () loc(#loc417)
        "ttnn.deallocate"(%arg212) <{force = false}> : (tensor<1024x512x1x1xbf16, #ttnn_layout15>) -> () loc(#loc417)
        %557 = "ttnn.reshape"(%556) <{shape = [1 : i32, 14 : i32, 14 : i32, 1024 : i32]}> : (tensor<1x1x196x1024xbf16, #ttnn_layout58>) -> tensor<1x14x14x1024xbf16, #ttnn_layout59> loc(#loc529)
        "ttnn.deallocate"(%556) <{force = false}> : (tensor<1x1x196x1024xbf16, #ttnn_layout58>) -> () loc(#loc529)
        %558 = "ttnn.permute"(%557) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x14x14x1024xbf16, #ttnn_layout59>) -> tensor<1x1024x14x14xbf16, #ttnn_layout11> loc(#loc417)
        "ttnn.deallocate"(%557) <{force = false}> : (tensor<1x14x14x1024xbf16, #ttnn_layout59>) -> () loc(#loc417)
        %559 = "ttnn.batch_norm_inference"(%558, %23, %196, %156, %166) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> tensor<1x1024x14x14xbf16, #ttnn_layout11> loc(#loc21)
        "ttnn.deallocate"(%558) <{force = false}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> () loc(#loc21)
        "ttnn.deallocate"(%196) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc21)
        "ttnn.deallocate"(%166) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc21)
        "ttnn.deallocate"(%156) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc21)
        "ttnn.deallocate"(%23) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc21)
        %560 = "ttnn.add"(%559, %536) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>, tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> tensor<1x1024x14x14xbf16, #ttnn_layout11> loc(#loc418)
        "ttnn.deallocate"(%559) <{force = false}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> () loc(#loc418)
        "ttnn.deallocate"(%536) <{force = false}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> () loc(#loc418)
        %561 = "ttnn.relu"(%560) : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> tensor<1x1024x14x14xbf16, #ttnn_layout11> loc(#loc419)
        "ttnn.deallocate"(%560) <{force = false}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> () loc(#loc419)
        %562 = "ttnn.permute"(%561) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> tensor<1x14x14x1024xbf16, #ttnn_layout59> loc(#loc419)
        "ttnn.deallocate"(%561) <{force = false}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> () loc(#loc419)
        %563 = "ttnn.reshape"(%562) <{shape = [1 : i32, 1 : i32, 196 : i32, 1024 : i32]}> : (tensor<1x14x14x1024xbf16, #ttnn_layout59>) -> tensor<1x1x196x1024xbf16, #ttnn_layout58> loc(#loc419)
        "ttnn.deallocate"(%562) <{force = false}> : (tensor<1x14x14x1024xbf16, #ttnn_layout59>) -> () loc(#loc419)
        %564 = "ttnn.to_layout"(%563) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x196x1024xbf16, #ttnn_layout58>) -> tensor<1x1x196x1024xbf16, #ttnn_layout60> loc(#loc530)
        %565 = "ttnn.conv2d"(%564, %arg237, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 1024 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 1024 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x196x1024xbf16, #ttnn_layout60>, tensor<1024x1024x1x1xbf16, #ttnn_layout31>, !ttnn.device) -> tensor<1x1x196x1024xbf16, #ttnn_layout58> loc(#loc420)
        "ttnn.deallocate"(%564) <{force = false}> : (tensor<1x1x196x1024xbf16, #ttnn_layout60>) -> () loc(#loc420)
        "ttnn.deallocate"(%arg237) <{force = false}> : (tensor<1024x1024x1x1xbf16, #ttnn_layout31>) -> () loc(#loc420)
        %566 = "ttnn.reshape"(%565) <{shape = [1 : i32, 14 : i32, 14 : i32, 1024 : i32]}> : (tensor<1x1x196x1024xbf16, #ttnn_layout58>) -> tensor<1x14x14x1024xbf16, #ttnn_layout59> loc(#loc531)
        "ttnn.deallocate"(%565) <{force = false}> : (tensor<1x1x196x1024xbf16, #ttnn_layout58>) -> () loc(#loc531)
        %567 = "ttnn.permute"(%566) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x14x14x1024xbf16, #ttnn_layout59>) -> tensor<1x1024x14x14xbf16, #ttnn_layout11> loc(#loc420)
        "ttnn.deallocate"(%566) <{force = false}> : (tensor<1x14x14x1024xbf16, #ttnn_layout59>) -> () loc(#loc420)
        %568 = "ttnn.batch_norm_inference"(%567, %62, %51, %56, %113) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> tensor<1x1024x14x14xbf16, #ttnn_layout11> loc(#loc39)
        "ttnn.deallocate"(%567) <{force = false}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> () loc(#loc39)
        "ttnn.deallocate"(%113) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc39)
        "ttnn.deallocate"(%62) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc39)
        "ttnn.deallocate"(%56) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc39)
        "ttnn.deallocate"(%51) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc39)
        %569 = "ttnn.relu"(%568) : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> tensor<1x1024x14x14xbf16, #ttnn_layout11> loc(#loc421)
        "ttnn.deallocate"(%568) <{force = false}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> () loc(#loc421)
        %570 = "ttnn.permute"(%569) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> tensor<1x14x14x1024xbf16, #ttnn_layout59> loc(#loc421)
        "ttnn.deallocate"(%569) <{force = false}> : (tensor<1x1024x14x14xbf16, #ttnn_layout11>) -> () loc(#loc421)
        %571 = "ttnn.reshape"(%570) <{shape = [1 : i32, 1 : i32, 196 : i32, 1024 : i32]}> : (tensor<1x14x14x1024xbf16, #ttnn_layout59>) -> tensor<1x1x196x1024xbf16, #ttnn_layout58> loc(#loc421)
        "ttnn.deallocate"(%570) <{force = false}> : (tensor<1x14x14x1024xbf16, #ttnn_layout59>) -> () loc(#loc421)
        %572 = "ttnn.to_layout"(%571) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x196x1024xbf16, #ttnn_layout58>) -> tensor<1x1x196x1024xbf16, #ttnn_layout60> loc(#loc532)
        "ttnn.deallocate"(%571) <{force = false}> : (tensor<1x1x196x1024xbf16, #ttnn_layout58>) -> () loc(#loc532)
        %573 = "ttnn.conv2d"(%572, %arg232, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 1024 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 3, 3>, out_channels = 1024 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 2, 2>}> : (tensor<1x1x196x1024xbf16, #ttnn_layout60>, tensor<1024x1024x3x3xbf16, #ttnn_layout30>, !ttnn.device) -> tensor<1x1x49x1024xbf16, #ttnn_layout61> loc(#loc422)
        "ttnn.deallocate"(%572) <{force = false}> : (tensor<1x1x196x1024xbf16, #ttnn_layout60>) -> () loc(#loc422)
        "ttnn.deallocate"(%arg232) <{force = false}> : (tensor<1024x1024x3x3xbf16, #ttnn_layout30>) -> () loc(#loc422)
        %574 = "ttnn.reshape"(%573) <{shape = [1 : i32, 7 : i32, 7 : i32, 1024 : i32]}> : (tensor<1x1x49x1024xbf16, #ttnn_layout61>) -> tensor<1x7x7x1024xbf16, #ttnn_layout62> loc(#loc533)
        "ttnn.deallocate"(%573) <{force = false}> : (tensor<1x1x49x1024xbf16, #ttnn_layout61>) -> () loc(#loc533)
        %575 = "ttnn.permute"(%574) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x7x7x1024xbf16, #ttnn_layout62>) -> tensor<1x1024x7x7xbf16, #ttnn_layout11> loc(#loc422)
        "ttnn.deallocate"(%574) <{force = false}> : (tensor<1x7x7x1024xbf16, #ttnn_layout62>) -> () loc(#loc422)
        %576 = "ttnn.batch_norm_inference"(%575, %189, %163, %21, %10) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x1024x7x7xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> tensor<1x1024x7x7xbf16, #ttnn_layout11> loc(#loc11)
        "ttnn.deallocate"(%575) <{force = false}> : (tensor<1x1024x7x7xbf16, #ttnn_layout11>) -> () loc(#loc11)
        "ttnn.deallocate"(%189) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc11)
        "ttnn.deallocate"(%163) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc11)
        "ttnn.deallocate"(%21) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc11)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc11)
        %577 = "ttnn.relu"(%576) : (tensor<1x1024x7x7xbf16, #ttnn_layout11>) -> tensor<1x1024x7x7xbf16, #ttnn_layout11> loc(#loc423)
        "ttnn.deallocate"(%576) <{force = false}> : (tensor<1x1024x7x7xbf16, #ttnn_layout11>) -> () loc(#loc423)
        %578 = "ttnn.permute"(%577) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1024x7x7xbf16, #ttnn_layout11>) -> tensor<1x7x7x1024xbf16, #ttnn_layout62> loc(#loc423)
        "ttnn.deallocate"(%577) <{force = false}> : (tensor<1x1024x7x7xbf16, #ttnn_layout11>) -> () loc(#loc423)
        %579 = "ttnn.reshape"(%578) <{shape = [1 : i32, 1 : i32, 49 : i32, 1024 : i32]}> : (tensor<1x7x7x1024xbf16, #ttnn_layout62>) -> tensor<1x1x49x1024xbf16, #ttnn_layout61> loc(#loc423)
        "ttnn.deallocate"(%578) <{force = false}> : (tensor<1x7x7x1024xbf16, #ttnn_layout62>) -> () loc(#loc423)
        %580 = "ttnn.to_layout"(%579) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x49x1024xbf16, #ttnn_layout61>) -> tensor<1x1x49x1024xbf16, #ttnn_layout63> loc(#loc534)
        "ttnn.deallocate"(%579) <{force = false}> : (tensor<1x1x49x1024xbf16, #ttnn_layout61>) -> () loc(#loc534)
        %581 = "ttnn.conv2d"(%580, %arg227, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 1024 : i32, input_height = 7 : i32, input_width = 7 : i32, kernel_size = array<i32: 1, 1>, out_channels = 2048 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x49x1024xbf16, #ttnn_layout63>, tensor<2048x1024x1x1xbf16, #ttnn_layout14>, !ttnn.device) -> tensor<1x1x49x2048xbf16, #ttnn_layout64> loc(#loc424)
        "ttnn.deallocate"(%580) <{force = false}> : (tensor<1x1x49x1024xbf16, #ttnn_layout63>) -> () loc(#loc424)
        "ttnn.deallocate"(%arg227) <{force = false}> : (tensor<2048x1024x1x1xbf16, #ttnn_layout14>) -> () loc(#loc424)
        %582 = "ttnn.reshape"(%581) <{shape = [1 : i32, 7 : i32, 7 : i32, 2048 : i32]}> : (tensor<1x1x49x2048xbf16, #ttnn_layout64>) -> tensor<1x7x7x2048xbf16, #ttnn_layout65> loc(#loc535)
        "ttnn.deallocate"(%581) <{force = false}> : (tensor<1x1x49x2048xbf16, #ttnn_layout64>) -> () loc(#loc535)
        %583 = "ttnn.permute"(%582) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x7x7x2048xbf16, #ttnn_layout65>) -> tensor<1x2048x7x7xbf16, #ttnn_layout9> loc(#loc424)
        "ttnn.deallocate"(%582) <{force = false}> : (tensor<1x7x7x2048xbf16, #ttnn_layout65>) -> () loc(#loc424)
        %584 = "ttnn.batch_norm_inference"(%583, %45, %41, %205, %132) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x2048x7x7xbf16, #ttnn_layout9>, tensor<1x2048x1x1xbf16, #ttnn_layout9>, tensor<1x2048x1x1xbf16, #ttnn_layout9>, tensor<1x2048x1x1xbf16, #ttnn_layout9>, tensor<1x2048x1x1xbf16, #ttnn_layout9>) -> tensor<1x2048x7x7xbf16, #ttnn_layout9> loc(#loc32)
        "ttnn.deallocate"(%583) <{force = false}> : (tensor<1x2048x7x7xbf16, #ttnn_layout9>) -> () loc(#loc32)
        "ttnn.deallocate"(%205) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout9>) -> () loc(#loc32)
        "ttnn.deallocate"(%132) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout9>) -> () loc(#loc32)
        "ttnn.deallocate"(%45) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout9>) -> () loc(#loc32)
        "ttnn.deallocate"(%41) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout9>) -> () loc(#loc32)
        %585 = "ttnn.to_layout"(%563) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x196x1024xbf16, #ttnn_layout58>) -> tensor<1x1x196x1024xbf16, #ttnn_layout60> loc(#loc536)
        "ttnn.deallocate"(%563) <{force = false}> : (tensor<1x1x196x1024xbf16, #ttnn_layout58>) -> () loc(#loc536)
        %586 = "ttnn.conv2d"(%585, %arg6, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 1024 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 2048 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x196x1024xbf16, #ttnn_layout60>, tensor<2048x1024x1x1xbf16, #ttnn_layout14>, !ttnn.device) -> tensor<1x1x49x2048xbf16, #ttnn_layout64> loc(#loc425)
        "ttnn.deallocate"(%585) <{force = false}> : (tensor<1x1x196x1024xbf16, #ttnn_layout60>) -> () loc(#loc425)
        "ttnn.deallocate"(%arg6) <{force = false}> : (tensor<2048x1024x1x1xbf16, #ttnn_layout14>) -> () loc(#loc425)
        %587 = "ttnn.reshape"(%586) <{shape = [1 : i32, 7 : i32, 7 : i32, 2048 : i32]}> : (tensor<1x1x49x2048xbf16, #ttnn_layout64>) -> tensor<1x7x7x2048xbf16, #ttnn_layout65> loc(#loc537)
        "ttnn.deallocate"(%586) <{force = false}> : (tensor<1x1x49x2048xbf16, #ttnn_layout64>) -> () loc(#loc537)
        %588 = "ttnn.permute"(%587) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x7x7x2048xbf16, #ttnn_layout65>) -> tensor<1x2048x7x7xbf16, #ttnn_layout9> loc(#loc425)
        "ttnn.deallocate"(%587) <{force = false}> : (tensor<1x7x7x2048xbf16, #ttnn_layout65>) -> () loc(#loc425)
        %589 = "ttnn.batch_norm_inference"(%588, %116, %148, %181, %84) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x2048x7x7xbf16, #ttnn_layout9>, tensor<1x2048x1x1xbf16, #ttnn_layout9>, tensor<1x2048x1x1xbf16, #ttnn_layout9>, tensor<1x2048x1x1xbf16, #ttnn_layout9>, tensor<1x2048x1x1xbf16, #ttnn_layout9>) -> tensor<1x2048x7x7xbf16, #ttnn_layout9> loc(#loc47)
        "ttnn.deallocate"(%588) <{force = false}> : (tensor<1x2048x7x7xbf16, #ttnn_layout9>) -> () loc(#loc47)
        "ttnn.deallocate"(%181) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout9>) -> () loc(#loc47)
        "ttnn.deallocate"(%148) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout9>) -> () loc(#loc47)
        "ttnn.deallocate"(%116) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout9>) -> () loc(#loc47)
        "ttnn.deallocate"(%84) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout9>) -> () loc(#loc47)
        %590 = "ttnn.add"(%584, %589) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x2048x7x7xbf16, #ttnn_layout9>, tensor<1x2048x7x7xbf16, #ttnn_layout9>) -> tensor<1x2048x7x7xbf16, #ttnn_layout9> loc(#loc426)
        "ttnn.deallocate"(%589) <{force = false}> : (tensor<1x2048x7x7xbf16, #ttnn_layout9>) -> () loc(#loc426)
        "ttnn.deallocate"(%584) <{force = false}> : (tensor<1x2048x7x7xbf16, #ttnn_layout9>) -> () loc(#loc426)
        %591 = "ttnn.relu"(%590) : (tensor<1x2048x7x7xbf16, #ttnn_layout9>) -> tensor<1x2048x7x7xbf16, #ttnn_layout9> loc(#loc427)
        "ttnn.deallocate"(%590) <{force = false}> : (tensor<1x2048x7x7xbf16, #ttnn_layout9>) -> () loc(#loc427)
        %592 = "ttnn.permute"(%591) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x2048x7x7xbf16, #ttnn_layout9>) -> tensor<1x7x7x2048xbf16, #ttnn_layout65> loc(#loc538)
        %593 = "ttnn.reshape"(%592) <{shape = [1 : i32, 1 : i32, 49 : i32, 2048 : i32]}> : (tensor<1x7x7x2048xbf16, #ttnn_layout65>) -> tensor<1x1x49x2048xbf16, #ttnn_layout64> loc(#loc556)
        "ttnn.deallocate"(%592) <{force = false}> : (tensor<1x7x7x2048xbf16, #ttnn_layout65>) -> () loc(#loc556)
        %594 = "ttnn.to_layout"(%593) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x49x2048xbf16, #ttnn_layout64>) -> tensor<1x1x49x2048xbf16, #ttnn_layout66> loc(#loc539)
        "ttnn.deallocate"(%593) <{force = false}> : (tensor<1x1x49x2048xbf16, #ttnn_layout64>) -> () loc(#loc539)
        %595 = "ttnn.conv2d"(%594, %arg252, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 2048 : i32, input_height = 7 : i32, input_width = 7 : i32, kernel_size = array<i32: 1, 1>, out_channels = 1024 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x49x2048xbf16, #ttnn_layout66>, tensor<1024x2048x1x1xbf16, #ttnn_layout32>, !ttnn.device) -> tensor<1x1x49x1024xbf16, #ttnn_layout61> loc(#loc428)
        "ttnn.deallocate"(%594) <{force = false}> : (tensor<1x1x49x2048xbf16, #ttnn_layout66>) -> () loc(#loc428)
        "ttnn.deallocate"(%arg252) <{force = false}> : (tensor<1024x2048x1x1xbf16, #ttnn_layout32>) -> () loc(#loc428)
        %596 = "ttnn.reshape"(%595) <{shape = [1 : i32, 7 : i32, 7 : i32, 1024 : i32]}> : (tensor<1x1x49x1024xbf16, #ttnn_layout61>) -> tensor<1x7x7x1024xbf16, #ttnn_layout62> loc(#loc540)
        "ttnn.deallocate"(%595) <{force = false}> : (tensor<1x1x49x1024xbf16, #ttnn_layout61>) -> () loc(#loc540)
        %597 = "ttnn.permute"(%596) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x7x7x1024xbf16, #ttnn_layout62>) -> tensor<1x1024x7x7xbf16, #ttnn_layout11> loc(#loc428)
        "ttnn.deallocate"(%596) <{force = false}> : (tensor<1x7x7x1024xbf16, #ttnn_layout62>) -> () loc(#loc428)
        %598 = "ttnn.batch_norm_inference"(%597, %88, %34, %91, %152) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x1024x7x7xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> tensor<1x1024x7x7xbf16, #ttnn_layout11> loc(#loc28)
        "ttnn.deallocate"(%597) <{force = false}> : (tensor<1x1024x7x7xbf16, #ttnn_layout11>) -> () loc(#loc28)
        "ttnn.deallocate"(%152) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc28)
        "ttnn.deallocate"(%91) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc28)
        "ttnn.deallocate"(%88) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc28)
        "ttnn.deallocate"(%34) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc28)
        %599 = "ttnn.relu"(%598) : (tensor<1x1024x7x7xbf16, #ttnn_layout11>) -> tensor<1x1024x7x7xbf16, #ttnn_layout11> loc(#loc429)
        "ttnn.deallocate"(%598) <{force = false}> : (tensor<1x1024x7x7xbf16, #ttnn_layout11>) -> () loc(#loc429)
        %600 = "ttnn.permute"(%599) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1024x7x7xbf16, #ttnn_layout11>) -> tensor<1x7x7x1024xbf16, #ttnn_layout62> loc(#loc429)
        "ttnn.deallocate"(%599) <{force = false}> : (tensor<1x1024x7x7xbf16, #ttnn_layout11>) -> () loc(#loc429)
        %601 = "ttnn.reshape"(%600) <{shape = [1 : i32, 1 : i32, 49 : i32, 1024 : i32]}> : (tensor<1x7x7x1024xbf16, #ttnn_layout62>) -> tensor<1x1x49x1024xbf16, #ttnn_layout61> loc(#loc429)
        "ttnn.deallocate"(%600) <{force = false}> : (tensor<1x7x7x1024xbf16, #ttnn_layout62>) -> () loc(#loc429)
        %602 = "ttnn.to_layout"(%601) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x49x1024xbf16, #ttnn_layout61>) -> tensor<1x1x49x1024xbf16, #ttnn_layout63> loc(#loc541)
        "ttnn.deallocate"(%601) <{force = false}> : (tensor<1x1x49x1024xbf16, #ttnn_layout61>) -> () loc(#loc541)
        %603 = "ttnn.conv2d"(%602, %arg247, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 1024 : i32, input_height = 7 : i32, input_width = 7 : i32, kernel_size = array<i32: 3, 3>, out_channels = 1024 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x49x1024xbf16, #ttnn_layout63>, tensor<1024x1024x3x3xbf16, #ttnn_layout30>, !ttnn.device) -> tensor<1x1x49x1024xbf16, #ttnn_layout61> loc(#loc430)
        "ttnn.deallocate"(%602) <{force = false}> : (tensor<1x1x49x1024xbf16, #ttnn_layout63>) -> () loc(#loc430)
        "ttnn.deallocate"(%arg247) <{force = false}> : (tensor<1024x1024x3x3xbf16, #ttnn_layout30>) -> () loc(#loc430)
        %604 = "ttnn.reshape"(%603) <{shape = [1 : i32, 7 : i32, 7 : i32, 1024 : i32]}> : (tensor<1x1x49x1024xbf16, #ttnn_layout61>) -> tensor<1x7x7x1024xbf16, #ttnn_layout62> loc(#loc542)
        "ttnn.deallocate"(%603) <{force = false}> : (tensor<1x1x49x1024xbf16, #ttnn_layout61>) -> () loc(#loc542)
        %605 = "ttnn.permute"(%604) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x7x7x1024xbf16, #ttnn_layout62>) -> tensor<1x1024x7x7xbf16, #ttnn_layout11> loc(#loc430)
        "ttnn.deallocate"(%604) <{force = false}> : (tensor<1x7x7x1024xbf16, #ttnn_layout62>) -> () loc(#loc430)
        %606 = "ttnn.batch_norm_inference"(%605, %68, %80, %28, %151) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x1024x7x7xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> tensor<1x1024x7x7xbf16, #ttnn_layout11> loc(#loc24)
        "ttnn.deallocate"(%605) <{force = false}> : (tensor<1x1024x7x7xbf16, #ttnn_layout11>) -> () loc(#loc24)
        "ttnn.deallocate"(%151) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc24)
        "ttnn.deallocate"(%80) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc24)
        "ttnn.deallocate"(%68) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc24)
        "ttnn.deallocate"(%28) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc24)
        %607 = "ttnn.relu"(%606) : (tensor<1x1024x7x7xbf16, #ttnn_layout11>) -> tensor<1x1024x7x7xbf16, #ttnn_layout11> loc(#loc431)
        "ttnn.deallocate"(%606) <{force = false}> : (tensor<1x1024x7x7xbf16, #ttnn_layout11>) -> () loc(#loc431)
        %608 = "ttnn.permute"(%607) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1024x7x7xbf16, #ttnn_layout11>) -> tensor<1x7x7x1024xbf16, #ttnn_layout62> loc(#loc431)
        "ttnn.deallocate"(%607) <{force = false}> : (tensor<1x1024x7x7xbf16, #ttnn_layout11>) -> () loc(#loc431)
        %609 = "ttnn.reshape"(%608) <{shape = [1 : i32, 1 : i32, 49 : i32, 1024 : i32]}> : (tensor<1x7x7x1024xbf16, #ttnn_layout62>) -> tensor<1x1x49x1024xbf16, #ttnn_layout61> loc(#loc431)
        "ttnn.deallocate"(%608) <{force = false}> : (tensor<1x7x7x1024xbf16, #ttnn_layout62>) -> () loc(#loc431)
        %610 = "ttnn.to_layout"(%609) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x49x1024xbf16, #ttnn_layout61>) -> tensor<1x1x49x1024xbf16, #ttnn_layout63> loc(#loc543)
        "ttnn.deallocate"(%609) <{force = false}> : (tensor<1x1x49x1024xbf16, #ttnn_layout61>) -> () loc(#loc543)
        %611 = "ttnn.conv2d"(%610, %arg242, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 1024 : i32, input_height = 7 : i32, input_width = 7 : i32, kernel_size = array<i32: 1, 1>, out_channels = 2048 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x49x1024xbf16, #ttnn_layout63>, tensor<2048x1024x1x1xbf16, #ttnn_layout14>, !ttnn.device) -> tensor<1x1x49x2048xbf16, #ttnn_layout64> loc(#loc432)
        "ttnn.deallocate"(%610) <{force = false}> : (tensor<1x1x49x1024xbf16, #ttnn_layout63>) -> () loc(#loc432)
        "ttnn.deallocate"(%arg242) <{force = false}> : (tensor<2048x1024x1x1xbf16, #ttnn_layout14>) -> () loc(#loc432)
        %612 = "ttnn.reshape"(%611) <{shape = [1 : i32, 7 : i32, 7 : i32, 2048 : i32]}> : (tensor<1x1x49x2048xbf16, #ttnn_layout64>) -> tensor<1x7x7x2048xbf16, #ttnn_layout65> loc(#loc544)
        "ttnn.deallocate"(%611) <{force = false}> : (tensor<1x1x49x2048xbf16, #ttnn_layout64>) -> () loc(#loc544)
        %613 = "ttnn.permute"(%612) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x7x7x2048xbf16, #ttnn_layout65>) -> tensor<1x2048x7x7xbf16, #ttnn_layout9> loc(#loc432)
        "ttnn.deallocate"(%612) <{force = false}> : (tensor<1x7x7x2048xbf16, #ttnn_layout65>) -> () loc(#loc432)
        %614 = "ttnn.batch_norm_inference"(%613, %131, %83, %71, %212) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x2048x7x7xbf16, #ttnn_layout9>, tensor<1x2048x1x1xbf16, #ttnn_layout9>, tensor<1x2048x1x1xbf16, #ttnn_layout9>, tensor<1x2048x1x1xbf16, #ttnn_layout9>, tensor<1x2048x1x1xbf16, #ttnn_layout9>) -> tensor<1x2048x7x7xbf16, #ttnn_layout9> loc(#loc43)
        "ttnn.deallocate"(%613) <{force = false}> : (tensor<1x2048x7x7xbf16, #ttnn_layout9>) -> () loc(#loc43)
        "ttnn.deallocate"(%212) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout9>) -> () loc(#loc43)
        "ttnn.deallocate"(%131) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout9>) -> () loc(#loc43)
        "ttnn.deallocate"(%83) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout9>) -> () loc(#loc43)
        "ttnn.deallocate"(%71) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout9>) -> () loc(#loc43)
        %615 = "ttnn.add"(%614, %591) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x2048x7x7xbf16, #ttnn_layout9>, tensor<1x2048x7x7xbf16, #ttnn_layout9>) -> tensor<1x2048x7x7xbf16, #ttnn_layout9> loc(#loc433)
        "ttnn.deallocate"(%614) <{force = false}> : (tensor<1x2048x7x7xbf16, #ttnn_layout9>) -> () loc(#loc433)
        "ttnn.deallocate"(%591) <{force = false}> : (tensor<1x2048x7x7xbf16, #ttnn_layout9>) -> () loc(#loc433)
        %616 = "ttnn.relu"(%615) : (tensor<1x2048x7x7xbf16, #ttnn_layout9>) -> tensor<1x2048x7x7xbf16, #ttnn_layout9> loc(#loc434)
        "ttnn.deallocate"(%615) <{force = false}> : (tensor<1x2048x7x7xbf16, #ttnn_layout9>) -> () loc(#loc434)
        %617 = "ttnn.permute"(%616) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x2048x7x7xbf16, #ttnn_layout9>) -> tensor<1x7x7x2048xbf16, #ttnn_layout65> loc(#loc545)
        %618 = "ttnn.reshape"(%617) <{shape = [1 : i32, 1 : i32, 49 : i32, 2048 : i32]}> : (tensor<1x7x7x2048xbf16, #ttnn_layout65>) -> tensor<1x1x49x2048xbf16, #ttnn_layout64> loc(#loc557)
        "ttnn.deallocate"(%617) <{force = false}> : (tensor<1x7x7x2048xbf16, #ttnn_layout65>) -> () loc(#loc557)
        %619 = "ttnn.to_layout"(%618) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x49x2048xbf16, #ttnn_layout64>) -> tensor<1x1x49x2048xbf16, #ttnn_layout66> loc(#loc546)
        "ttnn.deallocate"(%618) <{force = false}> : (tensor<1x1x49x2048xbf16, #ttnn_layout64>) -> () loc(#loc546)
        %620 = "ttnn.conv2d"(%619, %arg267, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 2048 : i32, input_height = 7 : i32, input_width = 7 : i32, kernel_size = array<i32: 1, 1>, out_channels = 1024 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x49x2048xbf16, #ttnn_layout66>, tensor<1024x2048x1x1xbf16, #ttnn_layout32>, !ttnn.device) -> tensor<1x1x49x1024xbf16, #ttnn_layout61> loc(#loc435)
        "ttnn.deallocate"(%619) <{force = false}> : (tensor<1x1x49x2048xbf16, #ttnn_layout66>) -> () loc(#loc435)
        "ttnn.deallocate"(%arg267) <{force = false}> : (tensor<1024x2048x1x1xbf16, #ttnn_layout32>) -> () loc(#loc435)
        %621 = "ttnn.reshape"(%620) <{shape = [1 : i32, 7 : i32, 7 : i32, 1024 : i32]}> : (tensor<1x1x49x1024xbf16, #ttnn_layout61>) -> tensor<1x7x7x1024xbf16, #ttnn_layout62> loc(#loc547)
        "ttnn.deallocate"(%620) <{force = false}> : (tensor<1x1x49x1024xbf16, #ttnn_layout61>) -> () loc(#loc547)
        %622 = "ttnn.permute"(%621) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x7x7x1024xbf16, #ttnn_layout62>) -> tensor<1x1024x7x7xbf16, #ttnn_layout11> loc(#loc435)
        "ttnn.deallocate"(%621) <{force = false}> : (tensor<1x7x7x1024xbf16, #ttnn_layout62>) -> () loc(#loc435)
        %623 = "ttnn.batch_norm_inference"(%622, %64, %139, %172, %17) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x1024x7x7xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> tensor<1x1024x7x7xbf16, #ttnn_layout11> loc(#loc17)
        "ttnn.deallocate"(%622) <{force = false}> : (tensor<1x1024x7x7xbf16, #ttnn_layout11>) -> () loc(#loc17)
        "ttnn.deallocate"(%172) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc17)
        "ttnn.deallocate"(%139) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc17)
        "ttnn.deallocate"(%64) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc17)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc17)
        %624 = "ttnn.relu"(%623) : (tensor<1x1024x7x7xbf16, #ttnn_layout11>) -> tensor<1x1024x7x7xbf16, #ttnn_layout11> loc(#loc436)
        "ttnn.deallocate"(%623) <{force = false}> : (tensor<1x1024x7x7xbf16, #ttnn_layout11>) -> () loc(#loc436)
        %625 = "ttnn.permute"(%624) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1024x7x7xbf16, #ttnn_layout11>) -> tensor<1x7x7x1024xbf16, #ttnn_layout62> loc(#loc436)
        "ttnn.deallocate"(%624) <{force = false}> : (tensor<1x1024x7x7xbf16, #ttnn_layout11>) -> () loc(#loc436)
        %626 = "ttnn.reshape"(%625) <{shape = [1 : i32, 1 : i32, 49 : i32, 1024 : i32]}> : (tensor<1x7x7x1024xbf16, #ttnn_layout62>) -> tensor<1x1x49x1024xbf16, #ttnn_layout61> loc(#loc436)
        "ttnn.deallocate"(%625) <{force = false}> : (tensor<1x7x7x1024xbf16, #ttnn_layout62>) -> () loc(#loc436)
        %627 = "ttnn.to_layout"(%626) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x49x1024xbf16, #ttnn_layout61>) -> tensor<1x1x49x1024xbf16, #ttnn_layout63> loc(#loc548)
        "ttnn.deallocate"(%626) <{force = false}> : (tensor<1x1x49x1024xbf16, #ttnn_layout61>) -> () loc(#loc548)
        %628 = "ttnn.conv2d"(%627, %arg262, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 1024 : i32, input_height = 7 : i32, input_width = 7 : i32, kernel_size = array<i32: 3, 3>, out_channels = 1024 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x49x1024xbf16, #ttnn_layout63>, tensor<1024x1024x3x3xbf16, #ttnn_layout30>, !ttnn.device) -> tensor<1x1x49x1024xbf16, #ttnn_layout61> loc(#loc437)
        "ttnn.deallocate"(%627) <{force = false}> : (tensor<1x1x49x1024xbf16, #ttnn_layout63>) -> () loc(#loc437)
        "ttnn.deallocate"(%arg262) <{force = false}> : (tensor<1024x1024x3x3xbf16, #ttnn_layout30>) -> () loc(#loc437)
        %629 = "ttnn.reshape"(%628) <{shape = [1 : i32, 7 : i32, 7 : i32, 1024 : i32]}> : (tensor<1x1x49x1024xbf16, #ttnn_layout61>) -> tensor<1x7x7x1024xbf16, #ttnn_layout62> loc(#loc549)
        "ttnn.deallocate"(%628) <{force = false}> : (tensor<1x1x49x1024xbf16, #ttnn_layout61>) -> () loc(#loc549)
        %630 = "ttnn.permute"(%629) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x7x7x1024xbf16, #ttnn_layout62>) -> tensor<1x1024x7x7xbf16, #ttnn_layout11> loc(#loc437)
        "ttnn.deallocate"(%629) <{force = false}> : (tensor<1x7x7x1024xbf16, #ttnn_layout62>) -> () loc(#loc437)
        %631 = "ttnn.batch_norm_inference"(%630, %184, %188, %50, %144) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x1024x7x7xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>, tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> tensor<1x1024x7x7xbf16, #ttnn_layout11> loc(#loc38)
        "ttnn.deallocate"(%630) <{force = false}> : (tensor<1x1024x7x7xbf16, #ttnn_layout11>) -> () loc(#loc38)
        "ttnn.deallocate"(%188) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc38)
        "ttnn.deallocate"(%184) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc38)
        "ttnn.deallocate"(%144) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc38)
        "ttnn.deallocate"(%50) <{force = false}> : (tensor<1x1024x1x1xbf16, #ttnn_layout11>) -> () loc(#loc38)
        %632 = "ttnn.relu"(%631) : (tensor<1x1024x7x7xbf16, #ttnn_layout11>) -> tensor<1x1024x7x7xbf16, #ttnn_layout11> loc(#loc438)
        "ttnn.deallocate"(%631) <{force = false}> : (tensor<1x1024x7x7xbf16, #ttnn_layout11>) -> () loc(#loc438)
        %633 = "ttnn.permute"(%632) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x1024x7x7xbf16, #ttnn_layout11>) -> tensor<1x7x7x1024xbf16, #ttnn_layout62> loc(#loc438)
        "ttnn.deallocate"(%632) <{force = false}> : (tensor<1x1024x7x7xbf16, #ttnn_layout11>) -> () loc(#loc438)
        %634 = "ttnn.reshape"(%633) <{shape = [1 : i32, 1 : i32, 49 : i32, 1024 : i32]}> : (tensor<1x7x7x1024xbf16, #ttnn_layout62>) -> tensor<1x1x49x1024xbf16, #ttnn_layout61> loc(#loc438)
        "ttnn.deallocate"(%633) <{force = false}> : (tensor<1x7x7x1024xbf16, #ttnn_layout62>) -> () loc(#loc438)
        %635 = "ttnn.to_layout"(%634) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x49x1024xbf16, #ttnn_layout61>) -> tensor<1x1x49x1024xbf16, #ttnn_layout63> loc(#loc550)
        "ttnn.deallocate"(%634) <{force = false}> : (tensor<1x1x49x1024xbf16, #ttnn_layout61>) -> () loc(#loc550)
        %636 = "ttnn.conv2d"(%635, %arg257, %213) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<bf16>, groups = 1 : i32, in_channels = 1024 : i32, input_height = 7 : i32, input_width = 7 : i32, kernel_size = array<i32: 1, 1>, out_channels = 2048 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 1, 1>}> : (tensor<1x1x49x1024xbf16, #ttnn_layout63>, tensor<2048x1024x1x1xbf16, #ttnn_layout14>, !ttnn.device) -> tensor<1x1x49x2048xbf16, #ttnn_layout64> loc(#loc439)
        "ttnn.deallocate"(%635) <{force = false}> : (tensor<1x1x49x1024xbf16, #ttnn_layout63>) -> () loc(#loc439)
        "ttnn.deallocate"(%arg257) <{force = false}> : (tensor<2048x1024x1x1xbf16, #ttnn_layout14>) -> () loc(#loc439)
        %637 = "ttnn.reshape"(%636) <{shape = [1 : i32, 7 : i32, 7 : i32, 2048 : i32]}> : (tensor<1x1x49x2048xbf16, #ttnn_layout64>) -> tensor<1x7x7x2048xbf16, #ttnn_layout65> loc(#loc551)
        "ttnn.deallocate"(%636) <{force = false}> : (tensor<1x1x49x2048xbf16, #ttnn_layout64>) -> () loc(#loc551)
        %638 = "ttnn.permute"(%637) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x7x7x2048xbf16, #ttnn_layout65>) -> tensor<1x2048x7x7xbf16, #ttnn_layout9> loc(#loc439)
        "ttnn.deallocate"(%637) <{force = false}> : (tensor<1x7x7x2048xbf16, #ttnn_layout65>) -> () loc(#loc439)
        %639 = "ttnn.batch_norm_inference"(%638, %191, %206, %52, %5) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x2048x7x7xbf16, #ttnn_layout9>, tensor<1x2048x1x1xbf16, #ttnn_layout9>, tensor<1x2048x1x1xbf16, #ttnn_layout9>, tensor<1x2048x1x1xbf16, #ttnn_layout9>, tensor<1x2048x1x1xbf16, #ttnn_layout9>) -> tensor<1x2048x7x7xbf16, #ttnn_layout9> loc(#loc6)
        "ttnn.deallocate"(%638) <{force = false}> : (tensor<1x2048x7x7xbf16, #ttnn_layout9>) -> () loc(#loc6)
        "ttnn.deallocate"(%206) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout9>) -> () loc(#loc6)
        "ttnn.deallocate"(%191) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout9>) -> () loc(#loc6)
        "ttnn.deallocate"(%52) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout9>) -> () loc(#loc6)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x2048x1x1xbf16, #ttnn_layout9>) -> () loc(#loc6)
        %640 = "ttnn.add"(%639, %616) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x2048x7x7xbf16, #ttnn_layout9>, tensor<1x2048x7x7xbf16, #ttnn_layout9>) -> tensor<1x2048x7x7xbf16, #ttnn_layout9> loc(#loc440)
        "ttnn.deallocate"(%639) <{force = false}> : (tensor<1x2048x7x7xbf16, #ttnn_layout9>) -> () loc(#loc440)
        "ttnn.deallocate"(%616) <{force = false}> : (tensor<1x2048x7x7xbf16, #ttnn_layout9>) -> () loc(#loc440)
        %641 = "ttnn.relu"(%640) : (tensor<1x2048x7x7xbf16, #ttnn_layout9>) -> tensor<1x2048x7x7xbf16, #ttnn_layout9> loc(#loc441)
        "ttnn.deallocate"(%640) <{force = false}> : (tensor<1x2048x7x7xbf16, #ttnn_layout9>) -> () loc(#loc441)
        %642 = "ttnn.mean"(%641) <{dim_arg = [2 : i32, 3 : i32], keep_dim = false}> : (tensor<1x2048x7x7xbf16, #ttnn_layout9>) -> tensor<1x2048xbf16, #ttnn_layout67> loc(#loc552)
        "ttnn.deallocate"(%641) <{force = false}> : (tensor<1x2048x7x7xbf16, #ttnn_layout9>) -> () loc(#loc552)
        %643 = "ttnn.matmul"(%642, %arg1) <{transpose_a = false, transpose_b = true}> : (tensor<1x2048xbf16, #ttnn_layout67>, tensor<1000x2048xbf16, #ttnn_layout13>) -> tensor<1x1000xbf16, #ttnn_layout12> loc(#loc553)
        "ttnn.deallocate"(%642) <{force = false}> : (tensor<1x2048xbf16, #ttnn_layout67>) -> () loc(#loc553)
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<1000x2048xbf16, #ttnn_layout13>) -> () loc(#loc553)
        %644 = "ttnn.add"(%643, %44) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1000xbf16, #ttnn_layout12>, tensor<1x1000xbf16, #ttnn_layout12>) -> tensor<1x1000xbf16, #ttnn_layout12> loc(#loc554)
        "ttnn.deallocate"(%643) <{force = false}> : (tensor<1x1000xbf16, #ttnn_layout12>) -> () loc(#loc554)
        "ttnn.deallocate"(%44) <{force = false}> : (tensor<1x1000xbf16, #ttnn_layout12>) -> () loc(#loc554)
        return %644 : tensor<1x1000xbf16, #ttnn_layout12> loc(#loc)
      } loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc1 = loc("batch-norm-inference.124")
#loc2 = loc("batch-norm-inference.725")
#loc3 = loc("batch-norm-inference.542")
#loc4 = loc("batch-norm-inference.716")
#loc5 = loc("batch-norm-inference.632")
#loc6 = loc("batch-norm-inference.1661")
#loc7 = loc("batch-norm-inference.383")
#loc8 = loc("batch-norm-inference.365")
#loc9 = loc("batch-norm-inference.1133")
#loc10 = loc("batch-norm-inference.884")
#loc11 = loc("batch-norm-inference.1484")
#loc12 = loc("batch-norm-inference.467")
#loc13 = loc("batch-norm-inference.800")
#loc14 = loc("batch-norm-inference.374")
#loc15 = loc("batch-norm-inference.1235")
#loc16 = loc("batch-norm-inference.563")
#loc17 = loc("batch-norm-inference.1643")
#loc18 = loc("batch-norm-inference.707")
#loc19 = loc("batch-norm-inference.905")
#loc20 = loc("batch-norm-inference.1142")
#loc21 = loc("batch-norm-inference.1403")
#loc22 = loc("batch-norm-inference.551")
#loc23 = loc("batch-norm-inference.449")
#loc24 = loc("batch-norm-inference.1568")
#loc25 = loc("batch-norm-inference.1319")
#loc26 = loc("batch-norm-inference.1385")
#loc27 = loc("batch-norm-inference.983")
#loc28 = loc("batch-norm-inference.1559")
#loc29 = loc("batch-norm-inference.809")
#loc30 = loc("batch-norm-inference.1217")
#loc31 = loc("batch-norm-inference.305")
#loc32 = loc("batch-norm-inference.1493")
#loc33 = loc("batch-norm-inference.641")
#loc34 = loc("batch-norm-inference.965")
#loc35 = loc("broadcast.1695")
#loc36 = loc("batch-norm-inference.1226")
#loc37 = loc("batch-norm-inference.1394")
#loc38 = loc("batch-norm-inference.1652")
#loc39 = loc("batch-norm-inference.1475")
#loc40 = loc("batch-norm-inference.1058")
#loc41 = loc("batch-norm-inference.1301")
#loc42 = loc("batch-norm-inference.791")
#loc43 = loc("batch-norm-inference.1577")
#loc44 = loc("batch-norm-inference.623")
#loc45 = loc("batch-norm-inference.893")
#loc46 = loc("batch-norm-inference.1151")
#loc47 = loc("batch-norm-inference.1415")
#loc48 = loc("batch-norm-inference.974")
#loc49 = loc("batch-norm-inference.1067")
#loc50 = loc("batch-norm-inference.533")
#loc51 = loc("batch-norm-inference.1310")
#loc52 = loc("batch-norm-inference.1049")
#loc53 = loc("batch-norm-inference.875")
#loc54 = loc("batch-norm-inference.458")
#loc323 = loc("convolution.123")
#loc324 = loc("maximum.131")
#loc325 = loc("reduce-window.143")
#loc326 = loc("convolution.364")
#loc327 = loc("maximum.372")
#loc328 = loc("convolution.373")
#loc329 = loc("maximum.381")
#loc330 = loc("convolution.382")
#loc331 = loc("convolution.304")
#loc332 = loc("add.390")
#loc333 = loc("maximum.393")
#loc334 = loc("convolution.448")
#loc335 = loc("maximum.456")
#loc336 = loc("convolution.457")
#loc337 = loc("maximum.465")
#loc338 = loc("convolution.466")
#loc339 = loc("add.474")
#loc340 = loc("maximum.477")
#loc341 = loc("convolution.532")
#loc342 = loc("maximum.540")
#loc343 = loc("convolution.541")
#loc344 = loc("maximum.549")
#loc345 = loc("convolution.550")
#loc346 = loc("add.558")
#loc347 = loc("maximum.561")
#loc348 = loc("convolution.622")
#loc349 = loc("maximum.630")
#loc350 = loc("convolution.631")
#loc351 = loc("maximum.639")
#loc352 = loc("convolution.640")
#loc353 = loc("convolution.562")
#loc354 = loc("add.648")
#loc355 = loc("maximum.651")
#loc356 = loc("convolution.706")
#loc357 = loc("maximum.714")
#loc358 = loc("convolution.715")
#loc359 = loc("maximum.723")
#loc360 = loc("convolution.724")
#loc361 = loc("add.732")
#loc362 = loc("maximum.735")
#loc363 = loc("convolution.790")
#loc364 = loc("maximum.798")
#loc365 = loc("convolution.799")
#loc366 = loc("maximum.807")
#loc367 = loc("convolution.808")
#loc368 = loc("add.816")
#loc369 = loc("maximum.819")
#loc370 = loc("convolution.874")
#loc371 = loc("maximum.882")
#loc372 = loc("convolution.883")
#loc373 = loc("maximum.891")
#loc374 = loc("convolution.892")
#loc375 = loc("add.900")
#loc376 = loc("maximum.903")
#loc377 = loc("convolution.964")
#loc378 = loc("maximum.972")
#loc379 = loc("convolution.973")
#loc380 = loc("maximum.981")
#loc381 = loc("convolution.982")
#loc382 = loc("convolution.904")
#loc383 = loc("add.990")
#loc384 = loc("maximum.993")
#loc385 = loc("convolution.1048")
#loc386 = loc("maximum.1056")
#loc387 = loc("convolution.1057")
#loc388 = loc("maximum.1065")
#loc389 = loc("convolution.1066")
#loc390 = loc("add.1074")
#loc391 = loc("maximum.1077")
#loc392 = loc("convolution.1132")
#loc393 = loc("maximum.1140")
#loc394 = loc("convolution.1141")
#loc395 = loc("maximum.1149")
#loc396 = loc("convolution.1150")
#loc397 = loc("add.1158")
#loc398 = loc("maximum.1161")
#loc399 = loc("convolution.1216")
#loc400 = loc("maximum.1224")
#loc401 = loc("convolution.1225")
#loc402 = loc("maximum.1233")
#loc403 = loc("convolution.1234")
#loc404 = loc("add.1242")
#loc405 = loc("maximum.1245")
#loc406 = loc("convolution.1300")
#loc407 = loc("maximum.1308")
#loc408 = loc("convolution.1309")
#loc409 = loc("maximum.1317")
#loc410 = loc("convolution.1318")
#loc411 = loc("add.1326")
#loc412 = loc("maximum.1329")
#loc413 = loc("convolution.1384")
#loc414 = loc("maximum.1392")
#loc415 = loc("convolution.1393")
#loc416 = loc("maximum.1401")
#loc417 = loc("convolution.1402")
#loc418 = loc("add.1410")
#loc419 = loc("maximum.1413")
#loc420 = loc("convolution.1474")
#loc421 = loc("maximum.1482")
#loc422 = loc("convolution.1483")
#loc423 = loc("maximum.1491")
#loc424 = loc("convolution.1492")
#loc425 = loc("convolution.1414")
#loc426 = loc("add.1500")
#loc427 = loc("maximum.1503")
#loc428 = loc("convolution.1558")
#loc429 = loc("maximum.1566")
#loc430 = loc("convolution.1567")
#loc431 = loc("maximum.1575")
#loc432 = loc("convolution.1576")
#loc433 = loc("add.1584")
#loc434 = loc("maximum.1587")
#loc435 = loc("convolution.1642")
#loc436 = loc("maximum.1650")
#loc437 = loc("convolution.1651")
#loc438 = loc("maximum.1659")
#loc439 = loc("convolution.1660")
#loc440 = loc("add.1668")
#loc441 = loc("maximum.1671")
#loc442 = loc("reduce.1678")
#loc443 = loc("add.1696")
#loc444 = loc("convolution.123_input"(#loc323))
#loc445 = loc("convolution.123_workaround"(#loc323))
#loc446 = loc("convolution.123_reshape"(#loc323))
#loc447 = loc("reduce-window.143_workaround"(#loc325))
#loc448 = loc("convolution.364_reshape"(#loc326))
#loc449 = loc("convolution.373_workaround"(#loc328))
#loc450 = loc("convolution.373_reshape"(#loc328))
#loc451 = loc("convolution.382_workaround"(#loc330))
#loc452 = loc("convolution.382_reshape"(#loc330))
#loc453 = loc("convolution.304_reshape"(#loc331))
#loc454 = loc("convolution.448_workaround"(#loc334))
#loc455 = loc("convolution.448_reshape"(#loc334))
#loc456 = loc("convolution.457_workaround"(#loc336))
#loc457 = loc("convolution.457_reshape"(#loc336))
#loc458 = loc("convolution.466_workaround"(#loc338))
#loc459 = loc("convolution.466_reshape"(#loc338))
#loc460 = loc("convolution.532_workaround"(#loc341))
#loc461 = loc("convolution.532_reshape"(#loc341))
#loc462 = loc("convolution.541_workaround"(#loc343))
#loc463 = loc("convolution.541_reshape"(#loc343))
#loc464 = loc("convolution.550_workaround"(#loc345))
#loc465 = loc("convolution.550_reshape"(#loc345))
#loc466 = loc("convolution.622_workaround"(#loc348))
#loc467 = loc("convolution.622_reshape"(#loc348))
#loc468 = loc("convolution.631_workaround"(#loc350))
#loc469 = loc("convolution.631_reshape"(#loc350))
#loc470 = loc("convolution.640_workaround"(#loc352))
#loc471 = loc("convolution.640_reshape"(#loc352))
#loc472 = loc("convolution.562_workaround"(#loc353))
#loc473 = loc("convolution.562_reshape"(#loc353))
#loc474 = loc("convolution.706_workaround"(#loc356))
#loc475 = loc("convolution.706_reshape"(#loc356))
#loc476 = loc("convolution.715_workaround"(#loc358))
#loc477 = loc("convolution.715_reshape"(#loc358))
#loc478 = loc("convolution.724_workaround"(#loc360))
#loc479 = loc("convolution.724_reshape"(#loc360))
#loc480 = loc("convolution.790_workaround"(#loc363))
#loc481 = loc("convolution.790_reshape"(#loc363))
#loc482 = loc("convolution.799_workaround"(#loc365))
#loc483 = loc("convolution.799_reshape"(#loc365))
#loc484 = loc("convolution.808_workaround"(#loc367))
#loc485 = loc("convolution.808_reshape"(#loc367))
#loc486 = loc("convolution.874_workaround"(#loc370))
#loc487 = loc("convolution.874_reshape"(#loc370))
#loc488 = loc("convolution.883_workaround"(#loc372))
#loc489 = loc("convolution.883_reshape"(#loc372))
#loc490 = loc("convolution.892_workaround"(#loc374))
#loc491 = loc("convolution.892_reshape"(#loc374))
#loc492 = loc("convolution.964_workaround"(#loc377))
#loc493 = loc("convolution.964_reshape"(#loc377))
#loc494 = loc("convolution.973_workaround"(#loc379))
#loc495 = loc("convolution.973_reshape"(#loc379))
#loc496 = loc("convolution.982_workaround"(#loc381))
#loc497 = loc("convolution.982_reshape"(#loc381))
#loc498 = loc("convolution.904_workaround"(#loc382))
#loc499 = loc("convolution.904_reshape"(#loc382))
#loc500 = loc("convolution.1048_workaround"(#loc385))
#loc501 = loc("convolution.1048_reshape"(#loc385))
#loc502 = loc("convolution.1057_workaround"(#loc387))
#loc503 = loc("convolution.1057_reshape"(#loc387))
#loc504 = loc("convolution.1066_workaround"(#loc389))
#loc505 = loc("convolution.1066_reshape"(#loc389))
#loc506 = loc("convolution.1132_workaround"(#loc392))
#loc507 = loc("convolution.1132_reshape"(#loc392))
#loc508 = loc("convolution.1141_workaround"(#loc394))
#loc509 = loc("convolution.1141_reshape"(#loc394))
#loc510 = loc("convolution.1150_workaround"(#loc396))
#loc511 = loc("convolution.1150_reshape"(#loc396))
#loc512 = loc("convolution.1216_workaround"(#loc399))
#loc513 = loc("convolution.1216_reshape"(#loc399))
#loc514 = loc("convolution.1225_workaround"(#loc401))
#loc515 = loc("convolution.1225_reshape"(#loc401))
#loc516 = loc("convolution.1234_workaround"(#loc403))
#loc517 = loc("convolution.1234_reshape"(#loc403))
#loc518 = loc("convolution.1300_workaround"(#loc406))
#loc519 = loc("convolution.1300_reshape"(#loc406))
#loc520 = loc("convolution.1309_workaround"(#loc408))
#loc521 = loc("convolution.1309_reshape"(#loc408))
#loc522 = loc("convolution.1318_workaround"(#loc410))
#loc523 = loc("convolution.1318_reshape"(#loc410))
#loc524 = loc("convolution.1384_workaround"(#loc413))
#loc525 = loc("convolution.1384_reshape"(#loc413))
#loc526 = loc("convolution.1393_workaround"(#loc415))
#loc527 = loc("convolution.1393_reshape"(#loc415))
#loc528 = loc("convolution.1402_workaround"(#loc417))
#loc529 = loc("convolution.1402_reshape"(#loc417))
#loc530 = loc("convolution.1474_workaround"(#loc420))
#loc531 = loc("convolution.1474_reshape"(#loc420))
#loc532 = loc("convolution.1483_workaround"(#loc422))
#loc533 = loc("convolution.1483_reshape"(#loc422))
#loc534 = loc("convolution.1492_workaround"(#loc424))
#loc535 = loc("convolution.1492_reshape"(#loc424))
#loc536 = loc("convolution.1414_workaround"(#loc425))
#loc537 = loc("convolution.1414_reshape"(#loc425))
#loc538 = loc("convolution.1558_input"(#loc428))
#loc539 = loc("convolution.1558_workaround"(#loc428))
#loc540 = loc("convolution.1558_reshape"(#loc428))
#loc541 = loc("convolution.1567_workaround"(#loc430))
#loc542 = loc("convolution.1567_reshape"(#loc430))
#loc543 = loc("convolution.1576_workaround"(#loc432))
#loc544 = loc("convolution.1576_reshape"(#loc432))
#loc545 = loc("convolution.1642_input"(#loc435))
#loc546 = loc("convolution.1642_workaround"(#loc435))
#loc547 = loc("convolution.1642_reshape"(#loc435))
#loc548 = loc("convolution.1651_workaround"(#loc437))
#loc549 = loc("convolution.1651_reshape"(#loc437))
#loc550 = loc("convolution.1660_workaround"(#loc439))
#loc551 = loc("convolution.1660_reshape"(#loc439))
#loc552 = loc("reduce.1678_mean"(#loc442))
#loc553 = loc("add.1696_decomp_matmul"(#loc443))
#loc554 = loc("add.1696_decomp_add"(#loc443))
#loc555 = loc("convolution.123_input_reshape"(#loc444))
#loc556 = loc("convolution.1558_input_reshape"(#loc538))
#loc557 = loc("convolution.1642_input_reshape"(#loc545))
